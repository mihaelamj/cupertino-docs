{
  "abstract" : "Use your machine learning model to focus on a specific subject.",
  "codeExamples" : [
    {
      "code" : "captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection)\n\nlet pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer)",
      "language" : "swift"
    },
    {
      "code" : "let cameraInfo = DockAccessory.CameraInformation(captureDevice: cameraDevice!.deviceType, cameraPosition: cameraDevice!.position, orientation: .LandscapeRight, cameraIntrinsics: self.intrinsics, referenceDimensions: self.referenceDimensions)\nlet obs = DockAccessory.Observation(identifier: 0, type: .humanFace, rect: rect, faceYawAngle: Measurement(value: self.faceYaw, unit: UnitAngle.radians))\nlet observations = [obs]\n\ntry await dockAccessory.track(observations, cameraInformation: cameraInfo)",
      "language" : "swift"
    }
  ],
  "contentHash" : "ee7931b9695d73488407c91ce5e92329ec4fb1cc8e9a46fdda322fb90c23155c",
  "crawledAt" : "2025-12-03T04:08:08Z",
  "id" : "1B17E586-542B-423F-9DF2-BF6BC79E2292",
  "kind" : "article",
  "language" : "swift",
  "module" : "DockKit",
  "overview" : "## Overview\n\nYou can use your own custom machine learning model, also known as *inference*. This allows DockKit to track a custom subject, such as a dog or a cat. Once you’ve applied your machine learning model on a set of video frames, use DockKit to generate and provide tracking vectors.\n\n## Obtain video images\n\nBegin by obtaining images from the video capture source.\n\n## Provide observations\n\nOnce you’ve passed your video frames to your custom machine learning model, supply these observations to the accessory using [doc:\/\/com.apple.DockKit\/documentation\/DockKit\/DockAccessory\/track(_:cameraInformation:)-4yl9b].\n\nFormat the output of your object detections as an array of [doc:\/\/com.apple.DockKit\/documentation\/DockKit\/DockAccessory\/Observation]  objects. If you want to write your own custom tracking logic, or generate your own position and velocity-based vectors, see [doc:\/\/com.apple.DockKit\/documentation\/DockKit\/Modify-rotation-and-positioning-behavior-programmatically].\n\nCall [doc:\/\/com.apple.DockKit\/documentation\/DockKit\/DockAccessory\/track(_:cameraInformation:)-4yl9b] at an interval between 10 and 30 times per second. When calling this method , the tracking system analyzes the subjects and determines which one to focus on and how to frame it. The tracking vector derives from these two choices, and the accessory keeps the subjects framed appropriately.\n\nThe tracking vector arrives at the accessory which controls the motors. This work occurs in the background and can take some time to complete.\n\nIn some cases, the call to [doc:\/\/com.apple.DockKit\/documentation\/DockKit\/DockAccessory\/track(_:cameraInformation:)-4yl9b] might not generate a vector, or the vector may only keep the accessory at rest.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/DockKit\/Track-custom-objects-in-a-frame\ncrawled: 2025-12-03T04:08:08Z\n---\n\n# Track custom objects in a frame\n\n**Article**\n\nUse your machine learning model to focus on a specific subject.\n\n## Overview\n\nYou can use your own custom machine learning model, also known as *inference*. This allows DockKit to track a custom subject, such as a dog or a cat. Once you’ve applied your machine learning model on a set of video frames, use DockKit to generate and provide tracking vectors.\n\n## Obtain video images\n\nBegin by obtaining images from the video capture source.\n\n```swift\ncaptureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection)\n\nlet pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer)\n```\n\n## Provide observations\n\nOnce you’ve passed your video frames to your custom machine learning model, supply these observations to the accessory using [doc:\/\/com.apple.DockKit\/documentation\/DockKit\/DockAccessory\/track(_:cameraInformation:)-4yl9b].\n\n```swift\nlet cameraInfo = DockAccessory.CameraInformation(captureDevice: cameraDevice!.deviceType, cameraPosition: cameraDevice!.position, orientation: .LandscapeRight, cameraIntrinsics: self.intrinsics, referenceDimensions: self.referenceDimensions)\nlet obs = DockAccessory.Observation(identifier: 0, type: .humanFace, rect: rect, faceYawAngle: Measurement(value: self.faceYaw, unit: UnitAngle.radians))\nlet observations = [obs]\n\ntry await dockAccessory.track(observations, cameraInformation: cameraInfo)\n```\n\nFormat the output of your object detections as an array of [doc:\/\/com.apple.DockKit\/documentation\/DockKit\/DockAccessory\/Observation]  objects. If you want to write your own custom tracking logic, or generate your own position and velocity-based vectors, see [doc:\/\/com.apple.DockKit\/documentation\/DockKit\/Modify-rotation-and-positioning-behavior-programmatically].\n\nCall [doc:\/\/com.apple.DockKit\/documentation\/DockKit\/DockAccessory\/track(_:cameraInformation:)-4yl9b] at an interval between 10 and 30 times per second. When calling this method , the tracking system analyzes the subjects and determines which one to focus on and how to frame it. The tracking vector derives from these two choices, and the accessory keeps the subjects framed appropriately.\n\nThe tracking vector arrives at the accessory which controls the motors. This work occurs in the background and can take some time to complete.\n\nIn some cases, the call to [doc:\/\/com.apple.DockKit\/documentation\/DockKit\/DockAccessory\/track(_:cameraInformation:)-4yl9b] might not generate a vector, or the vector may only keep the accessory at rest.\n\n## Customizing tracking behavior\n\n- **Modify rotation and positioning programmatically**: Perform custom control of the dock accessory.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Perform custom control of the dock accessory.",
          "name" : "Modify rotation and positioning programmatically",
          "url" : "https:\/\/developer.apple.com\/documentation\/DockKit\/Modify-rotation-and-positioning-behavior-programmatically"
        }
      ],
      "title" : "Customizing tracking behavior"
    }
  ],
  "source" : "appleJSON",
  "title" : "Track custom objects in a frame",
  "url" : "https:\/\/developer.apple.com\/documentation\/DockKit\/Track-custom-objects-in-a-frame"
}