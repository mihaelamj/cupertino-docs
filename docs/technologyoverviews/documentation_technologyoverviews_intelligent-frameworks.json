{
  "abstract" : "Analyze images, sound, and language without needing to be a machine learning expert.",
  "codeExamples" : [

  ],
  "contentHash" : "015dfeb1f7ee17b253b9de733afec17d358013ffbad620173810ada7d255e9db",
  "crawledAt" : "2025-12-02T15:41:42Z",
  "id" : "01D9E08E-CEDE-44F5-A006-2395C3062FBA",
  "kind" : "article",
  "language" : "swift",
  "module" : "Technology Overviews",
  "overview" : "## Overview\n\nBuilding an app that uses machine learning doesn’t have to be complicated. There are several on-device models that many frameworks lean on to perform analysis. Apple trains and optimizes these on-device models for efficiency on device, allowing frameworks to perform the heavy lifting for you so you can focus on building your features. When you use a framework that’s powered by machine learning, you don’t have to worry about collecting a large amount of data and taking the time to train and optimize a model by yourself.\n\nStarting with one of the available frameworks provides a great introduction to thinking about machine learning and building intelligent apps. Whether you want to analyze images, sound, or language, you make requests to the system that performs the work and returns the result that you use in your app.\n\n## Know more about images by analyzing photos or video frames\n\nComputer vision allows for better understanding of the world around us. When you work with photos and videos, you might want to know more about what’s happening in them to create the feature you want in your app. For example, you don’t have to start from zero to [doc:\/\/com.apple.documentation\/documentation\/Vision\/analyzing-a-selfie-and-visualizing-its-content]. The [doc:\/\/com.apple.documentation\/documentation\/Vision] and [doc:\/\/com.apple.documentation\/documentation\/VisionKit] frameworks perform a wide variety of tasks that do the heavy lifting for you, and provides more than 25 types of image analysis tasks, like:\n\nTo prevent people from viewing unwanted image content in your app, [doc:\/\/com.apple.documentation\/documentation\/SensitiveContentAnalysis\/detecting-nudity-in-media-and-providing-intervention-options] to help detect it.\n\n## Perform speech recognition in recorded or live audio\n\nSpeech recognition transforms spoken words into text to help you with things like dictating notes in a note-taking app. Or, you might allow people to speak voice commands to control a smart thermostat. The Speech framework helps you [doc:\/\/com.apple.documentation\/documentation\/Speech\/bringing-advanced-speech-to-text-capabilities-to-your-app] into text with very little code, and entirely on device. The framework uses the same speech-to-text model powering Siri and is especially good for long-form and distant audio, like meetings and lectures.\n\nThe framework allows you to provide audio from either prerecorded files or a live source like a microphone. You pass audio to a [doc:\/\/com.apple.documentation\/documentation\/Speech\/SpeechAnalyzer] that uses a speech-to-text model to predict the text that matches the audio. Analysis is asynchronous and the framework decouples the input and output to allow you to work with results as they happen while the framework processes the input.\n\n## Detect the song that’s being played\n\nAudio recognition allows you to identify a piece of audio being played in an environment to get more information about it or to perform actions that are in sync with it. For example, if you’re building a trivia app you might want to know when an interactive moment occurs in the audio stream so you can [doc:\/\/com.apple.documentation\/documentation\/ShazamKit\/building-a-custom-catalog-and-matching-audio] with it. The [doc:\/\/com.apple.documentation\/documentation\/ShazamKit] framework helps you match audio against the vast music catalog that Shazam provides, or match against your own prerecorded reference audio in a custom catalog.\n\nShazamKit lets you perform a match by converting audio into a special format called [doc:\/\/com.apple.documentation\/documentation\/ShazamKit\/SHSignature]. You pass in a stream of audio buffers — or the signature data — into a session, and the session uses the data to find a match in the Shazam catalog or in the catalogs you provide. If there’s a match, the session returns a match object that contains the media items that represent the metadata of the match.\n\nFor heavy-duty audio recognition workflows, macOS includes a command line utility for Shazam under the hood. For additional details, open Terminal and run `man shazam`.\n\n## Classify the type of sound in the audio\n\nReal-time sound recognition capabilities can enhance the accessibility of the app you create. Integrate sound analysis capabilities when your app needs to [doc:\/\/com.apple.documentation\/documentation\/SoundAnalysis\/classifying-sounds-in-an-audio-file] in real time to identify environmental sounds, like glass breaking or dog barks. Or if you’re building a music making app, use sound analysis to identify the instrument being played.\n\nOptimized for hardware acceleration, the sound classification API provides a specialized machine learning audio expertise by classifying over 300 categories of sounds on-device. On-device processing helps preserve user privacy because the audio isn’t sent to the cloud.\n\nFor apps that have a particular sound that needs to be detected, use the [https:\/\/developer.apple.com\/machine-learning\/create-ml\/] to customize a sound analysis model with your own data.\n\n## Segment natural language into lexical units\n\nNatural language processing helps you understand and process human language and allows your app to extract meaning from text. If you’re working on a multilingual app, you may want to [doc:\/\/com.apple.documentation\/documentation\/NaturalLanguage\/identifying-the-language-in-text] in text to help convert it on behalf of the person using your app. For example, a language learning app might identify that the input a person provides matches the language the app expects. The framework breaks strings down into lexical units — like words or sentences — and ensures correct behavior in multiple script languages. For example, Chinese doesn’t use spaces to delimit words.\n\nThe framework helps you perform tasks like [doc:\/\/com.apple.documentation\/documentation\/NaturalLanguage\/finding-similarities-between-pieces-of-text], which can improve search in your app by including related words to an original search query.\n\n## Add language translation\n\nLanguage translation helps people understand the world around them. If a person using your app is traveling to a foreign country, your app might help them translate the transit signs they see in a subway or train station. Or an app that’s made for exploring a museum might translate the text that describes an exhibit. The [doc:\/\/com.apple.documentation\/documentation\/Translation] framework helps you perform text translations between multiple languages, and uses the same on-device model shared across the OS for language translation.\n\nThe framework allows you to translate between many [doc:\/\/com.apple.documentation\/documentation\/Translation\/LanguageAvailability\/supportedLanguages], and provides your app access to the languages a person has already downloaded. Start translating text in your app by applying a translation [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/View\/translationPresentation(isPresented:text:attachmentAnchor:arrowEdge:replacementAction:)] to show the system UI translation popover.\n\nWhen you need to [doc:\/\/com.apple.documentation\/documentation\/Translation\/translating-text-within-your-app] in your app, use a translation session directly. The system provides a session automatically when you attach the [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/View\/translationTask(source:target:action:)] to your view, and allows for performing a single translation or a [doc:\/\/com.apple.documentation\/documentation\/Translation\/TranslationSession\/translate(batch:)] to translate several strings at once.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/TechnologyOverviews\/intelligent-frameworks\ncrawled: 2025-12-02T15:41:42Z\n---\n\n# Intelligent frameworks\n\nAnalyze images, sound, and language without needing to be a machine learning expert.\n\n## Overview\n\nBuilding an app that uses machine learning doesn’t have to be complicated. There are several on-device models that many frameworks lean on to perform analysis. Apple trains and optimizes these on-device models for efficiency on device, allowing frameworks to perform the heavy lifting for you so you can focus on building your features. When you use a framework that’s powered by machine learning, you don’t have to worry about collecting a large amount of data and taking the time to train and optimize a model by yourself.\n\nStarting with one of the available frameworks provides a great introduction to thinking about machine learning and building intelligent apps. Whether you want to analyze images, sound, or language, you make requests to the system that performs the work and returns the result that you use in your app.\n\n\n\n## Know more about images by analyzing photos or video frames\n\nComputer vision allows for better understanding of the world around us. When you work with photos and videos, you might want to know more about what’s happening in them to create the feature you want in your app. For example, you don’t have to start from zero to [doc:\/\/com.apple.documentation\/documentation\/Vision\/analyzing-a-selfie-and-visualizing-its-content]. The [doc:\/\/com.apple.documentation\/documentation\/Vision] and [doc:\/\/com.apple.documentation\/documentation\/VisionKit] frameworks perform a wide variety of tasks that do the heavy lifting for you, and provides more than 25 types of image analysis tasks, like:\n\n- Capture text within the camera frame by enabling [doc:\/\/com.apple.documentation\/documentation\/VisionKit\/enabling-live-text-interactions-with-images].\n- Identify objects, text, bar codes, documents, and more in images or the [doc:\/\/com.apple.documentation\/documentation\/VisionKit\/scanning-data-with-the-camera].\n- Track the movement of [doc:\/\/com.apple.documentation\/documentation\/Vision\/TrackObjectRequest] across images or video frames.\n- Detect face and body poses for [doc:\/\/com.apple.documentation\/documentation\/Vision\/DetectHumanBodyPoseRequest] and [doc:\/\/com.apple.documentation\/documentation\/Vision\/DetectAnimalBodyPoseRequest].\n- Determine the [doc:\/\/com.apple.documentation\/documentation\/Vision\/DetectTrajectoriesRequest].\n\nTo prevent people from viewing unwanted image content in your app, [doc:\/\/com.apple.documentation\/documentation\/SensitiveContentAnalysis\/detecting-nudity-in-media-and-providing-intervention-options] to help detect it.\n\n\n\n## Perform speech recognition in recorded or live audio\n\nSpeech recognition transforms spoken words into text to help you with things like dictating notes in a note-taking app. Or, you might allow people to speak voice commands to control a smart thermostat. The Speech framework helps you [doc:\/\/com.apple.documentation\/documentation\/Speech\/bringing-advanced-speech-to-text-capabilities-to-your-app] into text with very little code, and entirely on device. The framework uses the same speech-to-text model powering Siri and is especially good for long-form and distant audio, like meetings and lectures.\n\nThe framework allows you to provide audio from either prerecorded files or a live source like a microphone. You pass audio to a [doc:\/\/com.apple.documentation\/documentation\/Speech\/SpeechAnalyzer] that uses a speech-to-text model to predict the text that matches the audio. Analysis is asynchronous and the framework decouples the input and output to allow you to work with results as they happen while the framework processes the input.\n\n\n\n## Detect the song that’s being played\n\nAudio recognition allows you to identify a piece of audio being played in an environment to get more information about it or to perform actions that are in sync with it. For example, if you’re building a trivia app you might want to know when an interactive moment occurs in the audio stream so you can [doc:\/\/com.apple.documentation\/documentation\/ShazamKit\/building-a-custom-catalog-and-matching-audio] with it. The [doc:\/\/com.apple.documentation\/documentation\/ShazamKit] framework helps you match audio against the vast music catalog that Shazam provides, or match against your own prerecorded reference audio in a custom catalog.\n\nShazamKit lets you perform a match by converting audio into a special format called [doc:\/\/com.apple.documentation\/documentation\/ShazamKit\/SHSignature]. You pass in a stream of audio buffers — or the signature data — into a session, and the session uses the data to find a match in the Shazam catalog or in the catalogs you provide. If there’s a match, the session returns a match object that contains the media items that represent the metadata of the match.\n\nFor heavy-duty audio recognition workflows, macOS includes a command line utility for Shazam under the hood. For additional details, open Terminal and run `man shazam`.\n\n\n\n## Classify the type of sound in the audio\n\nReal-time sound recognition capabilities can enhance the accessibility of the app you create. Integrate sound analysis capabilities when your app needs to [doc:\/\/com.apple.documentation\/documentation\/SoundAnalysis\/classifying-sounds-in-an-audio-file] in real time to identify environmental sounds, like glass breaking or dog barks. Or if you’re building a music making app, use sound analysis to identify the instrument being played.\n\nOptimized for hardware acceleration, the sound classification API provides a specialized machine learning audio expertise by classifying over 300 categories of sounds on-device. On-device processing helps preserve user privacy because the audio isn’t sent to the cloud.\n\nFor apps that have a particular sound that needs to be detected, use the [https:\/\/developer.apple.com\/machine-learning\/create-ml\/] to customize a sound analysis model with your own data.\n\n\n\n## Segment natural language into lexical units\n\nNatural language processing helps you understand and process human language and allows your app to extract meaning from text. If you’re working on a multilingual app, you may want to [doc:\/\/com.apple.documentation\/documentation\/NaturalLanguage\/identifying-the-language-in-text] in text to help convert it on behalf of the person using your app. For example, a language learning app might identify that the input a person provides matches the language the app expects. The framework breaks strings down into lexical units — like words or sentences — and ensures correct behavior in multiple script languages. For example, Chinese doesn’t use spaces to delimit words.\n\nThe framework helps you perform tasks like [doc:\/\/com.apple.documentation\/documentation\/NaturalLanguage\/finding-similarities-between-pieces-of-text], which can improve search in your app by including related words to an original search query.\n\n\n\n## Add language translation\n\nLanguage translation helps people understand the world around them. If a person using your app is traveling to a foreign country, your app might help them translate the transit signs they see in a subway or train station. Or an app that’s made for exploring a museum might translate the text that describes an exhibit. The [doc:\/\/com.apple.documentation\/documentation\/Translation] framework helps you perform text translations between multiple languages, and uses the same on-device model shared across the OS for language translation.\n\nThe framework allows you to translate between many [doc:\/\/com.apple.documentation\/documentation\/Translation\/LanguageAvailability\/supportedLanguages], and provides your app access to the languages a person has already downloaded. Start translating text in your app by applying a translation [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/View\/translationPresentation(isPresented:text:attachmentAnchor:arrowEdge:replacementAction:)] to show the system UI translation popover.\n\nWhen you need to [doc:\/\/com.apple.documentation\/documentation\/Translation\/translating-text-within-your-app] in your app, use a translation session directly. The system provides a session automatically when you attach the [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/View\/translationTask(source:target:action:)] to your view, and allows for performing a single translation or a [doc:\/\/com.apple.documentation\/documentation\/Translation\/TranslationSession\/translate(batch:)] to translate several strings at once.\n\n",
  "sections" : [

  ],
  "source" : "appleJSON",
  "title" : "Intelligent frameworks",
  "url" : "https:\/\/developer.apple.com\/documentation\/TechnologyOverviews\/intelligent-frameworks"
}