{
  "abstract" : "Create, optimize, and deploy models for on-device execution.",
  "codeExamples" : [

  ],
  "contentHash" : "9e121d1d8e36bff3fcb5585de4dd8982453a79c67d3d472fdc57550131e373af",
  "crawledAt" : "2025-12-02T15:59:59Z",
  "id" : "B0464FF3-FCD9-4F5E-BAB8-39AA844DB8DB",
  "kind" : "article",
  "language" : "swift",
  "module" : "Technology Overviews",
  "overview" : "## Overview\n\nWhen the available intelligent frameworks or generative technologies don’t provide the features you need, Apple provides machine learning frameworks that help you:\n\nBuild models to analyze text, images, or other types of data your app needs. If you already have your own machine learning models, convert them to the Core ML model format and integrate them into your app. Apple also provides frameworks to help with highly demanding machine learning tasks that involve graphics and real-time signal processing.\n\nAs you design your models, it’s important to keep the intended experience of your app in mind. The HIG offers [https:\/\/developer.apple.com\/design\/human-interface-guidelines\/machine-learning] to help you create apps that use machine learning.\n\n## Collect and prepare your data for training\n\nWhen you create a new model the starting point is always the same — your training and testing data. The quality of your data determines the quality of your results, so choose data that reflects a wide variety of possibility for your training use case. For example, when you [doc:\/\/com.apple.documentation\/documentation\/CreateML\/creating-an-image-classifier-model#Gather-Your-Data] model to recognize animals, begin by gathering at least 10 images per animal that best represent what you expect the model to see. Create ML supports several types of data sources, each with its [doc:\/\/com.apple.documentation\/documentation\/CreateML\/creating-an-action-classifier-model#Organize-the-Example-Videos] within a parent folder. In you parent folder, organize data into subfolders and use the folder name as your training label.\n\nIf you use the [doc:\/\/com.apple.documentation\/documentation\/CreateML] framework to programmatically create and train a model — like a text classifier that identifies the sentiment expressed in a sentence — [doc:\/\/com.apple.documentation\/documentation\/CreateML\/creating-a-text-classifier-model#Import-your-data] by using [doc:\/\/com.apple.documentation\/documentation\/TabularData].\n\n## Build and train on-device models with no code\n\nMany system frameworks can be extended or customized to your specific use case. If you’re working in a specialized domain that requires using your own data — or you want to extend the capabilities of an existing framework – the [https:\/\/developer.apple.com\/machine-learning\/create-ml\/] makes it easier to adapt system models. For example, if you use the [doc:\/\/com.apple.documentation\/documentation\/SoundAnalysis] framework and notice it can’t classify your sound, use the Create ML app to train a sound classification model that’s trained to identify your sound. After you train the model, [doc:\/\/com.apple.documentation\/documentation\/SoundAnalysis\/classifying-sounds-in-an-audio-file#Create-a-Sound-Classification-Request] framework.\n\nBuilt with the [doc:\/\/com.apple.documentation\/documentation\/CreateML] and [doc:\/\/com.apple.documentation\/documentation\/CreateMLComponents] frameworks, the Create ML app provides an approachable interface for creating models using your own data. In Xcode, choose Xcode > Open Developer Tool > Create ML. Choose a template that aligns with the task you want to customize, then provide your data, train, evaluate, and iterate on your model.\n\n\n\nAfter training your model, use the Create ML app to visualize and debug your annotations by clicking on the data source. The default view shows a distribution of your data, and the Explore page lets you see into specific object or class labels to visualize your annotations. The app [doc:\/\/com.apple.documentation\/documentation\/CoreML\/getting-a-core-ml-model] that’s ready to integrate into your app with Core ML.\n\n## Model conversion and optimization\n\nBring any model to the device if you want to experiment with it or deploy it. All you need is the model to be in the Core ML format. [doc:\/\/com.apple.documentation\/documentation\/CoreML] is the go-to framework for deploying models on-device, and you can [https:\/\/developer.apple.com\/machine-learning\/models\/] that are already in the Core ML format to experiment with or use for your feature.\n\nIf you created a model using training libraries like [https:\/\/ml-explore.github.io\/mlx\/] or [https:\/\/pytorch.org\/], use [https:\/\/coremltools.readme.io\/] to convert it to the Core ML format. Core ML Tools provides utilities and workflows for transforming trained models to the Core ML format. The workflows that Core ML Tools provide apply optimizations for on-device execution. Converting your model optimizes it for Apple devices, which requires less space on device, uses less power, and reduces latency when making predictions. Core ML Tools provides a number of compression techniques to help you optimize the model representation and parameters, while maintaining good accuracy.\n\nWhen you have an optimized and prepared model, you’re ready to integrate it with system frameworks. For example, if your model performs image analysis, [doc:\/\/com.apple.documentation\/documentation\/Vision\/CoreMLRequest] with the Vision framework.\n\n## Analyze the performance of your model with Xcode\n\nEvaluating the performance of models is an important task of machine learning. In Xcode, preview your model’s behavior by using sample data files or using the device’s camera and microphone. Review the performance of your model’s predictions directly from Xcode, or profile your app in Instruments to get a thorough performance analysis. After you add a model to your project, select it to get insights about the expected prediction latency, load times, and introspect where a particular operation is supported and run.\n\n\n\nTo build a deeper understanding of the model you’re working with, Xcode allows you to visualize the structure of the full model architecture and dive into the details of any operation. This visualization helps you debug issues and find performance enhancing opportunities.\n\n## Model deployment and execution on device\n\nYou use [doc:\/\/com.apple.documentation\/documentation\/CoreML] to integrate and run your model directly into your app. At runtime, Core ML makes use of all available compute and optimizes task execution across CPU, GPU, and Neural Engine. There are several technologies that underly Core ML that are available when you need fine-grained control over machine learning task execution.\n\nTo sequence and integrate machine learning with demanding graphics workloads, use Core ML models with both [doc:\/\/com.apple.documentation\/documentation\/MetalPerformanceShadersGraph] and [doc:\/\/com.apple.documentation\/documentation\/Metal]. MPS Graph enables you to sequence tasks with other workloads, which optimizes GPU utilization. Use MPS Graph to load your Core ML model or programmatically build, compile, and execute computational graphs.\n\nWhen running real-time signal processing on the CPU, use the BNNS Graph API in [doc:\/\/com.apple.documentation\/documentation\/Accelerate]. BNNS Graph works with Core ML models to enable real-time and latency-sensitive inference on the CPU, along with strict control over memory allocations. Use the Graph Builder to create graphs of operations that allow for writing routines or even small machine learning models to run in real-time on the CPU.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/TechnologyOverviews\/machine-learning\ncrawled: 2025-12-02T15:59:59Z\n---\n\n# Machine learning models\n\nCreate, optimize, and deploy models for on-device execution.\n\n## Overview\n\nWhen the available intelligent frameworks or generative technologies don’t provide the features you need, Apple provides machine learning frameworks that help you:\n\n- Create models from your own training data.\n- Run generative models, stateful models, and transformer models efficiently.\n- Convert models from other training libraries to run on-device.\n- Preview your model’s behavior from sample data or live inputs.\n- Analayze the performance of your model in Xcode and Instruments.\n\nBuild models to analyze text, images, or other types of data your app needs. If you already have your own machine learning models, convert them to the Core ML model format and integrate them into your app. Apple also provides frameworks to help with highly demanding machine learning tasks that involve graphics and real-time signal processing.\n\nAs you design your models, it’s important to keep the intended experience of your app in mind. The HIG offers [https:\/\/developer.apple.com\/design\/human-interface-guidelines\/machine-learning] to help you create apps that use machine learning.\n\n\n\n## Collect and prepare your data for training\n\nWhen you create a new model the starting point is always the same — your training and testing data. The quality of your data determines the quality of your results, so choose data that reflects a wide variety of possibility for your training use case. For example, when you [doc:\/\/com.apple.documentation\/documentation\/CreateML\/creating-an-image-classifier-model#Gather-Your-Data] model to recognize animals, begin by gathering at least 10 images per animal that best represent what you expect the model to see. Create ML supports several types of data sources, each with its [doc:\/\/com.apple.documentation\/documentation\/CreateML\/creating-an-action-classifier-model#Organize-the-Example-Videos] within a parent folder. In you parent folder, organize data into subfolders and use the folder name as your training label.\n\nIf you use the [doc:\/\/com.apple.documentation\/documentation\/CreateML] framework to programmatically create and train a model — like a text classifier that identifies the sentiment expressed in a sentence — [doc:\/\/com.apple.documentation\/documentation\/CreateML\/creating-a-text-classifier-model#Import-your-data] by using [doc:\/\/com.apple.documentation\/documentation\/TabularData].\n\n\n\n## Build and train on-device models with no code\n\nMany system frameworks can be extended or customized to your specific use case. If you’re working in a specialized domain that requires using your own data — or you want to extend the capabilities of an existing framework – the [https:\/\/developer.apple.com\/machine-learning\/create-ml\/] makes it easier to adapt system models. For example, if you use the [doc:\/\/com.apple.documentation\/documentation\/SoundAnalysis] framework and notice it can’t classify your sound, use the Create ML app to train a sound classification model that’s trained to identify your sound. After you train the model, [doc:\/\/com.apple.documentation\/documentation\/SoundAnalysis\/classifying-sounds-in-an-audio-file#Create-a-Sound-Classification-Request] framework.\n\nBuilt with the [doc:\/\/com.apple.documentation\/documentation\/CreateML] and [doc:\/\/com.apple.documentation\/documentation\/CreateMLComponents] frameworks, the Create ML app provides an approachable interface for creating models using your own data. In Xcode, choose Xcode > Open Developer Tool > Create ML. Choose a template that aligns with the task you want to customize, then provide your data, train, evaluate, and iterate on your model.\n\n\n\nAfter training your model, use the Create ML app to visualize and debug your annotations by clicking on the data source. The default view shows a distribution of your data, and the Explore page lets you see into specific object or class labels to visualize your annotations. The app [doc:\/\/com.apple.documentation\/documentation\/CoreML\/getting-a-core-ml-model] that’s ready to integrate into your app with Core ML.\n\n\n\n## Model conversion and optimization\n\nBring any model to the device if you want to experiment with it or deploy it. All you need is the model to be in the Core ML format. [doc:\/\/com.apple.documentation\/documentation\/CoreML] is the go-to framework for deploying models on-device, and you can [https:\/\/developer.apple.com\/machine-learning\/models\/] that are already in the Core ML format to experiment with or use for your feature.\n\nIf you created a model using training libraries like [https:\/\/ml-explore.github.io\/mlx\/] or [https:\/\/pytorch.org\/], use [https:\/\/coremltools.readme.io\/] to convert it to the Core ML format. Core ML Tools provides utilities and workflows for transforming trained models to the Core ML format. The workflows that Core ML Tools provide apply optimizations for on-device execution. Converting your model optimizes it for Apple devices, which requires less space on device, uses less power, and reduces latency when making predictions. Core ML Tools provides a number of compression techniques to help you optimize the model representation and parameters, while maintaining good accuracy.\n\nWhen you have an optimized and prepared model, you’re ready to integrate it with system frameworks. For example, if your model performs image analysis, [doc:\/\/com.apple.documentation\/documentation\/Vision\/CoreMLRequest] with the Vision framework.\n\n\n\n## Analyze the performance of your model with Xcode\n\nEvaluating the performance of models is an important task of machine learning. In Xcode, preview your model’s behavior by using sample data files or using the device’s camera and microphone. Review the performance of your model’s predictions directly from Xcode, or profile your app in Instruments to get a thorough performance analysis. After you add a model to your project, select it to get insights about the expected prediction latency, load times, and introspect where a particular operation is supported and run.\n\n\n\nTo build a deeper understanding of the model you’re working with, Xcode allows you to visualize the structure of the full model architecture and dive into the details of any operation. This visualization helps you debug issues and find performance enhancing opportunities.\n\n\n\n## Model deployment and execution on device\n\nYou use [doc:\/\/com.apple.documentation\/documentation\/CoreML] to integrate and run your model directly into your app. At runtime, Core ML makes use of all available compute and optimizes task execution across CPU, GPU, and Neural Engine. There are several technologies that underly Core ML that are available when you need fine-grained control over machine learning task execution.\n\nTo sequence and integrate machine learning with demanding graphics workloads, use Core ML models with both [doc:\/\/com.apple.documentation\/documentation\/MetalPerformanceShadersGraph] and [doc:\/\/com.apple.documentation\/documentation\/Metal]. MPS Graph enables you to sequence tasks with other workloads, which optimizes GPU utilization. Use MPS Graph to load your Core ML model or programmatically build, compile, and execute computational graphs.\n\nWhen running real-time signal processing on the CPU, use the BNNS Graph API in [doc:\/\/com.apple.documentation\/documentation\/Accelerate]. BNNS Graph works with Core ML models to enable real-time and latency-sensitive inference on the CPU, along with strict control over memory allocations. Use the Graph Builder to create graphs of operations that allow for writing routines or even small machine learning models to run in real-time on the CPU.\n\n",
  "sections" : [

  ],
  "source" : "appleJSON",
  "title" : "Machine learning models",
  "url" : "https:\/\/developer.apple.com\/documentation\/TechnologyOverviews\/machine-learning"
}