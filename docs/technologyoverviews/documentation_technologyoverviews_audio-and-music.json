{
  "abstract" : "Play, record, and process audio, and find and play content from Apple Music and someone’s music library.",
  "codeExamples" : [

  ],
  "contentHash" : "118fcae21e88a4bbf7ecbed1ec901839ece0d2d5d6456323d258eace7bcbb3ee",
  "crawledAt" : "2025-12-02T17:01:49Z",
  "id" : "3DC53C11-38FF-4477-8D32-773404B688F3",
  "kind" : "article",
  "language" : "swift",
  "module" : "Technology Overviews",
  "overview" : "Whether you’re adding audio touches to your app’s UI, or building your own audio-editing app, Apple has the audio technologies you need. Audio plays an important role in many apps, with specialized audio hardware built in to all Apple devices. The software you use to access that hardware ensures playback is efficient, and helps you work with a wide variety of audio formats.\n\n### Add user interface sounds and haptic feedback\n\nYou can enhance your app with audio and haptic feedback. Consider the feedback that the system apps provide: Mail plays a whoosh when you send an email, Camera plays a shutter click when you take a photo, and Photos provides haptic feedback when editing photos. These sensory cues create engaging and tactile experiences.\n\nApple offers several APIs that support audio playback, including:\n\nHaptic feedback delivers a tactile effect that draws attention and reinforces actions and events. While many system-provided UI components like pickers, switches, and sliders automatically provide haptic feedback, you can also add feedback to custom views and controls in your app:\n\n### Play music from the Apple Music catalog and user libraries\n\nAdd music playback to your app for a rich, engaging experience that feels deeply personal. You might create an app to help people find new music or rediscover their old favorites, a fitness app that plays a motivating soundtrack to power someone’s workout, or a meditation app with a playlist of ambient soundscapes.\n\nPlay music from Apple Music’s vast catalog with [https:\/\/developer.apple.com\/musickit\/], a combination of client frameworks and the [doc:\/\/com.apple.documentation\/documentation\/AppleMusicAPI]. Discover and fetch content from Apple Music, including artists, albums, playlists, and more, with features that enable you search the catalog and browse the latest charts. You can also use MusicKit to provide access to personalized features, including a person’s music library, recommendations, and recently played history. MusicKit client frameworks provide the APIs needed to authenticate subscribers and start and control playback on a device. Client frameworks are available for apps on [doc:\/\/com.apple.documentation\/documentation\/MusicKit], [https:\/\/js-cdn.music.apple.com\/musickit\/v1\/index.html] using JavaScript, and [https:\/\/developer.apple.com\/musickit\/android\/] using the available SDK.\n\n### Perform basic playback and recording of audio files\n\nYou can provide audio feedback in response to user input, play background music for your app or game, or record audio in a journaling or note-taking app.\n\nTwo classes in the [doc:\/\/com.apple.documentation\/documentation\/AVFAudio] framework provide high-level interfaces for audio playback and recording across Apple platforms. These classes simplify working with audio, which makes them accessible to developers without requiring an understanding how to perform low-level audio processing:\n\nTogether, these classes provide a straightforward way to add rich audio features into your app.\n\n### Perform advanced playback, recording, and processing\n\nWhen your needs grow beyond simple playback and recording of local audio files, Apple provides APIs that support advanced cases. For example, you can use these APIs to play streaming audio, enable musicians to mix custom backing tracks for practice or performance, or support a singer in a karaoke app by adding effects like reverb or delay.\n\nUse the [doc:\/\/com.apple.documentation\/documentation\/AVFAudio] framework’s [doc:\/\/com.apple.documentation\/documentation\/AVFAudio\/AVAudioEngine] API to perform real-time audio processing and mixing. It provides a node-based design that enables you to connect audio sources, effects, and outputs, making it ideal for apps that need to create advanced audio pipelines for real-time audio processing. For example, a game could use `AVAudioEngine` to blend background music, sound effects, and spatial audio cues, or a podcasting app could apply live noise cancellation or equalization.\n\n### Play spatial audio in games and immersive experiences\n\nSpatial audio enhances games and immersive experiences by providing realistic, directional sounds that react to a person’s position and environment. In games, you can use it to simulate sounds like footsteps approaching from behind, explosions on a distant battlefield, or ambient noises tied to virtual locations, making gameplay more engaging. For immersive experiences on Apple Vision Pro, spatial audio supports environmental sounds that change with the listener’s position and head movements. These use cases rely on precise sound positioning, head-tracking, and environmental effects to align audio with visual and interactive elements.\n\nApple provides several frameworks that you can use to play Spatial Audio, depending on the features you need:\n\n### Intelligently identify music and sounds\n\nBuild apps that intelligently identify sounds and music by creating dynamic, context-aware interactions that respond to music and audio. You can use this functionality to add recognition of songs, ambient sounds, or specific audio cues, and to power features like identifying a song that someone hears while walking down the street, syncing workout apps with music tempo for custom exercise cues, or prompting questions in a companion app during the presentation of educational content.\n\nTwo frameworks perform intelligent identification of music and sounds:\n\n### Support speech synthesis and transcription\n\nIntegrating speech synthesis or transcription into your app can create intuitive and accessible user experiences. Speech synthesis enables apps to vocalize text, which you can use to read content aloud to people with visual impairments, power language learning tools with accurate pronunciation, or enhance navigation apps with spoken directions. Speech transcription converts spoken words to text, which enables features like hands-free note taking or real-time captioning during video playback or voice calls. These capabilities make apps more inclusive and capable by supporting a wide variety of use cases.\n\nTo synthesize speech, use [doc:\/\/com.apple.documentation\/documentation\/AVFAudio\/AVSpeechSynthesizer] from the AVFAudio framework. This API allows you to generate spoken audio from text, supports multiple languages, and uses built-in or [https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10033\/] voices. It’s ideal for creating natural-sounding narration in apps, like reading books, providing audio feedback in educational apps, or voice prompts in navigation tools. It provides a variety of controls to customize pitch, rate, and volume, ensuring custom audio experiences that enhance accessibility and user engagement.\n\nThe [doc:\/\/com.apple.documentation\/documentation\/Speech] framework provides speech transcription, converting audio to text in real-time or from recordings. It supports voice command interfaces, dictation for note-taking apps, or live captioning for accessibility in video conferencing. With on-device and cloud-based options, developers can balance privacy and accuracy, enabling features like transcribing voice memos or live speeches.\n\n### Play and process MIDI data\n\nMIDI (Musical Instrument Digital Interface) is a protocol that enables precise control of virtual instruments, enabling you to compose music, trigger synthesized sounds or audio samples, or teach music theory through interactive playback. Recording MIDI data enables people to capture performances, edit compositions, or integrate with digital audio workstations (DAWs), making it ideal for music creation apps, virtual instrument simulators, or educational tools that provide real-time feedback to students.\n\nApple provides several technologies to play and process MIDI data:\n\n### Create and host Audio Unit plug-ins\n\nCreating Audio Unit plug-ins enables you to build advanced audio processing tools for music production, sound design, or multimedia apps. These plug-ins enable people to apply effects like equalization, delay, or reverb, or generate sounds through virtual instruments. Developing Audio Unit plug-ins enables you to provide customizable audio solutions for musicians and producers, while hosting plugins in your app expands its capabilities for people seeking professional-grade audio processing. This is ideal for building DAWs, audio editors, or live performance tools within Apple’s audio ecosystem.\n\nYou create Audio Unit plug-ins using the APIs available in the [doc:\/\/com.apple.documentation\/documentation\/AudioToolbox] framework. Here you’ll find the support for creating [doc:\/\/com.apple.documentation\/documentation\/AudioToolbox\/audio-unit-v3-plug-ins] plug-ins, which is the latest standard built on the [https:\/\/developer.apple.com\/app-extensions\/] model, or the [doc:\/\/com.apple.documentation\/documentation\/AudioToolbox\/audio-unit-v2-c-api] standard. These APIs allow you to create plug-ins for effects, instruments, or MIDI processing, providing people with customizable parameters to dynamically tailor processing to meet their needs. With support for real-time, low-latency processing, AudioToolbox ensures DAWs can integrate plugins efficiently, offering a robust platform for professional audio workflows compatible with apps like Logic Pro and GarageBand.\n\nTo host Audio Units, use [doc:\/\/com.apple.documentation\/documentation\/AudioToolbox\/audio-components] and [doc:\/\/com.apple.documentation\/documentation\/AVFAudio\/AVAudioUnit] to discover and load Audio Unit plug-ins across the system. You can use these APIs to support third-party plugins, ensuring compatibility with a broad ecosystem of audio tools. You can use Audio Unit plug-ins in the audio processing graph using AVAudioEngine. You can instantiate plugins for effects or instruments, enabling dynamic sound generation or processing. With features like parameter automation and preset management, `AVAudioUnit` supports DAWs in delivering flexible, high-quality audio that’s ideal for music creation or mixing environments.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/TechnologyOverviews\/audio-and-music\ncrawled: 2025-12-02T17:01:49Z\n---\n\n# Audio and music\n\nPlay, record, and process audio, and find and play content from Apple Music and someone’s music library.\n\nWhether you’re adding audio touches to your app’s UI, or building your own audio-editing app, Apple has the audio technologies you need. Audio plays an important role in many apps, with specialized audio hardware built in to all Apple devices. The software you use to access that hardware ensures playback is efficient, and helps you work with a wide variety of audio formats.\n\n### Add user interface sounds and haptic feedback\n\nYou can enhance your app with audio and haptic feedback. Consider the feedback that the system apps provide: Mail plays a whoosh when you send an email, Camera plays a shutter click when you take a photo, and Photos provides haptic feedback when editing photos. These sensory cues create engaging and tactile experiences.\n\nApple offers several APIs that support audio playback, including:\n\n\n\nHaptic feedback delivers a tactile effect that draws attention and reinforces actions and events. While many system-provided UI components like pickers, switches, and sliders automatically provide haptic feedback, you can also add feedback to custom views and controls in your app:\n\n- The simplest way to support custom [https:\/\/developer.apple.com\/documentation\/ApplePencil\/playing-haptic-feedback-in-your-app] is to use the support available in your app’s UI framework. For example, provide [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/SensoryFeedback] in SwiftUI or  [doc:\/\/com.apple.documentation\/documentation\/UIKit\/UIFeedbackGenerator] in UIKit, to provide feedback for common interactions like selection events or value changes.\n- When you require more advanced control, use [doc:\/\/com.apple.documentation\/documentation\/CoreHaptics] to deliver customized haptic feedback on supported devices like iPhone and Apple Watch. This framework allows you to deliver nuanced tactile feedback by combining haptic patterns with audio. Using Core Haptics, you can define continuous or transient haptic events with precise control over intensity and sharpness, making it ideal for enhancing interactions in games, music apps, or productivity tools.\n\n### Play music from the Apple Music catalog and user libraries\n\nAdd music playback to your app for a rich, engaging experience that feels deeply personal. You might create an app to help people find new music or rediscover their old favorites, a fitness app that plays a motivating soundtrack to power someone’s workout, or a meditation app with a playlist of ambient soundscapes.\n\nPlay music from Apple Music’s vast catalog with [https:\/\/developer.apple.com\/musickit\/], a combination of client frameworks and the [doc:\/\/com.apple.documentation\/documentation\/AppleMusicAPI]. Discover and fetch content from Apple Music, including artists, albums, playlists, and more, with features that enable you search the catalog and browse the latest charts. You can also use MusicKit to provide access to personalized features, including a person’s music library, recommendations, and recently played history. MusicKit client frameworks provide the APIs needed to authenticate subscribers and start and control playback on a device. Client frameworks are available for apps on [doc:\/\/com.apple.documentation\/documentation\/MusicKit], [https:\/\/js-cdn.music.apple.com\/musickit\/v1\/index.html] using JavaScript, and [https:\/\/developer.apple.com\/musickit\/android\/] using the available SDK.\n\n### Perform basic playback and recording of audio files\n\nYou can provide audio feedback in response to user input, play background music for your app or game, or record audio in a journaling or note-taking app.\n\nTwo classes in the [doc:\/\/com.apple.documentation\/documentation\/AVFAudio] framework provide high-level interfaces for audio playback and recording across Apple platforms. These classes simplify working with audio, which makes them accessible to developers without requiring an understanding how to perform low-level audio processing:\n\n- Use [https:\/\/developer.apple.com\/documentation\/avfaudio\/avaudioplayer] to play audio files in a variety of formats, including AAC, WAV, and MP3. It supports features that enable you to start and stop playback, adjust the volume level and playback rate, enable seamless looping, and more. It also provides features that you can use to respond to events like playback completion or decoding errors. Use it for a variety of audio playback tasks, including background music, user interface sound effects, and synchronizing multiple players.\n- Use [https:\/\/developer.apple.com\/documentation\/avfaudio\/avaudioplayer] to add basic microphone capture and recording capabilities to your app. It provides the interface to configure the recording settings, such as sample rate, number of channels, and audio format, and provides real-time metering of the input signal. It offers you a robust feature set to create apps that capture voice memos, audio notes, and more.\n\nTogether, these classes provide a straightforward way to add rich audio features into your app.\n\n### Perform advanced playback, recording, and processing\n\nWhen your needs grow beyond simple playback and recording of local audio files, Apple provides APIs that support advanced cases. For example, you can use these APIs to play streaming audio, enable musicians to mix custom backing tracks for practice or performance, or support a singer in a karaoke app by adding effects like reverb or delay.\n\nUse the [doc:\/\/com.apple.documentation\/documentation\/AVFAudio] framework’s [doc:\/\/com.apple.documentation\/documentation\/AVFAudio\/AVAudioEngine] API to perform real-time audio processing and mixing. It provides a node-based design that enables you to connect audio sources, effects, and outputs, making it ideal for apps that need to create advanced audio pipelines for real-time audio processing. For example, a game could use `AVAudioEngine` to blend background music, sound effects, and spatial audio cues, or a podcasting app could apply live noise cancellation or equalization.\n\n\n\n### Play spatial audio in games and immersive experiences\n\nSpatial audio enhances games and immersive experiences by providing realistic, directional sounds that react to a person’s position and environment. In games, you can use it to simulate sounds like footsteps approaching from behind, explosions on a distant battlefield, or ambient noises tied to virtual locations, making gameplay more engaging. For immersive experiences on Apple Vision Pro, spatial audio supports environmental sounds that change with the listener’s position and head movements. These use cases rely on precise sound positioning, head-tracking, and environmental effects to align audio with visual and interactive elements.\n\nApple provides several frameworks that you can use to play Spatial Audio, depending on the features you need:\n\n- [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/scene-content-audio] creates immersive experiences, particularly for games and apps built for Apple Vision Pro. It allows you to attach audio sources to entities in a 3D scene, and automatically handle spatialization based on their position relative to the listener. With support for head-tracked audio and environmental effects like occlusion, it tightly integrates with RealityKit’s visual rendering, making it ideal for apps or games.\n- [doc:\/\/com.apple.documentation\/documentation\/AVFAudio\/AVAudioEngine] supports spatial audio mixing with its [doc:\/\/com.apple.documentation\/documentation\/AVFAudio\/AVAudioEnvironmentNode], which enables 3D sound positioning, reverb, and distance attenuation. It’s integrated into a flexible node-based architecture, which makes it ideal for games or apps that need to support spatial audio mixing alongside general audio tasks like playback, recording, or effects processing. While effective for simpler spatial audio needs, it’s best suited for apps with mixed audio requirements.\n- [doc:\/\/com.apple.documentation\/documentation\/PHASE] is Apple’s most advanced framework for immersive spatial audio, specifically intended for 3D games and immersive experiences. It manages a dynamic environment where sound sources and listeners are positioned in 3D space, and supports head-tracked audio, occlusion, reverb, and distance-based effects. Its focus is spatial audio over general audio processing, making it ideal for highly immersive apps.\n\n### Intelligently identify music and sounds\n\nBuild apps that intelligently identify sounds and music by creating dynamic, context-aware interactions that respond to music and audio. You can use this functionality to add recognition of songs, ambient sounds, or specific audio cues, and to power features like identifying a song that someone hears while walking down the street, syncing workout apps with music tempo for custom exercise cues, or prompting questions in a companion app during the presentation of educational content.\n\nTwo frameworks perform intelligent identification of music and sounds:\n\n- [doc:\/\/com.apple.documentation\/documentation\/ShazamKit] integrates audio recognition into your apps, enabling identification of music from Shazam’s expansive catalog or custom audio from your own catalog. Use ShazamKit to synchronize your app state with audio cues to enhance its experience with context-aware interactions.\n- [doc:\/\/com.apple.documentation\/documentation\/SoundAnalysis] enables you to analyze and classify sounds in real-time or from recorded files, including environmental sound detection and activity identification. For example, it can identify sounds like a dog barking, a siren, or glass breaking to drive app actions, such as alerting people to emergencies, enhancing accessibility apps by describing ambient sounds, or powering interactive experiences that respond to specific noises. You can train custom models with tools like Create ML to recognize unique sounds relevant to your app, such as musical instruments for a music education tool or appliance beeps for a smart home app. The framework’s on-device processing ensures private, low-latency performance.\n\n### Support speech synthesis and transcription\n\nIntegrating speech synthesis or transcription into your app can create intuitive and accessible user experiences. Speech synthesis enables apps to vocalize text, which you can use to read content aloud to people with visual impairments, power language learning tools with accurate pronunciation, or enhance navigation apps with spoken directions. Speech transcription converts spoken words to text, which enables features like hands-free note taking or real-time captioning during video playback or voice calls. These capabilities make apps more inclusive and capable by supporting a wide variety of use cases.\n\nTo synthesize speech, use [doc:\/\/com.apple.documentation\/documentation\/AVFAudio\/AVSpeechSynthesizer] from the AVFAudio framework. This API allows you to generate spoken audio from text, supports multiple languages, and uses built-in or [https:\/\/developer.apple.com\/videos\/play\/wwdc2023\/10033\/] voices. It’s ideal for creating natural-sounding narration in apps, like reading books, providing audio feedback in educational apps, or voice prompts in navigation tools. It provides a variety of controls to customize pitch, rate, and volume, ensuring custom audio experiences that enhance accessibility and user engagement.\n\nThe [doc:\/\/com.apple.documentation\/documentation\/Speech] framework provides speech transcription, converting audio to text in real-time or from recordings. It supports voice command interfaces, dictation for note-taking apps, or live captioning for accessibility in video conferencing. With on-device and cloud-based options, developers can balance privacy and accuracy, enabling features like transcribing voice memos or live speeches.\n\n### Play and process MIDI data\n\nMIDI (Musical Instrument Digital Interface) is a protocol that enables precise control of virtual instruments, enabling you to compose music, trigger synthesized sounds or audio samples, or teach music theory through interactive playback. Recording MIDI data enables people to capture performances, edit compositions, or integrate with digital audio workstations (DAWs), making it ideal for music creation apps, virtual instrument simulators, or educational tools that provide real-time feedback to students.\n\nApple provides several technologies to play and process MIDI data:\n\n\n\n### Create and host Audio Unit plug-ins\n\nCreating Audio Unit plug-ins enables you to build advanced audio processing tools for music production, sound design, or multimedia apps. These plug-ins enable people to apply effects like equalization, delay, or reverb, or generate sounds through virtual instruments. Developing Audio Unit plug-ins enables you to provide customizable audio solutions for musicians and producers, while hosting plugins in your app expands its capabilities for people seeking professional-grade audio processing. This is ideal for building DAWs, audio editors, or live performance tools within Apple’s audio ecosystem.\n\nYou create Audio Unit plug-ins using the APIs available in the [doc:\/\/com.apple.documentation\/documentation\/AudioToolbox] framework. Here you’ll find the support for creating [doc:\/\/com.apple.documentation\/documentation\/AudioToolbox\/audio-unit-v3-plug-ins] plug-ins, which is the latest standard built on the [https:\/\/developer.apple.com\/app-extensions\/] model, or the [doc:\/\/com.apple.documentation\/documentation\/AudioToolbox\/audio-unit-v2-c-api] standard. These APIs allow you to create plug-ins for effects, instruments, or MIDI processing, providing people with customizable parameters to dynamically tailor processing to meet their needs. With support for real-time, low-latency processing, AudioToolbox ensures DAWs can integrate plugins efficiently, offering a robust platform for professional audio workflows compatible with apps like Logic Pro and GarageBand.\n\nTo host Audio Units, use [doc:\/\/com.apple.documentation\/documentation\/AudioToolbox\/audio-components] and [doc:\/\/com.apple.documentation\/documentation\/AVFAudio\/AVAudioUnit] to discover and load Audio Unit plug-ins across the system. You can use these APIs to support third-party plugins, ensuring compatibility with a broad ecosystem of audio tools. You can use Audio Unit plug-ins in the audio processing graph using AVAudioEngine. You can instantiate plugins for effects or instruments, enabling dynamic sound generation or processing. With features like parameter automation and preset management, `AVAudioUnit` supports DAWs in delivering flexible, high-quality audio that’s ideal for music creation or mixing environments.\n\n",
  "sections" : [

  ],
  "source" : "appleJSON",
  "title" : "Audio and music",
  "url" : "https:\/\/developer.apple.com\/documentation\/TechnologyOverviews\/audio-and-music"
}