{
  "abstract" : "Perform text recognition on a photo using the Vision framework’s text-recognition request.",
  "codeExamples" : [
    {
      "code" : "CameraPreview(camera: $camera)\n    .task {\n        \/\/\/ If the app has access to the camera, set up and display a live capture preview.\n        if await camera.checkCameraAuthorization() {\n            didSetup = camera.setup()\n        \/\/\/ If the app doesn't have access, dismiss the camera and display an error.\n        } else {\n            showAccessError = true\n            showCamera = false\n        }\n        \n        if !didSetup {\n            print(\"Camera setup failed.\")\n            showCamera = false\n        }\n    }",
      "language" : "swift"
    },
    {
      "code" : "func updateRequestSettings() {\n    \/\/\/ A Boolean value that indicates whether the system applies the language-correction model.\n    imageOCR.request.usesLanguageCorrection = languageCorrection\n    \n    imageOCR.request.recognitionLanguages = [selectedLanguage]\n    \n    switch selectedRecognitionLevel {\n    case \"Fast\":\n        imageOCR.request.recognitionLevel = .fast\n    default:\n        imageOCR.request.recognitionLevel = .accurate\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "@Observable\nclass OCR {\n    \/\/\/ The array of `RecognizedTextObservation` objects to hold the request's results.\n    var observations = [RecognizedTextObservation]()\n    \n    \/\/\/ The Vision request.\n    var request = RecognizeTextRequest()\n    \n    func performOCR(imageData: Data) async throws {\n        \/\/\/ Clear the `observations` array for photo recapture.\n        observations.removeAll()\n        \n        \/\/\/ Perform the request on the image data and return the results.\n        let results = try await request.perform(on: imageData)\n        \n        \/\/\/ Add each observation to the `observations` array.\n        for observation in results {\n            observations.append(observation)\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : ".onChange(of: settingChanges, initial: true) {\n    updateRequestSettings()\n    Task {\n        try await imageOCR.performOCR(imageData: imageData)\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "struct Box: Shape {\n    private let normalizedRect: NormalizedRect\n    \n    init(observation: any BoundingBoxProviding) {\n        normalizedRect = observation.boundingBox\n    }\n    \n    func path(in rect: CGRect) -> Path {\n        let rect = normalizedRect.toImageCoordinates(rect.size, origin: .upperLeft)\n        return Path(rect)\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : ".overlay {\n    ForEach(imageOCR.observations, id: \\.uuid) { observation in\n        Box(observation: observation)\n            .stroke(.red, lineWidth: 1)\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ Display the text from the captured image.\nForEach(imageOCR.observations, id: \\.self) { observation in\n    Text(observation.topCandidates(1).first?.string ?? \"No text recognized\")\n        .textSelection(.enabled)\n}\n.foregroundStyle(.gray)",
      "language" : "swift"
    }
  ],
  "contentHash" : "3966166856f2cc39368f3f6e57ab727e1cc9b1f5958036cb2522ce26bbf9f082",
  "crawledAt" : "2025-12-02T15:52:46Z",
  "id" : "3C04D42C-D977-4FD7-B073-F8FA35344CDD",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "Vision",
  "overview" : "## Overview\n\nThis sample code project demonstrates the Vision framework’s ability to perform optical character recognition (OCR) on an image you capture using your device’s camera. The [doc:\/\/com.apple.documentation\/documentation\/Vision\/RecognizeTextRequest] structure generates a collection of objects that extract and describe an image’s textual content. These objects provide information like the text string, the confidence of the observation’s accuracy, and the bounding box around the text’s location.\n\nAlong with extracting and displaying text from the image, the sample app helps you visualize where an observation occurs by using the bounding boxes to draw red rectangles around the text.\n\n### Configure the sample code project\n\nTo run this sample app, you need the following:\n\nConnect the iPhone to your Mac over USB-C. The first time you run this sample app, the system prompts you to grant the app access to the camera. You need to allow the sample app to access the camera for it to function correctly.\n\n### Set up the camera\n\nThe sample app performs the request on a photo from a physical device. To enable use of the camera, the sample uses [doc:\/\/com.apple.documentation\/documentation\/AVFoundation] to create a custom camera interface. Tapping the Take a Photo button on the initial view presents the camera and calls the `setup` method. This is where the camera work begins in the sample.\n\nFor more information on how to integrate the camera, see [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/setting-up-a-capture-session] and [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/capturing-still-and-live-photos].\n\n### Customize the request\n\nThe Vision framework provides the ability to customize the way it handles text recognition. Through its text-recognition path, the app demonstrates how to change whether the request prioritizes speed or accuracy. It also shows how to customize the languages the request detects, and whether the request applies a language-correction model during the recognition process.\n\nThe two text-recognition paths are: fast and accurate. The fast path is similar to a traditional OCR approach, and the accurate path uses a neural network that’s similar to how humans read text. By default, the request uses the accurate path, so the system sets the [doc:\/\/com.apple.documentation\/documentation\/Vision\/RecognizeTextRequest\/recognitionLevel-swift.property] property to `accurate`.\n\nDepending on the recognition level and language correction settings, the available recognition languages change. To dynamically generate a list of available languages to choose from, the app uses the [doc:\/\/com.apple.documentation\/documentation\/Vision\/RecognizeTextRequest\/supportedRecognitionLanguages] method. The app sets the recognition languages with an array of [doc:\/\/com.apple.documentation\/documentation\/Foundation\/Locale\/Language-swift.struct] objects, and prioritizes the first element.\n\n### Perform the request\n\nAfter capturing a photo, the app creates an instance of the custom OCR class. This class provides an array to hold the results, the [doc:\/\/com.apple.documentation\/documentation\/Vision\/RecognizeTextRequest], and the `performOCR` method to handle the text recognition. The `performOCR` method performs the request on the image, which returns an array of [doc:\/\/com.apple.documentation\/documentation\/Vision\/RecognizedTextObservation] objects. The method then adds each observation to the observations array.\n\nInitially, the app performs the request once when it captures an image. If the app detects any changes to the request settings (for example, the recognition level), it performs the request again. The Vision framework’s perform method is asynchronous, so the system wraps the method in a [doc:\/\/com.apple.documentation\/documentation\/Swift\/Task] block.\n\n### Create and display bounding boxes\n\nThis sample provides a custom implementation to display red bounding boxes where an observation occurs. An observation contains the location and the dimensions of the [doc:\/\/com.apple.documentation\/documentation\/Vision\/RecognizedTextObservation\/boundingBox] in the form of a [doc:\/\/com.apple.documentation\/documentation\/Vision\/NormalizedRect]. To create a bounding box, the app first converts the `NormalizedRect` to a [doc:\/\/com.apple.documentation\/documentation\/CoreFoundation\/CGRect], and then returns a [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/Path] to draw the rectangle.\n\nTo display the bounding boxes, the app uses the [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/View\/overlay(alignment:content:)] method on the image, and creates a bounding box for each of the observations.\n\n### Access the results\n\nFinally, the app displays the extracted text from the image by iterating through the observations array. If the request doesn’t recognize any text in the image, the app displays the “No text recognized” string.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/Vision\/locating-and-displaying-recognized-text\ncrawled: 2025-12-02T15:52:46Z\n---\n\n# Locating and displaying recognized text\n\n**Sample Code**\n\nPerform text recognition on a photo using the Vision framework’s text-recognition request.\n\n## Overview\n\nThis sample code project demonstrates the Vision framework’s ability to perform optical character recognition (OCR) on an image you capture using your device’s camera. The [doc:\/\/com.apple.documentation\/documentation\/Vision\/RecognizeTextRequest] structure generates a collection of objects that extract and describe an image’s textual content. These objects provide information like the text string, the confidence of the observation’s accuracy, and the bounding box around the text’s location.\n\nAlong with extracting and displaying text from the image, the sample app helps you visualize where an observation occurs by using the bounding boxes to draw red rectangles around the text.\n\n### Configure the sample code project\n\nTo run this sample app, you need the following:\n\n- Xcode 16 or later\n- iPhone with iOS 18 or later\n\nConnect the iPhone to your Mac over USB-C. The first time you run this sample app, the system prompts you to grant the app access to the camera. You need to allow the sample app to access the camera for it to function correctly.\n\n### Set up the camera\n\nThe sample app performs the request on a photo from a physical device. To enable use of the camera, the sample uses [doc:\/\/com.apple.documentation\/documentation\/AVFoundation] to create a custom camera interface. Tapping the Take a Photo button on the initial view presents the camera and calls the `setup` method. This is where the camera work begins in the sample.\n\n```swift\nCameraPreview(camera: $camera)\n    .task {\n        \/\/\/ If the app has access to the camera, set up and display a live capture preview.\n        if await camera.checkCameraAuthorization() {\n            didSetup = camera.setup()\n        \/\/\/ If the app doesn't have access, dismiss the camera and display an error.\n        } else {\n            showAccessError = true\n            showCamera = false\n        }\n        \n        if !didSetup {\n            print(\"Camera setup failed.\")\n            showCamera = false\n        }\n    }\n```\n\nFor more information on how to integrate the camera, see [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/setting-up-a-capture-session] and [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/capturing-still-and-live-photos].\n\n### Customize the request\n\nThe Vision framework provides the ability to customize the way it handles text recognition. Through its text-recognition path, the app demonstrates how to change whether the request prioritizes speed or accuracy. It also shows how to customize the languages the request detects, and whether the request applies a language-correction model during the recognition process.\n\nThe two text-recognition paths are: fast and accurate. The fast path is similar to a traditional OCR approach, and the accurate path uses a neural network that’s similar to how humans read text. By default, the request uses the accurate path, so the system sets the [doc:\/\/com.apple.documentation\/documentation\/Vision\/RecognizeTextRequest\/recognitionLevel-swift.property] property to `accurate`.\n\nDepending on the recognition level and language correction settings, the available recognition languages change. To dynamically generate a list of available languages to choose from, the app uses the [doc:\/\/com.apple.documentation\/documentation\/Vision\/RecognizeTextRequest\/supportedRecognitionLanguages] method. The app sets the recognition languages with an array of [doc:\/\/com.apple.documentation\/documentation\/Foundation\/Locale\/Language-swift.struct] objects, and prioritizes the first element.\n\n```swift\nfunc updateRequestSettings() {\n    \/\/\/ A Boolean value that indicates whether the system applies the language-correction model.\n    imageOCR.request.usesLanguageCorrection = languageCorrection\n    \n    imageOCR.request.recognitionLanguages = [selectedLanguage]\n    \n    switch selectedRecognitionLevel {\n    case \"Fast\":\n        imageOCR.request.recognitionLevel = .fast\n    default:\n        imageOCR.request.recognitionLevel = .accurate\n    }\n}\n```\n\n### Perform the request\n\nAfter capturing a photo, the app creates an instance of the custom OCR class. This class provides an array to hold the results, the [doc:\/\/com.apple.documentation\/documentation\/Vision\/RecognizeTextRequest], and the `performOCR` method to handle the text recognition. The `performOCR` method performs the request on the image, which returns an array of [doc:\/\/com.apple.documentation\/documentation\/Vision\/RecognizedTextObservation] objects. The method then adds each observation to the observations array.\n\n```swift\n@Observable\nclass OCR {\n    \/\/\/ The array of `RecognizedTextObservation` objects to hold the request's results.\n    var observations = [RecognizedTextObservation]()\n    \n    \/\/\/ The Vision request.\n    var request = RecognizeTextRequest()\n    \n    func performOCR(imageData: Data) async throws {\n        \/\/\/ Clear the `observations` array for photo recapture.\n        observations.removeAll()\n        \n        \/\/\/ Perform the request on the image data and return the results.\n        let results = try await request.perform(on: imageData)\n        \n        \/\/\/ Add each observation to the `observations` array.\n        for observation in results {\n            observations.append(observation)\n        }\n    }\n}\n```\n\nInitially, the app performs the request once when it captures an image. If the app detects any changes to the request settings (for example, the recognition level), it performs the request again. The Vision framework’s perform method is asynchronous, so the system wraps the method in a [doc:\/\/com.apple.documentation\/documentation\/Swift\/Task] block.\n\n```swift\n.onChange(of: settingChanges, initial: true) {\n    updateRequestSettings()\n    Task {\n        try await imageOCR.performOCR(imageData: imageData)\n    }\n}\n```\n\n### Create and display bounding boxes\n\nThis sample provides a custom implementation to display red bounding boxes where an observation occurs. An observation contains the location and the dimensions of the [doc:\/\/com.apple.documentation\/documentation\/Vision\/RecognizedTextObservation\/boundingBox] in the form of a [doc:\/\/com.apple.documentation\/documentation\/Vision\/NormalizedRect]. To create a bounding box, the app first converts the `NormalizedRect` to a [doc:\/\/com.apple.documentation\/documentation\/CoreFoundation\/CGRect], and then returns a [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/Path] to draw the rectangle.\n\n```swift\nstruct Box: Shape {\n    private let normalizedRect: NormalizedRect\n    \n    init(observation: any BoundingBoxProviding) {\n        normalizedRect = observation.boundingBox\n    }\n    \n    func path(in rect: CGRect) -> Path {\n        let rect = normalizedRect.toImageCoordinates(rect.size, origin: .upperLeft)\n        return Path(rect)\n    }\n}\n```\n\nTo display the bounding boxes, the app uses the [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/View\/overlay(alignment:content:)] method on the image, and creates a bounding box for each of the observations.\n\n```swift\n.overlay {\n    ForEach(imageOCR.observations, id: \\.uuid) { observation in\n        Box(observation: observation)\n            .stroke(.red, lineWidth: 1)\n    }\n}\n```\n\n### Access the results\n\nFinally, the app displays the extracted text from the image by iterating through the observations array. If the request doesn’t recognize any text in the image, the app displays the “No text recognized” string.\n\n```swift\n\/\/\/ Display the text from the captured image.\nForEach(imageOCR.observations, id: \\.self) { observation in\n    Text(observation.topCandidates(1).first?.string ?? \"No text recognized\")\n        .textSelection(.enabled)\n}\n.foregroundStyle(.gray)\n```\n\n## Text detection\n\n- **Recognizing tables within a document**: Scan a document that contains a table and extract its content in a formatted way.\n- **RecognizeDocumentsRequest**: An image-analysis request to scan an image of a document and provide information about its structure.\n- **DocumentObservation**: Information about the sections of content that an image-analysis request detects in a document.\n- **DetectTextRectanglesRequest**: An image-analysis request that finds regions of visible text in an image.\n- **RecognizeTextRequest**: An image-analysis request that recognizes text in an image.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Scan a document that contains a table and extract its content in a formatted way.",
          "name" : "Recognizing tables within a document",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/recognize-tables-within-a-document"
        },
        {
          "description" : "An image-analysis request to scan an image of a document and provide information about its structure.",
          "name" : "RecognizeDocumentsRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/RecognizeDocumentsRequest"
        },
        {
          "description" : "Information about the sections of content that an image-analysis request detects in a document.",
          "name" : "DocumentObservation",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/DocumentObservation"
        },
        {
          "description" : "An image-analysis request that finds regions of visible text in an image.",
          "name" : "DetectTextRectanglesRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/DetectTextRectanglesRequest"
        },
        {
          "description" : "An image-analysis request that recognizes text in an image.",
          "name" : "RecognizeTextRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/RecognizeTextRequest"
        }
      ],
      "title" : "Text detection"
    }
  ],
  "source" : "appleJSON",
  "title" : "Locating and displaying recognized text",
  "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/locating-and-displaying-recognized-text"
}