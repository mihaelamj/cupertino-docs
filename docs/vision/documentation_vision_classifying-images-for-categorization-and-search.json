{
  "abstract" : "Analyze and label images using a Vision classification request.",
  "codeExamples" : [
    {
      "code" : "\/\/ Returns an `ImageFile` object based on the `ClassifyImageRequest` results.\nfunc classifyImage(url: URL) async throws -> ImageFile {\n    var image = ImageFile(url: url)\n    \n    \/\/ Vision request to classify an image.\n    let request = ClassifyImageRequest()\n    \n    \/\/ Perform the request on the image, and return an array of `ClassificationObservation` objects.\n    let results = try await request.perform(on: url)\n        \/\/ Use `hasMinimumPrecision` for a high-recall filter.\n        .filter { $0.hasMinimumPrecision(0.1, forRecall: 0.8) }\n        \/\/ Use `hasMinimumRecall` for a high-precision filter.\n        \/\/ .filter { $0.hasMinimumRecall(0.01, forPrecision: 0.9) }\n    \n    \/\/ Add each classification identifier and its respective confidence level into the observations dictionary.\n    for classification in results {\n        image.observations[classification.identifier] = classification.confidence\n    }\n    \n    return image\n}",
      "language" : "swift"
    },
    {
      "code" : "func classifyAllImages(urls: [URL]) async throws -> [ImageFile] {\n    var images = [ImageFile]()\n    \n    try await withThrowingTaskGroup(of: ImageFile.self) { group in\n        for url in urls {\n            group.addTask {\n                return try await classifyImage(url: url)\n            }\n        }\n        \n        for try await image in group {\n            images.append(image)\n        }\n    }\n    \n    return images\n}",
      "language" : "swift"
    },
    {
      "code" : "var searchResults: [ImageFile] {\n    if searchTerm.isEmpty {\n        \/\/ If the search bar is empty, keep all of the images available.\n        return images\n    } else {\n        \/\/ The only images that are available are those that contain classification labels equal to the search term.\n        return images.filter({ $0.observations.keys.contains(searchTerm) })\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "List {\n    if image.observations.isEmpty {\n        Text(\"No observations found with significant confidence.\")\n            .font(.title2)\n            .padding(10)\n    }\n                \n    ForEach(image.observations.sorted(by: { $0.value > $1.value }), id: \\.key) { key, value in\n        Text(\"\\(value, specifier: \"%.2f\"): \\(key.capitalized)\")\n            .font(.title2)\n            .padding(10)\n    }\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "fd9ffcfbcb26e1aba7517b57836f0f95a936b5d040b1c2582814a364037145f7",
  "crawledAt" : "2025-12-03T18:55:11Z",
  "id" : "F1A753CC-DCAB-490A-8972-376EF2E0EABC",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "Vision",
  "overview" : "## Overview\n\nWith the help of machine learning to perform image analysis, the Vision framework can identify, recognize, and label objects in an image you provide. The [doc:\/\/com.apple.documentation\/documentation\/Vision\/ClassifyImageRequest] structure generates the image analysis results in the form of [doc:\/\/com.apple.documentation\/documentation\/Vision\/ClassificationObservation] objects. Each object contains a classification label (such as *bicycle*), and an associated confidence value to indicate the level of certainty in the observation.\n\nThis sample app demonstrates how to set up and perform the classification request on a single image or on a collection of images. For a collection of images, the sample shows you how to process the images concurrently using Swift concurrency. Finally, the sample shows how you access and display the results from the request.\n\n### Perform the request\n\nFirst, the app creates the `ClassifyImageRequest` and performs it on an image. The request returns the entire taxonomy as `ClassificationObservation` objects, and stores the objects in an array. For each observation, the request assigns a label in the form of an identifier, along with a floating-point confidence value in the range of `0.0` to `1.0`. A value of `0.0` indicates that the observation is not likely in the image, while a value of `1.0` indicates that the observation is certain to be in the image.\n\nWhen the request returns the results, the app performs a high-recall [doc:\/\/com.apple.documentation\/documentation\/Swift\/String\/filter(_:)], and only retains the observations that meet a confidence threshold. This threshold is set by the app with the [doc:\/\/com.apple.documentation\/documentation\/Vision\/ClassificationObservation\/hasMinimumPrecision(_:forRecall:)] method from `Vision`. A high-recall filter provides a much broader range of observations, but can result in more false positive results. For example, with this filter, an ambiguous image of a bear may result in an observation with the label *dog*.\n\nIf an app can’t tolerate false positive results, the [doc:\/\/com.apple.documentation\/documentation\/Vision\/ClassificationObservation\/hasMinimumRecall(_:forPrecision:)] method allows for a high-precision filter. A high-precision filter retains a smaller number of observations, but less chance to contain false positives. Increasing precision decreases recall, and increasing recall decreases precision. Testing can help determine the balance point that returns the best results for a specific use case.\n\nThe app stores the final results in the `observations` dictionary. The dictionary’s key is the identifier (the label of the observation), and its value is the confidence level.\n\nProcessing a large collection of images can take time, so the app uses Swift concurrency to help with speed and efficiency. Using [doc:\/\/com.apple.documentation\/documentation\/Swift\/TaskGroup], the app processes the images in parallel instead of sequentially in a loop. When the request returns results, the app adds each image’s observations to an array. For more information on using Swift concurrency, see [https:\/\/docs.swift.org\/swift-book\/documentation\/the-swift-programming-language\/concurrency\/].\n\n### Retrieve and search the results\n\nThe sample provides the ability to search for images by their classification labels. If the search bar is empty, the app presents all the images. If the search bar is not empty, the app only presents images with classification labels equal to the search term.\n\nClicking an image navigates to the results view and displays the results of the image analysis. Using a `ForEach` loop, the app iterates through the observations. The [doc:\/\/com.apple.documentation\/documentation\/Swift\/Array\/sorted(by:)] method sorts the observations in descending order of their confidence levels. The `ForEach` loop accesses the values of the observation with the dictionary’s key and value keywords. Note that any confidence values of `0.0` are greater than zero due to rounding.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/vision\/classifying-images-for-categorization-and-search\ncrawled: 2025-12-03T18:55:11Z\n---\n\n# Classifying images for categorization and search\n\n**Sample Code**\n\nAnalyze and label images using a Vision classification request.\n\n## Overview\n\nWith the help of machine learning to perform image analysis, the Vision framework can identify, recognize, and label objects in an image you provide. The [doc:\/\/com.apple.documentation\/documentation\/Vision\/ClassifyImageRequest] structure generates the image analysis results in the form of [doc:\/\/com.apple.documentation\/documentation\/Vision\/ClassificationObservation] objects. Each object contains a classification label (such as *bicycle*), and an associated confidence value to indicate the level of certainty in the observation.\n\nThis sample app demonstrates how to set up and perform the classification request on a single image or on a collection of images. For a collection of images, the sample shows you how to process the images concurrently using Swift concurrency. Finally, the sample shows how you access and display the results from the request.\n\n### Perform the request\n\nFirst, the app creates the `ClassifyImageRequest` and performs it on an image. The request returns the entire taxonomy as `ClassificationObservation` objects, and stores the objects in an array. For each observation, the request assigns a label in the form of an identifier, along with a floating-point confidence value in the range of `0.0` to `1.0`. A value of `0.0` indicates that the observation is not likely in the image, while a value of `1.0` indicates that the observation is certain to be in the image.\n\nWhen the request returns the results, the app performs a high-recall [doc:\/\/com.apple.documentation\/documentation\/Swift\/String\/filter(_:)], and only retains the observations that meet a confidence threshold. This threshold is set by the app with the [doc:\/\/com.apple.documentation\/documentation\/Vision\/ClassificationObservation\/hasMinimumPrecision(_:forRecall:)] method from `Vision`. A high-recall filter provides a much broader range of observations, but can result in more false positive results. For example, with this filter, an ambiguous image of a bear may result in an observation with the label *dog*.\n\nIf an app can’t tolerate false positive results, the [doc:\/\/com.apple.documentation\/documentation\/Vision\/ClassificationObservation\/hasMinimumRecall(_:forPrecision:)] method allows for a high-precision filter. A high-precision filter retains a smaller number of observations, but less chance to contain false positives. Increasing precision decreases recall, and increasing recall decreases precision. Testing can help determine the balance point that returns the best results for a specific use case.\n\nThe app stores the final results in the `observations` dictionary. The dictionary’s key is the identifier (the label of the observation), and its value is the confidence level.\n\n```swift\n\/\/ Returns an `ImageFile` object based on the `ClassifyImageRequest` results.\nfunc classifyImage(url: URL) async throws -> ImageFile {\n    var image = ImageFile(url: url)\n    \n    \/\/ Vision request to classify an image.\n    let request = ClassifyImageRequest()\n    \n    \/\/ Perform the request on the image, and return an array of `ClassificationObservation` objects.\n    let results = try await request.perform(on: url)\n        \/\/ Use `hasMinimumPrecision` for a high-recall filter.\n        .filter { $0.hasMinimumPrecision(0.1, forRecall: 0.8) }\n        \/\/ Use `hasMinimumRecall` for a high-precision filter.\n        \/\/ .filter { $0.hasMinimumRecall(0.01, forPrecision: 0.9) }\n    \n    \/\/ Add each classification identifier and its respective confidence level into the observations dictionary.\n    for classification in results {\n        image.observations[classification.identifier] = classification.confidence\n    }\n    \n    return image\n}\n```\n\nProcessing a large collection of images can take time, so the app uses Swift concurrency to help with speed and efficiency. Using [doc:\/\/com.apple.documentation\/documentation\/Swift\/TaskGroup], the app processes the images in parallel instead of sequentially in a loop. When the request returns results, the app adds each image’s observations to an array. For more information on using Swift concurrency, see [https:\/\/docs.swift.org\/swift-book\/documentation\/the-swift-programming-language\/concurrency\/].\n\n```swift\nfunc classifyAllImages(urls: [URL]) async throws -> [ImageFile] {\n    var images = [ImageFile]()\n    \n    try await withThrowingTaskGroup(of: ImageFile.self) { group in\n        for url in urls {\n            group.addTask {\n                return try await classifyImage(url: url)\n            }\n        }\n        \n        for try await image in group {\n            images.append(image)\n        }\n    }\n    \n    return images\n}\n```\n\n### Retrieve and search the results\n\nThe sample provides the ability to search for images by their classification labels. If the search bar is empty, the app presents all the images. If the search bar is not empty, the app only presents images with classification labels equal to the search term.\n\n```swift\nvar searchResults: [ImageFile] {\n    if searchTerm.isEmpty {\n        \/\/ If the search bar is empty, keep all of the images available.\n        return images\n    } else {\n        \/\/ The only images that are available are those that contain classification labels equal to the search term.\n        return images.filter({ $0.observations.keys.contains(searchTerm) })\n    }\n}\n```\n\nClicking an image navigates to the results view and displays the results of the image analysis. Using a `ForEach` loop, the app iterates through the observations. The [doc:\/\/com.apple.documentation\/documentation\/Swift\/Array\/sorted(by:)] method sorts the observations in descending order of their confidence levels. The `ForEach` loop accesses the values of the observation with the dictionary’s key and value keywords. Note that any confidence values of `0.0` are greater than zero due to rounding.\n\n```swift\nList {\n    if image.observations.isEmpty {\n        Text(\"No observations found with significant confidence.\")\n            .font(.title2)\n            .padding(10)\n    }\n                \n    ForEach(image.observations.sorted(by: { $0.value > $1.value }), id: \\.key) { key, value in\n        Text(\"\\(value, specifier: \"%.2f\"): \\(key.capitalized)\")\n            .font(.title2)\n            .padding(10)\n    }\n}\n```\n\n## Image classification and recognition\n\n- **ClassifyImageRequest**: A request to classify an image.\n- **DetectHumanRectanglesRequest**: A request that finds rectangular regions that contain people in an image.\n- **RecognizeAnimalsRequest**: A request that recognizes animals in an image.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "A request to classify an image.",
          "name" : "ClassifyImageRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/ClassifyImageRequest"
        },
        {
          "description" : "A request that finds rectangular regions that contain people in an image.",
          "name" : "DetectHumanRectanglesRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/DetectHumanRectanglesRequest"
        },
        {
          "description" : "A request that recognizes animals in an image.",
          "name" : "RecognizeAnimalsRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/RecognizeAnimalsRequest"
        }
      ],
      "title" : "Image classification and recognition"
    }
  ],
  "source" : "appleJSON",
  "title" : "Classifying images for categorization and search",
  "url" : "https:\/\/developer.apple.com\/documentation\/vision\/classifying-images-for-categorization-and-search"
}