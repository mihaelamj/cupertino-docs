{
  "abstract" : "Identify the most visually pleasing frames in a video by using the image-aesthetics scores request.",
  "codeExamples" : [
    {
      "code" : "struct Frame {\n    \/\/\/ The timestamp of the frame.\n    let time: CMTime\n\n    \/\/\/ The score of the frame.\n    let score: Float\n\n    \/\/\/ The feature-print observation of the frame.\n    let observation: FeaturePrintObservation\n}",
      "language" : "swift"
    },
    {
      "code" : "class Thumbnail: Identifiable {\n    \/\/\/ The image that captures from the video frame.\n    let image: CGImage\n\n    \/\/\/ The frame that the thumbnail represents.\n    let frame: Frame\n\n    \/\/ ...\n}",
      "language" : "swift"
    },
    {
      "code" : "func processVideo(for videoURL: URL, progression: Binding<Float>) async -> [Thumbnail] {\n    \/\/\/ The instance of the `VideoProcessor` with the local path to the video file.\n    let videoProcessor = VideoProcessor(videoURL)\n\n    \/\/\/ The request to calculate the aesthetics score for each frame.\n    let aestheticsScoresRequest = CalculateImageAestheticsScoresRequest()\n\n    \/\/\/ The request to generate feature prints from an image.\n    let imageFeaturePrintRequest = GenerateImageFeaturePrintRequest()\n\n    \/\/\/ The array to store information for the frames with the highest scores.\n    var topFrames: [Frame] = []\n\n    \/\/ ...\n}",
      "language" : "swift"
    },
    {
      "code" : "do {\n    \/\/\/ The time interval for the video-processing cadence.\n    let interval = CMTime(\n        seconds: totalDuration \/ framesToEval,\n        preferredTimescale: timeScale\n    )\n\n    \/\/\/ The video-processing cadence to process only 100 frames.\n    let cadence = VideoProcessor.Cadence.timeInterval(interval)\n\n    \/\/\/ The stream that adds the aesthetics scores request to the video processor.\n    let aestheticsScoreStream = try await videoProcessor.addRequest(aestheticsScoresRequest, cadence: cadence)\n\n    \/\/\/ The stream that adds the image feature-print request to the video processor.\n    let featurePrintStream = try await videoProcessor.addRequest(imageFeaturePrintRequest, cadence: cadence)\n    \n    \/\/ Start to analyze the video.\n    videoProcessor.startAnalysis()\n\n    \/\/\/ The dictionary to store the timestamp and the aesthetics score.\n    var aestheticsResults: [CMTime: Float] = [:]\n\n    \/\/\/ The dictionary to store the timestamp and the feature-print observation.\n    var featurePrintResults: [CMTime: FeaturePrintObservation] = [:]\n\n    \/\/ ...\n\n    \/\/ Solve for the top-rated frames.\n    topFrames = await calculateTopFrames(aestheticsResults: aestheticsResults, featurePrintResults: featurePrintResults)\n} ",
      "language" : "swift"
    },
    {
      "code" : "for (time, score) in aestheticsResults {\n    \/\/\/ The `FeaturePrintObservation` for the timestamp.\n    guard let featurePrint = featurePrintResults[time] else { continue }\n\n    \/\/\/ The new frame at that timestamp.\n    let newFrame = Frame(time: time, score: score, observation: featurePrint)\n\n    \/\/\/ The variable that tracks whether to add the image based on image similarity.\n    var isSimilar = false\n\n    \/\/ Iterate through the current top-rated frames to check whether any of them\n    \/\/ are similar to the new frame and find the insertion index.\n    for (index, frame) in topFrames.enumerated() {\n        if let distance = try? featurePrint.distance(to: frame.observation), distance < similarityThreshold {\n            \/\/ Replace the frame if the new frame has a higher score.\n            if newFrame.score > frame.score {\n                topFrames[index] = newFrame\n            }\n            isSimilar = true\n            break\n        }\n\n        \/\/ ...\n    }\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "ab0c537dd60161c59f14cdfcef4ca57abd36ec29adbe4490573eff7023eb7286",
  "crawledAt" : "2025-12-02T15:34:34Z",
  "id" : "1F6B2FB7-E5EE-43AB-866E-861F626F1C82",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "Vision",
  "overview" : "## Overview\n\nUsing the [doc:\/\/com.apple.documentation\/documentation\/Vision] framework, you can process images and videos frame by frame, enabling image-analysis requests through [doc:\/\/com.apple.documentation\/documentation\/Vision\/VisionRequest]. The sample demonstrates how to process video input to identify the three frames with the highest aesthetic scores. By leveraging this capability, you can automatically select the best frames for creating promotional materials or for highlighting short films. The following image shows a preview of the sample:\n\n\n\nThe sample uses a [doc:\/\/com.apple.documentation\/documentation\/Vision\/VideoProcessor] to convert the video into a stream of frames, which [doc:\/\/com.apple.documentation\/documentation\/Vision\/CalculateImageAestheticsScoresRequest] analyzes to rate each frame with an overall aesthetic score. To avoid selecting similar thumbnails, the [doc:\/\/com.apple.documentation\/documentation\/Vision\/GenerateImageFeaturePrintRequest] compares the image similarity. Finally, [doc:\/\/com.apple.documentation\/documentation\/AVFoundation] extracts the images from the selected frames and presents them as thumbnails.\n\n### Set up the representation of frames and thumbnails\n\nThe `Frame` structure contains the information about a specific frame in the video that the sample uses to determine the best frame to use as a thumbnail. It accepts a `CMTime` to represent the timestamp of the frame, a `Float` value for the overall score of the frame, and a [doc:\/\/com.apple.documentation\/documentation\/Vision\/FeaturePrintObservation] to enable comparison to other frames:\n\nTo present the results of each frame, the sample creates a `Thumbnail` class that conforms to the [doc:\/\/com.apple.documentation\/documentation\/Swift\/Identifiable] protocal. This ensures that the class has a unique identity for [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/ForEach] to display the results. This class accepts a `CGImage` to store the image of the frame, and a `Frame` to establish a connection between the frame and the thumbnail:\n\n### Process the video\n\nThe sample processes videos frame by frame using the [doc:\/\/com.apple.documentation\/documentation\/Vision\/VideoProcessor]. The sample first initializes the video processor with the video URL, then create the instances of the requests the sample uses to process the video:\n\nThe [doc:\/\/com.apple.documentation\/documentation\/Vision\/CalculateImageAestheticsScoresRequest] calculates the aesthetic scores for each frame. To ensure that the thumbnail results represent different scenes rather than variations of the same frame, [doc:\/\/com.apple.documentation\/documentation\/Vision\/GenerateImageFeaturePrintRequest] computes the similarity between the frames.\n\nThe sample calculates a time interval for the video processor to process approximately 100 frames, adds the `aestheticsScoresRequest` and `imageFeaturePrintRequest` to the video processor, then starts the video-analysis process. To store the timestamp and the results from the video-processor stream, the sample creates two dictionaries: `aestheticsResults` and `featurePrintResults`:\n\nWhen the sample receives the results for both requests, call `calculateTopFrames(aestheticsResults:featurePrintResults:)` to solve for the top-rated frames.\n\n### Solve for the top-rated frames\n\nThe function uses `aestheticsResults` and `featurePrintResults` to identify three frames that have the highest aesthetic scores and are different from each other:\n\nFor each result, the sample creates a new frame based on the timestamp and attaches the score and `FeaturePrintObservation`. It checks for similar frames in `topFrames` with the `distance(to:)` function to compare the observations. If there is a match, the sample keeps the frame with the higher score and exits the loop.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/Vision\/generating-thumbnails-from-videos\ncrawled: 2025-12-02T15:34:34Z\n---\n\n# Generating high-quality thumbnails from videos\n\n**Sample Code**\n\nIdentify the most visually pleasing frames in a video by using the image-aesthetics scores request.\n\n## Overview\n\nUsing the [doc:\/\/com.apple.documentation\/documentation\/Vision] framework, you can process images and videos frame by frame, enabling image-analysis requests through [doc:\/\/com.apple.documentation\/documentation\/Vision\/VisionRequest]. The sample demonstrates how to process video input to identify the three frames with the highest aesthetic scores. By leveraging this capability, you can automatically select the best frames for creating promotional materials or for highlighting short films. The following image shows a preview of the sample:\n\n\n\nThe sample uses a [doc:\/\/com.apple.documentation\/documentation\/Vision\/VideoProcessor] to convert the video into a stream of frames, which [doc:\/\/com.apple.documentation\/documentation\/Vision\/CalculateImageAestheticsScoresRequest] analyzes to rate each frame with an overall aesthetic score. To avoid selecting similar thumbnails, the [doc:\/\/com.apple.documentation\/documentation\/Vision\/GenerateImageFeaturePrintRequest] compares the image similarity. Finally, [doc:\/\/com.apple.documentation\/documentation\/AVFoundation] extracts the images from the selected frames and presents them as thumbnails.\n\n### Set up the representation of frames and thumbnails\n\nThe `Frame` structure contains the information about a specific frame in the video that the sample uses to determine the best frame to use as a thumbnail. It accepts a `CMTime` to represent the timestamp of the frame, a `Float` value for the overall score of the frame, and a [doc:\/\/com.apple.documentation\/documentation\/Vision\/FeaturePrintObservation] to enable comparison to other frames:\n\n```swift\nstruct Frame {\n    \/\/\/ The timestamp of the frame.\n    let time: CMTime\n\n    \/\/\/ The score of the frame.\n    let score: Float\n\n    \/\/\/ The feature-print observation of the frame.\n    let observation: FeaturePrintObservation\n}\n```\n\nTo present the results of each frame, the sample creates a `Thumbnail` class that conforms to the [doc:\/\/com.apple.documentation\/documentation\/Swift\/Identifiable] protocal. This ensures that the class has a unique identity for [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/ForEach] to display the results. This class accepts a `CGImage` to store the image of the frame, and a `Frame` to establish a connection between the frame and the thumbnail:\n\n```swift\nclass Thumbnail: Identifiable {\n    \/\/\/ The image that captures from the video frame.\n    let image: CGImage\n\n    \/\/\/ The frame that the thumbnail represents.\n    let frame: Frame\n\n    \/\/ ...\n}\n```\n\n### Process the video\n\nThe sample processes videos frame by frame using the [doc:\/\/com.apple.documentation\/documentation\/Vision\/VideoProcessor]. The sample first initializes the video processor with the video URL, then create the instances of the requests the sample uses to process the video:\n\n```swift\nfunc processVideo(for videoURL: URL, progression: Binding<Float>) async -> [Thumbnail] {\n    \/\/\/ The instance of the `VideoProcessor` with the local path to the video file.\n    let videoProcessor = VideoProcessor(videoURL)\n\n    \/\/\/ The request to calculate the aesthetics score for each frame.\n    let aestheticsScoresRequest = CalculateImageAestheticsScoresRequest()\n\n    \/\/\/ The request to generate feature prints from an image.\n    let imageFeaturePrintRequest = GenerateImageFeaturePrintRequest()\n\n    \/\/\/ The array to store information for the frames with the highest scores.\n    var topFrames: [Frame] = []\n\n    \/\/ ...\n}\n```\n\nThe [doc:\/\/com.apple.documentation\/documentation\/Vision\/CalculateImageAestheticsScoresRequest] calculates the aesthetic scores for each frame. To ensure that the thumbnail results represent different scenes rather than variations of the same frame, [doc:\/\/com.apple.documentation\/documentation\/Vision\/GenerateImageFeaturePrintRequest] computes the similarity between the frames.\n\nThe sample calculates a time interval for the video processor to process approximately 100 frames, adds the `aestheticsScoresRequest` and `imageFeaturePrintRequest` to the video processor, then starts the video-analysis process. To store the timestamp and the results from the video-processor stream, the sample creates two dictionaries: `aestheticsResults` and `featurePrintResults`:\n\n```swift\ndo {\n    \/\/\/ The time interval for the video-processing cadence.\n    let interval = CMTime(\n        seconds: totalDuration \/ framesToEval,\n        preferredTimescale: timeScale\n    )\n\n    \/\/\/ The video-processing cadence to process only 100 frames.\n    let cadence = VideoProcessor.Cadence.timeInterval(interval)\n\n    \/\/\/ The stream that adds the aesthetics scores request to the video processor.\n    let aestheticsScoreStream = try await videoProcessor.addRequest(aestheticsScoresRequest, cadence: cadence)\n\n    \/\/\/ The stream that adds the image feature-print request to the video processor.\n    let featurePrintStream = try await videoProcessor.addRequest(imageFeaturePrintRequest, cadence: cadence)\n    \n    \/\/ Start to analyze the video.\n    videoProcessor.startAnalysis()\n\n    \/\/\/ The dictionary to store the timestamp and the aesthetics score.\n    var aestheticsResults: [CMTime: Float] = [:]\n\n    \/\/\/ The dictionary to store the timestamp and the feature-print observation.\n    var featurePrintResults: [CMTime: FeaturePrintObservation] = [:]\n\n    \/\/ ...\n\n    \/\/ Solve for the top-rated frames.\n    topFrames = await calculateTopFrames(aestheticsResults: aestheticsResults, featurePrintResults: featurePrintResults)\n} \n```\n\nWhen the sample receives the results for both requests, call `calculateTopFrames(aestheticsResults:featurePrintResults:)` to solve for the top-rated frames.\n\n### Solve for the top-rated frames\n\nThe function uses `aestheticsResults` and `featurePrintResults` to identify three frames that have the highest aesthetic scores and are different from each other:\n\n```swift\nfor (time, score) in aestheticsResults {\n    \/\/\/ The `FeaturePrintObservation` for the timestamp.\n    guard let featurePrint = featurePrintResults[time] else { continue }\n\n    \/\/\/ The new frame at that timestamp.\n    let newFrame = Frame(time: time, score: score, observation: featurePrint)\n\n    \/\/\/ The variable that tracks whether to add the image based on image similarity.\n    var isSimilar = false\n\n    \/\/ Iterate through the current top-rated frames to check whether any of them\n    \/\/ are similar to the new frame and find the insertion index.\n    for (index, frame) in topFrames.enumerated() {\n        if let distance = try? featurePrint.distance(to: frame.observation), distance < similarityThreshold {\n            \/\/ Replace the frame if the new frame has a higher score.\n            if newFrame.score > frame.score {\n                topFrames[index] = newFrame\n            }\n            isSimilar = true\n            break\n        }\n\n        \/\/ ...\n    }\n}\n```\n\nFor each result, the sample creates a new frame based on the timestamp and attaches the score and `FeaturePrintObservation`. It checks for similar frames in `topFrames` with the `distance(to:)` function to compare the observations. If there is a match, the sample keeps the frame with the higher score and exits the loop.\n\n## Image aesthetics analysis\n\n- **CalculateImageAestheticsScoresRequest**: A request that analyzes an image for aesthetically pleasing attributes.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "A request that analyzes an image for aesthetically pleasing attributes.",
          "name" : "CalculateImageAestheticsScoresRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/CalculateImageAestheticsScoresRequest"
        }
      ],
      "title" : "Image aesthetics analysis"
    }
  ],
  "source" : "appleJSON",
  "title" : "Generating high-quality thumbnails from videos",
  "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/generating-thumbnails-from-videos"
}