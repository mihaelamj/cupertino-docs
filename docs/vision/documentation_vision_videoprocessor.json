{
  "abstract" : "An object that performs offline analysis of video content.",
  "codeExamples" : [
    {
      "code" : "let videoURL = \/\/ A local `URL` path to the video you want to process.\nlet videoProcessor = VideoProcessor(videoURL)\n\n\/\/ Calculate the aesthetics score for each frame.\nlet aestheticsScoresRequest = CalculateImageAestheticsScoresRequest()\n\n\/\/ Perform face detection on each frame. \nlet faceDetectionRequest = DetectFaceRectanglesRequest()",
      "language" : "swift"
    },
    {
      "code" : "do {\n    let asset = AVURLAsset(url: videoURL)\n    let totalDuration = try await asset.load(.duration).seconds\n    let framesToEvaluate: Double = 100\n\n    \/\/ Create a time interval that processes 100 frames.\n    let interval = CMTime(\n        seconds: totalDuration \/ framesToEvaluate,\n        preferredTimescale: 600\n    )\n    let cadence = VideoProcessor.Cadence.timeInterval(interval)\n\n    \/\/ Add the requests to get an `AsyncSequence` stream that provides access \n    \/\/ to the observation results.\n    let aestheticsScoreStream = try await videoProcessor.addRequest(aestheticsScoresRequest, \n                                                                    cadence: cadence)\n    let faceDetectionStream = try await videoProcessor.addRequest(faceDetectionRequest, \n                                                                  cadence: cadence)\n\n    \/\/ Start the analysis.\n    videoProcessor.startAnalysis()\n} catch {\n    print(\"Error processing the video: \\(error.localizedDescription)\")\n}",
      "language" : "swift"
    },
    {
      "code" : "var aestheticsResults: [CMTime: Float] = [:]\nfor try await observation in aestheticsScoreStream {\n    if let timeRange = observation.timeRange {\n        aestheticsResults[timeRange.start] = observation.overallScore\n    }\n}",
      "language" : "swift"
    }
  ],
  "conformsTo" : [
    "Sendable",
    "SendableMetatype"
  ],
  "contentHash" : "fb78dd7054329b518b468c894a2899c49582bac019f5736cd183c5932d5b3c49",
  "crawledAt" : "2025-12-01T09:04:46Z",
  "declaration" : {
    "code" : "final class VideoProcessor",
    "language" : "swift"
  },
  "id" : "38F8E7BA-6F16-4D5C-8236-37C8E51EF8D9",
  "kind" : "class",
  "module" : "Vision",
  "overview" : "## Overview\n\nA video processor streamlines video content analysis through frame-by-frame processing. Instead of manually extracting frames, the video processor manages the processing pipeline and delivers results through convenient async streams. You can attach multiple different analysis requests to the same [doc:\/\/Vision\/documentation\/Vision\/VideoProcessor] instance and they’ll all operate on the same frames simultaneously. For example, you can provide a video and perform aesthetic scoring, face detection, and object recognition all at once without processing the video multiple times.\n\nProcessing every single video frame provides the most accuracy, but can be computationally expensive and time-consuming. Before you begin video analysis, determine how many frames to process by using [doc:\/\/Vision\/documentation\/Vision\/VideoProcessor\/Cadence]. [doc:\/\/Vision\/documentation\/Vision\/VideoProcessor\/Cadence\/timeInterval(_:)] processes frames at regular intervals, which provides consistent sampling throughout the video’s duration.\n\nAfter you start processing a video, access the observations that the framework provides through an `AsyncSequence` stream. For example, the following code stores the timestamp and the aesthetics score:",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst",
    "macOS",
    "tvOS",
    "visionOS"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/Vision\/VideoProcessor\ncrawled: 2025-12-01T09:04:46Z\n---\n\n# VideoProcessor\n\n**Class**\n\nAn object that performs offline analysis of video content.\n\n## Declaration\n\n```swift\nfinal class VideoProcessor\n```\n\n## Overview\n\nA video processor streamlines video content analysis through frame-by-frame processing. Instead of manually extracting frames, the video processor manages the processing pipeline and delivers results through convenient async streams. You can attach multiple different analysis requests to the same [doc:\/\/Vision\/documentation\/Vision\/VideoProcessor] instance and they’ll all operate on the same frames simultaneously. For example, you can provide a video and perform aesthetic scoring, face detection, and object recognition all at once without processing the video multiple times.\n\n```swift\nlet videoURL = \/\/ A local `URL` path to the video you want to process.\nlet videoProcessor = VideoProcessor(videoURL)\n\n\/\/ Calculate the aesthetics score for each frame.\nlet aestheticsScoresRequest = CalculateImageAestheticsScoresRequest()\n\n\/\/ Perform face detection on each frame. \nlet faceDetectionRequest = DetectFaceRectanglesRequest()\n```\n\nProcessing every single video frame provides the most accuracy, but can be computationally expensive and time-consuming. Before you begin video analysis, determine how many frames to process by using [doc:\/\/Vision\/documentation\/Vision\/VideoProcessor\/Cadence]. [doc:\/\/Vision\/documentation\/Vision\/VideoProcessor\/Cadence\/timeInterval(_:)] processes frames at regular intervals, which provides consistent sampling throughout the video’s duration.\n\n```swift\ndo {\n    let asset = AVURLAsset(url: videoURL)\n    let totalDuration = try await asset.load(.duration).seconds\n    let framesToEvaluate: Double = 100\n\n    \/\/ Create a time interval that processes 100 frames.\n    let interval = CMTime(\n        seconds: totalDuration \/ framesToEvaluate,\n        preferredTimescale: 600\n    )\n    let cadence = VideoProcessor.Cadence.timeInterval(interval)\n\n    \/\/ Add the requests to get an `AsyncSequence` stream that provides access \n    \/\/ to the observation results.\n    let aestheticsScoreStream = try await videoProcessor.addRequest(aestheticsScoresRequest, \n                                                                    cadence: cadence)\n    let faceDetectionStream = try await videoProcessor.addRequest(faceDetectionRequest, \n                                                                  cadence: cadence)\n\n    \/\/ Start the analysis.\n    videoProcessor.startAnalysis()\n} catch {\n    print(\"Error processing the video: \\(error.localizedDescription)\")\n}\n```\n\nAfter you start processing a video, access the observations that the framework provides through an `AsyncSequence` stream. For example, the following code stores the timestamp and the aesthetics score:\n\n```swift\nvar aestheticsResults: [CMTime: Float] = [:]\nfor try await observation in aestheticsScoreStream {\n    if let timeRange = observation.timeRange {\n        aestheticsResults[timeRange.start] = observation.overallScore\n    }\n}\n```\n\n## Creating a video processor\n\n- **init(_:)**: Creates a video processor to perform framework requests against the video asset you specify.\n\n## Adding and removing a request\n\n- **addRequest(_:cadence:)**: Adds a request to the video processor.\n- **VideoProcessor.Cadence**: A type that describes the video processing cadence.\n- **removeRequest(_:)**: Stops performing a request on future frames.\n\n## Starting the analysis\n\n- **startAnalysis(of:)**: Begins analyzing video frames.\n\n## Cancelling the analysis\n\n- **cancel()**: Stops the video processor.\n\n## Utilities\n\n- **ComputeStage**: Types that represent the compute stage.\n\n## Conforms To\n\n- Sendable\n- SendableMetatype\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Creates a video processor to perform framework requests against the video asset you specify.",
          "name" : "init(_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VideoProcessor\/init(_:)"
        }
      ],
      "title" : "Creating a video processor"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "Adds a request to the video processor.",
          "name" : "addRequest(_:cadence:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VideoProcessor\/addRequest(_:cadence:)"
        },
        {
          "description" : "A type that describes the video processing cadence.",
          "name" : "VideoProcessor.Cadence",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VideoProcessor\/Cadence"
        },
        {
          "description" : "Stops performing a request on future frames.",
          "name" : "removeRequest(_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VideoProcessor\/removeRequest(_:)"
        }
      ],
      "title" : "Adding and removing a request"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "Begins analyzing video frames.",
          "name" : "startAnalysis(of:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VideoProcessor\/startAnalysis(of:)"
        }
      ],
      "title" : "Starting the analysis"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "Stops the video processor.",
          "name" : "cancel()",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VideoProcessor\/cancel()"
        }
      ],
      "title" : "Cancelling the analysis"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "Types that represent the compute stage.",
          "name" : "ComputeStage",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/ComputeStage"
        }
      ],
      "title" : "Utilities"
    }
  ],
  "source" : "appleJSON",
  "title" : "VideoProcessor",
  "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VideoProcessor"
}