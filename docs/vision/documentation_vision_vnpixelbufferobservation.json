{
  "abstract" : "An object that represents an image that an image-analysis request produces.",
  "codeExamples" : [

  ],
  "conformsTo" : [
    "CVarArg",
    "CustomDebugStringConvertible",
    "CustomStringConvertible",
    "Equatable",
    "Hashable",
    "NSCoding",
    "NSCopying",
    "NSObjectProtocol",
    "NSSecureCoding",
    "VNRequestRevisionProviding"
  ],
  "contentHash" : "6752ab056c0e071f14e00d452bf5174500b36ffe1187a6e1ec6532e05c8cffa2",
  "crawledAt" : "2025-12-02T16:21:16Z",
  "declaration" : {
    "code" : "class VNPixelBufferObservation",
    "language" : "swift"
  },
  "id" : "41D3A772-7215-4FF0-A24F-68704CD45AC7",
  "inheritedBy" : [
    "VNSaliencyImageObservation"
  ],
  "kind" : "class",
  "language" : "swift",
  "module" : "Vision",
  "overview" : "## Overview\n\nThis type of observation results from performing a [doc:\/\/Vision\/documentation\/Vision\/VNCoreMLRequest] image analysis with a Core ML model that has an image-to-image processing role. For example, this observation might result from a model that analyzes the style of one image and then transfers that style to a different image.\n\nVision infers that an [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLModel] object is an image-to-image model if that model includes an image. Its [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLModel\/modelDescription] object includes an image-typed feature description in its [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLModelDescription\/outputDescriptionsByName] dictionary.",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst",
    "macOS",
    "tvOS",
    "visionOS"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/Vision\/VNPixelBufferObservation\ncrawled: 2025-12-02T16:21:16Z\n---\n\n# VNPixelBufferObservation\n\n**Class**\n\nAn object that represents an image that an image-analysis request produces.\n\n## Declaration\n\n```swift\nclass VNPixelBufferObservation\n```\n\n## Overview\n\nThis type of observation results from performing a [doc:\/\/Vision\/documentation\/Vision\/VNCoreMLRequest] image analysis with a Core ML model that has an image-to-image processing role. For example, this observation might result from a model that analyzes the style of one image and then transfers that style to a different image.\n\nVision infers that an [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLModel] object is an image-to-image model if that model includes an image. Its [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLModel\/modelDescription] object includes an image-typed feature description in its [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLModelDescription\/outputDescriptionsByName] dictionary.\n\n## Parsing Observation Content\n\n- **pixelBuffer**: The image that results from a request with image output.\n- **featureName**: A feature name that the CoreML model defines.\n\n## Getting the supported pixel formats\n\n- **supportedOutputPixelFormats()**: Returns a list of output pixel formats that the request supports.\n\n## Machine learning image analysis\n\n- **Classifying Images with Vision and Core ML**: Crop and scale photos using the Vision framework and classify them with a Core ML model.\n- **Training a Create ML Model to Classify Flowers**: Train a flower classifier using Create ML in Swift Playgrounds, and apply the resulting model to real-time image classification using Vision.\n- **VNCoreMLRequest**: An image-analysis request that uses a Core ML model to process images.\n- **VNClassificationObservation**: An object that represents classification information that an image-analysis request produces.\n- **VNCoreMLFeatureValueObservation**: An object that represents a collection of key-value information that a Core ML image-analysis request produces.\n\n## Inherits From\n\n- VNObservation\n\n## Inherited By\n\n- VNSaliencyImageObservation\n\n## Conforms To\n\n- CVarArg\n- CustomDebugStringConvertible\n- CustomStringConvertible\n- Equatable\n- Hashable\n- NSCoding\n- NSCopying\n- NSObjectProtocol\n- NSSecureCoding\n- VNRequestRevisionProviding\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "The image that results from a request with image output.",
          "name" : "pixelBuffer",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNPixelBufferObservation\/pixelBuffer"
        },
        {
          "description" : "A feature name that the CoreML model defines.",
          "name" : "featureName",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNPixelBufferObservation\/featureName"
        }
      ],
      "title" : "Parsing Observation Content"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "Returns a list of output pixel formats that the request supports.",
          "name" : "supportedOutputPixelFormats()",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNGeneratePersonSegmentationRequest\/supportedOutputPixelFormats()"
        }
      ],
      "title" : "Getting the supported pixel formats"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "Crop and scale photos using the Vision framework and classify them with a Core ML model.",
          "name" : "Classifying Images with Vision and Core ML",
          "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/classifying-images-with-vision-and-core-ml"
        },
        {
          "description" : "Train a flower classifier using Create ML in Swift Playgrounds, and apply the resulting model to real-time image classification using Vision.",
          "name" : "Training a Create ML Model to Classify Flowers",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/training-a-create-ml-model-to-classify-flowers"
        },
        {
          "description" : "An image-analysis request that uses a Core ML model to process images.",
          "name" : "VNCoreMLRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNCoreMLRequest"
        },
        {
          "description" : "An object that represents classification information that an image-analysis request produces.",
          "name" : "VNClassificationObservation",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNClassificationObservation"
        },
        {
          "description" : "An object that represents a collection of key-value information that a Core ML image-analysis request produces.",
          "name" : "VNCoreMLFeatureValueObservation",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNCoreMLFeatureValueObservation"
        }
      ],
      "title" : "Machine learning image analysis"
    },
    {
      "content" : "",
      "items" : [
        {
          "name" : "VNObservation"
        }
      ],
      "title" : "Inherits From"
    }
  ],
  "source" : "appleJSON",
  "title" : "VNPixelBufferObservation",
  "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNPixelBufferObservation"
}