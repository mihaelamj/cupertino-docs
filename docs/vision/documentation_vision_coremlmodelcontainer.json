{
  "abstract" : "A model container to use with an image-analysis request.",
  "codeExamples" : [

  ],
  "conformsTo" : [
    "Equatable",
    "Hashable",
    "Sendable",
    "SendableMetatype"
  ],
  "contentHash" : "6ebf09e1a51fe7e4f7fdd0cb056ed16a88d704048547348afc85604e6a9e23a2",
  "crawledAt" : "2025-12-03T19:15:07Z",
  "declaration" : {
    "code" : "struct CoreMLModelContainer",
    "language" : "swift"
  },
  "id" : "F1B5B0AE-0021-48E8-A3F1-8A4E50B4A404",
  "kind" : "struct",
  "language" : "swift",
  "module" : "Vision",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst",
    "macOS",
    "tvOS",
    "visionOS"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/Vision\/CoreMLModelContainer\ncrawled: 2025-12-03T19:15:07Z\n---\n\n# CoreMLModelContainer\n\n**Structure**\n\nA model container to use with an image-analysis request.\n\n## Declaration\n\n```swift\nstruct CoreMLModelContainer\n```\n\n## Creating a model container\n\n- **init(model:featureProvider:)**\n\n## Getting the feature name\n\n- **inputImageFeatureName**: The name of the feature value that Vision sets from the request handler.\n\n## Configuring a request\n\n- **supportedIdentifiers**: The classification identifiers supported by the request.\n- **modelContainer**: The model to base the image analysis request on.\n- **ComputeStage**: Types that represent the compute stage.\n- **cropAndScaleAction**: An optional setting that tells the Vision algorithm how to scale an input image.\n- **ImageCropAndScaleAction**: A scale to apply to an input image before performing a request.\n\n## Conforms To\n\n- Equatable\n- Hashable\n- Sendable\n- SendableMetatype\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "",
          "name" : "init(model:featureProvider:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/CoreMLModelContainer\/init(model:featureProvider:)"
        }
      ],
      "title" : "Creating a model container"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "The name of the feature value that Vision sets from the request handler.",
          "name" : "inputImageFeatureName",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/CoreMLModelContainer\/inputImageFeatureName"
        }
      ],
      "title" : "Getting the feature name"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "The classification identifiers supported by the request.",
          "name" : "supportedIdentifiers",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/CoreMLRequest\/supportedIdentifiers"
        },
        {
          "description" : "The model to base the image analysis request on.",
          "name" : "modelContainer",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/CoreMLRequest\/modelContainer"
        },
        {
          "description" : "Types that represent the compute stage.",
          "name" : "ComputeStage",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/ComputeStage"
        },
        {
          "description" : "An optional setting that tells the Vision algorithm how to scale an input image.",
          "name" : "cropAndScaleAction",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/CoreMLRequest\/cropAndScaleAction"
        },
        {
          "description" : "A scale to apply to an input image before performing a request.",
          "name" : "ImageCropAndScaleAction",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/ImageCropAndScaleAction"
        }
      ],
      "title" : "Configuring a request"
    }
  ],
  "source" : "appleJSON",
  "title" : "CoreMLModelContainer",
  "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/CoreMLModelContainer"
}