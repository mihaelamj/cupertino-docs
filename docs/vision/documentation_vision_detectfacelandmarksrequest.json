{
  "abstract" : "An image-analysis request that finds facial features like eyes and mouth in an image.",
  "codeExamples" : [

  ],
  "conformsTo" : [
    "CustomStringConvertible",
    "Equatable",
    "Hashable",
    "ImageProcessingRequest",
    "Sendable",
    "SendableMetatype",
    "VisionRequest"
  ],
  "contentHash" : "f5ab10df46c842ff7e5cef5233751ee497e735c53bfde190fcbab08f7f29bddc",
  "crawledAt" : "2025-12-02T16:20:20Z",
  "declaration" : {
    "code" : "struct DetectFaceLandmarksRequest",
    "language" : "swift"
  },
  "id" : "835F75A0-F630-40F9-B2CB-02DAC09442FB",
  "kind" : "struct",
  "language" : "swift",
  "module" : "Vision",
  "overview" : "## Overview\n\nBy default, a request for face landmarks first locates all faces in the input image, then analyzes each to detect facial features.\n\nIf you already located all the faces in an image, or want to detect landmarks in only a subset of the faces in the image, set the [doc:\/\/Vision\/documentation\/Vision\/DetectFaceCaptureQualityRequest\/inputFaceObservations] property to an array of [doc:\/\/Vision\/documentation\/Vision\/FaceObservation] objects representing the faces you want to analyze. You can either use face observations output by a [doc:\/\/Vision\/documentation\/Vision\/DetectFaceRectanglesRequest] or manually create [doc:\/\/Vision\/documentation\/Vision\/FaceObservation] instances with the bounding boxes of the faces you want to analyze.",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst",
    "macOS",
    "tvOS",
    "visionOS"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/Vision\/DetectFaceLandmarksRequest\ncrawled: 2025-12-02T16:20:20Z\n---\n\n# DetectFaceLandmarksRequest\n\n**Structure**\n\nAn image-analysis request that finds facial features like eyes and mouth in an image.\n\n## Declaration\n\n```swift\nstruct DetectFaceLandmarksRequest\n```\n\n## Overview\n\nBy default, a request for face landmarks first locates all faces in the input image, then analyzes each to detect facial features.\n\nIf you already located all the faces in an image, or want to detect landmarks in only a subset of the faces in the image, set the [doc:\/\/Vision\/documentation\/Vision\/DetectFaceCaptureQualityRequest\/inputFaceObservations] property to an array of [doc:\/\/Vision\/documentation\/Vision\/FaceObservation] objects representing the faces you want to analyze. You can either use face observations output by a [doc:\/\/Vision\/documentation\/Vision\/DetectFaceRectanglesRequest] or manually create [doc:\/\/Vision\/documentation\/Vision\/FaceObservation] instances with the bounding boxes of the faces you want to analyze.\n\n## Creating a request\n\n- **init(_:)**: Creates a face landmark detection request.\n\n## Getting the revision\n\n- **revision**: The algorithm or implementation the request uses.\n- **supportedRevisions**: The collection of revisions the request supports.\n- **DetectFaceLandmarksRequest.Revision**: A type that describes the algorithm or implementation that the request performs.\n\n## Inspecting a request\n\n- **inputFaceObservations**: An array of face-observation objects to process as part of the request.\n\n## Performing a request\n\n- **perform(on:orientation:)**: Performs the request on an image URL and produces observations.\n- **perform(on:orientation:)**: Performs the request on image data and produces observations.\n- **perform(on:orientation:)**: Performs the request on a Core Graphics image and produces observations.\n- **perform(on:orientation:)**: Performs the request on a pixel buffer and produces observations.\n- **perform(on:orientation:)**: Performs the request on a Core Media buffer and produces observations.\n- **perform(on:orientation:)**: Performs the request on a Core Image image and produces observations.\n- **FaceObservation**: An image-analysis request that identifies facial features in an image.\n\n## Face and body detection\n\n- **Analyzing a selfie and visualizing its content**: Calculate face-capture quality and visualize facial features for a collection of images using the Vision framework.\n- **DetectFaceRectanglesRequest**: A request that finds faces within an image.\n- **DetectFaceCaptureQualityRequest**: A request that produces a floating-point number that represents the capture quality of a face in a photo.\n- **DetectHumanRectanglesRequest**: A request that finds rectangular regions that contain people in an image.\n\n## Conforms To\n\n- CustomStringConvertible\n- Equatable\n- Hashable\n- ImageProcessingRequest\n- Sendable\n- SendableMetatype\n- VisionRequest\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Creates a face landmark detection request.",
          "name" : "init(_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/DetectFaceLandmarksRequest\/init(_:)"
        }
      ],
      "title" : "Creating a request"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "The algorithm or implementation the request uses.",
          "name" : "revision",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/DetectFaceLandmarksRequest\/revision-swift.property"
        },
        {
          "description" : "The collection of revisions the request supports.",
          "name" : "supportedRevisions",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/DetectFaceLandmarksRequest\/supportedRevisions"
        },
        {
          "description" : "A type that describes the algorithm or implementation that the request performs.",
          "name" : "DetectFaceLandmarksRequest.Revision",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/DetectFaceLandmarksRequest\/Revision-swift.enum"
        }
      ],
      "title" : "Getting the revision"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "An array of face-observation objects to process as part of the request.",
          "name" : "inputFaceObservations",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/DetectFaceLandmarksRequest\/inputFaceObservations"
        }
      ],
      "title" : "Inspecting a request"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "Performs the request on an image URL and produces observations.",
          "name" : "perform(on:orientation:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/ImageProcessingRequest\/perform(on:orientation:)-80bya"
        },
        {
          "description" : "Performs the request on image data and produces observations.",
          "name" : "perform(on:orientation:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/ImageProcessingRequest\/perform(on:orientation:)-3f3f1"
        },
        {
          "description" : "Performs the request on a Core Graphics image and produces observations.",
          "name" : "perform(on:orientation:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/ImageProcessingRequest\/perform(on:orientation:)-qxxx"
        },
        {
          "description" : "Performs the request on a pixel buffer and produces observations.",
          "name" : "perform(on:orientation:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/ImageProcessingRequest\/perform(on:orientation:)-xspx"
        },
        {
          "description" : "Performs the request on a Core Media buffer and produces observations.",
          "name" : "perform(on:orientation:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/ImageProcessingRequest\/perform(on:orientation:)-3hddl"
        },
        {
          "description" : "Performs the request on a Core Image image and produces observations.",
          "name" : "perform(on:orientation:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/ImageProcessingRequest\/perform(on:orientation:)-85ex1"
        },
        {
          "description" : "An image-analysis request that identifies facial features in an image.",
          "name" : "FaceObservation",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/FaceObservation"
        }
      ],
      "title" : "Performing a request"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "Calculate face-capture quality and visualize facial features for a collection of images using the Vision framework.",
          "name" : "Analyzing a selfie and visualizing its content",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/analyzing-a-selfie-and-visualizing-its-content"
        },
        {
          "description" : "A request that finds faces within an image.",
          "name" : "DetectFaceRectanglesRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/DetectFaceRectanglesRequest"
        },
        {
          "description" : "A request that produces a floating-point number that represents the capture quality of a face in a photo.",
          "name" : "DetectFaceCaptureQualityRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/DetectFaceCaptureQualityRequest"
        },
        {
          "description" : "A request that finds rectangular regions that contain people in an image.",
          "name" : "DetectHumanRectanglesRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/DetectHumanRectanglesRequest"
        }
      ],
      "title" : "Face and body detection"
    }
  ],
  "source" : "appleJSON",
  "title" : "DetectFaceLandmarksRequest",
  "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/DetectFaceLandmarksRequest"
}