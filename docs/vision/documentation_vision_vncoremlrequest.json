{
  "abstract" : "An image-analysis request that uses a Core ML model to process images.",
  "codeExamples" : [

  ],
  "conformsTo" : [
    "CVarArg",
    "CustomDebugStringConvertible",
    "CustomStringConvertible",
    "Equatable",
    "Hashable",
    "NSCopying",
    "NSObjectProtocol"
  ],
  "contentHash" : "576c8cefe6afc86b3fc4bca06746a5f8cf6acc4b13c4a1df8e9b21577545be7d",
  "crawledAt" : "2025-12-04T02:35:15Z",
  "declaration" : {
    "code" : "class VNCoreMLRequest",
    "language" : "swift"
  },
  "id" : "CC045B2C-74B1-4C7A-90CC-BA59101E261B",
  "kind" : "class",
  "language" : "swift",
  "module" : "Vision",
  "overview" : "## Overview\n\nThe results array of a Core ML-based image analysis request contains a different observation type, depending on the kind of [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLModel] object you use:",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst",
    "macOS",
    "tvOS",
    "visionOS"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/vision\/vncoremlrequest\ncrawled: 2025-12-04T02:35:15Z\n---\n\n# VNCoreMLRequest\n\n**Class**\n\nAn image-analysis request that uses a Core ML model to process images.\n\n## Declaration\n\n```swift\nclass VNCoreMLRequest\n```\n\n## Overview\n\nThe results array of a Core ML-based image analysis request contains a different observation type, depending on the kind of [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLModel] object you use:\n\n- If the model predicts a single feature, the model’s [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLModel\/modelDescription] object has a non-`nil` value for [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLModelDescription\/predictedFeatureName] and Vision treats the model as a classifier. The results are [doc:\/\/Vision\/documentation\/Vision\/VNClassificationObservation] objects.\n- If the model’s outputs include at least one output with a feature type of [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLFeatureType\/image], Vision treats that model as an image-to-image model. The results are [doc:\/\/Vision\/documentation\/Vision\/VNPixelBufferObservation] objects.\n- Otherwise, Vision treats the model as a general predictor model. The results are [doc:\/\/Vision\/documentation\/Vision\/VNCoreMLFeatureValueObservation] objects.\n\n\n\n## Initializing with a Core ML Model\n\n- **init(model:)**: Creates a model container to use with an image analysis request based on the model you provide.\n- **init(model:completionHandler:)**: Creates a model container to use with an image analysis request based on the model you provide, with an optional completion handler.\n- **model**: The model to base the image analysis request on.\n- **VNCoreMLModel**: A container for the model to use with Vision requests.\n\n## Configuring Image Options\n\n- **imageCropAndScaleOption**: An optional setting that tells the Vision algorithm how to scale an input image.\n- **VNImageCropAndScaleOption**: Options that define how Vision crops and scales an input-image.\n\n## Identifying Request Revisions\n\n- **VNCoreMLRequestRevision1**: A constant for specifying revision 1 of a Core ML request.\n\n## Machine learning image analysis\n\n- **Classifying Images with Vision and Core ML**: Crop and scale photos using the Vision framework and classify them with a Core ML model.\n- **Training a Create ML Model to Classify Flowers**: Train a flower classifier using Create ML in Swift Playgrounds, and apply the resulting model to real-time image classification using Vision.\n- **VNClassificationObservation**: An object that represents classification information that an image-analysis request produces.\n- **VNPixelBufferObservation**: An object that represents an image that an image-analysis request produces.\n- **VNCoreMLFeatureValueObservation**: An object that represents a collection of key-value information that a Core ML image-analysis request produces.\n\n## Inherits From\n\n- VNImageBasedRequest\n\n## Conforms To\n\n- CVarArg\n- CustomDebugStringConvertible\n- CustomStringConvertible\n- Equatable\n- Hashable\n- NSCopying\n- NSObjectProtocol\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Creates a model container to use with an image analysis request based on the model you provide.",
          "name" : "init(model:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNCoreMLRequest\/init(model:)"
        },
        {
          "description" : "Creates a model container to use with an image analysis request based on the model you provide, with an optional completion handler.",
          "name" : "init(model:completionHandler:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNCoreMLRequest\/init(model:completionHandler:)"
        },
        {
          "description" : "The model to base the image analysis request on.",
          "name" : "model",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNCoreMLRequest\/model"
        },
        {
          "description" : "A container for the model to use with Vision requests.",
          "name" : "VNCoreMLModel",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNCoreMLModel"
        }
      ],
      "title" : "Initializing with a Core ML Model"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "An optional setting that tells the Vision algorithm how to scale an input image.",
          "name" : "imageCropAndScaleOption",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNCoreMLRequest\/imageCropAndScaleOption"
        },
        {
          "description" : "Options that define how Vision crops and scales an input-image.",
          "name" : "VNImageCropAndScaleOption",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNImageCropAndScaleOption"
        }
      ],
      "title" : "Configuring Image Options"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "A constant for specifying revision 1 of a Core ML request.",
          "name" : "VNCoreMLRequestRevision1",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNCoreMLRequestRevision1"
        }
      ],
      "title" : "Identifying Request Revisions"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "Crop and scale photos using the Vision framework and classify them with a Core ML model.",
          "name" : "Classifying Images with Vision and Core ML",
          "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/classifying-images-with-vision-and-core-ml"
        },
        {
          "description" : "Train a flower classifier using Create ML in Swift Playgrounds, and apply the resulting model to real-time image classification using Vision.",
          "name" : "Training a Create ML Model to Classify Flowers",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/training-a-create-ml-model-to-classify-flowers"
        },
        {
          "description" : "An object that represents classification information that an image-analysis request produces.",
          "name" : "VNClassificationObservation",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNClassificationObservation"
        },
        {
          "description" : "An object that represents an image that an image-analysis request produces.",
          "name" : "VNPixelBufferObservation",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNPixelBufferObservation"
        },
        {
          "description" : "An object that represents a collection of key-value information that a Core ML image-analysis request produces.",
          "name" : "VNCoreMLFeatureValueObservation",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNCoreMLFeatureValueObservation"
        }
      ],
      "title" : "Machine learning image analysis"
    },
    {
      "content" : "",
      "items" : [
        {
          "name" : "VNImageBasedRequest"
        }
      ],
      "title" : "Inherits From"
    }
  ],
  "source" : "appleJSON",
  "title" : "VNCoreMLRequest",
  "url" : "https:\/\/developer.apple.com\/documentation\/vision\/vncoremlrequest"
}