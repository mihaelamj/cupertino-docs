{
  "abstract" : "Calculate face-capture quality and visualize facial features for a collection of images using the Vision framework.",
  "codeExamples" : [
    {
      "code" : "PhotosPicker(selection: $selectedPhotos, maxSelectionCount: 5, matching: .images) {\n    Text(\"Select Selfies\")\n}",
      "language" : "swift"
    },
    {
      "code" : "for photo in selectedPhotos {\n    if let image = try? await photo.loadTransferable(type: Data.self) {\n        selectedPhotosData.append(image)\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "func processSelfie(photo: Data) async throws -> Selfie {\n    \/\/\/ Instantiate the `Vision` requests.\n    let detectFacesRequest = DetectFaceRectanglesRequest()\n    var qualityRequest = DetectFaceCaptureQualityRequest()\n    var landmarksRequest = DetectFaceLandmarksRequest()\n    \n    \/\/\/ Perform `DetectFaceRectanglesRequest` to locate all faces in the photo.\n    let handler = ImageRequestHandler(photo)\n    let faceObservations = try await handler.perform(detectFacesRequest)\n    \n    \/\/\/ Set the faces that `DetectFaceLandmarksRequest` and `DetectFaceCaptureQualityRequest` analyze.\n    landmarksRequest.inputFaceObservations = faceObservations\n    qualityRequest.inputFaceObservations = faceObservations\n        \n    \/\/\/ Perform `DetectFaceCaptureQualityRequest` and `DetectFaceLandmarksRequest` on the photo.\n    let (qualityResults, landmarksResults) = try await handler.perform(qualityRequest, landmarksRequest)\n    \n    var score: Float = 0\n    \/\/\/ Set the capture-quality score of the photo if `Vision` detects one face.\n    if qualityResults.count == 1 {\n        score = qualityResults[0].captureQuality!.score\n    \/\/\/ Set the average capture-quality score if `Vision` detects multiple faces.\n    } else if qualityResults.count > 1 {\n        for face in qualityResults {\n            score += face.captureQuality!.score\n        }\n        score \/= Float(qualityResults.count)\n    }\n        \n    return Selfie(photo: photo, score: score, landmarksResults: landmarksResults)\n}",
      "language" : "swift"
    },
    {
      "code" : "func processAllSelfies(photos: [Data]) async throws -> [Selfie] {\n    var selfies = [Selfie]()\n    \n    try await withThrowingTaskGroup(of: Selfie.self) { group in\n        for photo in photos {\n            group.addTask {\n                return try await processSelfie(photo: photo)\n            }\n        }\n        \n        \/\/\/ Only add the photo to the `selfies` array if Vision detects a face.\n        for try await selfie in group where selfie.facesDetected > 0 {\n            selfies.append(selfie)\n        }\n    }\n    \n    \/\/\/ Sort the selfies in descending order of their capture-quality scores.\n    selfies.sort { $0.score > $1.score }\n    \n    return selfies\n}",
      "language" : "swift"
    },
    {
      "code" : "struct BoundingBox: Shape {\n    private let normalizedRect: NormalizedRect\n    \n    init(observation: any BoundingBoxProviding) {\n        normalizedRect = observation.boundingBox\n    }\n    \n    func path(in rect: CGRect) -> Path {\n        let rect = normalizedRect.toImageCoordinates(rect.size, origin: .upperLeft)\n        return Path(rect)\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : ".overlay {\n    ForEach(selfie.landmarkResults, id: \\.self) { observation in\n        BoundingBox(observation: observation)\n            .stroke(.red, lineWidth: 2)\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "struct FaceLandmark: Shape {\n    let region: FaceObservation.Landmarks2D.Region\n    \n    func path(in rect: CGRect) -> Path {\n        let points = region.pointsInImageCoordinates(rect.size, origin: .upperLeft)\n        let path = CGMutablePath()\n        \n        path.move(to: points[0])\n        \n        for index in 1..<points.count {\n            path.addLine(to: points[index])\n        }\n        \n        if region.pointsClassification == .closedPath {\n            path.closeSubpath()\n        }\n        \n        return Path(path)\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : ".overlay {\n    ForEach(selfie.landmarksResults, id: \\.self) { observation in\n        FaceLandmark(region: observation.landmarks!.faceContour)\n            .stroke(.white, lineWidth: 2)\n        \n        \/\/ ..\n    }\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "4cd730f833443bd39449d3cbb6cc62f0ee2ff70b718b5f0338f66ec5b94f61a7",
  "crawledAt" : "2025-12-02T15:52:36Z",
  "id" : "B3269D50-3834-47EF-B199-DE2765E7EF3A",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "Vision",
  "overview" : "## Overview\n\nUse the `Vision` framework to detect faces and facial features in a photo. This framework can analyze a photo to retrieve metrics such as face-capture quality and visual information like facial landmarks and face rectangles. This sample demonstrates how to locate all the faces in a selfie through the [doc:\/\/com.apple.documentation\/documentation\/Vision\/DetectFaceRectanglesRequest]. The sample then uses [doc:\/\/com.apple.documentation\/documentation\/Vision\/DetectFaceCaptureQualityRequest] to obtain capture-quality scores, and [doc:\/\/com.apple.documentation\/documentation\/Vision\/DetectFaceLandmarksRequest] to display outlines around each facial landmark, like the eyes or nose.\n\n\n\nFace-capture quality is a holistic measure that considers scene lighting, blur, occlusion, expression, pose, focus, and more. It provides a score that the app uses to sort the collection of selfies from best to worst. The pretrained machine-learning model scores a capture lower if, for example, the image contains low light or bad focus, or if the person has a negative expression. These scores are floating-point values between 0.0 and 1.0.\n\n### Configure the sample code project\n\nTo run this sample app, you need the following:\n\n### Selecting the selfies\n\nThe sample uses [doc:\/\/com.apple.documentation\/documentation\/PhotosUI\/PhotosPicker] to allow a person to select the selfies to analyze, and sets the maximum number of images to 5 through the `maxSelectionCount` parameter. For more information on using `PhotosPicker`, see [doc:\/\/com.apple.documentation\/documentation\/PhotoKit\/bringing-photos-picker-to-your-swiftui-app]:\n\nThe sample performs the `Vision` requests and displays the images using data, so the app converts each [doc:\/\/com.apple.documentation\/documentation\/PhotosUI\/PhotosPickerItem] to data:\n\n### Perform the requests and analyze the selfies\n\nTo analyze a selfie, the sample first instantiates the three `Vision` requests. The sample then performs `DetectFaceRectanglesRequest` to locate the faces in the photo, and uses the returned `FaceObservation` objects as the objects the other two requests process. The app sets this functionality through the [doc:\/\/com.apple.documentation\/documentation\/Vision\/DetectFaceLandmarksRequest\/inputFaceObservations] property.\n\nBy default, `DetectFaceLandmarksRequest` and `DetectFaceCaptureQualityRequest` need to locate the faces first before performing the rest of the request. Setting the `inputFaceObservations` property prevents the sample from performing `DetectFaceRectanglesRequest` more than once (which is unnecessary).\n\nUsing the `score` method on a `FaceObservation`, the sample sets the selfie’s score. The function returns the new `Selfie` object, which holds the photo, the score, and the results of the `DetectFaceLandmarksRequest`:\n\nAnalyzing a large collection of selfies with `Vision` requests can take time, so the app uses Swift concurrency to help with speed and efficiency. Using [doc:\/\/com.apple.documentation\/documentation\/Swift\/TaskGroup], the app processes the images in parallel. When the `processSelfie` method returns a new `Selfie` object, the app adds it to the `selfies` array. After the app processes all the images, it sorts the `selfies` array by capture-quality score. The function returns the new array of `Selfie` objects:\n\n### Display face rectangles\n\nThe sample provides custom `Shape` implementations to draw a rectangle around each face, and the face landmarks. For face rectangles, the app uses the [doc:\/\/com.apple.documentation\/documentation\/Vision\/FaceObservation\/boundingBox] property on a `FaceObservation`. The `boundingBox` property contains the location and dimensions of the box in the form of a [doc:\/\/com.apple.documentation\/documentation\/Vision\/NormalizedRect]. The sample converts the `NormalizedRect` to a [doc:\/\/com.apple.documentation\/documentation\/CoreFoundation\/CGRect], and returns a [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/Path] to draw the rectangle:\n\nThe sample creates a `BoundingBox` object for each face in the photo, and overlays them on the image:\n\n### Display face landmarks\n\nTo create and display face landmarks on the image, the sample uses the custom `FaceLandmark` structure. Each `FaceObservation` from the `DetectFaceLandmarksRequest` contains a collection of landmarks as regions. A region contains all the points the sample needs to draw the outline. The possible regions are `faceContour`, `innerLips`, `leftEye`, `leftEyebrow`, `leftPupil`, `medianLine`, `nose`, `noseCrest`, `outerLips`, `rightEye`, `rightEyebrow`, and `rightPupil`.\n\nThe sample converts a region’s [doc:\/\/com.apple.documentation\/documentation\/Vision\/NormalizedPoint] collection to a [doc:\/\/com.apple.documentation\/documentation\/CoreFoundation\/CGPoint] collection, and draws a path from one point to the next. When it reaches the last point, the sample closes the path:\n\nFor each face `Vision` detects in an image, the sample creates `FaceLandmark` objects and overlays them on the image:",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/Vision\/analyzing-a-selfie-and-visualizing-its-content\ncrawled: 2025-12-02T15:52:36Z\n---\n\n# Analyzing a selfie and visualizing its content\n\n**Sample Code**\n\nCalculate face-capture quality and visualize facial features for a collection of images using the Vision framework.\n\n## Overview\n\nUse the `Vision` framework to detect faces and facial features in a photo. This framework can analyze a photo to retrieve metrics such as face-capture quality and visual information like facial landmarks and face rectangles. This sample demonstrates how to locate all the faces in a selfie through the [doc:\/\/com.apple.documentation\/documentation\/Vision\/DetectFaceRectanglesRequest]. The sample then uses [doc:\/\/com.apple.documentation\/documentation\/Vision\/DetectFaceCaptureQualityRequest] to obtain capture-quality scores, and [doc:\/\/com.apple.documentation\/documentation\/Vision\/DetectFaceLandmarksRequest] to display outlines around each facial landmark, like the eyes or nose.\n\n\n\nFace-capture quality is a holistic measure that considers scene lighting, blur, occlusion, expression, pose, focus, and more. It provides a score that the app uses to sort the collection of selfies from best to worst. The pretrained machine-learning model scores a capture lower if, for example, the image contains low light or bad focus, or if the person has a negative expression. These scores are floating-point values between 0.0 and 1.0.\n\n### Configure the sample code project\n\nTo run this sample app, you need the following:\n\n- Xcode 16 or later\n- iPhone with iOS 18 or later\n\n### Selecting the selfies\n\nThe sample uses [doc:\/\/com.apple.documentation\/documentation\/PhotosUI\/PhotosPicker] to allow a person to select the selfies to analyze, and sets the maximum number of images to 5 through the `maxSelectionCount` parameter. For more information on using `PhotosPicker`, see [doc:\/\/com.apple.documentation\/documentation\/PhotoKit\/bringing-photos-picker-to-your-swiftui-app]:\n\n```swift\nPhotosPicker(selection: $selectedPhotos, maxSelectionCount: 5, matching: .images) {\n    Text(\"Select Selfies\")\n}\n```\n\nThe sample performs the `Vision` requests and displays the images using data, so the app converts each [doc:\/\/com.apple.documentation\/documentation\/PhotosUI\/PhotosPickerItem] to data:\n\n```swift\nfor photo in selectedPhotos {\n    if let image = try? await photo.loadTransferable(type: Data.self) {\n        selectedPhotosData.append(image)\n    }\n}\n```\n\n### Perform the requests and analyze the selfies\n\nTo analyze a selfie, the sample first instantiates the three `Vision` requests. The sample then performs `DetectFaceRectanglesRequest` to locate the faces in the photo, and uses the returned `FaceObservation` objects as the objects the other two requests process. The app sets this functionality through the [doc:\/\/com.apple.documentation\/documentation\/Vision\/DetectFaceLandmarksRequest\/inputFaceObservations] property.\n\nBy default, `DetectFaceLandmarksRequest` and `DetectFaceCaptureQualityRequest` need to locate the faces first before performing the rest of the request. Setting the `inputFaceObservations` property prevents the sample from performing `DetectFaceRectanglesRequest` more than once (which is unnecessary).\n\nUsing the `score` method on a `FaceObservation`, the sample sets the selfie’s score. The function returns the new `Selfie` object, which holds the photo, the score, and the results of the `DetectFaceLandmarksRequest`:\n\n```swift\nfunc processSelfie(photo: Data) async throws -> Selfie {\n    \/\/\/ Instantiate the `Vision` requests.\n    let detectFacesRequest = DetectFaceRectanglesRequest()\n    var qualityRequest = DetectFaceCaptureQualityRequest()\n    var landmarksRequest = DetectFaceLandmarksRequest()\n    \n    \/\/\/ Perform `DetectFaceRectanglesRequest` to locate all faces in the photo.\n    let handler = ImageRequestHandler(photo)\n    let faceObservations = try await handler.perform(detectFacesRequest)\n    \n    \/\/\/ Set the faces that `DetectFaceLandmarksRequest` and `DetectFaceCaptureQualityRequest` analyze.\n    landmarksRequest.inputFaceObservations = faceObservations\n    qualityRequest.inputFaceObservations = faceObservations\n        \n    \/\/\/ Perform `DetectFaceCaptureQualityRequest` and `DetectFaceLandmarksRequest` on the photo.\n    let (qualityResults, landmarksResults) = try await handler.perform(qualityRequest, landmarksRequest)\n    \n    var score: Float = 0\n    \/\/\/ Set the capture-quality score of the photo if `Vision` detects one face.\n    if qualityResults.count == 1 {\n        score = qualityResults[0].captureQuality!.score\n    \/\/\/ Set the average capture-quality score if `Vision` detects multiple faces.\n    } else if qualityResults.count > 1 {\n        for face in qualityResults {\n            score += face.captureQuality!.score\n        }\n        score \/= Float(qualityResults.count)\n    }\n        \n    return Selfie(photo: photo, score: score, landmarksResults: landmarksResults)\n}\n```\n\nAnalyzing a large collection of selfies with `Vision` requests can take time, so the app uses Swift concurrency to help with speed and efficiency. Using [doc:\/\/com.apple.documentation\/documentation\/Swift\/TaskGroup], the app processes the images in parallel. When the `processSelfie` method returns a new `Selfie` object, the app adds it to the `selfies` array. After the app processes all the images, it sorts the `selfies` array by capture-quality score. The function returns the new array of `Selfie` objects:\n\n```swift\nfunc processAllSelfies(photos: [Data]) async throws -> [Selfie] {\n    var selfies = [Selfie]()\n    \n    try await withThrowingTaskGroup(of: Selfie.self) { group in\n        for photo in photos {\n            group.addTask {\n                return try await processSelfie(photo: photo)\n            }\n        }\n        \n        \/\/\/ Only add the photo to the `selfies` array if Vision detects a face.\n        for try await selfie in group where selfie.facesDetected > 0 {\n            selfies.append(selfie)\n        }\n    }\n    \n    \/\/\/ Sort the selfies in descending order of their capture-quality scores.\n    selfies.sort { $0.score > $1.score }\n    \n    return selfies\n}\n```\n\n### Display face rectangles\n\nThe sample provides custom `Shape` implementations to draw a rectangle around each face, and the face landmarks. For face rectangles, the app uses the [doc:\/\/com.apple.documentation\/documentation\/Vision\/FaceObservation\/boundingBox] property on a `FaceObservation`. The `boundingBox` property contains the location and dimensions of the box in the form of a [doc:\/\/com.apple.documentation\/documentation\/Vision\/NormalizedRect]. The sample converts the `NormalizedRect` to a [doc:\/\/com.apple.documentation\/documentation\/CoreFoundation\/CGRect], and returns a [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/Path] to draw the rectangle:\n\n```swift\nstruct BoundingBox: Shape {\n    private let normalizedRect: NormalizedRect\n    \n    init(observation: any BoundingBoxProviding) {\n        normalizedRect = observation.boundingBox\n    }\n    \n    func path(in rect: CGRect) -> Path {\n        let rect = normalizedRect.toImageCoordinates(rect.size, origin: .upperLeft)\n        return Path(rect)\n    }\n}\n```\n\nThe sample creates a `BoundingBox` object for each face in the photo, and overlays them on the image:\n\n```swift\n.overlay {\n    ForEach(selfie.landmarkResults, id: \\.self) { observation in\n        BoundingBox(observation: observation)\n            .stroke(.red, lineWidth: 2)\n    }\n}\n```\n\n### Display face landmarks\n\nTo create and display face landmarks on the image, the sample uses the custom `FaceLandmark` structure. Each `FaceObservation` from the `DetectFaceLandmarksRequest` contains a collection of landmarks as regions. A region contains all the points the sample needs to draw the outline. The possible regions are `faceContour`, `innerLips`, `leftEye`, `leftEyebrow`, `leftPupil`, `medianLine`, `nose`, `noseCrest`, `outerLips`, `rightEye`, `rightEyebrow`, and `rightPupil`.\n\nThe sample converts a region’s [doc:\/\/com.apple.documentation\/documentation\/Vision\/NormalizedPoint] collection to a [doc:\/\/com.apple.documentation\/documentation\/CoreFoundation\/CGPoint] collection, and draws a path from one point to the next. When it reaches the last point, the sample closes the path:\n\n```swift\nstruct FaceLandmark: Shape {\n    let region: FaceObservation.Landmarks2D.Region\n    \n    func path(in rect: CGRect) -> Path {\n        let points = region.pointsInImageCoordinates(rect.size, origin: .upperLeft)\n        let path = CGMutablePath()\n        \n        path.move(to: points[0])\n        \n        for index in 1..<points.count {\n            path.addLine(to: points[index])\n        }\n        \n        if region.pointsClassification == .closedPath {\n            path.closeSubpath()\n        }\n        \n        return Path(path)\n    }\n}\n```\n\nFor each face `Vision` detects in an image, the sample creates `FaceLandmark` objects and overlays them on the image:\n\n```swift\n.overlay {\n    ForEach(selfie.landmarksResults, id: \\.self) { observation in\n        FaceLandmark(region: observation.landmarks!.faceContour)\n            .stroke(.white, lineWidth: 2)\n        \n        \/\/ ..\n    }\n}\n```\n\n## Face and body detection\n\n- **DetectFaceRectanglesRequest**: A request that finds faces within an image.\n- **DetectFaceLandmarksRequest**: An image-analysis request that finds facial features like eyes and mouth in an image.\n- **DetectFaceCaptureQualityRequest**: A request that produces a floating-point number that represents the capture quality of a face in a photo.\n- **DetectHumanRectanglesRequest**: A request that finds rectangular regions that contain people in an image.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "A request that finds faces within an image.",
          "name" : "DetectFaceRectanglesRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/DetectFaceRectanglesRequest"
        },
        {
          "description" : "An image-analysis request that finds facial features like eyes and mouth in an image.",
          "name" : "DetectFaceLandmarksRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/DetectFaceLandmarksRequest"
        },
        {
          "description" : "A request that produces a floating-point number that represents the capture quality of a face in a photo.",
          "name" : "DetectFaceCaptureQualityRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/DetectFaceCaptureQualityRequest"
        },
        {
          "description" : "A request that finds rectangular regions that contain people in an image.",
          "name" : "DetectHumanRectanglesRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/DetectHumanRectanglesRequest"
        }
      ],
      "title" : "Face and body detection"
    }
  ],
  "source" : "appleJSON",
  "title" : "Analyzing a selfie and visualizing its content",
  "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/analyzing-a-selfie-and-visualizing-its-content"
}