{
  "abstract" : "An object that represents a collection of key-value information that a Core ML image-analysis request produces.",
  "codeExamples" : [

  ],
  "conformsTo" : [
    "CVarArg",
    "CustomDebugStringConvertible",
    "CustomStringConvertible",
    "Equatable",
    "Hashable",
    "NSCoding",
    "NSCopying",
    "NSObjectProtocol",
    "NSSecureCoding",
    "VNRequestRevisionProviding"
  ],
  "contentHash" : "47bf699a586cd077e3f05e14ec6d1faf5504a2038ca14e6f8814fa83baa82179",
  "crawledAt" : "2025-12-04T01:03:41Z",
  "declaration" : {
    "code" : "class VNCoreMLFeatureValueObservation",
    "language" : "swift"
  },
  "id" : "AB1EB24B-F521-4BE1-A60E-F86D23BFD75E",
  "kind" : "class",
  "language" : "swift",
  "module" : "Vision",
  "overview" : "## Overview\n\nThis type of observation results from performing a [doc:\/\/Vision\/documentation\/Vision\/VNCoreMLRequest] image analysis with a Core ML model whose role is prediction rather than classification or image-to-image processing.\n\nVision infers that an [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLModel] object is a predictor model if that model predicts multiple features. You can tell that a model predicts multiple features when its [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLModel\/modelDescription] object has a `nil` value for its [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLModelDescription\/predictedFeatureName] property, or when it inserts its output in an [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLModelDescription\/outputDescriptionsByName] dictionary.",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst",
    "macOS",
    "tvOS",
    "visionOS"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/vision\/vncoremlfeaturevalueobservation\ncrawled: 2025-12-04T01:03:41Z\n---\n\n# VNCoreMLFeatureValueObservation\n\n**Class**\n\nAn object that represents a collection of key-value information that a Core ML image-analysis request produces.\n\n## Declaration\n\n```swift\nclass VNCoreMLFeatureValueObservation\n```\n\n## Overview\n\nThis type of observation results from performing a [doc:\/\/Vision\/documentation\/Vision\/VNCoreMLRequest] image analysis with a Core ML model whose role is prediction rather than classification or image-to-image processing.\n\nVision infers that an [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLModel] object is a predictor model if that model predicts multiple features. You can tell that a model predicts multiple features when its [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLModel\/modelDescription] object has a `nil` value for its [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLModelDescription\/predictedFeatureName] property, or when it inserts its output in an [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLModelDescription\/outputDescriptionsByName] dictionary.\n\n## Obtaining Feature Values\n\n- **featureValue**: The feature result of a [doc:\/\/Vision\/documentation\/Vision\/VNCoreMLRequest] that outputs neither a classification nor an image.\n- **featureName**: The name used in the model description of the CoreML model that produced this observation.\n\n## Machine learning image analysis\n\n- **Classifying Images with Vision and Core ML**: Crop and scale photos using the Vision framework and classify them with a Core ML model.\n- **Training a Create ML Model to Classify Flowers**: Train a flower classifier using Create ML in Swift Playgrounds, and apply the resulting model to real-time image classification using Vision.\n- **VNCoreMLRequest**: An image-analysis request that uses a Core ML model to process images.\n- **VNClassificationObservation**: An object that represents classification information that an image-analysis request produces.\n- **VNPixelBufferObservation**: An object that represents an image that an image-analysis request produces.\n\n## Inherits From\n\n- VNObservation\n\n## Conforms To\n\n- CVarArg\n- CustomDebugStringConvertible\n- CustomStringConvertible\n- Equatable\n- Hashable\n- NSCoding\n- NSCopying\n- NSObjectProtocol\n- NSSecureCoding\n- VNRequestRevisionProviding\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "The feature result of a [doc:\/\/Vision\/documentation\/Vision\/VNCoreMLRequest] that outputs neither a classification nor an image.",
          "name" : "featureValue",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNCoreMLFeatureValueObservation\/featureValue"
        },
        {
          "description" : "The name used in the model description of the CoreML model that produced this observation.",
          "name" : "featureName",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNCoreMLFeatureValueObservation\/featureName"
        }
      ],
      "title" : "Obtaining Feature Values"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "Crop and scale photos using the Vision framework and classify them with a Core ML model.",
          "name" : "Classifying Images with Vision and Core ML",
          "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/classifying-images-with-vision-and-core-ml"
        },
        {
          "description" : "Train a flower classifier using Create ML in Swift Playgrounds, and apply the resulting model to real-time image classification using Vision.",
          "name" : "Training a Create ML Model to Classify Flowers",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/training-a-create-ml-model-to-classify-flowers"
        },
        {
          "description" : "An image-analysis request that uses a Core ML model to process images.",
          "name" : "VNCoreMLRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNCoreMLRequest"
        },
        {
          "description" : "An object that represents classification information that an image-analysis request produces.",
          "name" : "VNClassificationObservation",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNClassificationObservation"
        },
        {
          "description" : "An object that represents an image that an image-analysis request produces.",
          "name" : "VNPixelBufferObservation",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNPixelBufferObservation"
        }
      ],
      "title" : "Machine learning image analysis"
    },
    {
      "content" : "",
      "items" : [
        {
          "name" : "VNObservation"
        }
      ],
      "title" : "Inherits From"
    }
  ],
  "source" : "appleJSON",
  "title" : "VNCoreMLFeatureValueObservation",
  "url" : "https:\/\/developer.apple.com\/documentation\/vision\/vncoremlfeaturevalueobservation"
}