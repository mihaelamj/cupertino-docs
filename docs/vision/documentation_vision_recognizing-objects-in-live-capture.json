{
  "abstract" : "Apply Vision algorithms to identify objects in real-time video.",
  "codeExamples" : [
    {
      "code" : "private let session = AVCaptureSession()",
      "language" : "swift"
    },
    {
      "code" : "let videoDevice = AVCaptureDevice.DiscoverySession(deviceTypes: [.builtInWideAngleCamera], mediaType: .video, position: .back).devices.first\ndo {\n    deviceInput = try AVCaptureDeviceInput(device: videoDevice!)\n} catch {\n    print(\"Could not create video device input: \\(error)\")\n    return\n}\n\nsession.beginConfiguration()\nsession.sessionPreset = .vga640x480 \/\/ Model image size is smaller.",
      "language" : "swift"
    },
    {
      "code" : "guard session.canAddInput(deviceInput) else {\n    print(\"Could not add video device input to the session\")\n    session.commitConfiguration()\n    return\n}\nsession.addInput(deviceInput)",
      "language" : "swift"
    },
    {
      "code" : "if session.canAddOutput(videoDataOutput) {\n    session.addOutput(videoDataOutput)\n    \/\/ Add a video data output\n    videoDataOutput.alwaysDiscardsLateVideoFrames = true\n    videoDataOutput.videoSettings = [kCVPixelBufferPixelFormatTypeKey as String: Int(kCVPixelFormatType_420YpCbCr8BiPlanarFullRange)]\n    videoDataOutput.setSampleBufferDelegate(self, queue: videoDataOutputQueue)\n} else {\n    print(\"Could not add video data output to the session\")\n    session.commitConfiguration()\n    return\n}",
      "language" : "swift"
    },
    {
      "code" : "let captureConnection = videoDataOutput.connection(with: .video)\n\/\/ Always process the frames\ncaptureConnection?.isEnabled = true\ndo {\n    try  videoDevice!.lockForConfiguration()\n    let dimensions = CMVideoFormatDescriptionGetDimensions((videoDevice?.activeFormat.formatDescription)!)\n    bufferSize.width = CGFloat(dimensions.width)\n    bufferSize.height = CGFloat(dimensions.height)\n    videoDevice!.unlockForConfiguration()\n} catch {\n    print(error)\n}",
      "language" : "swift"
    },
    {
      "code" : "session.commitConfiguration()",
      "language" : "swift"
    },
    {
      "code" : "previewLayer = AVCaptureVideoPreviewLayer(session: session)\npreviewLayer.videoGravity = AVLayerVideoGravity.resizeAspectFill\nrootLayer = previewView.layer\npreviewLayer.frame = rootLayer.bounds\nrootLayer.addSublayer(previewLayer)",
      "language" : "swift"
    },
    {
      "code" : "let curDeviceOrientation = UIDevice.current.orientation\nlet exifOrientation: CGImagePropertyOrientation\n\nswitch curDeviceOrientation {\ncase UIDeviceOrientation.portraitUpsideDown:  \/\/ Device oriented vertically, home button on the top\n    exifOrientation = .left\ncase UIDeviceOrientation.landscapeLeft:       \/\/ Device oriented horizontally, home button on the right\n    exifOrientation = .upMirrored\ncase UIDeviceOrientation.landscapeRight:      \/\/ Device oriented horizontally, home button on the left\n    exifOrientation = .down\ncase UIDeviceOrientation.portrait:            \/\/ Device oriented vertically, home button on the bottom\n    exifOrientation = .up\ndefault:\n    exifOrientation = .up\n}",
      "language" : "swift"
    },
    {
      "code" : "let visionModel = try VNCoreMLModel(for: MLModel(contentsOf: modelURL))",
      "language" : "swift"
    },
    {
      "code" : "let objectRecognition = VNCoreMLRequest(model: visionModel, completionHandler: { (request, error) in\n    DispatchQueue.main.async(execute: {\n        \/\/ perform all the UI updates on the main queue\n        if let results = request.results {\n            self.drawVisionRequestResults(results)\n        }\n    })\n})",
      "language" : "swift"
    },
    {
      "code" : "for observation in results where observation is VNRecognizedObjectObservation {\n    guard let objectObservation = observation as? VNRecognizedObjectObservation else {\n        continue\n    }\n    \/\/ Select only the label with the highest confidence.\n    let topLabelObservation = objectObservation.labels[0]\n    let objectBounds = VNImageRectForNormalizedRect(objectObservation.boundingBox, Int(bufferSize.width), Int(bufferSize.height))\n    \n    let shapeLayer = self.createRoundedRectLayerWithBounds(objectBounds)\n    \n    let textLayer = self.createTextSubLayerInBounds(objectBounds,\n                                                    identifier: topLabelObservation.identifier,\n                                                    confidence: topLabelObservation.confidence)\n    shapeLayer.addSublayer(textLayer)\n    detectionOverlay.addSublayer(shapeLayer)\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "c6c14a7286340074b6a96f3a97c46467b88ab0b5240aff7b0437f0e68e2f24b6",
  "crawledAt" : "2025-12-04T02:34:59Z",
  "id" : "7CF6D69F-5053-476C-9F65-2D82015CEEE9",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "Vision",
  "overview" : "## Overview\n\nWith the [https:\/\/developer.apple.com\/documentation\/vision] framework, you can recognize objects in live capture.  Starting in iOS 12, macOS 10.14, and tvOS 12, Vision requests made with a Core ML model return results as  [https:\/\/developer.apple.com\/documentation\/vision\/vnrecognizedobjectobservation] objects, which identify objects found in the captured scene.\n\nThis sample app shows you how to set up your camera for live capture, incorporate a Core ML model into Vision, and parse results as classified objects.\n\n\n\n### Set Up Live Capture\n\nAlthough implementing AV live capture is similar from one capture app to another, configuring the camera to work best with Vision algorithms involves some subtle differences.\n\n**Configure the camera to use for capture.**  This sample app feeds camera output from AVFoundation into the main view controller.  Start by configuring an  [https:\/\/developer.apple.com\/documentation\/avfoundation\/avcapturesession]:\n\n**Set your device and session resolution.** It’s important to choose the right resolution for your app.  Don’t simply select the highest resolution available if your app doesn’t require it.  It’s better to select a lower resolution so Vision can process results more efficiently.  Check the model parameters in Xcode to find out if your app requires a resolution smaller than 640 x 480 pixels.\n\nSet the camera resolution to the nearest resolution that is greater than or equal to the resolution of images used in the model:\n\nVision will perform the remaining scaling.\n\n**Add video input to your session by adding the camera as a device:**\n\n**Add video output to your session, being sure to specify the pixel format:**\n\n**Process every frame, but don’t hold on to more than one Vision request at a time.**  The camera will stop working if the buffer queue overflows available memory.  To simplify buffer management, in the capture output, Vision blocks the call for as long as the previous request requires.  As a result, AVFoundation may drop frames, if necessary.  The sample app keeps a queue size of 1; if a Vision request is already queued up for processing when another becomes available, skip it instead of holding on to extras.\n\n**Commit the session configuration:**\n\nSet up a preview layer on your view controller, so the camera can feed its frames into your app’s UI:\n\n### Specify Device Orientation\n\nYou must input the camera’s orientation properly using the device orientation.  Vision algorithms aren’t orientation-agnostic, so when you make a request, use an orientation that’s relative to that of the capture device.\n\n### Designate Labels Using a Core ML Classifier\n\nThe Core ML model you include in your app determines which labels are used in Vision’s object identifiers.  The model in this sample app was trained in Turi Create 4.3.2 using Darknet YOLO (You Only Look Once). See [https:\/\/apple.github.io\/turicreate\/docs\/userguide\/object_detection\/] to learn how to generate your own models using Turi Create. Vision analyzes these models and returns observations as [https:\/\/developer.apple.com\/documentation\/vision\/vnrecognizedobjectobservation] objects.\n\nLoad the model using a [https:\/\/developer.apple.com\/documentation\/vision\/vncoremlmodel]:\n\nCreate a [https:\/\/developer.apple.com\/documentation\/vision\/vncoremlrequest] with that model:\n\nThe completion handler could execute on a background queue, so perform UI updates on the main queue to provide immediate visual feedback.\n\nAccess results in the request’s completion handler, or through the `requests` property.\n\n### Parse Recognized Object Observations\n\nThe `results` property is an array of observations, each with a set of labels and bounding boxes. Parse those observations by iterating through the array, as follows:\n\nThe `labels` array lists each classification `identifier` along with its `confidence` value, ordered from highest confidence to lowest.  The sample app notes only the classification with the highest `confidence` score, at element `0`.  It then displays this classification and confidence in a textual overlay.\n\nThe bounding box tells where the object was observed. The sample uses this location to draw a bounding box around the object.\n\nThis sample simplifies classification by returning only the top classification; the array is ordered in decreasing order of confidence score.  However, your app could analyze the confidence score and show multiple classifications, either to further describe your detected objects, or to show competing classifications.\n\nYou can also use the [https:\/\/developer.apple.com\/documentation\/vision\/vnrecognizedobjectobservation] resulting from object recognition to initialize an object tracker such as [https:\/\/developer.apple.com\/documentation\/vision\/vntrackobjectrequest].  For more information about tracking, see the article on object tracking: [https:\/\/developer.apple.com\/documentation\/vision\/tracking-multiple-objects-or-rectangles-in-video].",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/vision\/recognizing-objects-in-live-capture\ncrawled: 2025-12-04T02:34:59Z\n---\n\n# Recognizing Objects in Live Capture\n\n**Sample Code**\n\nApply Vision algorithms to identify objects in real-time video.\n\n## Overview\n\nWith the [https:\/\/developer.apple.com\/documentation\/vision] framework, you can recognize objects in live capture.  Starting in iOS 12, macOS 10.14, and tvOS 12, Vision requests made with a Core ML model return results as  [https:\/\/developer.apple.com\/documentation\/vision\/vnrecognizedobjectobservation] objects, which identify objects found in the captured scene.\n\nThis sample app shows you how to set up your camera for live capture, incorporate a Core ML model into Vision, and parse results as classified objects.\n\n\n\n### Set Up Live Capture\n\nAlthough implementing AV live capture is similar from one capture app to another, configuring the camera to work best with Vision algorithms involves some subtle differences.\n\n**Configure the camera to use for capture.**  This sample app feeds camera output from AVFoundation into the main view controller.  Start by configuring an  [https:\/\/developer.apple.com\/documentation\/avfoundation\/avcapturesession]:\n\n```swift\nprivate let session = AVCaptureSession()\n```\n\n**Set your device and session resolution.** It’s important to choose the right resolution for your app.  Don’t simply select the highest resolution available if your app doesn’t require it.  It’s better to select a lower resolution so Vision can process results more efficiently.  Check the model parameters in Xcode to find out if your app requires a resolution smaller than 640 x 480 pixels.\n\nSet the camera resolution to the nearest resolution that is greater than or equal to the resolution of images used in the model:\n\n```swift\nlet videoDevice = AVCaptureDevice.DiscoverySession(deviceTypes: [.builtInWideAngleCamera], mediaType: .video, position: .back).devices.first\ndo {\n    deviceInput = try AVCaptureDeviceInput(device: videoDevice!)\n} catch {\n    print(\"Could not create video device input: \\(error)\")\n    return\n}\n\nsession.beginConfiguration()\nsession.sessionPreset = .vga640x480 \/\/ Model image size is smaller.\n```\n\nVision will perform the remaining scaling.\n\n**Add video input to your session by adding the camera as a device:**\n\n```swift\nguard session.canAddInput(deviceInput) else {\n    print(\"Could not add video device input to the session\")\n    session.commitConfiguration()\n    return\n}\nsession.addInput(deviceInput)\n```\n\n**Add video output to your session, being sure to specify the pixel format:**\n\n```swift\nif session.canAddOutput(videoDataOutput) {\n    session.addOutput(videoDataOutput)\n    \/\/ Add a video data output\n    videoDataOutput.alwaysDiscardsLateVideoFrames = true\n    videoDataOutput.videoSettings = [kCVPixelBufferPixelFormatTypeKey as String: Int(kCVPixelFormatType_420YpCbCr8BiPlanarFullRange)]\n    videoDataOutput.setSampleBufferDelegate(self, queue: videoDataOutputQueue)\n} else {\n    print(\"Could not add video data output to the session\")\n    session.commitConfiguration()\n    return\n}\n```\n\n**Process every frame, but don’t hold on to more than one Vision request at a time.**  The camera will stop working if the buffer queue overflows available memory.  To simplify buffer management, in the capture output, Vision blocks the call for as long as the previous request requires.  As a result, AVFoundation may drop frames, if necessary.  The sample app keeps a queue size of 1; if a Vision request is already queued up for processing when another becomes available, skip it instead of holding on to extras.\n\n```swift\nlet captureConnection = videoDataOutput.connection(with: .video)\n\/\/ Always process the frames\ncaptureConnection?.isEnabled = true\ndo {\n    try  videoDevice!.lockForConfiguration()\n    let dimensions = CMVideoFormatDescriptionGetDimensions((videoDevice?.activeFormat.formatDescription)!)\n    bufferSize.width = CGFloat(dimensions.width)\n    bufferSize.height = CGFloat(dimensions.height)\n    videoDevice!.unlockForConfiguration()\n} catch {\n    print(error)\n}\n```\n\n**Commit the session configuration:**\n\n```swift\nsession.commitConfiguration()\n```\n\nSet up a preview layer on your view controller, so the camera can feed its frames into your app’s UI:\n\n```swift\npreviewLayer = AVCaptureVideoPreviewLayer(session: session)\npreviewLayer.videoGravity = AVLayerVideoGravity.resizeAspectFill\nrootLayer = previewView.layer\npreviewLayer.frame = rootLayer.bounds\nrootLayer.addSublayer(previewLayer)\n```\n\n### Specify Device Orientation\n\nYou must input the camera’s orientation properly using the device orientation.  Vision algorithms aren’t orientation-agnostic, so when you make a request, use an orientation that’s relative to that of the capture device.\n\n```swift\nlet curDeviceOrientation = UIDevice.current.orientation\nlet exifOrientation: CGImagePropertyOrientation\n\nswitch curDeviceOrientation {\ncase UIDeviceOrientation.portraitUpsideDown:  \/\/ Device oriented vertically, home button on the top\n    exifOrientation = .left\ncase UIDeviceOrientation.landscapeLeft:       \/\/ Device oriented horizontally, home button on the right\n    exifOrientation = .upMirrored\ncase UIDeviceOrientation.landscapeRight:      \/\/ Device oriented horizontally, home button on the left\n    exifOrientation = .down\ncase UIDeviceOrientation.portrait:            \/\/ Device oriented vertically, home button on the bottom\n    exifOrientation = .up\ndefault:\n    exifOrientation = .up\n}\n```\n\n### Designate Labels Using a Core ML Classifier\n\nThe Core ML model you include in your app determines which labels are used in Vision’s object identifiers.  The model in this sample app was trained in Turi Create 4.3.2 using Darknet YOLO (You Only Look Once). See [https:\/\/apple.github.io\/turicreate\/docs\/userguide\/object_detection\/] to learn how to generate your own models using Turi Create. Vision analyzes these models and returns observations as [https:\/\/developer.apple.com\/documentation\/vision\/vnrecognizedobjectobservation] objects.\n\nLoad the model using a [https:\/\/developer.apple.com\/documentation\/vision\/vncoremlmodel]:\n\n```swift\nlet visionModel = try VNCoreMLModel(for: MLModel(contentsOf: modelURL))\n```\n\nCreate a [https:\/\/developer.apple.com\/documentation\/vision\/vncoremlrequest] with that model:\n\n```swift\nlet objectRecognition = VNCoreMLRequest(model: visionModel, completionHandler: { (request, error) in\n    DispatchQueue.main.async(execute: {\n        \/\/ perform all the UI updates on the main queue\n        if let results = request.results {\n            self.drawVisionRequestResults(results)\n        }\n    })\n})\n```\n\nThe completion handler could execute on a background queue, so perform UI updates on the main queue to provide immediate visual feedback.\n\nAccess results in the request’s completion handler, or through the `requests` property.\n\n### Parse Recognized Object Observations\n\nThe `results` property is an array of observations, each with a set of labels and bounding boxes. Parse those observations by iterating through the array, as follows:\n\n```swift\nfor observation in results where observation is VNRecognizedObjectObservation {\n    guard let objectObservation = observation as? VNRecognizedObjectObservation else {\n        continue\n    }\n    \/\/ Select only the label with the highest confidence.\n    let topLabelObservation = objectObservation.labels[0]\n    let objectBounds = VNImageRectForNormalizedRect(objectObservation.boundingBox, Int(bufferSize.width), Int(bufferSize.height))\n    \n    let shapeLayer = self.createRoundedRectLayerWithBounds(objectBounds)\n    \n    let textLayer = self.createTextSubLayerInBounds(objectBounds,\n                                                    identifier: topLabelObservation.identifier,\n                                                    confidence: topLabelObservation.confidence)\n    shapeLayer.addSublayer(textLayer)\n    detectionOverlay.addSublayer(shapeLayer)\n}\n```\n\nThe `labels` array lists each classification `identifier` along with its `confidence` value, ordered from highest confidence to lowest.  The sample app notes only the classification with the highest `confidence` score, at element `0`.  It then displays this classification and confidence in a textual overlay.\n\nThe bounding box tells where the object was observed. The sample uses this location to draw a bounding box around the object.\n\nThis sample simplifies classification by returning only the top classification; the array is ordered in decreasing order of confidence score.  However, your app could analyze the confidence score and show multiple classifications, either to further describe your detected objects, or to show competing classifications.\n\nYou can also use the [https:\/\/developer.apple.com\/documentation\/vision\/vnrecognizedobjectobservation] resulting from object recognition to initialize an object tracker such as [https:\/\/developer.apple.com\/documentation\/vision\/vntrackobjectrequest].  For more information about tracking, see the article on object tracking: [https:\/\/developer.apple.com\/documentation\/vision\/tracking-multiple-objects-or-rectangles-in-video].\n\n## Object recognition\n\n- **Understanding a Dice Roll with Vision and Object Detection**: Detect dice position and values shown in a camera frame, and determine the end of a roll by leveraging a dice detection model.\n- **VNRecognizedObjectObservation**: A detected object observation with an array of classification labels that classify the recognized object.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Detect dice position and values shown in a camera frame, and determine the end of a roll by leveraging a dice detection model.",
          "name" : "Understanding a Dice Roll with Vision and Object Detection",
          "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/understanding-a-dice-roll-with-vision-and-object-detection"
        },
        {
          "description" : "A detected object observation with an array of classification labels that classify the recognized object.",
          "name" : "VNRecognizedObjectObservation",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNRecognizedObjectObservation"
        }
      ],
      "title" : "Object recognition"
    }
  ],
  "source" : "appleJSON",
  "title" : "Recognizing Objects in Live Capture",
  "url" : "https:\/\/developer.apple.com\/documentation\/vision\/recognizing-objects-in-live-capture"
}