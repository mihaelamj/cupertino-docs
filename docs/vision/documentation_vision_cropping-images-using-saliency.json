{
  "abstract" : "Isolate regions in an image that are most likely to draw people’s attention.",
  "codeExamples" : [

  ],
  "contentHash" : "c8ddcd88be423849219e027ada29d9dd787df09dc2eba3b1369eed0eade04049",
  "crawledAt" : "2025-12-04T01:02:37Z",
  "id" : "EF36C0D7-8AB9-4615-94F2-2A9179266CAC",
  "kind" : "article",
  "language" : "swift",
  "module" : "Vision",
  "overview" : "## Overview\n\n*Saliency* refers to what’s noticeable or important in an image. The Vision framework supports two types of saliency: *object-based* and *attention-based*.\n\nAttention-based saliency highlights what people are likely to look at. Object-based saliency highlights foreground objects and provides a coarse segmentation of the main subjects in an image. The range of saliency is [0,1], where higher values indicate higher potential for interest or inclusion in the foreground.\n\nSaliency has many applications:\n\nThis article describes one application of saliency: cropping an input image to fit a given aspect ratio while keeping the most interesting element in the image. This technique works by focusing on the region most likely to draw attention at a glance.\n\n### Specify the Type of Saliency\n\nBoth models of saliency are based on deep-learning neural networks. The model used for object-based saliency is trained on foreground objects that have been segmented from the background. The model used for attention-based saliency is trained using eye-tracking data from human subjects looking at images.\n\nThe class of the request to generate saliency differs depending on the type of saliency you want Vision to compute:\n\nObject-based saliency and attention-based saliency have different use cases. If you’re deciding what to keep in an image thumbnail based on what’s most interesting, use attention-based saliency.\n\n### Parse the Output Heatmap\n\nBoth types of saliency requests return their results as heatmaps, which are 68 x 68 pixel buffers of floating-point saliency values. Think of each entry in the heatmap as a cell region of your original image. The heatmap quantifies how salient the pixels in the cell are for the chosen saliency approach.\n\nIf your app overlays the saliency heatmap on the original input image, upsample the heatmap and apply a colormap before showing it to the user.\n\n### Merge Salient Regions for Object-Based Saliency\n\nFor object-based saliency requests, which return up to three bounding boxes, you can use either the most salient bounding box directly to crop a region of your image, or the union of all boxes. Each bounding box comes with a score that you use to rank the relevance of regions within the image.\n\nIf the object-based saliency map value is close to zero everywhere, the image contains nothing that’s salient.\n\n### Crop the Image\n\nAttention-based saliency requests return only one bounding box, which you use directly to crop the image and drop uninteresting content. Use this bounding box to find the region on which to focus:\n\nIf nothing is salient, the attention heatmap that’s returned highlights the central part of the image. This behavior reflects the fact that people tend to look at the center of an image if nothing in particular stands out to them.\n\n### Feed Saliency Results into Other Vision Requests\n\nOnce you know which regions of an image are interesting, you can use the output of a saliency request as the input to another Vision request, like text recognition. The bounding boxes from saliency requests also help you localize the regions you’d like to search, so you can prioritize recognition on the most salient parts of an image, like signs or posters.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/vision\/cropping-images-using-saliency\ncrawled: 2025-12-04T01:02:37Z\n---\n\n# Cropping Images Using Saliency\n\n**Article**\n\nIsolate regions in an image that are most likely to draw people’s attention.\n\n## Overview\n\n*Saliency* refers to what’s noticeable or important in an image. The Vision framework supports two types of saliency: *object-based* and *attention-based*.\n\nAttention-based saliency highlights what people are likely to look at. Object-based saliency highlights foreground objects and provides a coarse segmentation of the main subjects in an image. The range of saliency is [0,1], where higher values indicate higher potential for interest or inclusion in the foreground.\n\nSaliency has many applications:\n\n- Automate image cropping to fit key elements.\n- Generate thumbnails from an image set.\n- Guide the camera to focus on a key area for blur estimation or white balance.\n- Guide postprocessing by determining candidates for sharpening or lighting enhancement.\n- Pan the camera to relevant shots in a photo or video album.\n\nThis article describes one application of saliency: cropping an input image to fit a given aspect ratio while keeping the most interesting element in the image. This technique works by focusing on the region most likely to draw attention at a glance.\n\n### Specify the Type of Saliency\n\nBoth models of saliency are based on deep-learning neural networks. The model used for object-based saliency is trained on foreground objects that have been segmented from the background. The model used for attention-based saliency is trained using eye-tracking data from human subjects looking at images.\n\nThe class of the request to generate saliency differs depending on the type of saliency you want Vision to compute:\n\n\n\nObject-based saliency and attention-based saliency have different use cases. If you’re deciding what to keep in an image thumbnail based on what’s most interesting, use attention-based saliency.\n\n### Parse the Output Heatmap\n\nBoth types of saliency requests return their results as heatmaps, which are 68 x 68 pixel buffers of floating-point saliency values. Think of each entry in the heatmap as a cell region of your original image. The heatmap quantifies how salient the pixels in the cell are for the chosen saliency approach.\n\n\n\nIf your app overlays the saliency heatmap on the original input image, upsample the heatmap and apply a colormap before showing it to the user.\n\n### Merge Salient Regions for Object-Based Saliency\n\nFor object-based saliency requests, which return up to three bounding boxes, you can use either the most salient bounding box directly to crop a region of your image, or the union of all boxes. Each bounding box comes with a score that you use to rank the relevance of regions within the image.\n\n\n\nIf the object-based saliency map value is close to zero everywhere, the image contains nothing that’s salient.\n\n### Crop the Image\n\nAttention-based saliency requests return only one bounding box, which you use directly to crop the image and drop uninteresting content. Use this bounding box to find the region on which to focus:\n\n\n\nIf nothing is salient, the attention heatmap that’s returned highlights the central part of the image. This behavior reflects the fact that people tend to look at the center of an image if nothing in particular stands out to them.\n\n### Feed Saliency Results into Other Vision Requests\n\nOnce you know which regions of an image are interesting, you can use the output of a saliency request as the input to another Vision request, like text recognition. The bounding boxes from saliency requests also help you localize the regions you’d like to search, so you can prioritize recognition on the most salient parts of an image, like signs or posters.\n\n\n\n## Saliency analysis\n\n- **Highlighting Areas of Interest in an Image Using Saliency**: Quantify and visualize where people are likely to look in an image.\n- **VNGenerateAttentionBasedSaliencyImageRequest**: An object that produces a heat map that identifies the parts of an image most likely to draw attention.\n- **VNGenerateObjectnessBasedSaliencyImageRequest**: A request that generates a heat map that identifies the parts of an image most likely to represent objects.\n- **VNSaliencyImageObservation**: An observation that contains a grayscale heat map of important areas across an image.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Quantify and visualize where people are likely to look in an image.",
          "name" : "Highlighting Areas of Interest in an Image Using Saliency",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/highlighting-areas-of-interest-in-an-image-using-saliency"
        },
        {
          "description" : "An object that produces a heat map that identifies the parts of an image most likely to draw attention.",
          "name" : "VNGenerateAttentionBasedSaliencyImageRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNGenerateAttentionBasedSaliencyImageRequest"
        },
        {
          "description" : "A request that generates a heat map that identifies the parts of an image most likely to represent objects.",
          "name" : "VNGenerateObjectnessBasedSaliencyImageRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNGenerateObjectnessBasedSaliencyImageRequest"
        },
        {
          "description" : "An observation that contains a grayscale heat map of important areas across an image.",
          "name" : "VNSaliencyImageObservation",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNSaliencyImageObservation"
        }
      ],
      "title" : "Saliency analysis"
    }
  ],
  "source" : "appleJSON",
  "title" : "Cropping Images Using Saliency",
  "url" : "https:\/\/developer.apple.com\/documentation\/vision\/cropping-images-using-saliency"
}