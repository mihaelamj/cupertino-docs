{
  "abstract" : "Scan a document that contains a table and extract its content in a formatted way.",
  "codeExamples" : [
    {
      "code" : "\/\/ Performs the initial capture session configuration.\nprivate func setUpSession() throws {\n    \/\/ Return early if already set up.\n    guard !isSetUp else { return }\n\n    \/\/ Retrieve the default camera.\n    guard let defaultCamera = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back) else {\n        throw CameraError.deviceUnavailable\n    }\n\n    \/\/ Add inputs for the default camera and microphone devices.\n    activeVideoInput = try addInput(for: defaultCamera)\n\n    \/\/ Configure the session preset based on the current capture mode.\n    captureSession.sessionPreset = .photo\n    \/\/ Add the photo capture output as the default output type.\n    try addOutput(photoCapture.output)\n\n    isSetUp = true\n}",
      "language" : "swift"
    },
    {
      "code" : "func capturePhoto() async throws -> Data {\n    try await photoCapture.capturePhoto()\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ Process an image and return the first table detected.\nprivate func extractTable(from image: Data) async throws -> DocumentObservation.Container.Table {\n\n    \/\/ The Vision request.\n    let request = RecognizeDocumentsRequest()\n\n    \/\/ Perform the request on the image data and return the results.\n    let observations = try await request.perform(on: image)\n\n    \/\/ Get the first observation from the array.\n    guard let document = observations.first?.document else {\n        throw AppError.noDocument\n    }\n\n    \/\/ Extract the first table detected.\n    guard let table = document.tables.first else {\n        throw AppError.noTable\n    }\n\n    return table\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ Extract the name, email address, and phone number from a table into a list of contacts.\nprivate func parseTable(_ table: DocumentObservation.Container.Table) -> [Contact] {\n    var contacts = [Contact]()\n\n    \/\/ Iterate over each row in the table.\n    for row in table.rows {\n        \/\/ Take the contact name from the first column.\n        guard let firstCell = row.first else {\n            continue\n        }\n        \/\/ Extract the text content from the transcript.\n        let name = firstCell.content.text.transcript\n\n        \/\/ Look for email addresses and phone numbers in the remaining cells.\n        var detectedPhone: String? = nil\n        var detectedEmail: String? = nil\n\n        for cell in row.dropFirst() {\n            \/\/ Get all detected data in the cell, then match emails and phone numbers to a contact. \n            for data in cell.content.text.detectedData {\n                switch data.match.details {\n                case .emailAddress(let email):\n                    detectedEmail = email.emailAddress\n                case .phoneNumber(let phoneNumber):\n                    detectedPhone = phoneNumber.phoneNumber\n                default:\n                    break\n                }\n            }\n        }\n\n        \/\/ Create a contact if an email was detected.\n        if let email = detectedEmail {\n            let contact = Contact(name: name, email: email, phoneNumber: detectedPhone)\n            contacts.append(contact)\n        }\n    }\n\n    return contacts\n}",
      "language" : "swift"
    },
    {
      "code" : "struct ContactView: View {\n    let contacts: [Contact]\n    var body: some View {\n        Text(\"Contacts\")\n        List(contacts, id: \\.name) { contact in\n            HStack {\n                Text(contact.name)\n                Spacer()\n                Text(contact.email)\n                Spacer()\n                Text(contact.phoneNumber ?? \"\")\n            }\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "extension DocumentObservation.Container.Table {\n    \/\/\/ Returns the contents of a cell that someone taps.\n    func cell(at point: NormalizedPoint) -> TableCell? {\n        let visionPoint = point.cgPoint\n        \/\/ Verify that the tap occurs inside the bounding region of the table.\n        guard self.boundingRegion.normalizedPath.contains(visionPoint) else {\n            return nil\n        }\n        \/\/ Inspect each cell.\n        for row in self.rows {\n            for cell in row {\n                \/\/ Check if the tap occurs inside the cell.\n                if cell.content.boundingRegion.normalizedPath.contains(visionPoint) {\n                    return TableCell(cell)\n                }\n            }\n        }\n        return nil\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ Convert the table into a TSV string format that's compatible with pasting into Notes or Numbers.\n\/\/\/\n\/\/\/ Tables have at most one line per cell, and no cells that span multiple rows or columns.\nfunc exportTable() async throws -> String {\n    guard let rows = self.table?.rows else {\n        throw AppError.noTable\n    }\n    \/\/ Map each row into a tab-delimited line.\n    let tableRowData = rows.map { row in\n        return row.map({ $0.content.text.transcript }).joined(separator: \"\\t\")\n    }\n    \/\/ Create a multiline string with one row per line.\n    return tableRowData.joined(separator: \"\\n\")\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "d9e4bb69c951c2eb8a5c6eddaf003a2ece194c7e3a6b9a10babf60863e1ed7db",
  "crawledAt" : "2025-12-05T10:57:34Z",
  "id" : "B12F3C8F-B949-4CAE-BA2A-8B3E98480676",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "Vision",
  "overview" : "## Overview\n\nThis sample app shows how to capture document images using the device camera and extract structured data from tables. The app uses the [doc:\/\/Vision\/documentation\/Vision\/RecognizeDocumentsRequest] API to detect a table and create a contact list from the extracted data.\n\nWhen you run the app, you point your device camera at a document that contains a table of information. After capturing a photo, the app analyzes the table data and displays the formatted data so you can export the information to apps like Notes or Numbers.\n\nThe sample demonstrates three main capabilities:\n\n## Configure the sample code project\n\nBecause this sample app requires camera access, you’ll need to build and run this sample on a device. When you first launch the app on a device, grant the app access to the camera. In the sample project’s assets folder, open the `sampleDocuments.png` file and use the rear camera on iPad or iPhone to take a picture of the document. Optionally, if you have access to a printer, print this file and take a picture of it with your device.\n\n## Capture a document photo\n\nThe app starts by showing a camera preview where you can frame and take a picture of the document. To setup this preview and capture, the app creates a capture session with `AVFoundation`:\n\nThe `defaultCamera` uses the device’s rear camera and the `.photo` preset sets up the session to capture a picture of the document.\n\nWhen you tap the capture button, the app calls `capturePhoto`:\n\nThis asynchronous method returns the captured photo as `Data`, which the app passes to the Vision model for analysis.\n\n## Detect tables in the document\n\nThe app uses Vision to find the table in the captured document image. To detect the table, the app uses [doc:\/\/Vision\/documentation\/Vision\/RecognizeDocumentsRequest]. The Vision framework uses a default method for image processing: pass in the image, run the request, and get the extracted contents in an observation.\n\nThe `request.perform(on:)` method runs the [doc:\/\/Vision\/documentation\/Vision\/RecognizeDocumentsRequest] on the image and returns a [doc:\/\/Vision\/documentation\/Vision\/DocumentObservation]. Each document is a container that holds text, tables, lists, or barcodes. The app accesses the table from the document’s [doc:\/\/Vision\/documentation\/Vision\/DocumentObservation\/Container\/Table] property.\n\nThe app highlights the detected table with a blue outline showing the boundaries.\n\n## Extract contact information from table cells\n\nWith the extracted structure, the app can access the data in the table cells. The app parses through the rows and columns to get the table data and converts it to an array of `Contact` objects:\n\nThe app takes the contact name from the first column and accesses the text content using the  [doc:\/\/com.apple.documentation\/documentation\/Vision\/DocumentObservation\/Container\/Text-swift.struct\/transcript] property.\n\nTo process the remaining columns, the app skips the first cell by using [doc:\/\/com.apple.documentation\/documentation\/Swift\/Array\/dropFirst(_:)] on the row. It uses the [doc:\/\/com.apple.documentation\/documentation\/DataDetection] framework to find email addresses and phone numbers in the `cell.content.text.detectedData` array.\n\nThe app creates a contact only when it finds an email address. After processing the table, the app stores all the contacts in an array and people can view it through the `ContactView`.\n\nA person can see this list of extracted contacts in the app by clicking the View Contacts button above the photo capture. Each entry shows the contact’s name and email address, with phone numbers included when detected in the table.\n\n## Interact with table cells\n\nThe app allows you to tap on the cells in the captured table and use the data within the cells to call or send a message. It uses the [doc:\/\/com.apple.documentation\/documentation\/Vision\/DocumentObservation\/Container\/Table\/boundingRegion] property of the `DocumentObservation` to access the selected cell and to ensure that people only tap within the table bounds.\n\nThe method first verifies that the tap occurs within the table’s overall `boundingRegion`, then iterates through each cell’s `boundingRegion` to find the one that contains the tap. Bounding regions use normalized coordinates (`0.0` to `1.0`) relative to the image dimensions, which makes them work at any display scale.\n\nWhen the method finds a cell, the app displays a popup showing the cell’s content. If the cell contains an email address, people can tap on the address to compose a message. For phone numbers, people can tap to call or send a text message.\n\n## Export table data\n\nYou can also export the table data in tab-separated values (TSV) format to copy and paste into compatible apps like Notes or Numbers:\n\nEach row becomes a line in the output string, with the `transcript` property providing the recognized text from each cell. The `\"\\t\"` separator creates the TSV format by placing tab characters between cells in the same row. The outer `joined(separator: \"\\n\")` call puts each row on its own line.\n\nPeople can tap the `Copy Table` button to copy this formatted text to the clipboard, then paste it into other apps. The table structure remains intact when pasted.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/vision\/recognize-tables-within-a-document\ncrawled: 2025-12-05T10:57:34Z\n---\n\n# Recognizing tables within a document\n\n**Sample Code**\n\nScan a document that contains a table and extract its content in a formatted way.\n\n## Overview\n\nThis sample app shows how to capture document images using the device camera and extract structured data from tables. The app uses the [doc:\/\/Vision\/documentation\/Vision\/RecognizeDocumentsRequest] API to detect a table and create a contact list from the extracted data.\n\nWhen you run the app, you point your device camera at a document that contains a table of information. After capturing a photo, the app analyzes the table data and displays the formatted data so you can export the information to apps like Notes or Numbers.\n\nThe sample demonstrates three main capabilities:\n\n1. Setting up camera capture with [doc:\/\/com.apple.documentation\/documentation\/AVFoundation] to photograph documents.\n2. Detecting a table in a document image using Vision.\n3. Parsing structured data from table cells using DataDetection.\n\n\n\n## Configure the sample code project\n\nBecause this sample app requires camera access, you’ll need to build and run this sample on a device. When you first launch the app on a device, grant the app access to the camera. In the sample project’s assets folder, open the `sampleDocuments.png` file and use the rear camera on iPad or iPhone to take a picture of the document. Optionally, if you have access to a printer, print this file and take a picture of it with your device.\n\n## Capture a document photo\n\nThe app starts by showing a camera preview where you can frame and take a picture of the document. To setup this preview and capture, the app creates a capture session with `AVFoundation`:\n\n```swift\n\/\/ Performs the initial capture session configuration.\nprivate func setUpSession() throws {\n    \/\/ Return early if already set up.\n    guard !isSetUp else { return }\n\n    \/\/ Retrieve the default camera.\n    guard let defaultCamera = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back) else {\n        throw CameraError.deviceUnavailable\n    }\n\n    \/\/ Add inputs for the default camera and microphone devices.\n    activeVideoInput = try addInput(for: defaultCamera)\n\n    \/\/ Configure the session preset based on the current capture mode.\n    captureSession.sessionPreset = .photo\n    \/\/ Add the photo capture output as the default output type.\n    try addOutput(photoCapture.output)\n\n    isSetUp = true\n}\n```\n\nThe `defaultCamera` uses the device’s rear camera and the `.photo` preset sets up the session to capture a picture of the document.\n\nWhen you tap the capture button, the app calls `capturePhoto`:\n\n```swift\nfunc capturePhoto() async throws -> Data {\n    try await photoCapture.capturePhoto()\n}\n```\n\nThis asynchronous method returns the captured photo as `Data`, which the app passes to the Vision model for analysis.\n\n## Detect tables in the document\n\nThe app uses Vision to find the table in the captured document image. To detect the table, the app uses [doc:\/\/Vision\/documentation\/Vision\/RecognizeDocumentsRequest]. The Vision framework uses a default method for image processing: pass in the image, run the request, and get the extracted contents in an observation.\n\n```swift\n\/\/\/ Process an image and return the first table detected.\nprivate func extractTable(from image: Data) async throws -> DocumentObservation.Container.Table {\n\n    \/\/ The Vision request.\n    let request = RecognizeDocumentsRequest()\n\n    \/\/ Perform the request on the image data and return the results.\n    let observations = try await request.perform(on: image)\n\n    \/\/ Get the first observation from the array.\n    guard let document = observations.first?.document else {\n        throw AppError.noDocument\n    }\n\n    \/\/ Extract the first table detected.\n    guard let table = document.tables.first else {\n        throw AppError.noTable\n    }\n\n    return table\n}\n```\n\nThe `request.perform(on:)` method runs the [doc:\/\/Vision\/documentation\/Vision\/RecognizeDocumentsRequest] on the image and returns a [doc:\/\/Vision\/documentation\/Vision\/DocumentObservation]. Each document is a container that holds text, tables, lists, or barcodes. The app accesses the table from the document’s [doc:\/\/Vision\/documentation\/Vision\/DocumentObservation\/Container\/Table] property.\n\nThe app highlights the detected table with a blue outline showing the boundaries.\n\n## Extract contact information from table cells\n\nWith the extracted structure, the app can access the data in the table cells. The app parses through the rows and columns to get the table data and converts it to an array of `Contact` objects:\n\n```swift\n\/\/\/ Extract the name, email address, and phone number from a table into a list of contacts.\nprivate func parseTable(_ table: DocumentObservation.Container.Table) -> [Contact] {\n    var contacts = [Contact]()\n\n    \/\/ Iterate over each row in the table.\n    for row in table.rows {\n        \/\/ Take the contact name from the first column.\n        guard let firstCell = row.first else {\n            continue\n        }\n        \/\/ Extract the text content from the transcript.\n        let name = firstCell.content.text.transcript\n\n        \/\/ Look for email addresses and phone numbers in the remaining cells.\n        var detectedPhone: String? = nil\n        var detectedEmail: String? = nil\n\n        for cell in row.dropFirst() {\n            \/\/ Get all detected data in the cell, then match emails and phone numbers to a contact. \n            for data in cell.content.text.detectedData {\n                switch data.match.details {\n                case .emailAddress(let email):\n                    detectedEmail = email.emailAddress\n                case .phoneNumber(let phoneNumber):\n                    detectedPhone = phoneNumber.phoneNumber\n                default:\n                    break\n                }\n            }\n        }\n\n        \/\/ Create a contact if an email was detected.\n        if let email = detectedEmail {\n            let contact = Contact(name: name, email: email, phoneNumber: detectedPhone)\n            contacts.append(contact)\n        }\n    }\n\n    return contacts\n}\n```\n\nThe app takes the contact name from the first column and accesses the text content using the  [doc:\/\/com.apple.documentation\/documentation\/Vision\/DocumentObservation\/Container\/Text-swift.struct\/transcript] property.\n\nTo process the remaining columns, the app skips the first cell by using [doc:\/\/com.apple.documentation\/documentation\/Swift\/Array\/dropFirst(_:)] on the row. It uses the [doc:\/\/com.apple.documentation\/documentation\/DataDetection] framework to find email addresses and phone numbers in the `cell.content.text.detectedData` array.\n\nThe app creates a contact only when it finds an email address. After processing the table, the app stores all the contacts in an array and people can view it through the `ContactView`.\n\n```swift\nstruct ContactView: View {\n    let contacts: [Contact]\n    var body: some View {\n        Text(\"Contacts\")\n        List(contacts, id: \\.name) { contact in\n            HStack {\n                Text(contact.name)\n                Spacer()\n                Text(contact.email)\n                Spacer()\n                Text(contact.phoneNumber ?? \"\")\n            }\n        }\n    }\n}\n```\n\nA person can see this list of extracted contacts in the app by clicking the View Contacts button above the photo capture. Each entry shows the contact’s name and email address, with phone numbers included when detected in the table.\n\n## Interact with table cells\n\nThe app allows you to tap on the cells in the captured table and use the data within the cells to call or send a message. It uses the [doc:\/\/com.apple.documentation\/documentation\/Vision\/DocumentObservation\/Container\/Table\/boundingRegion] property of the `DocumentObservation` to access the selected cell and to ensure that people only tap within the table bounds.\n\n```swift\nextension DocumentObservation.Container.Table {\n    \/\/\/ Returns the contents of a cell that someone taps.\n    func cell(at point: NormalizedPoint) -> TableCell? {\n        let visionPoint = point.cgPoint\n        \/\/ Verify that the tap occurs inside the bounding region of the table.\n        guard self.boundingRegion.normalizedPath.contains(visionPoint) else {\n            return nil\n        }\n        \/\/ Inspect each cell.\n        for row in self.rows {\n            for cell in row {\n                \/\/ Check if the tap occurs inside the cell.\n                if cell.content.boundingRegion.normalizedPath.contains(visionPoint) {\n                    return TableCell(cell)\n                }\n            }\n        }\n        return nil\n    }\n}\n```\n\nThe method first verifies that the tap occurs within the table’s overall `boundingRegion`, then iterates through each cell’s `boundingRegion` to find the one that contains the tap. Bounding regions use normalized coordinates (`0.0` to `1.0`) relative to the image dimensions, which makes them work at any display scale.\n\nWhen the method finds a cell, the app displays a popup showing the cell’s content. If the cell contains an email address, people can tap on the address to compose a message. For phone numbers, people can tap to call or send a text message.\n\n## Export table data\n\nYou can also export the table data in tab-separated values (TSV) format to copy and paste into compatible apps like Notes or Numbers:\n\n```swift\n\/\/\/ Convert the table into a TSV string format that's compatible with pasting into Notes or Numbers.\n\/\/\/\n\/\/\/ Tables have at most one line per cell, and no cells that span multiple rows or columns.\nfunc exportTable() async throws -> String {\n    guard let rows = self.table?.rows else {\n        throw AppError.noTable\n    }\n    \/\/ Map each row into a tab-delimited line.\n    let tableRowData = rows.map { row in\n        return row.map({ $0.content.text.transcript }).joined(separator: \"\\t\")\n    }\n    \/\/ Create a multiline string with one row per line.\n    return tableRowData.joined(separator: \"\\n\")\n}\n```\n\nEach row becomes a line in the output string, with the `transcript` property providing the recognized text from each cell. The `\"\\t\"` separator creates the TSV format by placing tab characters between cells in the same row. The outer `joined(separator: \"\\n\")` call puts each row on its own line.\n\nPeople can tap the `Copy Table` button to copy this formatted text to the clipboard, then paste it into other apps. The table structure remains intact when pasted.\n\n## Text and document analysis\n\n- **Locating and displaying recognized text**: Perform text recognition on a photo using the Vision framework’s text-recognition request.\n- **DetectBarcodesRequest**: A request that detects barcodes in an image.\n- **DetectDocumentSegmentationRequest**: A request that detects rectangular regions that contain text in the input image.\n- **DetectTextRectanglesRequest**: An image-analysis request that finds regions of visible text in an image.\n- **RecognizeDocumentsRequest**: An image-analysis request to scan an image of a document and provide information about its structure.\n- **RecognizeTextRequest**: An image-analysis request that recognizes text in an image.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Perform text recognition on a photo using the Vision framework’s text-recognition request.",
          "name" : "Locating and displaying recognized text",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/locating-and-displaying-recognized-text"
        },
        {
          "description" : "A request that detects barcodes in an image.",
          "name" : "DetectBarcodesRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/DetectBarcodesRequest"
        },
        {
          "description" : "A request that detects rectangular regions that contain text in the input image.",
          "name" : "DetectDocumentSegmentationRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/DetectDocumentSegmentationRequest"
        },
        {
          "description" : "An image-analysis request that finds regions of visible text in an image.",
          "name" : "DetectTextRectanglesRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/DetectTextRectanglesRequest"
        },
        {
          "description" : "An image-analysis request to scan an image of a document and provide information about its structure.",
          "name" : "RecognizeDocumentsRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/RecognizeDocumentsRequest"
        },
        {
          "description" : "An image-analysis request that recognizes text in an image.",
          "name" : "RecognizeTextRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/RecognizeTextRequest"
        }
      ],
      "title" : "Text and document analysis"
    }
  ],
  "source" : "appleJSON",
  "title" : "Recognizing tables within a document",
  "url" : "https:\/\/developer.apple.com\/documentation\/vision\/recognize-tables-within-a-document"
}