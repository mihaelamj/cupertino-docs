{
  "abstract" : "Detect three-dimensional human body poses using the Vision framework.",
  "codeExamples" : [
    {
      "code" : "import Vision\n\n\/\/ Get an image from the project bundle. \nguard let filePath = Bundle.main.path(forResource: \"bodypose\", ofType: \"heic\") else { \n    return \n} \nlet fileUrl = URL(fileURLWithPath: filePath) \n\n\/\/ Create an object to process the request.  \nlet requestHandler = VNImageRequestHandler(url: fileUrl) \n\n\/\/ Create a request to detect a body pose in 3D space. \nlet request = VNDetectHumanBodyPose3DRequest() \n\ndo {    \n    \/\/ Perform the body pose request.    \n    try requestHandler.perform([request])    \n\n    \/\/ Get the observation.    \n    if let observation = request.results?.first {        \n        \/\/ Handle the observation.    \n    }\n} catch {\n    print(\"Unable to perform the request: \\(error).\") \n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Get a recognized joint by using a joint name. \nlet leftShoulder = try observation.recognizedPoint(.leftShoulder) \nlet leftElbow = try observation.recognizedPoint(.leftElbow) \nlet leftWrist = try observation.recognizedPoint(.leftWrist) \n\n\/\/ Get a collection of joints by using a joint group name. \nlet leftArm = try observation.recognizedPoints(.leftArm)",
      "language" : "swift"
    },
    {
      "code" : "import simd \n\nvar angleVector: simd_float3 = simd_float3() \n\n\/\/ Get the position relative to the parent shoulder joint. \nlet childPosition = leftElbow.localPosition \nlet translationChild = simd_make_float3(childPosition.columns.3[0], \n                                        childPosition.columns.3[1], \n                                        childPosition.columns.3[2]) \n\n\/\/ The rotation around the x-axis. \nlet pitch = (Float.pi \/ 2) \n\n\/\/ The rotation around the y-axis. \nlet yaw = acos(translationChild.z \/ simd_length(translationChild)) \n\n\/\/ The rotation around the z-axis. \nlet roll = atan2((translationChild.y), (translationChild.x)) \n\n\/\/ The angle between the elbow and shoulder joint. \nangleVector = simd_float3(pitch, yaw, roll)",
      "language" : "swift"
    }
  ],
  "contentHash" : "fc8d52e1cea573f53dd19d6306acd8456a28b3d651759cb2a3f4e5ef9f590e42",
  "crawledAt" : "2025-12-02T16:20:51Z",
  "id" : "656C2090-D778-4FFE-98E6-C1C8EC8FE466",
  "kind" : "article",
  "language" : "swift",
  "module" : "Vision",
  "overview" : "## Overview\n\nVision allows you to take your app’s body pose detection into the third dimension. All photos — with people in them — are a 2D representation of people in a 3D world. Starting in iOS 17 and macOS 14, Vision detects human body poses and measures 17 individual joint locations in 3D space. You access a joint location using the joint name itself, or with a joint group name that returns a collection of joints. The following illustration of a 3D model identifies the 17 joint locations that Vision detects.\n\n\n\nImages you capture in Portrait mode using Camera — or using [doc:\/\/com.apple.documentation\/documentation\/AVFoundation] — contain depth data that helps to detect distance for each pixel. If your content contains depth data, Vision fetches it automatically. With Vision, you can build an app that tracks a person performing an exercise in 3D space, follows the arm movement during a golf swing, or captures character animations for a video game.\n\nFor more information about recognizing a body pose in 2D, see [doc:\/\/Vision\/documentation\/Vision\/detecting-human-body-poses-in-images].\n\n### Perform a body pose request\n\nDetecting the position of the joints on a human body in 3D space begins with [doc:\/\/Vision\/documentation\/Vision\/VNDetectHumanBodyPose3DRequest]. The request only supports the detection and results for the most prominent person in the frame, so it generates a single observation. For example, performing a request on an image with three people — with one person being closer to the camera — returns an observation that detects the person closest to the camera.\n\nThe observation contains a collection of 17 [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyRecognizedPoint3D] objects that contain the 3D position of a joint you specify, and the parent joint it connects to. The framework doesn’t return a partial list of joints, so you get all 17 joints or none.\n\nThe request doesn’t require images with depth data to run. However, providing depth data improves detection accuracy.\n\n### Handle the resulting observation\n\nA [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyPose3DObservation] provides body pose information in 3D space, in meters. The framework normalizes 2D points — from other framework requests — to a lower-left origin. Points that a 3D body pose request returns are relative to the scene in the real world, with an origin at the root joint, located between the [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyPose3DObservation\/JointName\/leftHip] and [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyPose3DObservation\/JointName\/rightHip].\n\nTo get a list of the available joint names, call [doc:\/\/Vision\/documentation\/Vision\/VNDetectHumanBodyPose3DRequest\/supportedJointNames]. Joints are also grouped by their location on the body. For example, the left arm group provides the left shoulder, elbow and wrist joints. To get a list of the group names, call [doc:\/\/Vision\/documentation\/Vision\/VNDetectHumanBodyPose3DRequest\/supportedJointsGroupNames].\n\nIf there’s enough depth metadata, [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyPose3DObservation\/bodyHeight] provides an estimated height of the subject, in meters; otherwise, [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyPose3DObservation\/bodyHeight] returns a reference height of `1.8` meters. The framework provides a measured height only when configuring an [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureSession] to use the LiDAR camera. For more information about configuring your session, see [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/capturing-depth-using-the-lidar-camera].\n\n\n\nUse [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyPose3DObservation\/cameraRelativePosition(_:)] to get an estimate of how far the person was away from a camera. To get an accurate understanding of where the camera was when capturing the image, use [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyPose3DObservation\/cameraOriginMatrix].\n\n### Work with positions in 3D space\n\nThe Vision framework represents a 3D position as a [doc:\/\/com.apple.documentation\/documentation\/simd\/simd_float4x4] matrix. A [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyRecognizedPoint3D] object contains the position of the joint, in meters, along with an identifier that corresponds to the joint name. It also contains the [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyRecognizedPoint3D\/localPosition] of the joint, which describes the position relative to a [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyRecognizedPoint3D\/parentJoint].\n\nUse [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyPose3DObservation\/pointInImage(_:)] to project a 3D joint coordinate back to a 2D input image for the body joint you specify. For instance, if you want to align the 3D body pose with the 2D input image, use [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyPose3DObservation\/pointInImage(_:)] to get the root joint position in the input image.\n\nA [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyRecognizedPoint3D\/localPosition] is useful if your app is only working with one area of the body, and simplifies determining the angle between a child and parent joint.\n\nFor more information about matrices, see [doc:\/\/com.apple.documentation\/documentation\/Accelerate\/working-with-matrices].\n\n### Use depth data as input\n\nDepth data contains the information the system needs to reconstruct a 3D scene, and includes camera calibration data. [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVDepthData] serves as the container class for interfacing with depth metadata. Starting in iOS 17 and macOS 14, provide depth data when you initialize a [doc:\/\/Vision\/documentation\/Vision\/VNImageRequestHandler] with sample or pixel buffers.\n\nImages you capture using Photo mode in Camera store depth as disparity maps — a 2D map reduced from 3D space — with calibration data. You can also configure your [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureSession] to use LiDAR to get depth data. For more information about working with depth, see [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/capturing-photos-with-depth] and [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/enhancing-live-video-by-leveraging-truedepth-camera-data].",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/Vision\/identifying-3d-human-body-poses-in-images\ncrawled: 2025-12-02T16:20:51Z\n---\n\n# Identifying 3D human body poses in images\n\n**Article**\n\nDetect three-dimensional human body poses using the Vision framework.\n\n## Overview\n\nVision allows you to take your app’s body pose detection into the third dimension. All photos — with people in them — are a 2D representation of people in a 3D world. Starting in iOS 17 and macOS 14, Vision detects human body poses and measures 17 individual joint locations in 3D space. You access a joint location using the joint name itself, or with a joint group name that returns a collection of joints. The following illustration of a 3D model identifies the 17 joint locations that Vision detects.\n\n\n\nImages you capture in Portrait mode using Camera — or using [doc:\/\/com.apple.documentation\/documentation\/AVFoundation] — contain depth data that helps to detect distance for each pixel. If your content contains depth data, Vision fetches it automatically. With Vision, you can build an app that tracks a person performing an exercise in 3D space, follows the arm movement during a golf swing, or captures character animations for a video game.\n\nFor more information about recognizing a body pose in 2D, see [doc:\/\/Vision\/documentation\/Vision\/detecting-human-body-poses-in-images].\n\n### Perform a body pose request\n\nDetecting the position of the joints on a human body in 3D space begins with [doc:\/\/Vision\/documentation\/Vision\/VNDetectHumanBodyPose3DRequest]. The request only supports the detection and results for the most prominent person in the frame, so it generates a single observation. For example, performing a request on an image with three people — with one person being closer to the camera — returns an observation that detects the person closest to the camera.\n\nThe observation contains a collection of 17 [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyRecognizedPoint3D] objects that contain the 3D position of a joint you specify, and the parent joint it connects to. The framework doesn’t return a partial list of joints, so you get all 17 joints or none.\n\nThe request doesn’t require images with depth data to run. However, providing depth data improves detection accuracy.\n\n```swift\nimport Vision\n\n\/\/ Get an image from the project bundle. \nguard let filePath = Bundle.main.path(forResource: \"bodypose\", ofType: \"heic\") else { \n    return \n} \nlet fileUrl = URL(fileURLWithPath: filePath) \n\n\/\/ Create an object to process the request.  \nlet requestHandler = VNImageRequestHandler(url: fileUrl) \n\n\/\/ Create a request to detect a body pose in 3D space. \nlet request = VNDetectHumanBodyPose3DRequest() \n\ndo {    \n    \/\/ Perform the body pose request.    \n    try requestHandler.perform([request])    \n\n    \/\/ Get the observation.    \n    if let observation = request.results?.first {        \n        \/\/ Handle the observation.    \n    }\n} catch {\n    print(\"Unable to perform the request: \\(error).\") \n}\n```\n\n### Handle the resulting observation\n\nA [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyPose3DObservation] provides body pose information in 3D space, in meters. The framework normalizes 2D points — from other framework requests — to a lower-left origin. Points that a 3D body pose request returns are relative to the scene in the real world, with an origin at the root joint, located between the [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyPose3DObservation\/JointName\/leftHip] and [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyPose3DObservation\/JointName\/rightHip].\n\nTo get a list of the available joint names, call [doc:\/\/Vision\/documentation\/Vision\/VNDetectHumanBodyPose3DRequest\/supportedJointNames]. Joints are also grouped by their location on the body. For example, the left arm group provides the left shoulder, elbow and wrist joints. To get a list of the group names, call [doc:\/\/Vision\/documentation\/Vision\/VNDetectHumanBodyPose3DRequest\/supportedJointsGroupNames].\n\n```swift\n\/\/ Get a recognized joint by using a joint name. \nlet leftShoulder = try observation.recognizedPoint(.leftShoulder) \nlet leftElbow = try observation.recognizedPoint(.leftElbow) \nlet leftWrist = try observation.recognizedPoint(.leftWrist) \n\n\/\/ Get a collection of joints by using a joint group name. \nlet leftArm = try observation.recognizedPoints(.leftArm)\n```\n\nIf there’s enough depth metadata, [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyPose3DObservation\/bodyHeight] provides an estimated height of the subject, in meters; otherwise, [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyPose3DObservation\/bodyHeight] returns a reference height of `1.8` meters. The framework provides a measured height only when configuring an [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureSession] to use the LiDAR camera. For more information about configuring your session, see [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/capturing-depth-using-the-lidar-camera].\n\n\n\nUse [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyPose3DObservation\/cameraRelativePosition(_:)] to get an estimate of how far the person was away from a camera. To get an accurate understanding of where the camera was when capturing the image, use [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyPose3DObservation\/cameraOriginMatrix].\n\n### Work with positions in 3D space\n\nThe Vision framework represents a 3D position as a [doc:\/\/com.apple.documentation\/documentation\/simd\/simd_float4x4] matrix. A [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyRecognizedPoint3D] object contains the position of the joint, in meters, along with an identifier that corresponds to the joint name. It also contains the [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyRecognizedPoint3D\/localPosition] of the joint, which describes the position relative to a [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyRecognizedPoint3D\/parentJoint].\n\n\n\nUse [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyPose3DObservation\/pointInImage(_:)] to project a 3D joint coordinate back to a 2D input image for the body joint you specify. For instance, if you want to align the 3D body pose with the 2D input image, use [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyPose3DObservation\/pointInImage(_:)] to get the root joint position in the input image.\n\nA [doc:\/\/Vision\/documentation\/Vision\/VNHumanBodyRecognizedPoint3D\/localPosition] is useful if your app is only working with one area of the body, and simplifies determining the angle between a child and parent joint.\n\n```swift\nimport simd \n\nvar angleVector: simd_float3 = simd_float3() \n\n\/\/ Get the position relative to the parent shoulder joint. \nlet childPosition = leftElbow.localPosition \nlet translationChild = simd_make_float3(childPosition.columns.3[0], \n                                        childPosition.columns.3[1], \n                                        childPosition.columns.3[2]) \n\n\/\/ The rotation around the x-axis. \nlet pitch = (Float.pi \/ 2) \n\n\/\/ The rotation around the y-axis. \nlet yaw = acos(translationChild.z \/ simd_length(translationChild)) \n\n\/\/ The rotation around the z-axis. \nlet roll = atan2((translationChild.y), (translationChild.x)) \n\n\/\/ The angle between the elbow and shoulder joint. \nangleVector = simd_float3(pitch, yaw, roll)\n```\n\nFor more information about matrices, see [doc:\/\/com.apple.documentation\/documentation\/Accelerate\/working-with-matrices].\n\n### Use depth data as input\n\nDepth data contains the information the system needs to reconstruct a 3D scene, and includes camera calibration data. [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVDepthData] serves as the container class for interfacing with depth metadata. Starting in iOS 17 and macOS 14, provide depth data when you initialize a [doc:\/\/Vision\/documentation\/Vision\/VNImageRequestHandler] with sample or pixel buffers.\n\nImages you capture using Photo mode in Camera store depth as disparity maps — a 2D map reduced from 3D space — with calibration data. You can also configure your [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureSession] to use LiDAR to get depth data. For more information about working with depth, see [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/capturing-photos-with-depth] and [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/enhancing-live-video-by-leveraging-truedepth-camera-data].\n\n## 3D body pose detection\n\n- **Detecting human body poses in 3D with Vision**: Render skeletons of 3D body pose points in a scene overlaying the input image.\n- **VNDetectHumanBodyPose3DRequest**: A request that detects points on human bodies in 3D space, relative to the camera.\n- **VNHumanBodyPose3DObservation**: An observation that provides the 3D body points the request recognizes.\n- **VNRecognizedPoints3DObservation**: An observation that provides the 3D points for a request.\n- **VNHumanBodyRecognizedPoint3D**: A recognized 3D point that includes a parent joint.\n- **VNPoint3D**: An object that represents a 3D point in an image.\n- **VNRecognizedPoint3D**: A 3D point that includes an identifier to the point.\n- **VNHumanBodyPose3DObservation.JointName**: The joint names for a 3D body pose.\n- **VNHumanBodyPose3DObservation.JointsGroupName**: The joint group names for a 3D body pose.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Render skeletons of 3D body pose points in a scene overlaying the input image.",
          "name" : "Detecting human body poses in 3D with Vision",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/detecting-human-body-poses-in-3d-with-vision"
        },
        {
          "description" : "A request that detects points on human bodies in 3D space, relative to the camera.",
          "name" : "VNDetectHumanBodyPose3DRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNDetectHumanBodyPose3DRequest"
        },
        {
          "description" : "An observation that provides the 3D body points the request recognizes.",
          "name" : "VNHumanBodyPose3DObservation",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNHumanBodyPose3DObservation"
        },
        {
          "description" : "An observation that provides the 3D points for a request.",
          "name" : "VNRecognizedPoints3DObservation",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNRecognizedPoints3DObservation"
        },
        {
          "description" : "A recognized 3D point that includes a parent joint.",
          "name" : "VNHumanBodyRecognizedPoint3D",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNHumanBodyRecognizedPoint3D"
        },
        {
          "description" : "An object that represents a 3D point in an image.",
          "name" : "VNPoint3D",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNPoint3D"
        },
        {
          "description" : "A 3D point that includes an identifier to the point.",
          "name" : "VNRecognizedPoint3D",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNRecognizedPoint3D"
        },
        {
          "description" : "The joint names for a 3D body pose.",
          "name" : "VNHumanBodyPose3DObservation.JointName",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNHumanBodyPose3DObservation\/JointName"
        },
        {
          "description" : "The joint group names for a 3D body pose.",
          "name" : "VNHumanBodyPose3DObservation.JointsGroupName",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNHumanBodyPose3DObservation\/JointsGroupName"
        }
      ],
      "title" : "3D body pose detection"
    }
  ],
  "source" : "appleJSON",
  "title" : "Identifying 3D human body poses in images",
  "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/identifying-3d-human-body-poses-in-images"
}