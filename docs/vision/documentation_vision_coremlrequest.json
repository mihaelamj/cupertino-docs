{
  "abstract" : "An image-analysis request that uses a Core ML model to process images.",
  "codeExamples" : [

  ],
  "conformsTo" : [
    "CustomStringConvertible",
    "Equatable",
    "Hashable",
    "ImageProcessingRequest",
    "Sendable",
    "SendableMetatype",
    "VisionRequest"
  ],
  "contentHash" : "18eb6d44a5dc736ca8e1ed69c1d2ceb5f432058952d8ed85eb0c26b90f224712",
  "crawledAt" : "2025-12-02T21:18:13Z",
  "declaration" : {
    "code" : "struct CoreMLRequest",
    "language" : "swift"
  },
  "id" : "A168AA3E-D8B0-44F0-92EA-286608E0C67F",
  "kind" : "struct",
  "language" : "swift",
  "module" : "Vision",
  "overview" : "## Overview\n\nThe results array of a [doc:\/\/com.apple.documentation\/documentation\/CoreML]-based image-analysis request contain a different observation type, depending on the kind of `MLModel` object you use:",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst",
    "macOS",
    "tvOS",
    "visionOS"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/Vision\/CoreMLRequest\ncrawled: 2025-12-02T21:18:13Z\n---\n\n# CoreMLRequest\n\n**Structure**\n\nAn image-analysis request that uses a Core ML model to process images.\n\n## Declaration\n\n```swift\nstruct CoreMLRequest\n```\n\n## Overview\n\nThe results array of a [doc:\/\/com.apple.documentation\/documentation\/CoreML]-based image-analysis request contain a different observation type, depending on the kind of `MLModel` object you use:\n\n- If the model predicts a single feature and the model’s [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLModelDescription] object has a non-`nil` value for [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLModelDescription\/predictedFeatureName], then Vision treats the model as a classifier. The results are [doc:\/\/Vision\/documentation\/Vision\/ClassificationObservation] objects.\n- If the model’s outputs include at least one output with a feature type of [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLFeatureType\/image], Vision treats that model as an image-to-image model. The results are [doc:\/\/Vision\/documentation\/Vision\/PixelBufferObservation] objects.\n- Otherwise, Vision treats the model as a general predictor model. The results are [doc:\/\/Vision\/documentation\/Vision\/CoreMLFeatureValueObservation] objects.\n\n\n\n## Creating a request\n\n- **init(model:_:)**: Creates a Core ML request.\n\n## Getting the revision\n\n- **revision**: The algorithm or implementation the request uses.\n- **supportedRevisions**: The collection of revisions the request supports.\n- **CoreMLRequest.Revision**: A type that describes the algorithm or implementation that the request performs.\n\n## Inspecting a request\n\n- **supportedIdentifiers**: The classification identifiers supported by the request.\n- **modelContainer**: The model to base the image analysis request on.\n- **CoreMLModelContainer**: A model container to use with an image-analysis request.\n- **ComputeStage**: Types that represent the compute stage.\n- **cropAndScaleAction**: An optional setting that tells the Vision algorithm how to scale an input image.\n- **ImageCropAndScaleAction**: A scale to apply to an input image before performing a request.\n\n## Performing a request\n\n- **perform(on:orientation:)**: Performs the request on an image URL and produces observations.\n- **perform(on:orientation:)**: Performs the request on image data and produces observations.\n- **perform(on:orientation:)**: Performs the request on a Core Graphics image and produces observations.\n- **perform(on:orientation:)**: Performs the request on a pixel buffer and produces observations.\n- **perform(on:orientation:)**: Performs the request on a Core Media buffer and produces observations.\n- **perform(on:orientation:)**: Performs the request on a Core Image image and produces observations.\n\n## Machine learning image analysis\n\n- **CoreMLFeatureValueObservation**: An object that represents a collection of key-value information that a Core ML image-analysis request produces.\n- **ClassificationObservation**: An object that represents classification information that an image-analysis request produces.\n- **PixelBufferObservation**: An object that represents an image that an image-analysis request produces.\n\n## Conforms To\n\n- CustomStringConvertible\n- Equatable\n- Hashable\n- ImageProcessingRequest\n- Sendable\n- SendableMetatype\n- VisionRequest\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Creates a Core ML request.",
          "name" : "init(model:_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/CoreMLRequest\/init(model:_:)"
        }
      ],
      "title" : "Creating a request"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "The algorithm or implementation the request uses.",
          "name" : "revision",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/CoreMLRequest\/revision-swift.property"
        },
        {
          "description" : "The collection of revisions the request supports.",
          "name" : "supportedRevisions",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/CoreMLRequest\/supportedRevisions"
        },
        {
          "description" : "A type that describes the algorithm or implementation that the request performs.",
          "name" : "CoreMLRequest.Revision",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/CoreMLRequest\/Revision-swift.enum"
        }
      ],
      "title" : "Getting the revision"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "The classification identifiers supported by the request.",
          "name" : "supportedIdentifiers",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/CoreMLRequest\/supportedIdentifiers"
        },
        {
          "description" : "The model to base the image analysis request on.",
          "name" : "modelContainer",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/CoreMLRequest\/modelContainer"
        },
        {
          "description" : "A model container to use with an image-analysis request.",
          "name" : "CoreMLModelContainer",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/CoreMLModelContainer"
        },
        {
          "description" : "Types that represent the compute stage.",
          "name" : "ComputeStage",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/ComputeStage"
        },
        {
          "description" : "An optional setting that tells the Vision algorithm how to scale an input image.",
          "name" : "cropAndScaleAction",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/CoreMLRequest\/cropAndScaleAction"
        },
        {
          "description" : "A scale to apply to an input image before performing a request.",
          "name" : "ImageCropAndScaleAction",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/ImageCropAndScaleAction"
        }
      ],
      "title" : "Inspecting a request"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "Performs the request on an image URL and produces observations.",
          "name" : "perform(on:orientation:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/ImageProcessingRequest\/perform(on:orientation:)-80bya"
        },
        {
          "description" : "Performs the request on image data and produces observations.",
          "name" : "perform(on:orientation:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/ImageProcessingRequest\/perform(on:orientation:)-3f3f1"
        },
        {
          "description" : "Performs the request on a Core Graphics image and produces observations.",
          "name" : "perform(on:orientation:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/ImageProcessingRequest\/perform(on:orientation:)-qxxx"
        },
        {
          "description" : "Performs the request on a pixel buffer and produces observations.",
          "name" : "perform(on:orientation:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/ImageProcessingRequest\/perform(on:orientation:)-xspx"
        },
        {
          "description" : "Performs the request on a Core Media buffer and produces observations.",
          "name" : "perform(on:orientation:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/ImageProcessingRequest\/perform(on:orientation:)-3hddl"
        },
        {
          "description" : "Performs the request on a Core Image image and produces observations.",
          "name" : "perform(on:orientation:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/ImageProcessingRequest\/perform(on:orientation:)-85ex1"
        }
      ],
      "title" : "Performing a request"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "An object that represents a collection of key-value information that a Core ML image-analysis request produces.",
          "name" : "CoreMLFeatureValueObservation",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/CoreMLFeatureValueObservation"
        },
        {
          "description" : "An object that represents classification information that an image-analysis request produces.",
          "name" : "ClassificationObservation",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/ClassificationObservation"
        },
        {
          "description" : "An object that represents an image that an image-analysis request produces.",
          "name" : "PixelBufferObservation",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/PixelBufferObservation"
        }
      ],
      "title" : "Machine learning image analysis"
    }
  ],
  "source" : "appleJSON",
  "title" : "CoreMLRequest",
  "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/CoreMLRequest"
}