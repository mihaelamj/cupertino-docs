{
  "abstract" : "Compare face-capture quality in a set of images by using Vision.",
  "codeExamples" : [
    {
      "code" : "let faceDetectionRequest = VNDetectFaceCaptureQualityRequest()\ndo {\n    try handler.perform([faceDetectionRequest])\n    guard let faceObservations = faceDetectionRequest.results as? [VNFaceObservation] else {\n        return\n    }\n    displayFaceObservations(faceObservations)\n    if isCapturingFaces {\n        saveFaceObservations(faceObservations, in: pixelBuffer)\n    }\n} catch {\n    print(\"Vision error: \\(error.localizedDescription)\")\n}",
      "language" : "swift"
    },
    {
      "code" : "let faceDetectionRequest = VNDetectFaceCaptureQualityRequest()\ndo {\n    try handler.perform([faceDetectionRequest])\n    guard let faceObservations = faceDetectionRequest.results as? [VNFaceObservation] else {\n        return\n    }\n    displayFaceObservations(faceObservations)\n    if isCapturingFaces {\n        saveFaceObservations(faceObservations, in: pixelBuffer)\n    }\n} catch {\n    print(\"Vision error: \\(error.localizedDescription)\")\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Sort faces in descending quality-score order.\nsavedFaces.sort { $0.qualityScore < $1.qualityScore }",
      "language" : "swift"
    },
    {
      "code" : "let savedFace = savedFaces[indexPath.item]\nlet faceImage = UIImage(contentsOfFile: savedFace.url.path)\ncell.imageView.image = faceImage\ncell.label.text = \"\\(savedFace.qualityScore)\"",
      "language" : "swift"
    }
  ],
  "contentHash" : "788b4e3e81c65716ebe29dbc92fa8e9277ba7139eb660c39ab23ce82682f7444",
  "crawledAt" : "2025-12-04T01:02:46Z",
  "id" : "8C70F5B5-0673-4BB9-8D4F-1DD9CACE5871",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "Vision",
  "overview" : "## Overview\n\nNew in iOS 13, the Vision framework adds the Face Capture Quality metric to represent the capture quality of a given face in a photo. The sample app shows you how to use this metric to evaluate a collection of images of the same person and identify which one has the best capture quality.\n\nFace Capture Quality is a holistic measure that considers scene lighting, blur, occlusion, expression, pose, focus, and more. It provides a score you can use to rank multiple captures of the same person. The pre-trained underlying models score a capture lower if, for example, the image contains low light or bad focus, or if the person has a negative expression. These scores are floating-point numbers normalized between `0.0` and `1.0`.\n\nFirst the app creates and performs a `VNDetectFaceCaptureQualityRequest` and obtains face observations from the results:\n\nThen the app passes the face observations to `saveFaceObservations(_:in:)`, where it retrieves the `faceCaptureQuality` score for each capture. When the user presses down on the capture button, the app saves each capture’s image data along with its quality score:\n\nNext, the app sorts the captures based on quality score:\n\nFinally, the app displays the saved faces with their quality scores:\n\n### Configure the sample code project\n\nTo run this sample app, you need the following:\n\nConnect the iPhone to the Mac over USB. The first time you run this sample app, the system prompts you to grant the app access to the camera. You must allow the sample app to access the camera for it to function correctly.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/vision\/selecting-a-selfie-based-on-capture-quality\ncrawled: 2025-12-04T01:02:46Z\n---\n\n# Selecting a selfie based on capture quality\n\n**Sample Code**\n\nCompare face-capture quality in a set of images by using Vision.\n\n## Overview\n\nNew in iOS 13, the Vision framework adds the Face Capture Quality metric to represent the capture quality of a given face in a photo. The sample app shows you how to use this metric to evaluate a collection of images of the same person and identify which one has the best capture quality.\n\nFace Capture Quality is a holistic measure that considers scene lighting, blur, occlusion, expression, pose, focus, and more. It provides a score you can use to rank multiple captures of the same person. The pre-trained underlying models score a capture lower if, for example, the image contains low light or bad focus, or if the person has a negative expression. These scores are floating-point numbers normalized between `0.0` and `1.0`.\n\nFirst the app creates and performs a `VNDetectFaceCaptureQualityRequest` and obtains face observations from the results:\n\n```swift\nlet faceDetectionRequest = VNDetectFaceCaptureQualityRequest()\ndo {\n    try handler.perform([faceDetectionRequest])\n    guard let faceObservations = faceDetectionRequest.results as? [VNFaceObservation] else {\n        return\n    }\n    displayFaceObservations(faceObservations)\n    if isCapturingFaces {\n        saveFaceObservations(faceObservations, in: pixelBuffer)\n    }\n} catch {\n    print(\"Vision error: \\(error.localizedDescription)\")\n}\n```\n\nThen the app passes the face observations to `saveFaceObservations(_:in:)`, where it retrieves the `faceCaptureQuality` score for each capture. When the user presses down on the capture button, the app saves each capture’s image data along with its quality score:\n\n```swift\nlet faceDetectionRequest = VNDetectFaceCaptureQualityRequest()\ndo {\n    try handler.perform([faceDetectionRequest])\n    guard let faceObservations = faceDetectionRequest.results as? [VNFaceObservation] else {\n        return\n    }\n    displayFaceObservations(faceObservations)\n    if isCapturingFaces {\n        saveFaceObservations(faceObservations, in: pixelBuffer)\n    }\n} catch {\n    print(\"Vision error: \\(error.localizedDescription)\")\n}\n```\n\nNext, the app sorts the captures based on quality score:\n\n```swift\n\/\/ Sort faces in descending quality-score order.\nsavedFaces.sort { $0.qualityScore < $1.qualityScore }\n```\n\nFinally, the app displays the saved faces with their quality scores:\n\n```swift\nlet savedFace = savedFaces[indexPath.item]\nlet faceImage = UIImage(contentsOfFile: savedFace.url.path)\ncell.imageView.image = faceImage\ncell.label.text = \"\\(savedFace.qualityScore)\"\n```\n\n### Configure the sample code project\n\nTo run this sample app, you need the following:\n\n- Xcode 11 or later\n- iPhone with iOS 13 or later\n\nConnect the iPhone to the Mac over USB. The first time you run this sample app, the system prompts you to grant the app access to the camera. You must allow the sample app to access the camera for it to function correctly.\n\n\n\n## Face and body detection\n\n- **VNDetectFaceCaptureQualityRequest**: A request that produces a floating-point number that represents the capture quality of a face in a photo.\n- **VNDetectFaceLandmarksRequest**: An image-analysis request that finds facial features like eyes and mouth in an image.\n- **VNDetectFaceRectanglesRequest**: A request that finds faces within an image.\n- **VNDetectHumanRectanglesRequest**: A request that finds rectangular regions that contain people in an image.\n- **VNHumanObservation**: An object that represents a person that the request detects.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "A request that produces a floating-point number that represents the capture quality of a face in a photo.",
          "name" : "VNDetectFaceCaptureQualityRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNDetectFaceCaptureQualityRequest"
        },
        {
          "description" : "An image-analysis request that finds facial features like eyes and mouth in an image.",
          "name" : "VNDetectFaceLandmarksRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNDetectFaceLandmarksRequest"
        },
        {
          "description" : "A request that finds faces within an image.",
          "name" : "VNDetectFaceRectanglesRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNDetectFaceRectanglesRequest"
        },
        {
          "description" : "A request that finds rectangular regions that contain people in an image.",
          "name" : "VNDetectHumanRectanglesRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNDetectHumanRectanglesRequest"
        },
        {
          "description" : "An object that represents a person that the request detects.",
          "name" : "VNHumanObservation",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNHumanObservation"
        }
      ],
      "title" : "Face and body detection"
    }
  ],
  "source" : "appleJSON",
  "title" : "Selecting a selfie based on capture quality",
  "url" : "https:\/\/developer.apple.com\/documentation\/vision\/selecting-a-selfie-based-on-capture-quality"
}