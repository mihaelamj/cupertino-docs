{
  "abstract" : "Identify the trajectory of a thrown object by using Vision.",
  "codeExamples" : [
    {
      "code" : "let visionHandler = VNImageRequestHandler(cmSampleBuffer: buffer,\n                                          orientation: orientation,\n                                          options: [:])",
      "language" : "swift"
    },
    {
      "code" : "detectTrajectoryRequest = VNDetectTrajectoriesRequest(frameAnalysisSpacing: CMTime(value: 10, timescale: 600),\n                                                      trajectoryLength: 6) { [weak self] (request: VNRequest, error: Error?) -> Void in\n    \n    guard let results = request.results as? [VNTrajectoryObservation] else {\n        return\n    }\n    \n    DispatchQueue.main.async {\n        self?.processTrajectoryObservation(results: results)\n    }\n    \n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Following optional bounds by checking for the moving average radius\n\/\/ of the trajectories the app is looking for.\ndetectTrajectoryRequest.objectMinimumNormalizedRadius = 10.0 \/ Float(1920.0)\ndetectTrajectoryRequest.objectMaximumNormalizedRadius = 30.0 \/ Float(1920.0)\n\n\/\/ Help manage the real-time use case to improve the precision versus delay tradeoff.\ndetectTrajectoryRequest.targetFrameTime = CMTimeMake(value: 1, timescale: 60)\n\n\/\/ The region of interest where the object is moving in the normalized image space.\ndetectTrajectoryRequest.regionOfInterest = normalizedFrame",
      "language" : "swift"
    },
    {
      "code" : "for trajectory in results {\n    \/\/ Filter the trajectory.\n    if filterParabola(trajectory: trajectory) {\n        \/\/ Verify and correct an incomplete path.\n        trajectoryView.points = correctTrajectoryPath(trajectoryToCorrect: trajectory)\n        \n        \/\/ Display a transition.\n        trajectoryView.performTransition(.fadeIn, duration: 0.05)\n        \n        \/\/ Determine the size of the moving object that the app tracks.\n        print(\"The object's moving average radius: \\(trajectory.movingAverageRadius)\")\n    }\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "61cc993417d9a965606c2c9438dab571611cc0d16ecf1d8b90734f7470f7b204",
  "crawledAt" : "2025-12-04T02:34:51Z",
  "id" : "7760DBB2-77CD-4D0F-A1A3-259C0C8DC230",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "Vision",
  "overview" : "## Overview\n\nThe Vision framework provides the ability to detect trajectories of objects in a video. The algorithm looks at the differences between frames of a video and identifies objects that travel along a parabolic path. A single object — like a bouncing ball — may produce multiple trajectories.\n\nThe sample code project shows you how to configure a trajectory detection request to analyze sample buffers. It explores how to set up a capture session to analyze a live-capture feed, and an asset reader to analyze a prerecorded video. When the sample app detects a trajectory, it illustrates it by displaying the detected path on the screen.\n\nFor more information about identifying trajectories, see [https:\/\/developer.apple.com\/documentation\/vision\/identifying-trajectories-in-video].\n\n### Configure the project and prepare your environment\n\nYou must run the sample code project on a physical device with an A12 processor or later.\n\nThe sample’s current configuration looks for a small object moving left to right in a video. Try the sample app by downloading and analyzing [https:\/\/developer.apple.com\/sample-code\/ml\/sample.mov] of a person tossing a bean bag. For live capture, the sample app requires a stable scene with a fixed camera position and stationary background, so mount your iOS device to a tripod and keep it fixed on the field of view. You need to modify the trajectory request’s configuration based on your conditions when you use your own video.\n\n### Create a trajectory request\n\nBefore the sample app creates a trajectory request, it gets sample buffers based on the selected source — live capture or a prerecorded video — in `CameraViewController`. After the [https:\/\/developer.apple.com\/documentation\/avfoundation\/avcapturesession] or [https:\/\/developer.apple.com\/documentation\/avfoundation\/avassetreader] retrieves a sample buffer, it passes to the `ContentAnalysisViewController`. The sample app creates a [https:\/\/developer.apple.com\/documentation\/vision\/vnimagerequesthandler] to perform the trajectory request with.\n\nThe sample app sets up a [https:\/\/developer.apple.com\/documentation\/vision\/vndetecttrajectoriesrequest] object to define what the sample app looks for. It looks for trajectories after `1\/60` second of video, and returns trajectories with a length of `6` or greater. Generally, developers use a shorter length for real-time apps, and longer lengths to observe finer and longer curves.\n\nAfter the sample app creates the `VNDetectTrajectoriesRequest`, it configures additional options that describe the radius of the object it’s looking for. To improve the efficiency of the analysis, it also specifies the region of interest.\n\nAfter the sample app configures the trajectory request for the buffer, it processes the list of [https:\/\/developer.apple.com\/documentation\/vision\/vntrajectoryobservation] results.\n\n### Process the trajectory observation results\n\nThe sample app configuration targets the prerecorded video from the configuration section. The `VNDetectTrajectoriesRequest` follows objects moving on a parabolic path, and requires more than a single data point (trajectory length). After a request gathers enough data points to recognize the trajectory — a length of at least 5 — it passes the observation results that contain the trajectory information.\n\nThe first step in processing the results involves filtering the `VNTrajectoryObservation` based on the following conditions:\n\nWhen the results meet the above conditions, the sample app deems the observation a valid trajectory. The sample app confirms the trajectory and makes any necessary correction to the path. If a left-to-right moving trajectory begins too far from a fixed region, the sample extrapolates it back to the region by using the available quadratic equation coefficients.\n\nThe sample app displays valid trajectories on the screen with particle effects by using [https:\/\/developer.apple.com\/documentation\/spritekit].",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/vision\/detecting-moving-objects-in-a-video\ncrawled: 2025-12-04T02:34:51Z\n---\n\n# Detecting moving objects in a video\n\n**Sample Code**\n\nIdentify the trajectory of a thrown object by using Vision.\n\n## Overview\n\nThe Vision framework provides the ability to detect trajectories of objects in a video. The algorithm looks at the differences between frames of a video and identifies objects that travel along a parabolic path. A single object — like a bouncing ball — may produce multiple trajectories.\n\nThe sample code project shows you how to configure a trajectory detection request to analyze sample buffers. It explores how to set up a capture session to analyze a live-capture feed, and an asset reader to analyze a prerecorded video. When the sample app detects a trajectory, it illustrates it by displaying the detected path on the screen.\n\nFor more information about identifying trajectories, see [https:\/\/developer.apple.com\/documentation\/vision\/identifying-trajectories-in-video].\n\n### Configure the project and prepare your environment\n\nYou must run the sample code project on a physical device with an A12 processor or later.\n\nThe sample’s current configuration looks for a small object moving left to right in a video. Try the sample app by downloading and analyzing [https:\/\/developer.apple.com\/sample-code\/ml\/sample.mov] of a person tossing a bean bag. For live capture, the sample app requires a stable scene with a fixed camera position and stationary background, so mount your iOS device to a tripod and keep it fixed on the field of view. You need to modify the trajectory request’s configuration based on your conditions when you use your own video.\n\n### Create a trajectory request\n\nBefore the sample app creates a trajectory request, it gets sample buffers based on the selected source — live capture or a prerecorded video — in `CameraViewController`. After the [https:\/\/developer.apple.com\/documentation\/avfoundation\/avcapturesession] or [https:\/\/developer.apple.com\/documentation\/avfoundation\/avassetreader] retrieves a sample buffer, it passes to the `ContentAnalysisViewController`. The sample app creates a [https:\/\/developer.apple.com\/documentation\/vision\/vnimagerequesthandler] to perform the trajectory request with.\n\n```swift\nlet visionHandler = VNImageRequestHandler(cmSampleBuffer: buffer,\n                                          orientation: orientation,\n                                          options: [:])\n```\n\nThe sample app sets up a [https:\/\/developer.apple.com\/documentation\/vision\/vndetecttrajectoriesrequest] object to define what the sample app looks for. It looks for trajectories after `1\/60` second of video, and returns trajectories with a length of `6` or greater. Generally, developers use a shorter length for real-time apps, and longer lengths to observe finer and longer curves.\n\n```swift\ndetectTrajectoryRequest = VNDetectTrajectoriesRequest(frameAnalysisSpacing: CMTime(value: 10, timescale: 600),\n                                                      trajectoryLength: 6) { [weak self] (request: VNRequest, error: Error?) -> Void in\n    \n    guard let results = request.results as? [VNTrajectoryObservation] else {\n        return\n    }\n    \n    DispatchQueue.main.async {\n        self?.processTrajectoryObservation(results: results)\n    }\n    \n}\n```\n\nAfter the sample app creates the `VNDetectTrajectoriesRequest`, it configures additional options that describe the radius of the object it’s looking for. To improve the efficiency of the analysis, it also specifies the region of interest.\n\n```swift\n\/\/ Following optional bounds by checking for the moving average radius\n\/\/ of the trajectories the app is looking for.\ndetectTrajectoryRequest.objectMinimumNormalizedRadius = 10.0 \/ Float(1920.0)\ndetectTrajectoryRequest.objectMaximumNormalizedRadius = 30.0 \/ Float(1920.0)\n\n\/\/ Help manage the real-time use case to improve the precision versus delay tradeoff.\ndetectTrajectoryRequest.targetFrameTime = CMTimeMake(value: 1, timescale: 60)\n\n\/\/ The region of interest where the object is moving in the normalized image space.\ndetectTrajectoryRequest.regionOfInterest = normalizedFrame\n```\n\nAfter the sample app configures the trajectory request for the buffer, it processes the list of [https:\/\/developer.apple.com\/documentation\/vision\/vntrajectoryobservation] results.\n\n### Process the trajectory observation results\n\nThe sample app configuration targets the prerecorded video from the configuration section. The `VNDetectTrajectoriesRequest` follows objects moving on a parabolic path, and requires more than a single data point (trajectory length). After a request gathers enough data points to recognize the trajectory — a length of at least 5 — it passes the observation results that contain the trajectory information.\n\nThe first step in processing the results involves filtering the `VNTrajectoryObservation` based on the following conditions:\n\n- The trajectory moves from left to right.\n- The trajectory starts in the first half of the region of interest.\n- The trajectory length increases to `8`, which indicates a throw instead of smaller movements.\n- The trajectory contains a parabolic equation constant `a`, less than or equal to `0`, and implies there are either straight lines or downward-facing lines.\n- The trajectory confidence is greater than `0.9`.\n\nWhen the results meet the above conditions, the sample app deems the observation a valid trajectory. The sample app confirms the trajectory and makes any necessary correction to the path. If a left-to-right moving trajectory begins too far from a fixed region, the sample extrapolates it back to the region by using the available quadratic equation coefficients.\n\n```swift\nfor trajectory in results {\n    \/\/ Filter the trajectory.\n    if filterParabola(trajectory: trajectory) {\n        \/\/ Verify and correct an incomplete path.\n        trajectoryView.points = correctTrajectoryPath(trajectoryToCorrect: trajectory)\n        \n        \/\/ Display a transition.\n        trajectoryView.performTransition(.fadeIn, duration: 0.05)\n        \n        \/\/ Determine the size of the moving object that the app tracks.\n        print(\"The object's moving average radius: \\(trajectory.movingAverageRadius)\")\n    }\n}\n```\n\nThe sample app displays valid trajectories on the screen with particle effects by using [https:\/\/developer.apple.com\/documentation\/spritekit].\n\n## Trajectory detection\n\n- **Identifying Trajectories in Video**: Gain new insights into your video data by using Vision to detect trajectories.\n- **VNDetectTrajectoriesRequest**: A request that detects the trajectories of shapes moving along a parabolic path.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Gain new insights into your video data by using Vision to detect trajectories.",
          "name" : "Identifying Trajectories in Video",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/identifying-trajectories-in-video"
        },
        {
          "description" : "A request that detects the trajectories of shapes moving along a parabolic path.",
          "name" : "VNDetectTrajectoriesRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNDetectTrajectoriesRequest"
        }
      ],
      "title" : "Trajectory detection"
    }
  ],
  "source" : "appleJSON",
  "title" : "Detecting moving objects in a video",
  "url" : "https:\/\/developer.apple.com\/documentation\/vision\/detecting-moving-objects-in-a-video"
}