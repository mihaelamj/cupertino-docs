{
  "abstract" : "Train a flower classifier using Create ML in Swift Playgrounds, and apply the resulting model to real-time image classification using Vision.",
  "codeExamples" : [
    {
      "code" : "let registrationRequest = VNTranslationalImageRegistrationRequest(targetedCVPixelBuffer: pixelBuffer)",
      "language" : "swift"
    },
    {
      "code" : "fileprivate func sceneStabilityAchieved() -> Bool {\n    \/\/ Determine if we have enough evidence of stability.\n    if transpositionHistoryPoints.count == maximumHistoryLength {\n        \/\/ Calculate the moving average.\n        var movingAverage: CGPoint = CGPoint.zero\n        for currentPoint in transpositionHistoryPoints {\n            movingAverage.x += currentPoint.x\n            movingAverage.y += currentPoint.y\n        }\n        let distance = abs(movingAverage.x) + abs(movingAverage.y)\n        if distance < 20 {\n            return true\n        }\n    }\n    return false\n}",
      "language" : "swift"
    },
    {
      "code" : "if self.sceneStabilityAchieved() {\n    showDetectionOverlay(true)\n    if currentlyAnalyzedPixelBuffer == nil {\n        \/\/ Retain the image buffer for Vision processing.\n        currentlyAnalyzedPixelBuffer = pixelBuffer\n        analyzeCurrentImage()\n    }\n} else {\n    showDetectionOverlay(false)\n}",
      "language" : "swift"
    },
    {
      "code" : "let barcodeDetection = VNDetectBarcodesRequest(completionHandler: { (request, error) in\n    if let results = request.results as? [VNBarcodeObservation] {\n        if let mainBarcode = results.first {\n            if let payloadString = mainBarcode.payloadStringValue {\n                self.showProductInfo(payloadString)\n            }\n        }\n    }\n})\nself.analysisRequests = ([barcodeDetection])\n\n\/\/ Setup a classification request.\nguard let modelURL = Bundle.main.url(forResource: \"FlowerShop\", withExtension: \"mlmodelc\") else {\n    return NSError(domain: \"VisionViewController\", code: -1, userInfo: [NSLocalizedDescriptionKey: \"The model file is missing.\"])\n}\nguard let objectRecognition = createClassificationRequest(modelURL: modelURL) else {\n    return NSError(domain: \"VisionViewController\", code: -1, userInfo: [NSLocalizedDescriptionKey: \"The classification request failed.\"])\n}\nself.analysisRequests.append(objectRecognition)",
      "language" : "swift"
    },
    {
      "code" : "let objectClassifier = try VNCoreMLModel(for: MLModel(contentsOf: modelURL))\nlet classificationRequest = VNCoreMLRequest(model: objectClassifier, completionHandler: { (request, error) in",
      "language" : "swift"
    },
    {
      "code" : "private func analyzeCurrentImage() {\n    \/\/ Most computer vision tasks are not rotation-agnostic, so it is important to pass in the orientation of the image with respect to device.\n    let orientation = exifOrientationFromDeviceOrientation()\n    \n    let requestHandler = VNImageRequestHandler(cvPixelBuffer: currentlyAnalyzedPixelBuffer!, orientation: orientation)\n    visionQueue.async {\n        do {\n            \/\/ Release the pixel buffer when done, allowing the next buffer to be processed.\n            defer { self.currentlyAnalyzedPixelBuffer = nil }\n            try requestHandler.perform(self.analysisRequests)\n        } catch {\n            print(\"Error: Vision request failed with error \\\"\\(error)\\\"\")\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "private let visionQueue = DispatchQueue(label: \"com.example.apple-samplecode.FlowerShop.serialVisionQueue\")",
      "language" : "swift"
    },
    {
      "code" : "if let results = request.results as? [VNClassificationObservation] {\n    print(\"\\(results.first!.identifier) : \\(results.first!.confidence)\")\n    if results.first!.confidence > 0.9 {\n        self.showProductInfo(results.first!.identifier)\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "self.currentlyAnalyzedPixelBuffer = nil",
      "language" : "swift"
    }
  ],
  "contentHash" : "8ab037b3a3142dbcc7f4045ef32e4d802eb30ec4470ebb3486f4b647787e7049",
  "crawledAt" : "2025-12-04T02:35:15Z",
  "id" : "A573E326-BF07-44E1-B907-8CA72159E8F4",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "Vision",
  "overview" : "## Overview\n\nTo classify images in real time, you need a classification model with the categories you’d like identified, and a way to capture images to feed to the classifier.\n\n\n\nThis sample code project contains two components: a Create ML model you train in Swift Playgrounds, and the iOS app, FlowerShop, which you use to classify different flower types. This project uses the same code as the robot shop demo in the WWDC 2018 session [https:\/\/developer.apple.com\/videos\/play\/wwdc2018\/717\/].\n\n### Getting Started\n\nTo see Create ML in action, run the Swift Playground `Training\/ImageClassifierPlayground.playground` in Xcode 10 on a Mac running macOS 10.14 or later. This Swift Playground performs classification on the training set and generates the Create ML model `ImageClassifier.mlmodel`.\n\nThe sample app, FlowerShop, requires the following:\n\nBecause Xcode can’t access the camera, FlowerShop won’t work in Simulator.\n\n### Train a Custom Classifier on an Organized Image Set\n\nThe sample images that Create ML uses to train the custom classifier were taken with a set of categories in mind. For your own app, decide on a set of classification labels before preparing images. Take the following measures to improve your data set:\n\nIf you’re using photos taken on an iOS device to train your model, you can use the macOS utility, Image Capture, to import the images onto your computer, and do the following:\n\nFor more information about configuring the resultant model, as well as screenshots of the Create ML UI, see [https:\/\/developer.apple.com\/documentation\/createml\/creating-an-image-classifier-model].\n\nCreate ML exports its trained results as a `.mlmodel` file, which you can import into your app in Xcode. After importing the model, you can examine  the prototypical image size by opening the model file in Xcode’s navigation menu. For example, a parameter such as “Color 299 x 299” indicates the size of the training image. You can also confirm the size of the model.\n\n### Build an iOS App Around the Classifier\n\nThe app leverages the trained model and uses Vision for both registration and classification:\n\n### Use Registration for Scene Stability\n\nRegistration takes and aligns two images to determine the relative difference. Vision’s registration operation uses an inexpensive, fast algorithm that tells the app if the subject is still and stable. Theoretically, the app could make a classification request on every frame buffer, but classification is a computationally expensive operation—so attempting to classify every frame could result in delays and poor performance with the UI. Classify the scene in a frame only if the registration algorithm determines that the scene and camera are still, indicating the user’s intent to classify an object.\n\nThe FlowerShop app uses [https:\/\/developer.apple.com\/documentation\/vision\/vnsequencerequesthandler] with [https:\/\/developer.apple.com\/documentation\/vision\/vntranslationalimageregistrationrequest] objects to compare consecutive frames, keeping a history of 15 frames. This amount of history amounts to half a second of capture at 30 frames per second and carries no special significance beyond empirical tuning. It takes the result of a request as `alignmentObservation.alignmentTransform` to determine if the scene is stable enough to perform classification. Check for scene stability by performing a request on the sequence request handler:\n\nThis algorithm deems a scene to be stable if the Manhattan distance between frames is less than 20:\n\nAfter registration has determined that the scene is longer varying, the app sends the stable frame to Vision for Core ML classification:\n\n### Perform Image Classification\n\nThe sample app wraps two request objects—a barcode detection request and an image classification request—in a single request execution so Vision can perform them together. Performing the combined request is faster than performing separate requests, since Vision can share the same visual data between both.\n\nClassification contains a setup stage and a performance stage. The setup stage involves initializing requests for the types of objects you’d like Vision to detect and defining completion handlers to tell the app how to handle detection results after the requests finish their work.\n\nThe sample code sets up both a classification request and a barcode detection request. FlowerShop uses barcode identification to label an object—fertilizer—for which it has no training data. For example, the curator of a museum exhibit or owner of a flower shop can place the barcode beside or in place of an actual item, so that scanning the barcode classifies the item.\n\nBy using it as a proxy for the actual item, the app can still provide a confident classification even if the user doesn’t scan the actual item. This kind of proxy works particularly well for items that Create ML may have trouble training through images, such as fertilizer, gasoline, transparent gases, and clear liquids. Set up this kind of barcode detection using a [https:\/\/developer.apple.com\/documentation\/vision\/vndetectbarcodesrequest] object:\n\nThe sample appends the normal model-based classification request to the same array. You can create both requests at once, but the sample code staggers the classification request to guard against failure to load the Core ML model. The classification request loads the Core ML classifier into a [https:\/\/developer.apple.com\/documentation\/vision\/vncoremlrequest] object:\n\nDefining the requests and completion handlers concludes the setup stage; the second stage performs identification in real time. The sample sends the stable frame to the classifier and tells Vision to perform classification by calling [https:\/\/developer.apple.com\/documentation\/vision\/vnimagerequesthandler\/2880297-perform]:\n\nPerform tasks asynchronously on a background queue, so the camera and user interface can keep running unhindered. Don’t continuously queue up every buffer that the camera provides; instead, drop buffers to keep the pipeline moving. The app works with a queue of one buffer, skipping subsequent frames so long as it is still processing that buffer. When one request finishes, it queues the next buffer and submits a classification request.\n\nEven if captured frames don’t match the size of the image under which you trained the Create ML model (299 × 299), the Vision framework crops and scales down its input images to match the model’s expected size on your behalf.\n\n### Interpret Classification Results\n\nCheck the results in the request’s completion handler. When you create and pass in a request, you handle results and errors and show the classification results in your app’s UI.\n\nThe sample app sets a confidence threshold of `0.9`—empirically tuned—to filter out false classifications. A score of `1.0` means only that the photo submitted for request satisfies the algorithm and trained classifier. The algorithm could output a score of `1.0` even when the classfication is wrong. When tuning your application for the optimal confidence threshold, use the output streamed to Xcode’s debugger window to gauge typical confidence values, making sure to note how far the confidence spikes on typical correct classifications. A white background with no object can still yield a confidence score of `0.6`.\n\nThe sample shows the top result, but in a search app, you can rank the labels by confidence, from most confident classification to least. The array of confidence scores and classifications is available, so use more than the top result if it fits your app’s context. Try different thresholds to determine the best balance of reducing false positives and surfacing real-world results when they are correct; a result can be correct at a lower confidence score, like `0.8`. Even though this app’s threshold is `0.9`, the ideal threshold may vary from model to model.\n\n### Release Your Buffers\n\nAfter processing your buffers, be sure to release them to prevent them from queuing up. Because the input is a capture device that is constantly streaming frames, your app will run out of memory quickly if you don’t discard extra frames. The sample app limits the number of queued frame buffers to only one, which prevents overflow from happening and clears the buffer by setting it to `nil`:",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/vision\/training-a-create-ml-model-to-classify-flowers\ncrawled: 2025-12-04T02:35:15Z\n---\n\n# Training a Create ML Model to Classify Flowers\n\n**Sample Code**\n\nTrain a flower classifier using Create ML in Swift Playgrounds, and apply the resulting model to real-time image classification using Vision.\n\n## Overview\n\nTo classify images in real time, you need a classification model with the categories you’d like identified, and a way to capture images to feed to the classifier.\n\n\n\nThis sample code project contains two components: a Create ML model you train in Swift Playgrounds, and the iOS app, FlowerShop, which you use to classify different flower types. This project uses the same code as the robot shop demo in the WWDC 2018 session [https:\/\/developer.apple.com\/videos\/play\/wwdc2018\/717\/].\n\n### Getting Started\n\nTo see Create ML in action, run the Swift Playground `Training\/ImageClassifierPlayground.playground` in Xcode 10 on a Mac running macOS 10.14 or later. This Swift Playground performs classification on the training set and generates the Create ML model `ImageClassifier.mlmodel`.\n\nThe sample app, FlowerShop, requires the following:\n\n- Xcode 10.\n- An iOS device running iOS 12 or later.\n\nBecause Xcode can’t access the camera, FlowerShop won’t work in Simulator.\n\n### Train a Custom Classifier on an Organized Image Set\n\nThe sample images that Create ML uses to train the custom classifier were taken with a set of categories in mind. For your own app, decide on a set of classification labels before preparing images. Take the following measures to improve your data set:\n\n- Aim for a minimum of 10 images per category—the more, the better.\n- Avoid highly unbalanced datasets by preparing a roughly equal number between categories.\n- Make your model more robust by enabling the Create ML UI’s Augmentation options: Crop, Rotate, Blur, Expose, Noise, and Flip.\n- Include redundancy in your training set: Take lots of images at different angles, on different backgrounds, and in different lighting conditions. Simulate real-world camera capture, including noise and motion blur.\n- Photograph sample objects in your hand to simulate real-world users that try to classify objects in their hands.\n- Remove other objects, especially ones that you’d like to classify differently, from view.\n\nIf you’re using photos taken on an iOS device to train your model, you can use the macOS utility, Image Capture, to import the images onto your computer, and do the following:\n\n1. In Xcode, open `ImageClassifierPlayground.playground` and display the Assistant Editor.\n2. Click Run on the last line of the Swift Playground; this opens the Create ML training environment.\n3. Place the training images you’d like to use into named folders (such as *Agapanthus*).\n4. Drag the set of folders into the Assistant Editor to perform image training.\n\nFor more information about configuring the resultant model, as well as screenshots of the Create ML UI, see [https:\/\/developer.apple.com\/documentation\/createml\/creating-an-image-classifier-model].\n\nCreate ML exports its trained results as a `.mlmodel` file, which you can import into your app in Xcode. After importing the model, you can examine  the prototypical image size by opening the model file in Xcode’s navigation menu. For example, a parameter such as “Color 299 x 299” indicates the size of the training image. You can also confirm the size of the model.\n\n### Build an iOS App Around the Classifier\n\nThe app leverages the trained model and uses Vision for both registration and classification:\n\n- It performs registration on subsequent video frame buffers to deem when the user is still enough for image capture.\n- When the user is holding the camera sufficiently still, it performs image classification on the frame, attempting to identify the focused object as one of the categories in the Create ML classifier.\n- If the confidence score associated with a classifier exceeds a high confidence threshold of `0.9`, the app shows its most confident classification through an overlay.\n\n### Use Registration for Scene Stability\n\nRegistration takes and aligns two images to determine the relative difference. Vision’s registration operation uses an inexpensive, fast algorithm that tells the app if the subject is still and stable. Theoretically, the app could make a classification request on every frame buffer, but classification is a computationally expensive operation—so attempting to classify every frame could result in delays and poor performance with the UI. Classify the scene in a frame only if the registration algorithm determines that the scene and camera are still, indicating the user’s intent to classify an object.\n\nThe FlowerShop app uses [https:\/\/developer.apple.com\/documentation\/vision\/vnsequencerequesthandler] with [https:\/\/developer.apple.com\/documentation\/vision\/vntranslationalimageregistrationrequest] objects to compare consecutive frames, keeping a history of 15 frames. This amount of history amounts to half a second of capture at 30 frames per second and carries no special significance beyond empirical tuning. It takes the result of a request as `alignmentObservation.alignmentTransform` to determine if the scene is stable enough to perform classification. Check for scene stability by performing a request on the sequence request handler:\n\n```swift\nlet registrationRequest = VNTranslationalImageRegistrationRequest(targetedCVPixelBuffer: pixelBuffer)\n```\n\nThis algorithm deems a scene to be stable if the Manhattan distance between frames is less than 20:\n\n```swift\nfileprivate func sceneStabilityAchieved() -> Bool {\n    \/\/ Determine if we have enough evidence of stability.\n    if transpositionHistoryPoints.count == maximumHistoryLength {\n        \/\/ Calculate the moving average.\n        var movingAverage: CGPoint = CGPoint.zero\n        for currentPoint in transpositionHistoryPoints {\n            movingAverage.x += currentPoint.x\n            movingAverage.y += currentPoint.y\n        }\n        let distance = abs(movingAverage.x) + abs(movingAverage.y)\n        if distance < 20 {\n            return true\n        }\n    }\n    return false\n}\n```\n\nAfter registration has determined that the scene is longer varying, the app sends the stable frame to Vision for Core ML classification:\n\n```swift\nif self.sceneStabilityAchieved() {\n    showDetectionOverlay(true)\n    if currentlyAnalyzedPixelBuffer == nil {\n        \/\/ Retain the image buffer for Vision processing.\n        currentlyAnalyzedPixelBuffer = pixelBuffer\n        analyzeCurrentImage()\n    }\n} else {\n    showDetectionOverlay(false)\n}\n```\n\n### Perform Image Classification\n\nThe sample app wraps two request objects—a barcode detection request and an image classification request—in a single request execution so Vision can perform them together. Performing the combined request is faster than performing separate requests, since Vision can share the same visual data between both.\n\nClassification contains a setup stage and a performance stage. The setup stage involves initializing requests for the types of objects you’d like Vision to detect and defining completion handlers to tell the app how to handle detection results after the requests finish their work.\n\nThe sample code sets up both a classification request and a barcode detection request. FlowerShop uses barcode identification to label an object—fertilizer—for which it has no training data. For example, the curator of a museum exhibit or owner of a flower shop can place the barcode beside or in place of an actual item, so that scanning the barcode classifies the item.\n\nBy using it as a proxy for the actual item, the app can still provide a confident classification even if the user doesn’t scan the actual item. This kind of proxy works particularly well for items that Create ML may have trouble training through images, such as fertilizer, gasoline, transparent gases, and clear liquids. Set up this kind of barcode detection using a [https:\/\/developer.apple.com\/documentation\/vision\/vndetectbarcodesrequest] object:\n\n```swift\nlet barcodeDetection = VNDetectBarcodesRequest(completionHandler: { (request, error) in\n    if let results = request.results as? [VNBarcodeObservation] {\n        if let mainBarcode = results.first {\n            if let payloadString = mainBarcode.payloadStringValue {\n                self.showProductInfo(payloadString)\n            }\n        }\n    }\n})\nself.analysisRequests = ([barcodeDetection])\n\n\/\/ Setup a classification request.\nguard let modelURL = Bundle.main.url(forResource: \"FlowerShop\", withExtension: \"mlmodelc\") else {\n    return NSError(domain: \"VisionViewController\", code: -1, userInfo: [NSLocalizedDescriptionKey: \"The model file is missing.\"])\n}\nguard let objectRecognition = createClassificationRequest(modelURL: modelURL) else {\n    return NSError(domain: \"VisionViewController\", code: -1, userInfo: [NSLocalizedDescriptionKey: \"The classification request failed.\"])\n}\nself.analysisRequests.append(objectRecognition)\n```\n\nThe sample appends the normal model-based classification request to the same array. You can create both requests at once, but the sample code staggers the classification request to guard against failure to load the Core ML model. The classification request loads the Core ML classifier into a [https:\/\/developer.apple.com\/documentation\/vision\/vncoremlrequest] object:\n\n```swift\nlet objectClassifier = try VNCoreMLModel(for: MLModel(contentsOf: modelURL))\nlet classificationRequest = VNCoreMLRequest(model: objectClassifier, completionHandler: { (request, error) in\n```\n\nDefining the requests and completion handlers concludes the setup stage; the second stage performs identification in real time. The sample sends the stable frame to the classifier and tells Vision to perform classification by calling [https:\/\/developer.apple.com\/documentation\/vision\/vnimagerequesthandler\/2880297-perform]:\n\n```swift\nprivate func analyzeCurrentImage() {\n    \/\/ Most computer vision tasks are not rotation-agnostic, so it is important to pass in the orientation of the image with respect to device.\n    let orientation = exifOrientationFromDeviceOrientation()\n    \n    let requestHandler = VNImageRequestHandler(cvPixelBuffer: currentlyAnalyzedPixelBuffer!, orientation: orientation)\n    visionQueue.async {\n        do {\n            \/\/ Release the pixel buffer when done, allowing the next buffer to be processed.\n            defer { self.currentlyAnalyzedPixelBuffer = nil }\n            try requestHandler.perform(self.analysisRequests)\n        } catch {\n            print(\"Error: Vision request failed with error \\\"\\(error)\\\"\")\n        }\n    }\n}\n```\n\nPerform tasks asynchronously on a background queue, so the camera and user interface can keep running unhindered. Don’t continuously queue up every buffer that the camera provides; instead, drop buffers to keep the pipeline moving. The app works with a queue of one buffer, skipping subsequent frames so long as it is still processing that buffer. When one request finishes, it queues the next buffer and submits a classification request.\n\n```swift\nprivate let visionQueue = DispatchQueue(label: \"com.example.apple-samplecode.FlowerShop.serialVisionQueue\")\n```\n\nEven if captured frames don’t match the size of the image under which you trained the Create ML model (299 × 299), the Vision framework crops and scales down its input images to match the model’s expected size on your behalf.\n\n### Interpret Classification Results\n\nCheck the results in the request’s completion handler. When you create and pass in a request, you handle results and errors and show the classification results in your app’s UI.\n\n```swift\nif let results = request.results as? [VNClassificationObservation] {\n    print(\"\\(results.first!.identifier) : \\(results.first!.confidence)\")\n    if results.first!.confidence > 0.9 {\n        self.showProductInfo(results.first!.identifier)\n    }\n}\n```\n\nThe sample app sets a confidence threshold of `0.9`—empirically tuned—to filter out false classifications. A score of `1.0` means only that the photo submitted for request satisfies the algorithm and trained classifier. The algorithm could output a score of `1.0` even when the classfication is wrong. When tuning your application for the optimal confidence threshold, use the output streamed to Xcode’s debugger window to gauge typical confidence values, making sure to note how far the confidence spikes on typical correct classifications. A white background with no object can still yield a confidence score of `0.6`.\n\nThe sample shows the top result, but in a search app, you can rank the labels by confidence, from most confident classification to least. The array of confidence scores and classifications is available, so use more than the top result if it fits your app’s context. Try different thresholds to determine the best balance of reducing false positives and surfacing real-world results when they are correct; a result can be correct at a lower confidence score, like `0.8`. Even though this app’s threshold is `0.9`, the ideal threshold may vary from model to model.\n\n### Release Your Buffers\n\nAfter processing your buffers, be sure to release them to prevent them from queuing up. Because the input is a capture device that is constantly streaming frames, your app will run out of memory quickly if you don’t discard extra frames. The sample app limits the number of queued frame buffers to only one, which prevents overflow from happening and clears the buffer by setting it to `nil`:\n\n```swift\nself.currentlyAnalyzedPixelBuffer = nil\n```\n\n## Machine learning image analysis\n\n- **Classifying Images with Vision and Core ML**: Crop and scale photos using the Vision framework and classify them with a Core ML model.\n- **VNCoreMLRequest**: An image-analysis request that uses a Core ML model to process images.\n- **VNClassificationObservation**: An object that represents classification information that an image-analysis request produces.\n- **VNPixelBufferObservation**: An object that represents an image that an image-analysis request produces.\n- **VNCoreMLFeatureValueObservation**: An object that represents a collection of key-value information that a Core ML image-analysis request produces.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Crop and scale photos using the Vision framework and classify them with a Core ML model.",
          "name" : "Classifying Images with Vision and Core ML",
          "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/classifying-images-with-vision-and-core-ml"
        },
        {
          "description" : "An image-analysis request that uses a Core ML model to process images.",
          "name" : "VNCoreMLRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNCoreMLRequest"
        },
        {
          "description" : "An object that represents classification information that an image-analysis request produces.",
          "name" : "VNClassificationObservation",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNClassificationObservation"
        },
        {
          "description" : "An object that represents an image that an image-analysis request produces.",
          "name" : "VNPixelBufferObservation",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNPixelBufferObservation"
        },
        {
          "description" : "An object that represents a collection of key-value information that a Core ML image-analysis request produces.",
          "name" : "VNCoreMLFeatureValueObservation",
          "url" : "https:\/\/developer.apple.com\/documentation\/Vision\/VNCoreMLFeatureValueObservation"
        }
      ],
      "title" : "Machine learning image analysis"
    }
  ],
  "source" : "appleJSON",
  "title" : "Training a Create ML Model to Classify Flowers",
  "url" : "https:\/\/developer.apple.com\/documentation\/vision\/training-a-create-ml-model-to-classify-flowers"
}