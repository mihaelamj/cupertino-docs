{
  "abstract" : "Capture photos and record video using the front and rear iPhone and iPad cameras.",
  "codeExamples" : [
    {
      "code" : "\/\/ Retrieve the default camera and microphone.\nlet defaultCamera = try deviceLookup.defaultCamera\nlet defaultMic = try deviceLookup.defaultMic\n\n\/\/ Add inputs for the default camera and microphone devices.\nactiveVideoInput = try addInput(for: defaultCamera)\ntry addInput(for: defaultMic)",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Adds an input to the capture session to connect the specified capture device.\n@discardableResult\nprivate func addInput(for device: AVCaptureDevice) throws -> AVCaptureDeviceInput {\n    let input = try AVCaptureDeviceInput(device: device)\n    if captureSession.canAddInput(input) {\n        captureSession.addInput(input)\n    } else {\n        throw CameraError.addInputFailed\n    }\n    return input\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Configure the session for photo capture by default.\ncaptureSession.sessionPreset = .photo\n\n\/\/ Add the photo capture output as the default output type.\nif captureSession.canAddOutput(photoCapture.output) {\n    captureSession.addOutput(photoCapture.output)\n} else {\n    throw CameraError.addOutputFailed\n}",
      "language" : "swift"
    },
    {
      "code" : "class PreviewView: UIView, PreviewTarget {\n    \n    \/\/ Use `AVCaptureVideoPreviewLayer` as the view's backing layer.\n    override class var layerClass: AnyClass {\n        AVCaptureVideoPreviewLayer.self\n    }\n    \n    var previewLayer: AVCaptureVideoPreviewLayer {\n        layer as! AVCaptureVideoPreviewLayer\n    }\n    \n    func setSession(_ session: AVCaptureSession) {\n        \/\/ Connects the session with the preview layer, which allows the layer\n        \/\/ to provide a live view of the captured content.\n        previewLayer.session = session\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "struct CameraPreview: UIViewRepresentable {\n    \n    private let source: PreviewSource\n    \n    init(source: PreviewSource) {\n        self.source = source\n    }\n    \n    func makeUIView(context: Context) -> PreviewView {\n        let preview = PreviewView()\n        \/\/ Connect the preview layer to the capture session.\n        source.connect(to: preview)\n        return preview\n    }\n    \n    func updateUIView(_ previewView: PreviewView, context: Context) {\n        \/\/ No implementation needed.\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "var isAuthorized: Bool {\n    get async {\n        let status = AVCaptureDevice.authorizationStatus(for: .video)\n        \/\/ Determine whether a person previously authorized camera access.\n        var isAuthorized = status == .authorized\n        \/\/ If the system hasn't determined their authorization status,\n        \/\/ explicitly prompt them for approval.\n        if status == .notDetermined {\n            isAuthorized = await AVCaptureDevice.requestAccess(for: .video)\n        }\n        return isAuthorized\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "func setCaptureMode(_ captureMode: CaptureMode) throws {\n    \n    self.captureMode = captureMode\n    \n    \/\/ Change the configuration atomically.\n    captureSession.beginConfiguration()\n    defer { captureSession.commitConfiguration() }\n    \n    \/\/ Configure the capture session for the selected capture mode.\n    switch captureMode {\n    case .photo:\n        \/\/ The app needs to remove the movie capture output to perform Live Photo capture.\n        captureSession.sessionPreset = .photo\n        captureSession.removeOutput(movieCapture.output)\n    case .video:\n        captureSession.sessionPreset = .high\n        try addOutput(movieCapture.output)\n    }\n\n    \/\/ Update the advertised capabilities after reconfiguration.\n    updateCaptureCapabilities()\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Changes the device the service uses for video capture.\nprivate func changeCaptureDevice(to device: AVCaptureDevice) {\n    \/\/ The service must have a valid video input prior to calling this method.\n    guard let currentInput = activeVideoInput else { fatalError() }\n    \n    \/\/ Bracket the following configuration in a begin\/commit configuration pair.\n    captureSession.beginConfiguration()\n    defer { captureSession.commitConfiguration() }\n    \n    \/\/ Remove the existing video input before attempting to connect a new one.\n    captureSession.removeInput(currentInput)\n    do {\n        \/\/ Attempt to connect a new input and device to the capture session.\n        activeVideoInput = try addInput(for: device)\n        \/\/ Configure a new rotation coordinator for the new device.\n        createRotationCoordinator(for: device)\n        \/\/ Register for device observations.\n        observeSubjectAreaChanges(of: device)\n        \/\/ Update the service's advertised capabilities.\n        updateCaptureCapabilities()\n    } catch {\n        \/\/ Reconnect the existing camera on failure.\n        captureSession.addInput(currentInput)\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ The app calls this method when the user taps the photo capture button.\nfunc capturePhoto(with features: EnabledPhotoFeatures) async throws -> Photo {\n    \/\/ Wrap the delegate-based capture API in a continuation to use it in an async context.\n    try await withCheckedThrowingContinuation { continuation in\n        \n        \/\/ Create a settings object to configure the photo capture.\n        let photoSettings = createPhotoSettings(with: features)\n        \n        let delegate = PhotoCaptureDelegate(continuation: continuation)\n        monitorProgress(of: delegate)\n        \n        \/\/ Capture a new photo with the specified settings.\n        photoOutput.capturePhoto(with: photoSettings, delegate: delegate)\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "func photoOutput(_ output: AVCapturePhotoOutput, didFinishCaptureFor resolvedSettings: AVCaptureResolvedPhotoSettings, error: Error?) {\n\n    \/\/ If an error occurs, resume the continuation by throwing an error, and return.\n    if let error {\n        continuation.resume(throwing: error)\n        return\n    }\n    \n    \/\/\/ Create a photo object to save to the `MediaLibrary`.\n    let photo = Photo(data: photoData, isProxy: isProxyPhoto, livePhotoMovieURL: livePhotoMovieURL)\n    \/\/ Resume the continuation by returning the captured photo.\n    continuation.resume(returning: photo)\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ Starts movie recording.\nfunc startRecording() {\n    \/\/ Return early if already recording.\n    guard !movieOutput.isRecording else { return }\n\n    \/\/ Start a timer to update the recording time.\n    startMonitoringDuration()\n    \n    delegate = MovieCaptureDelegate()\n    movieOutput.startRecording(to: URL.movieFileURL, recordingDelegate: delegate!)\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ Stops movie recording.\n\/\/\/ - Returns: A `Movie` object that represents the captured movie.\nfunc stopRecording() async throws -> Movie {\n    \/\/ Use a continuation to adapt the delegate-based capture API to an async interface.\n    return try await withCheckedThrowingContinuation { continuation in\n        \/\/ Set the continuation on the delegate to handle the capture result.\n        delegate?.continuation = continuation\n        \n        \/\/\/ Stops recording, which causes the output to call the `MovieCaptureDelegate` object.\n        movieOutput.stopRecording()\n        stopMonitoringDuration()\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "func fileOutput(_ output: AVCaptureFileOutput, didFinishRecordingTo outputFileURL: URL, from connections: [AVCaptureConnection], error: Error?) {\n    if let error {\n        \/\/ If an error occurs, throw it to the caller.\n        continuation?.resume(throwing: error)\n    } else {\n        \/\/ Return a new movie object.\n        continuation?.resume(returning: Movie(url: outputFileURL))\n    }\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "b4753acdd2b581dcaf0ff62d730ce407df2cfef2a4023dee2f997e338c2619e5",
  "crawledAt" : "2025-12-02T15:45:22Z",
  "id" : "5A857E4B-CC54-4348-ADAE-1B5D64D57402",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "AVFoundation",
  "overview" : "## Overview\n\nThe AVCam sample shows you how to build a basic camera app for iOS. It demonstrates how to use AVFoundation to access device cameras and microphones, configure a capture session, capture photos and videos, and much more. It also shows how to use the [doc:\/\/com.apple.documentation\/documentation\/PhotoKit] framework to save your captured media to the Photos library.\n\nThe sample uses SwiftUI and the features of Swift concurrency to build a responsive camera app. The following diagram describes the app’s design:\n\n\n\nThe key type the app defines is `CaptureService`, an actor that manages the interactions with the AVFoundation capture APIs. This object configures the capture pipeline and manages its life cycle, and defines an asynchronous interface to capture photos and videos. It delegates the handling of those operations to the app’s `PhotoCapture` and `MovieCapture` objects, respectively.\n\n### Configure the sample code project\n\nBecause Simulator doesn’t have access to device cameras, it isn’t suitable for running the app—you’ll need to run it on a device. To run this sample, you’ll need the following:\n\nAVCam adopts the [doc:\/\/com.apple.documentation\/documentation\/LockedCameraCapture] framework, which makes the app eligible to launch from the Lock Screen, Control Center, Action Button, and the Camera Control. To support this framework, the sample adds a capture extension target and a Control Center extension target to the main app target. Set your signing credentials on each target to build and run the sample.\n\n### Configure a capture session\n\nThe central object in any capture app is an instance of [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureSession]. A capture session is the central hub to which the app connects inputs from camera and microphone devices, and attaches them to outputs that capture media like photos and video. After configuring the session, the app uses it to control the flow of data through the capture pipeline.\n\n\n\nThe capture service performs the session configuration in its `setUpSession()` method. It retrieves the default camera and microphone for the host device and adds them as inputs to the capture session.\n\nTo add the inputs, it uses a helper method that creates a new [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDeviceInput] for the specified camera or microphone device and adds it to the capture session, if possible.\n\nAfter adding the device inputs, the method configures the capture session for the app’s default photo capture mode. It optimizes the pipeline for high-resolution photo quality output by setting the capture session’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureSession\/Preset\/photo] preset. Finally, to enable the app to capture photos, it adds an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput] instance to the session.\n\n### Set up a capture preview\n\nTo preview the content a camera is capturing, AVFoundation provides a Core Animation layer subclass called  [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureVideoPreviewLayer]. SwiftUI doesn’t support using layers directly, so instead, the app hosts this layer in a [doc:\/\/com.apple.documentation\/documentation\/UIKit\/UIView] subclass called `PreviewView`. It overrides the [doc:\/\/com.apple.documentation\/documentation\/UIKit\/UIView\/layerClass] property to make the preview layer the backing for the view.\n\nTo make this view accessible to SwiftUI, the app wraps it as a [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/UIViewRepresentable] type called `CameraPreview`.\n\nTo connect the preview to the capture session without directly exposing the capture service’s protected state, the sample defines app-specific `PreviewSource` and `PreviewTarget` protocols. The app passes the `CameraPreview` a preview source, which provides a reference to the capture session. Calling the preview source’s `connect(to:)` method sets the capture session on the `PreviewView` instance.\n\n### Request authorization\n\nThe initial capture configuration is complete, but before the app can successfully start the capture session, it needs to determine whether it has authorization to use device inputs. The system requires that a person explicitly authorize the app to capture input from cameras and microphones. To determine the app’s status, the capture service defines an asynchronous `isAuthorized` property as follows:\n\nThe property’s implementation uses the methods of [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDevice] to check the current status, and if the app hasn’t made a determination, requests authorization from the user. If the app has authorization, it starts the capture session to begin the flow of data. If not, it shows an error message in the user interface.\n\nTo learn more about the configuration required to access cameras and microphones, see [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/requesting-authorization-to-capture-and-save-media].\n\n### Change the capture mode\n\nThe app starts in photo capture mode. Changing modes requires a reconfiguration of the capture session as follows:\n\nIn photo capture mode, the app sets the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureSession\/Preset\/photo] preset on the capture session, which optimizes the capture pipeline for high-quality photo output. It also removes the movie capture output, which prevents the photo output from performing Live Photo capture. In video capture mode, it sets the session preset to [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureSession\/Preset\/high] and adds the movie file capture output to the session.\n\n### Select a new camera\n\nThe app provides a button that lets people switch between the front and back cameras and, in iPadOS, connected external cameras. To change the active camera, the app reconfigures the session as follows:\n\n[doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureSession] only allows attaching a single camera input at a time, so this method begins by removing the existing camera’s input. It then attempts to add an input for the new device and, if successful, performs some internal configuration to reflect the device change. If the capture session can’t add the new device, it reconnects the removed input.\n\n### Capture a photo\n\nThe capture service delegates handling of the app’s photo capture features to the `PhotoCapture` object, which manages the life cycle of and interaction with an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput]. The app captures photos with this object by calling its [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput\/capturePhoto(with:delegate:)] method, passing it an object that describes photo capture settings to enable and a delegate for the system to call as capture proceeds. To use this delegate-based API in an `async` context , the app wraps this call with a checked throwing continuation as follows:\n\nWhen the system finishes capturing a photo, it calls the delegate’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoCaptureDelegate\/photoOutput(_:didFinishCaptureFor:error:)] method. The delegate object’s implementation of this method uses the continuation to resume execution by returning a photo or throwing an error.\n\nTo learn more about capturing photos with AVFoundation, see [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/capturing-still-and-live-photos].\n\n### Record a movie\n\nThe capture service delegates handling of the app’s video capture features to the `MovieCapture` object, which manages the life cycle of and interaction with an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureMovieFileOutput]. To start recording a movie, the app calls the movie file output’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureFileOutput\/startRecording(to:recordingDelegate:)] method, which takes a URL to write the move to and a delegate for the system to call when recording completes.\n\nTo finish recording the video, the app calls the movie file output’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureFileOutput\/stopRecording()] method, which causes the system to call the delegate to handle the captured output. To adapt this delegate-based callback, the app wraps this interaction in a checked throwing continuation as follows:\n\nWhen the app calls the movie file output’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureFileOutput\/stopRecording()] method, the system calls the delegate, which resumes execution either by returning a movie or throwing an error.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/AVFoundation\/avcam-building-a-camera-app\ncrawled: 2025-12-02T15:45:22Z\n---\n\n# AVCam: Building a camera app\n\n**Sample Code**\n\nCapture photos and record video using the front and rear iPhone and iPad cameras.\n\n## Overview\n\nThe AVCam sample shows you how to build a basic camera app for iOS. It demonstrates how to use AVFoundation to access device cameras and microphones, configure a capture session, capture photos and videos, and much more. It also shows how to use the [doc:\/\/com.apple.documentation\/documentation\/PhotoKit] framework to save your captured media to the Photos library.\n\nThe sample uses SwiftUI and the features of Swift concurrency to build a responsive camera app. The following diagram describes the app’s design:\n\n\n\nThe key type the app defines is `CaptureService`, an actor that manages the interactions with the AVFoundation capture APIs. This object configures the capture pipeline and manages its life cycle, and defines an asynchronous interface to capture photos and videos. It delegates the handling of those operations to the app’s `PhotoCapture` and `MovieCapture` objects, respectively.\n\n\n\n### Configure the sample code project\n\nBecause Simulator doesn’t have access to device cameras, it isn’t suitable for running the app—you’ll need to run it on a device. To run this sample, you’ll need the following:\n\n- An iOS device with iOS 26 or later\n\nAVCam adopts the [doc:\/\/com.apple.documentation\/documentation\/LockedCameraCapture] framework, which makes the app eligible to launch from the Lock Screen, Control Center, Action Button, and the Camera Control. To support this framework, the sample adds a capture extension target and a Control Center extension target to the main app target. Set your signing credentials on each target to build and run the sample.\n\n### Configure a capture session\n\nThe central object in any capture app is an instance of [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureSession]. A capture session is the central hub to which the app connects inputs from camera and microphone devices, and attaches them to outputs that capture media like photos and video. After configuring the session, the app uses it to control the flow of data through the capture pipeline.\n\n\n\nThe capture service performs the session configuration in its `setUpSession()` method. It retrieves the default camera and microphone for the host device and adds them as inputs to the capture session.\n\n```swift\n\/\/ Retrieve the default camera and microphone.\nlet defaultCamera = try deviceLookup.defaultCamera\nlet defaultMic = try deviceLookup.defaultMic\n\n\/\/ Add inputs for the default camera and microphone devices.\nactiveVideoInput = try addInput(for: defaultCamera)\ntry addInput(for: defaultMic)\n```\n\nTo add the inputs, it uses a helper method that creates a new [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDeviceInput] for the specified camera or microphone device and adds it to the capture session, if possible.\n\n```swift\n\/\/ Adds an input to the capture session to connect the specified capture device.\n@discardableResult\nprivate func addInput(for device: AVCaptureDevice) throws -> AVCaptureDeviceInput {\n    let input = try AVCaptureDeviceInput(device: device)\n    if captureSession.canAddInput(input) {\n        captureSession.addInput(input)\n    } else {\n        throw CameraError.addInputFailed\n    }\n    return input\n}\n```\n\nAfter adding the device inputs, the method configures the capture session for the app’s default photo capture mode. It optimizes the pipeline for high-resolution photo quality output by setting the capture session’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureSession\/Preset\/photo] preset. Finally, to enable the app to capture photos, it adds an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput] instance to the session.\n\n```swift\n\/\/ Configure the session for photo capture by default.\ncaptureSession.sessionPreset = .photo\n\n\/\/ Add the photo capture output as the default output type.\nif captureSession.canAddOutput(photoCapture.output) {\n    captureSession.addOutput(photoCapture.output)\n} else {\n    throw CameraError.addOutputFailed\n}\n```\n\n### Set up a capture preview\n\nTo preview the content a camera is capturing, AVFoundation provides a Core Animation layer subclass called  [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureVideoPreviewLayer]. SwiftUI doesn’t support using layers directly, so instead, the app hosts this layer in a [doc:\/\/com.apple.documentation\/documentation\/UIKit\/UIView] subclass called `PreviewView`. It overrides the [doc:\/\/com.apple.documentation\/documentation\/UIKit\/UIView\/layerClass] property to make the preview layer the backing for the view.\n\n```swift\nclass PreviewView: UIView, PreviewTarget {\n    \n    \/\/ Use `AVCaptureVideoPreviewLayer` as the view's backing layer.\n    override class var layerClass: AnyClass {\n        AVCaptureVideoPreviewLayer.self\n    }\n    \n    var previewLayer: AVCaptureVideoPreviewLayer {\n        layer as! AVCaptureVideoPreviewLayer\n    }\n    \n    func setSession(_ session: AVCaptureSession) {\n        \/\/ Connects the session with the preview layer, which allows the layer\n        \/\/ to provide a live view of the captured content.\n        previewLayer.session = session\n    }\n}\n```\n\nTo make this view accessible to SwiftUI, the app wraps it as a [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/UIViewRepresentable] type called `CameraPreview`.\n\n```swift\nstruct CameraPreview: UIViewRepresentable {\n    \n    private let source: PreviewSource\n    \n    init(source: PreviewSource) {\n        self.source = source\n    }\n    \n    func makeUIView(context: Context) -> PreviewView {\n        let preview = PreviewView()\n        \/\/ Connect the preview layer to the capture session.\n        source.connect(to: preview)\n        return preview\n    }\n    \n    func updateUIView(_ previewView: PreviewView, context: Context) {\n        \/\/ No implementation needed.\n    }\n}\n```\n\nTo connect the preview to the capture session without directly exposing the capture service’s protected state, the sample defines app-specific `PreviewSource` and `PreviewTarget` protocols. The app passes the `CameraPreview` a preview source, which provides a reference to the capture session. Calling the preview source’s `connect(to:)` method sets the capture session on the `PreviewView` instance.\n\n### Request authorization\n\nThe initial capture configuration is complete, but before the app can successfully start the capture session, it needs to determine whether it has authorization to use device inputs. The system requires that a person explicitly authorize the app to capture input from cameras and microphones. To determine the app’s status, the capture service defines an asynchronous `isAuthorized` property as follows:\n\n```swift\nvar isAuthorized: Bool {\n    get async {\n        let status = AVCaptureDevice.authorizationStatus(for: .video)\n        \/\/ Determine whether a person previously authorized camera access.\n        var isAuthorized = status == .authorized\n        \/\/ If the system hasn't determined their authorization status,\n        \/\/ explicitly prompt them for approval.\n        if status == .notDetermined {\n            isAuthorized = await AVCaptureDevice.requestAccess(for: .video)\n        }\n        return isAuthorized\n    }\n}\n```\n\nThe property’s implementation uses the methods of [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDevice] to check the current status, and if the app hasn’t made a determination, requests authorization from the user. If the app has authorization, it starts the capture session to begin the flow of data. If not, it shows an error message in the user interface.\n\nTo learn more about the configuration required to access cameras and microphones, see [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/requesting-authorization-to-capture-and-save-media].\n\n### Change the capture mode\n\nThe app starts in photo capture mode. Changing modes requires a reconfiguration of the capture session as follows:\n\n```swift\nfunc setCaptureMode(_ captureMode: CaptureMode) throws {\n    \n    self.captureMode = captureMode\n    \n    \/\/ Change the configuration atomically.\n    captureSession.beginConfiguration()\n    defer { captureSession.commitConfiguration() }\n    \n    \/\/ Configure the capture session for the selected capture mode.\n    switch captureMode {\n    case .photo:\n        \/\/ The app needs to remove the movie capture output to perform Live Photo capture.\n        captureSession.sessionPreset = .photo\n        captureSession.removeOutput(movieCapture.output)\n    case .video:\n        captureSession.sessionPreset = .high\n        try addOutput(movieCapture.output)\n    }\n\n    \/\/ Update the advertised capabilities after reconfiguration.\n    updateCaptureCapabilities()\n}\n```\n\nIn photo capture mode, the app sets the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureSession\/Preset\/photo] preset on the capture session, which optimizes the capture pipeline for high-quality photo output. It also removes the movie capture output, which prevents the photo output from performing Live Photo capture. In video capture mode, it sets the session preset to [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureSession\/Preset\/high] and adds the movie file capture output to the session.\n\n### Select a new camera\n\nThe app provides a button that lets people switch between the front and back cameras and, in iPadOS, connected external cameras. To change the active camera, the app reconfigures the session as follows:\n\n```swift\n\/\/ Changes the device the service uses for video capture.\nprivate func changeCaptureDevice(to device: AVCaptureDevice) {\n    \/\/ The service must have a valid video input prior to calling this method.\n    guard let currentInput = activeVideoInput else { fatalError() }\n    \n    \/\/ Bracket the following configuration in a begin\/commit configuration pair.\n    captureSession.beginConfiguration()\n    defer { captureSession.commitConfiguration() }\n    \n    \/\/ Remove the existing video input before attempting to connect a new one.\n    captureSession.removeInput(currentInput)\n    do {\n        \/\/ Attempt to connect a new input and device to the capture session.\n        activeVideoInput = try addInput(for: device)\n        \/\/ Configure a new rotation coordinator for the new device.\n        createRotationCoordinator(for: device)\n        \/\/ Register for device observations.\n        observeSubjectAreaChanges(of: device)\n        \/\/ Update the service's advertised capabilities.\n        updateCaptureCapabilities()\n    } catch {\n        \/\/ Reconnect the existing camera on failure.\n        captureSession.addInput(currentInput)\n    }\n}\n```\n\n[doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureSession] only allows attaching a single camera input at a time, so this method begins by removing the existing camera’s input. It then attempts to add an input for the new device and, if successful, performs some internal configuration to reflect the device change. If the capture session can’t add the new device, it reconnects the removed input.\n\n\n\n### Capture a photo\n\nThe capture service delegates handling of the app’s photo capture features to the `PhotoCapture` object, which manages the life cycle of and interaction with an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput]. The app captures photos with this object by calling its [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput\/capturePhoto(with:delegate:)] method, passing it an object that describes photo capture settings to enable and a delegate for the system to call as capture proceeds. To use this delegate-based API in an `async` context , the app wraps this call with a checked throwing continuation as follows:\n\n```swift\n\/\/\/ The app calls this method when the user taps the photo capture button.\nfunc capturePhoto(with features: EnabledPhotoFeatures) async throws -> Photo {\n    \/\/ Wrap the delegate-based capture API in a continuation to use it in an async context.\n    try await withCheckedThrowingContinuation { continuation in\n        \n        \/\/ Create a settings object to configure the photo capture.\n        let photoSettings = createPhotoSettings(with: features)\n        \n        let delegate = PhotoCaptureDelegate(continuation: continuation)\n        monitorProgress(of: delegate)\n        \n        \/\/ Capture a new photo with the specified settings.\n        photoOutput.capturePhoto(with: photoSettings, delegate: delegate)\n    }\n}\n```\n\nWhen the system finishes capturing a photo, it calls the delegate’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoCaptureDelegate\/photoOutput(_:didFinishCaptureFor:error:)] method. The delegate object’s implementation of this method uses the continuation to resume execution by returning a photo or throwing an error.\n\n```swift\nfunc photoOutput(_ output: AVCapturePhotoOutput, didFinishCaptureFor resolvedSettings: AVCaptureResolvedPhotoSettings, error: Error?) {\n\n    \/\/ If an error occurs, resume the continuation by throwing an error, and return.\n    if let error {\n        continuation.resume(throwing: error)\n        return\n    }\n    \n    \/\/\/ Create a photo object to save to the `MediaLibrary`.\n    let photo = Photo(data: photoData, isProxy: isProxyPhoto, livePhotoMovieURL: livePhotoMovieURL)\n    \/\/ Resume the continuation by returning the captured photo.\n    continuation.resume(returning: photo)\n}\n```\n\nTo learn more about capturing photos with AVFoundation, see [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/capturing-still-and-live-photos].\n\n### Record a movie\n\nThe capture service delegates handling of the app’s video capture features to the `MovieCapture` object, which manages the life cycle of and interaction with an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureMovieFileOutput]. To start recording a movie, the app calls the movie file output’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureFileOutput\/startRecording(to:recordingDelegate:)] method, which takes a URL to write the move to and a delegate for the system to call when recording completes.\n\n```swift\n\/\/\/ Starts movie recording.\nfunc startRecording() {\n    \/\/ Return early if already recording.\n    guard !movieOutput.isRecording else { return }\n\n    \/\/ Start a timer to update the recording time.\n    startMonitoringDuration()\n    \n    delegate = MovieCaptureDelegate()\n    movieOutput.startRecording(to: URL.movieFileURL, recordingDelegate: delegate!)\n}\n```\n\nTo finish recording the video, the app calls the movie file output’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureFileOutput\/stopRecording()] method, which causes the system to call the delegate to handle the captured output. To adapt this delegate-based callback, the app wraps this interaction in a checked throwing continuation as follows:\n\n```swift\n\/\/\/ Stops movie recording.\n\/\/\/ - Returns: A `Movie` object that represents the captured movie.\nfunc stopRecording() async throws -> Movie {\n    \/\/ Use a continuation to adapt the delegate-based capture API to an async interface.\n    return try await withCheckedThrowingContinuation { continuation in\n        \/\/ Set the continuation on the delegate to handle the capture result.\n        delegate?.continuation = continuation\n        \n        \/\/\/ Stops recording, which causes the output to call the `MovieCaptureDelegate` object.\n        movieOutput.stopRecording()\n        stopMonitoringDuration()\n    }\n}\n```\n\nWhen the app calls the movie file output’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureFileOutput\/stopRecording()] method, the system calls the delegate, which resumes execution either by returning a movie or throwing an error.\n\n```swift\nfunc fileOutput(_ output: AVCaptureFileOutput, didFinishRecordingTo outputFileURL: URL, from connections: [AVCaptureConnection], error: Error?) {\n    if let error {\n        \/\/ If an error occurs, throw it to the caller.\n        continuation?.resume(throwing: error)\n    } else {\n        \/\/ Return a new movie object.\n        continuation?.resume(returning: Movie(url: outputFileURL))\n    }\n}\n```\n\n## Capture sessions\n\n- **Setting up a capture session**: Configure input devices, output media, preview views, and basic settings before capturing photos or video.\n- **Accessing the camera while multitasking on iPad**: Operate the camera in Split View, Slide Over, Picture in Picture, and Stage Manager modes.\n- **Capturing Cinematic video**: Capture video with an adjustable depth of field and focus points.\n- **AVMultiCamPiP: Capturing from Multiple Cameras**: Simultaneously record the output from the front and back cameras into a single movie file by using a multi-camera capture session.\n- **AVCamBarcode: detecting barcodes and faces**: Identify machine readable codes or faces by using the camera.\n- **AVCaptureSession**: An object that configures capture behavior and coordinates the flow of data from input devices to capture outputs.\n- **AVCaptureMultiCamSession**: A capture session that supports simultaneous capture from multiple inputs of the same media type.\n- **AVCaptureInput**: An abstract superclass for objects that provide input data to a capture session.\n- **AVCaptureOutput**: An abstract superclass for objects that provide media output destinations for a capture session.\n- **AVCaptureConnection**: An object that represents a connection from a capture input to a capture output.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Configure input devices, output media, preview views, and basic settings before capturing photos or video.",
          "name" : "Setting up a capture session",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/setting-up-a-capture-session"
        },
        {
          "description" : "Operate the camera in Split View, Slide Over, Picture in Picture, and Stage Manager modes.",
          "name" : "Accessing the camera while multitasking on iPad",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVKit\/accessing-the-camera-while-multitasking-on-ipad"
        },
        {
          "description" : "Capture video with an adjustable depth of field and focus points.",
          "name" : "Capturing Cinematic video",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/capturing-cinematic-video"
        },
        {
          "description" : "Simultaneously record the output from the front and back cameras into a single movie file by using a multi-camera capture session.",
          "name" : "AVMultiCamPiP: Capturing from Multiple Cameras",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/avmulticampip-capturing-from-multiple-cameras"
        },
        {
          "description" : "Identify machine readable codes or faces by using the camera.",
          "name" : "AVCamBarcode: detecting barcodes and faces",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/avcambarcode-detecting-barcodes-and-faces"
        },
        {
          "description" : "An object that configures capture behavior and coordinates the flow of data from input devices to capture outputs.",
          "name" : "AVCaptureSession",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVCaptureSession"
        },
        {
          "description" : "A capture session that supports simultaneous capture from multiple inputs of the same media type.",
          "name" : "AVCaptureMultiCamSession",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVCaptureMultiCamSession"
        },
        {
          "description" : "An abstract superclass for objects that provide input data to a capture session.",
          "name" : "AVCaptureInput",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVCaptureInput"
        },
        {
          "description" : "An abstract superclass for objects that provide media output destinations for a capture session.",
          "name" : "AVCaptureOutput",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVCaptureOutput"
        },
        {
          "description" : "An object that represents a connection from a capture input to a capture output.",
          "name" : "AVCaptureConnection",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVCaptureConnection"
        }
      ],
      "title" : "Capture sessions"
    }
  ],
  "source" : "appleJSON",
  "title" : "AVCam: Building a camera app",
  "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/avcam-building-a-camera-app"
}