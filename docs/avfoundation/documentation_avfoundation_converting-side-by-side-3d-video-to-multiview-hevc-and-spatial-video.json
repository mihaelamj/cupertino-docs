{
  "abstract" : "Create video content for visionOS by converting an existing 3D HEVC file to a multiview HEVC format, optionally adding spatial metadata to create a spatial video.",
  "codeExamples" : [
    {
      "code" : "let asset = AVURLAsset(url: url)\nreader = try AVAssetReader(asset: asset)\n\n\/\/ Get the side-by-side video track.\nguard let videoTrack = try await asset.loadTracks(withMediaCharacteristic: .visual).first else {\n    fatalError(\"Error loading side-by-side video input\")\n}",
      "language" : "swift"
    },
    {
      "code" : "sideBySideFrameSize = try await videoTrack.load(.naturalSize)\neyeFrameSize = CGSize(width: sideBySideFrameSize.width \/ 2, height: sideBySideFrameSize.height)",
      "language" : "swift"
    },
    {
      "code" : "let readerSettings: [String: Any] = [\n    kCVPixelBufferIOSurfacePropertiesKey as String: [String: String]()\n]\nsideBySideTrack = AVAssetReaderTrackOutput(track: videoTrack, outputSettings: readerSettings)\n\nif reader.canAdd(sideBySideTrack) {\n    reader.add(sideBySideTrack)\n}\n\nif !reader.startReading() {\n    fatalError(reader.error?.localizedDescription ?? \"Unknown error during track read start\")\n}",
      "language" : "swift"
    },
    {
      "code" : "var multiviewCompressionProperties: [CFString: Any] = [\n    kVTCompressionPropertyKey_MVHEVCVideoLayerIDs: MVHEVCVideoLayerIDs,\n    kVTCompressionPropertyKey_MVHEVCViewIDs: MVHEVCViewIDs,\n    kVTCompressionPropertyKey_MVHEVCLeftAndRightViewIDs: MVHEVCLeftAndRightViewIDs,\n    kVTCompressionPropertyKey_HasLeftStereoEyeView: true,\n    kVTCompressionPropertyKey_HasRightStereoEyeView: true\n]",
      "language" : "swift"
    },
    {
      "code" : "let MVHEVCVideoLayerIDs = [0, 1]\n\n\/\/ For simplicity, choose view IDs that match the layer IDs.\nlet MVHEVCViewIDs = [0, 1]\n\n\/\/ The first element in this array is the view ID of the left eye.\nlet MVHEVCLeftAndRightViewIDs = [0, 1]",
      "language" : "swift"
    },
    {
      "code" : "if let spatialMetadata {\n\n    let baselineInMicrometers = UInt32(1000.0 * spatialMetadata.baselineInMillimeters)\n    let encodedHorizontalFOV = UInt32(1000.0 * spatialMetadata.horizontalFOV)\n    let encodedDisparityAdjustment = Int32(10_000.0 * spatialMetadata.disparityAdjustment)\n\n    multiviewCompressionProperties[kVTCompressionPropertyKey_ProjectionKind] = kCMFormatDescriptionProjectionKind_Rectilinear\n    multiviewCompressionProperties[kVTCompressionPropertyKey_StereoCameraBaseline] = baselineInMicrometers\n    multiviewCompressionProperties[kVTCompressionPropertyKey_HorizontalFieldOfView] = encodedHorizontalFOV\n    multiviewCompressionProperties[kVTCompressionPropertyKey_HorizontalDisparityAdjustment] = encodedDisparityAdjustment\n\n}",
      "language" : "swift"
    },
    {
      "code" : "let multiviewSettings: [String: Any] = [\n    AVVideoCodecKey: AVVideoCodecType.hevc,\n    AVVideoWidthKey: self.eyeFrameSize.width,\n    AVVideoHeightKey: self.eyeFrameSize.height,\n    AVVideoCompressionPropertiesKey: multiviewCompressionProperties\n]\n\nguard multiviewWriter.canApply(outputSettings: multiviewSettings, forMediaType: AVMediaType.video) else {\n    fatalError(\"Error applying output settings\")\n}\n\nlet frameInput = AVAssetWriterInput(mediaType: .video, outputSettings: multiviewSettings)\n\nlet sourcePixelAttributes: [String: Any] = [\n    kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32ARGB,\n    kCVPixelBufferWidthKey as String: self.sideBySideFrameSize.width,\n    kCVPixelBufferHeightKey as String: self.sideBySideFrameSize.height\n]\n\nlet bufferInputAdapter = AVAssetWriterInputTaggedPixelBufferGroupAdaptor(assetWriterInput: frameInput, sourcePixelBufferAttributes: sourcePixelAttributes)",
      "language" : "swift"
    },
    {
      "code" : "guard multiviewWriter.canAdd(frameInput) else {\n    fatalError(\"Error adding side-by-side video frames as input\")\n}\nmultiviewWriter.add(frameInput)",
      "language" : "swift"
    },
    {
      "code" : "guard multiviewWriter.startWriting() else {\n    fatalError(\"Failed to start writing multiview output file\")\n}\nmultiviewWriter.startSession(atSourceTime: CMTime.zero)\n\n\/\/ The dispatch queue executes the closure when media reads from the input file are available.\nframeInput.requestMediaDataWhenReady(on: DispatchQueue(label: \"Multiview HEVC Writer\")) {",
      "language" : "swift"
    },
    {
      "code" : "var session: VTPixelTransferSession? = nil\nguard VTPixelTransferSessionCreate(allocator: kCFAllocatorDefault, pixelTransferSessionOut: &session) == noErr, let session else {\n    fatalError(\"Failed to create pixel transfer\")\n}\nguard let pixelBufferPool = bufferInputAdapter.pixelBufferPool else {\n    fatalError(\"Failed to retrieve existing pixel buffer pool\")\n}",
      "language" : "swift"
    },
    {
      "code" : "while frameInput.isReadyForMoreMediaData && bufferInputAdapter.assetWriterInput.isReadyForMoreMediaData {\n    if let sampleBuffer = self.sideBySideTrack.copyNextSampleBuffer() {\n        guard let imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else {\n            fatalError(\"Failed to load source samples as an image buffer\")\n        }\n        let taggedBuffers = self.convertFrame(fromSideBySide: imageBuffer, with: pixelBufferPool, in: session)\n        let newPTS = sampleBuffer.outputPresentationTimeStamp\n        if !bufferInputAdapter.appendTaggedBuffers(taggedBuffers, withPresentationTime: newPTS) {\n            fatalError(\"Failed to append tagged buffers to multiview output\")\n        }",
      "language" : "swift"
    },
    {
      "code" : "frameInput.markAsFinished()\nmultiviewWriter.finishWriting {\n    continuation.resume()\n}\n\nbreak",
      "language" : "swift"
    },
    {
      "code" : "var pixelBuffer: CVPixelBuffer?\nCVPixelBufferPoolCreatePixelBuffer(kCFAllocatorDefault, pixelBufferPool, &pixelBuffer)\nguard let pixelBuffer else {\n    fatalError(\"Failed to create pixel buffer for layer \\(layerID)\")\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Crop the transfer region to the current eye.\nlet apertureOffset = -(self.eyeFrameSize.width \/ 2) + CGFloat(layerID) * self.eyeFrameSize.width\nlet cropRectDict = [\n    kCVImageBufferCleanApertureHorizontalOffsetKey: apertureOffset,\n    kCVImageBufferCleanApertureVerticalOffsetKey: 0,\n    kCVImageBufferCleanApertureWidthKey: self.eyeFrameSize.width,\n    kCVImageBufferCleanApertureHeightKey: self.eyeFrameSize.height\n]\nCVBufferSetAttachment(imageBuffer, kCVImageBufferCleanApertureKey, cropRectDict as CFDictionary, CVAttachmentMode.shouldPropagate)\nVTSessionSetProperty(session, key: kVTPixelTransferPropertyKey_ScalingMode, value: kVTScalingMode_CropSourceToCleanAperture)\n\n\/\/ Transfer the image to the pixel buffer.\nguard VTPixelTransferSessionTransferImage(session, from: imageBuffer, to: pixelBuffer) == noErr else {\n    fatalError(\"Error during pixel transfer session for layer \\(layerID)\")\n}",
      "language" : "swift"
    },
    {
      "code" : "let tags: [CMTag] = [.videoLayerID(Int64(layerID)), .stereoView(eye)]\nlet buffer = CMTaggedBuffer(tags: tags, buffer: .pixelBuffer(pixelBuffer))\ntaggedBuffers.append(buffer)",
      "language" : "swift"
    }
  ],
  "contentHash" : "22d7db88be32a7459b684074fb24970ce34037f369e814432716522c67ec4c97",
  "crawledAt" : "2025-12-02T15:30:02Z",
  "id" : "4797B28B-9807-4B86-8219-AE293A2EEAEB",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "AVFoundation",
  "overview" : "## Overview\n\nIn visionOS, 3D video uses the *Multiview High Efficiency Video Encoding* (MV-HEVC) format, supported by MPEG4 and QuickTime. Unlike other 3D media, MV-HEVC stores a single track containing multiple layers for the video, where the track and layers share a frame size. This track frame size is different from other 3D video types, such as *side-by-side video*. Side-by-side videos use a single track, and place the left- and right-eye images next to each other as part of a single video frame.\n\nTo convert side-by-side video to MV-HEVC, you load the source video, extract each frame, and then split the frame horizontally. Then copy the left and right sides of the split frame into the left- and right-eye layers, writing a frame containing both layers to the output.\n\nThis sample app demonstrates the process for converting side-by-side video files to MV-HEVC, encoding the output as a QuickTime file. The output is placed in the same directory as the input file, with `_MVHEVC` appended to the original filename.\n\nFor videos you capture with a consistent camera configuration, you can optionally add spatial metadata to the output file. *Spatial metadata* describes properties of the left- and right-eye cameras that captured the stereo scene.\n\nAdding spatial metadata to a stereo MV-HEVC video prompts Apple platforms to consider the video as *spatial* instead of just stereo, and opts the video into visual treatments on Apple Vision Pro that can minimize common causes of stereo viewing discomfort.\n\nTo learn more about when to provide spatial metadata for a stereo MV-HEVC video and the metadata values to provide, see [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/Creating-spatial-photos-and-videos-with-spatial-metadata].\n\nYou can verify this sample’s MV-HEVC output by opening it with the sample project from [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/reading-multiview-3d-video-files].\n\nFor the full details of the MV-HEVC format, see [https:\/\/developer.apple.com\/av-foundation\/HEVC-Stereo-Video-Profile.pdf] and [https:\/\/developer.apple.com\/av-foundation\/Stereo-Video-ISOBMFF-Extensions.pdf].\n\n### Configure the sample code project\n\nThe app takes a path to a side-by-side stereo input video file as a single command-line argument. To run the app in Xcode, select Product > Scheme > Edit Scheme (Command-<), and add the path to your file to Arguments Passed On Launch.\n\nTo also add spatial metadata to the file, add four additional arguments to Arguments Passed On Launch:\n\nBy default, the project’s scheme loads a side-by-side video from the Xcode project folder named `Hummingbird.mov`. This video is a sequence of renders of a 3D scene, showing an animated hummingbird model. By default, the app converts this example video to a stereo MV-HEVC file, without spatial metadata.\n\nTo add spatial metadata to the hummingbird video during conversion, choose Product > Scheme > Edit Scheme (Command-<), and select the checkbox to the left of the second row of arguments in the Arguments Passed On Launch field. This enables the following additional arguments: `--spatial --baseline 64.0 --fov 80.0 --disparityAdjustment 0.02`.\n\nThe `--spatial` argument tells the app to write spatial metadata to the output video. The virtual cameras used to create these renders were positioned 64mm apart with a horizontal field of view of 80 degrees, and so the value for the `--baseline` argument is `64.0`, and the value of the `--fov` argument is `80.0`.\n\nFor this video, a disparity adjustment of +2% of the image width produces a comfortable depth effect when the spatial video is presented in a window on Apple Vision Pro. This 2% disparity adjustment value positions the nearest object in the spatial video — the hummingbird — just behind the front of the window, while still keeping an effective illusion of depth between the hummingbird and the background. The scheme’s arguments express the +2% disparity adjustment with a `--disparityAdjustment` value of `0.02`.\n\n### Load the side-by-side video\n\nThe app starts by loading the side-by-side video, creating an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetReader]. The app calls [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAsset\/loadTracks(withMediaCharacteristic:completionHandler:)] to load video tracks, and then selects the first track available as the side-by-side input.\n\nThe app also stores the frame size for the side-by-side video, and calculates the size of the output frames.\n\nTo finish loading the video, the app creates an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetReaderTrackOutput] and then adds this output stream to the `AVAssetReader`.\n\nWhen creating the reader track output, the app specifies the file’s pixel format and [doc:\/\/com.apple.documentation\/documentation\/IOSurface] settings in the `readerSettings` dictionary. The app indicates that output goes to a 32-bit ARGB pixel buffer, using [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/kCVPixelBufferPixelFormatTypeKey] with a value of [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/kCVPixelFormatType_32ARGB]. The sample app also manages its own pixel buffer allocations, passing an empty array as the value for [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/kCVPixelBufferIOSurfacePropertiesKey].\n\n### Configure the output MV-HEVC file\n\nWith the reader initialized, the app calls the `async` method `transcodeToMVHEVC(output:spatialMetadata:)` to generate the output file. First, the app creates a new [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriter] pointing to the video output location, and then configures the necessary information on the output to indicate that the file contains MV-HEVC video.\n\n[doc:\/\/com.apple.documentation\/documentation\/VideoToolbox\/kVTCompressionPropertyKey_HasLeftStereoEyeView] and [doc:\/\/com.apple.documentation\/documentation\/VideoToolbox\/kVTCompressionPropertyKey_HasRightStereoEyeView] are `true`, because the output contains a layer for each eye. [doc:\/\/com.apple.documentation\/documentation\/VideoToolbox\/kVTCompressionPropertyKey_MVHEVCVideoLayerIDs], [doc:\/\/com.apple.documentation\/documentation\/VideoToolbox\/kVTCompressionPropertyKey_MVHEVCViewIDs], and [doc:\/\/com.apple.documentation\/documentation\/VideoToolbox\/kVTCompressionPropertyKey_MVHEVCLeftAndRightViewIDs] define the layer and view IDs to use for multiview HEVC encoding. In the sample app, these are all the same.\n\nThe sample app uses `0` for the left eye layer\/view ID and `1` for the right eye layer\/view ID.\n\n### Include spatial metadata\n\nIf the person calling this command-line app requested to add spatial metadata to the output file, and provided the necessary spatial metadata, the app converts that metadata to expected units and scales, and adds an additional compression property key for each metadata value. The app also specifies that the input uses a rectilinear projection, to indicate that it has the expected projection for spatial video.\n\n### Configure the MV-HEVC input source\n\nThe app transcodes video by directly copying pixels from the source frame. Writing track data to a video file requires an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriterInput]. The sample app uses an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriterInputTaggedPixelBufferGroupAdaptor] to provide pixel data from the source, writing to the output.\n\nThe `AVAssetWriterInput` source uses the same `outputSettings` as `videoWriter`, and the created pixel buffer adapter has the same frame size as the source. The app follows the best practice of calling [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriter\/canAdd(_:)-6al7j] to check the input adapter compatibility before calling [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriter\/add(_:)-4c4d0] to use it as a source.\n\n### Process input as it becomes available\n\nThe app calls [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriter\/startWriting()] and [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriter\/startSession(atSourceTime:)] in sequence to start the video writing process, and then iterates over available frame inputs with [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriterInput\/requestMediaDataWhenReady(on:using:)].\n\nThe closure argument of `requestMediaDataWhenReady(on:using:)` runs on the provided [doc:\/\/com.apple.documentation\/documentation\/Dispatch\/DispatchQueue] when the first data read is available. The closure itself is responsible for managing resources that process the media data, and running a loop to process data efficiently.\n\n### Create the video frame transfer session and output pixel buffer pool\n\nTo perform the data transfer from the source track, the pixel input adapter requires a pixel buffer as a source. The app creates a [doc:\/\/com.apple.documentation\/documentation\/VideoToolbox\/VTPixelTransferSession] to allow for reading data from the video source, and uses the `AVAssetWriterInputTaggedPixelBufferGroupAdaptor`’s existing pixel buffer pool to allocate pixel buffers for the new multiview eye layers.\n\n### Copy frame images from input to output\n\nAfter preparing resources, the app then begins a loop to process frames until there’s no more data, or the input read has stopped to buffer data. The [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriterInput\/isReadyForMoreMediaData] property of an input source is `true` if another frame is immediately available to process. When a frame is ready, a [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/CVImageBuffer] instance is created from it.\n\nThe app is now ready to handle sampling. If there’s an available sample, the app processes it in the `convertFrame` method, then calls [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriterInputTaggedPixelBufferGroupAdaptor\/appendTaggedBuffers(_:withPresentationTime:)], copying the side-by-side sample buffer’s [doc:\/\/com.apple.documentation\/documentation\/CoreMedia\/CMSampleBuffer\/outputPresentationTimeStamp] timestamp to the new multiview timestamp.\n\nInput reading finishes when there are no more sample buffers to process from the input stream. The app calls [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriterInput\/markAsFinished()] to close the stream, and [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriter\/finishWriting(completionHandler:)] to complete the multiview video write. The app also calls [doc:\/\/com.apple.documentation\/documentation\/Swift\/CheckedContinuation\/resume()] on its associated [doc:\/\/com.apple.documentation\/documentation\/Swift\/CheckedContinuation], to return to the `await` call, then breaks from the processing loop.\n\n### Convert side-by-side inputs into video layer outputs\n\nIn the `convertFrame` method, the app processes the left- and right-eye images for the frame by `layerID`, using `0` for the left eye and `1` for the right. First, the app creates a pixel buffer from the pool.\n\nThe method then uses its passed `VTPixelTransferSession` to copy the pixels from the side-by-side source, placing them into the created output sample buffer by cropping the frame to include only one eye’s image.\n\nSetting aperture view properties on [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/CVBufferSetAttachment(_:_:_:_:)] defines how to capture and crop input images. The aperture here is the size of an eye image, and the center of the capture frame offset with [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/kCVImageBufferCleanApertureHorizontalOffsetKey] by `-0.5 * width` for the left eye and `+0.5 * width` for the right eye, to capture the correct half of the side-by-side frame.\n\nThe app then calls [doc:\/\/com.apple.documentation\/documentation\/VideoToolbox\/VTSessionSetProperty(_:key:value:)] to crop the image to the aperture frame with [doc:\/\/com.apple.documentation\/documentation\/VideoToolbox\/kVTScalingMode_CropSourceToCleanAperture]. Next, the app calls [doc:\/\/com.apple.documentation\/documentation\/VideoToolbox\/VTPixelTransferSessionTransferImage(_:from:to:)] to copy source pixels to the destination buffer.\n\nThe final step is to create a [doc:\/\/com.apple.documentation\/documentation\/CoreMedia\/CMTaggedBuffer] for the eye image to return to the calling output writer.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/AVFoundation\/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video\ncrawled: 2025-12-02T15:30:02Z\n---\n\n# Converting side-by-side 3D video to multiview HEVC and spatial video\n\n**Sample Code**\n\nCreate video content for visionOS by converting an existing 3D HEVC file to a multiview HEVC format, optionally adding spatial metadata to create a spatial video.\n\n## Overview\n\nIn visionOS, 3D video uses the *Multiview High Efficiency Video Encoding* (MV-HEVC) format, supported by MPEG4 and QuickTime. Unlike other 3D media, MV-HEVC stores a single track containing multiple layers for the video, where the track and layers share a frame size. This track frame size is different from other 3D video types, such as *side-by-side video*. Side-by-side videos use a single track, and place the left- and right-eye images next to each other as part of a single video frame.\n\nTo convert side-by-side video to MV-HEVC, you load the source video, extract each frame, and then split the frame horizontally. Then copy the left and right sides of the split frame into the left- and right-eye layers, writing a frame containing both layers to the output.\n\nThis sample app demonstrates the process for converting side-by-side video files to MV-HEVC, encoding the output as a QuickTime file. The output is placed in the same directory as the input file, with `_MVHEVC` appended to the original filename.\n\nFor videos you capture with a consistent camera configuration, you can optionally add spatial metadata to the output file. *Spatial metadata* describes properties of the left- and right-eye cameras that captured the stereo scene.\n\nAdding spatial metadata to a stereo MV-HEVC video prompts Apple platforms to consider the video as *spatial* instead of just stereo, and opts the video into visual treatments on Apple Vision Pro that can minimize common causes of stereo viewing discomfort.\n\nTo learn more about when to provide spatial metadata for a stereo MV-HEVC video and the metadata values to provide, see [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/Creating-spatial-photos-and-videos-with-spatial-metadata].\n\nYou can verify this sample’s MV-HEVC output by opening it with the sample project from [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/reading-multiview-3d-video-files].\n\nFor the full details of the MV-HEVC format, see [https:\/\/developer.apple.com\/av-foundation\/HEVC-Stereo-Video-Profile.pdf] and [https:\/\/developer.apple.com\/av-foundation\/Stereo-Video-ISOBMFF-Extensions.pdf].\n\n### Configure the sample code project\n\nThe app takes a path to a side-by-side stereo input video file as a single command-line argument. To run the app in Xcode, select Product > Scheme > Edit Scheme (Command-<), and add the path to your file to Arguments Passed On Launch.\n\nTo also add spatial metadata to the file, add four additional arguments to Arguments Passed On Launch:\n\n- `--spatial` (or `-s`) to indicate that you want to include spatial metadata\n- `--baseline` (or `-b`) to provide a baseline in millimeters (for example, `--baseline 64.0` for a 64mm baseline)\n- `--fov` (or `-f`) to provide a horizontal field of view in degrees (for example, `--fov 80.0` for an 80-degree field of view)\n- `--disparityAdjustment` (or `-d`) to provide a disparity adjustment (for example, `--disparityAdjustment 0.02` for a 2% positive disparity shift)\n\nBy default, the project’s scheme loads a side-by-side video from the Xcode project folder named `Hummingbird.mov`. This video is a sequence of renders of a 3D scene, showing an animated hummingbird model. By default, the app converts this example video to a stereo MV-HEVC file, without spatial metadata.\n\nTo add spatial metadata to the hummingbird video during conversion, choose Product > Scheme > Edit Scheme (Command-<), and select the checkbox to the left of the second row of arguments in the Arguments Passed On Launch field. This enables the following additional arguments: `--spatial --baseline 64.0 --fov 80.0 --disparityAdjustment 0.02`.\n\nThe `--spatial` argument tells the app to write spatial metadata to the output video. The virtual cameras used to create these renders were positioned 64mm apart with a horizontal field of view of 80 degrees, and so the value for the `--baseline` argument is `64.0`, and the value of the `--fov` argument is `80.0`.\n\nFor this video, a disparity adjustment of +2% of the image width produces a comfortable depth effect when the spatial video is presented in a window on Apple Vision Pro. This 2% disparity adjustment value positions the nearest object in the spatial video — the hummingbird — just behind the front of the window, while still keeping an effective illusion of depth between the hummingbird and the background. The scheme’s arguments express the +2% disparity adjustment with a `--disparityAdjustment` value of `0.02`.\n\n### Load the side-by-side video\n\nThe app starts by loading the side-by-side video, creating an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetReader]. The app calls [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAsset\/loadTracks(withMediaCharacteristic:completionHandler:)] to load video tracks, and then selects the first track available as the side-by-side input.\n\n```swift\nlet asset = AVURLAsset(url: url)\nreader = try AVAssetReader(asset: asset)\n\n\/\/ Get the side-by-side video track.\nguard let videoTrack = try await asset.loadTracks(withMediaCharacteristic: .visual).first else {\n    fatalError(\"Error loading side-by-side video input\")\n}\n```\n\nThe app also stores the frame size for the side-by-side video, and calculates the size of the output frames.\n\n```swift\nsideBySideFrameSize = try await videoTrack.load(.naturalSize)\neyeFrameSize = CGSize(width: sideBySideFrameSize.width \/ 2, height: sideBySideFrameSize.height)\n```\n\nTo finish loading the video, the app creates an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetReaderTrackOutput] and then adds this output stream to the `AVAssetReader`.\n\n```swift\nlet readerSettings: [String: Any] = [\n    kCVPixelBufferIOSurfacePropertiesKey as String: [String: String]()\n]\nsideBySideTrack = AVAssetReaderTrackOutput(track: videoTrack, outputSettings: readerSettings)\n\nif reader.canAdd(sideBySideTrack) {\n    reader.add(sideBySideTrack)\n}\n\nif !reader.startReading() {\n    fatalError(reader.error?.localizedDescription ?? \"Unknown error during track read start\")\n}\n```\n\nWhen creating the reader track output, the app specifies the file’s pixel format and [doc:\/\/com.apple.documentation\/documentation\/IOSurface] settings in the `readerSettings` dictionary. The app indicates that output goes to a 32-bit ARGB pixel buffer, using [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/kCVPixelBufferPixelFormatTypeKey] with a value of [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/kCVPixelFormatType_32ARGB]. The sample app also manages its own pixel buffer allocations, passing an empty array as the value for [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/kCVPixelBufferIOSurfacePropertiesKey].\n\n### Configure the output MV-HEVC file\n\nWith the reader initialized, the app calls the `async` method `transcodeToMVHEVC(output:spatialMetadata:)` to generate the output file. First, the app creates a new [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriter] pointing to the video output location, and then configures the necessary information on the output to indicate that the file contains MV-HEVC video.\n\n```swift\nvar multiviewCompressionProperties: [CFString: Any] = [\n    kVTCompressionPropertyKey_MVHEVCVideoLayerIDs: MVHEVCVideoLayerIDs,\n    kVTCompressionPropertyKey_MVHEVCViewIDs: MVHEVCViewIDs,\n    kVTCompressionPropertyKey_MVHEVCLeftAndRightViewIDs: MVHEVCLeftAndRightViewIDs,\n    kVTCompressionPropertyKey_HasLeftStereoEyeView: true,\n    kVTCompressionPropertyKey_HasRightStereoEyeView: true\n]\n```\n\n[doc:\/\/com.apple.documentation\/documentation\/VideoToolbox\/kVTCompressionPropertyKey_HasLeftStereoEyeView] and [doc:\/\/com.apple.documentation\/documentation\/VideoToolbox\/kVTCompressionPropertyKey_HasRightStereoEyeView] are `true`, because the output contains a layer for each eye. [doc:\/\/com.apple.documentation\/documentation\/VideoToolbox\/kVTCompressionPropertyKey_MVHEVCVideoLayerIDs], [doc:\/\/com.apple.documentation\/documentation\/VideoToolbox\/kVTCompressionPropertyKey_MVHEVCViewIDs], and [doc:\/\/com.apple.documentation\/documentation\/VideoToolbox\/kVTCompressionPropertyKey_MVHEVCLeftAndRightViewIDs] define the layer and view IDs to use for multiview HEVC encoding. In the sample app, these are all the same.\n\nThe sample app uses `0` for the left eye layer\/view ID and `1` for the right eye layer\/view ID.\n\n```swift\nlet MVHEVCVideoLayerIDs = [0, 1]\n\n\/\/ For simplicity, choose view IDs that match the layer IDs.\nlet MVHEVCViewIDs = [0, 1]\n\n\/\/ The first element in this array is the view ID of the left eye.\nlet MVHEVCLeftAndRightViewIDs = [0, 1]\n```\n\n### Include spatial metadata\n\nIf the person calling this command-line app requested to add spatial metadata to the output file, and provided the necessary spatial metadata, the app converts that metadata to expected units and scales, and adds an additional compression property key for each metadata value. The app also specifies that the input uses a rectilinear projection, to indicate that it has the expected projection for spatial video.\n\n```swift\nif let spatialMetadata {\n\n    let baselineInMicrometers = UInt32(1000.0 * spatialMetadata.baselineInMillimeters)\n    let encodedHorizontalFOV = UInt32(1000.0 * spatialMetadata.horizontalFOV)\n    let encodedDisparityAdjustment = Int32(10_000.0 * spatialMetadata.disparityAdjustment)\n\n    multiviewCompressionProperties[kVTCompressionPropertyKey_ProjectionKind] = kCMFormatDescriptionProjectionKind_Rectilinear\n    multiviewCompressionProperties[kVTCompressionPropertyKey_StereoCameraBaseline] = baselineInMicrometers\n    multiviewCompressionProperties[kVTCompressionPropertyKey_HorizontalFieldOfView] = encodedHorizontalFOV\n    multiviewCompressionProperties[kVTCompressionPropertyKey_HorizontalDisparityAdjustment] = encodedDisparityAdjustment\n\n}\n```\n\n### Configure the MV-HEVC input source\n\nThe app transcodes video by directly copying pixels from the source frame. Writing track data to a video file requires an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriterInput]. The sample app uses an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriterInputTaggedPixelBufferGroupAdaptor] to provide pixel data from the source, writing to the output.\n\n```swift\nlet multiviewSettings: [String: Any] = [\n    AVVideoCodecKey: AVVideoCodecType.hevc,\n    AVVideoWidthKey: self.eyeFrameSize.width,\n    AVVideoHeightKey: self.eyeFrameSize.height,\n    AVVideoCompressionPropertiesKey: multiviewCompressionProperties\n]\n\nguard multiviewWriter.canApply(outputSettings: multiviewSettings, forMediaType: AVMediaType.video) else {\n    fatalError(\"Error applying output settings\")\n}\n\nlet frameInput = AVAssetWriterInput(mediaType: .video, outputSettings: multiviewSettings)\n\nlet sourcePixelAttributes: [String: Any] = [\n    kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32ARGB,\n    kCVPixelBufferWidthKey as String: self.sideBySideFrameSize.width,\n    kCVPixelBufferHeightKey as String: self.sideBySideFrameSize.height\n]\n\nlet bufferInputAdapter = AVAssetWriterInputTaggedPixelBufferGroupAdaptor(assetWriterInput: frameInput, sourcePixelBufferAttributes: sourcePixelAttributes)\n```\n\nThe `AVAssetWriterInput` source uses the same `outputSettings` as `videoWriter`, and the created pixel buffer adapter has the same frame size as the source. The app follows the best practice of calling [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriter\/canAdd(_:)-6al7j] to check the input adapter compatibility before calling [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriter\/add(_:)-4c4d0] to use it as a source.\n\n```swift\nguard multiviewWriter.canAdd(frameInput) else {\n    fatalError(\"Error adding side-by-side video frames as input\")\n}\nmultiviewWriter.add(frameInput)\n```\n\n### Process input as it becomes available\n\nThe app calls [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriter\/startWriting()] and [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriter\/startSession(atSourceTime:)] in sequence to start the video writing process, and then iterates over available frame inputs with [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriterInput\/requestMediaDataWhenReady(on:using:)].\n\n```swift\nguard multiviewWriter.startWriting() else {\n    fatalError(\"Failed to start writing multiview output file\")\n}\nmultiviewWriter.startSession(atSourceTime: CMTime.zero)\n\n\/\/ The dispatch queue executes the closure when media reads from the input file are available.\nframeInput.requestMediaDataWhenReady(on: DispatchQueue(label: \"Multiview HEVC Writer\")) {\n```\n\nThe closure argument of `requestMediaDataWhenReady(on:using:)` runs on the provided [doc:\/\/com.apple.documentation\/documentation\/Dispatch\/DispatchQueue] when the first data read is available. The closure itself is responsible for managing resources that process the media data, and running a loop to process data efficiently.\n\n### Create the video frame transfer session and output pixel buffer pool\n\nTo perform the data transfer from the source track, the pixel input adapter requires a pixel buffer as a source. The app creates a [doc:\/\/com.apple.documentation\/documentation\/VideoToolbox\/VTPixelTransferSession] to allow for reading data from the video source, and uses the `AVAssetWriterInputTaggedPixelBufferGroupAdaptor`’s existing pixel buffer pool to allocate pixel buffers for the new multiview eye layers.\n\n```swift\nvar session: VTPixelTransferSession? = nil\nguard VTPixelTransferSessionCreate(allocator: kCFAllocatorDefault, pixelTransferSessionOut: &session) == noErr, let session else {\n    fatalError(\"Failed to create pixel transfer\")\n}\nguard let pixelBufferPool = bufferInputAdapter.pixelBufferPool else {\n    fatalError(\"Failed to retrieve existing pixel buffer pool\")\n}\n```\n\n### Copy frame images from input to output\n\nAfter preparing resources, the app then begins a loop to process frames until there’s no more data, or the input read has stopped to buffer data. The [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriterInput\/isReadyForMoreMediaData] property of an input source is `true` if another frame is immediately available to process. When a frame is ready, a [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/CVImageBuffer] instance is created from it.\n\nThe app is now ready to handle sampling. If there’s an available sample, the app processes it in the `convertFrame` method, then calls [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriterInputTaggedPixelBufferGroupAdaptor\/appendTaggedBuffers(_:withPresentationTime:)], copying the side-by-side sample buffer’s [doc:\/\/com.apple.documentation\/documentation\/CoreMedia\/CMSampleBuffer\/outputPresentationTimeStamp] timestamp to the new multiview timestamp.\n\n```swift\nwhile frameInput.isReadyForMoreMediaData && bufferInputAdapter.assetWriterInput.isReadyForMoreMediaData {\n    if let sampleBuffer = self.sideBySideTrack.copyNextSampleBuffer() {\n        guard let imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else {\n            fatalError(\"Failed to load source samples as an image buffer\")\n        }\n        let taggedBuffers = self.convertFrame(fromSideBySide: imageBuffer, with: pixelBufferPool, in: session)\n        let newPTS = sampleBuffer.outputPresentationTimeStamp\n        if !bufferInputAdapter.appendTaggedBuffers(taggedBuffers, withPresentationTime: newPTS) {\n            fatalError(\"Failed to append tagged buffers to multiview output\")\n        }\n```\n\nInput reading finishes when there are no more sample buffers to process from the input stream. The app calls [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriterInput\/markAsFinished()] to close the stream, and [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriter\/finishWriting(completionHandler:)] to complete the multiview video write. The app also calls [doc:\/\/com.apple.documentation\/documentation\/Swift\/CheckedContinuation\/resume()] on its associated [doc:\/\/com.apple.documentation\/documentation\/Swift\/CheckedContinuation], to return to the `await` call, then breaks from the processing loop.\n\n```swift\nframeInput.markAsFinished()\nmultiviewWriter.finishWriting {\n    continuation.resume()\n}\n\nbreak\n```\n\n### Convert side-by-side inputs into video layer outputs\n\nIn the `convertFrame` method, the app processes the left- and right-eye images for the frame by `layerID`, using `0` for the left eye and `1` for the right. First, the app creates a pixel buffer from the pool.\n\n```swift\nvar pixelBuffer: CVPixelBuffer?\nCVPixelBufferPoolCreatePixelBuffer(kCFAllocatorDefault, pixelBufferPool, &pixelBuffer)\nguard let pixelBuffer else {\n    fatalError(\"Failed to create pixel buffer for layer \\(layerID)\")\n}\n```\n\nThe method then uses its passed `VTPixelTransferSession` to copy the pixels from the side-by-side source, placing them into the created output sample buffer by cropping the frame to include only one eye’s image.\n\n```swift\n\/\/ Crop the transfer region to the current eye.\nlet apertureOffset = -(self.eyeFrameSize.width \/ 2) + CGFloat(layerID) * self.eyeFrameSize.width\nlet cropRectDict = [\n    kCVImageBufferCleanApertureHorizontalOffsetKey: apertureOffset,\n    kCVImageBufferCleanApertureVerticalOffsetKey: 0,\n    kCVImageBufferCleanApertureWidthKey: self.eyeFrameSize.width,\n    kCVImageBufferCleanApertureHeightKey: self.eyeFrameSize.height\n]\nCVBufferSetAttachment(imageBuffer, kCVImageBufferCleanApertureKey, cropRectDict as CFDictionary, CVAttachmentMode.shouldPropagate)\nVTSessionSetProperty(session, key: kVTPixelTransferPropertyKey_ScalingMode, value: kVTScalingMode_CropSourceToCleanAperture)\n\n\/\/ Transfer the image to the pixel buffer.\nguard VTPixelTransferSessionTransferImage(session, from: imageBuffer, to: pixelBuffer) == noErr else {\n    fatalError(\"Error during pixel transfer session for layer \\(layerID)\")\n}\n```\n\nSetting aperture view properties on [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/CVBufferSetAttachment(_:_:_:_:)] defines how to capture and crop input images. The aperture here is the size of an eye image, and the center of the capture frame offset with [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/kCVImageBufferCleanApertureHorizontalOffsetKey] by `-0.5 * width` for the left eye and `+0.5 * width` for the right eye, to capture the correct half of the side-by-side frame.\n\nThe app then calls [doc:\/\/com.apple.documentation\/documentation\/VideoToolbox\/VTSessionSetProperty(_:key:value:)] to crop the image to the aperture frame with [doc:\/\/com.apple.documentation\/documentation\/VideoToolbox\/kVTScalingMode_CropSourceToCleanAperture]. Next, the app calls [doc:\/\/com.apple.documentation\/documentation\/VideoToolbox\/VTPixelTransferSessionTransferImage(_:from:to:)] to copy source pixels to the destination buffer.\n\nThe final step is to create a [doc:\/\/com.apple.documentation\/documentation\/CoreMedia\/CMTaggedBuffer] for the eye image to return to the calling output writer.\n\n```swift\nlet tags: [CMTag] = [.videoLayerID(Int64(layerID)), .stereoView(eye)]\nlet buffer = CMTaggedBuffer(tags: tags, buffer: .pixelBuffer(pixelBuffer))\ntaggedBuffers.append(buffer)\n```\n\n## Media writing\n\n- **Converting projected video to Apple Projected Media Profile**: Convert content with equirectangular or half-equirectangular projection to APMP.\n- **Writing fragmented MPEG-4 files for HTTP Live Streaming**: Create an HTTP Live Streaming presentation by turning a movie file into a sequence of fragmented MPEG-4 files.\n- **Creating spatial photos and videos with spatial metadata**: Add spatial metadata to stereo photos and videos to create spatial media for viewing on Apple Vision Pro.\n- **Tagging media with video color information**: Inspect and set video color space information when writing and transcoding media.\n- **Evaluating an app’s video color**: Check color reproduction for a video in your app by using test patterns, video test equipment, and light-measurement instruments.\n- **AVOutputSettingsAssistant**: An object that builds audio and video output settings dictionaries.\n- **AVAssetWriter**: An object that writes media data to a container file.\n- **AVAssetWriterInput**: An object that appends media samples to a track in an asset writer’s output file.\n- **AVAssetWriterInputPixelBufferAdaptor**: An object that appends video samples to an asset writer input.\n- **AVAssetWriterInputTaggedPixelBufferGroupAdaptor**: An object that appends tagged buffer groups to an asset writer input.\n- **AVAssetWriterInputMetadataAdaptor**: An object that appends timed metadata groups to an asset writer input.\n- **AVAssetWriterInputGroup**: A group of inputs with tracks that are mutually exclusive to each other for playback or processing.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Convert content with equirectangular or half-equirectangular projection to APMP.",
          "name" : "Converting projected video to Apple Projected Media Profile",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/converting-projected-video-to-apple-projected-media-profile"
        },
        {
          "description" : "Create an HTTP Live Streaming presentation by turning a movie file into a sequence of fragmented MPEG-4 files.",
          "name" : "Writing fragmented MPEG-4 files for HTTP Live Streaming",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/writing-fragmented-mpeg-4-files-for-http-live-streaming"
        },
        {
          "description" : "Add spatial metadata to stereo photos and videos to create spatial media for viewing on Apple Vision Pro.",
          "name" : "Creating spatial photos and videos with spatial metadata",
          "url" : "https:\/\/developer.apple.com\/documentation\/ImageIO\/Creating-spatial-photos-and-videos-with-spatial-metadata"
        },
        {
          "description" : "Inspect and set video color space information when writing and transcoding media.",
          "name" : "Tagging media with video color information",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/tagging-media-with-video-color-information"
        },
        {
          "description" : "Check color reproduction for a video in your app by using test patterns, video test equipment, and light-measurement instruments.",
          "name" : "Evaluating an app’s video color",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/evaluating-an-app-s-video-color"
        },
        {
          "description" : "An object that builds audio and video output settings dictionaries.",
          "name" : "AVOutputSettingsAssistant",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVOutputSettingsAssistant"
        },
        {
          "description" : "An object that writes media data to a container file.",
          "name" : "AVAssetWriter",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVAssetWriter"
        },
        {
          "description" : "An object that appends media samples to a track in an asset writer’s output file.",
          "name" : "AVAssetWriterInput",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVAssetWriterInput"
        },
        {
          "description" : "An object that appends video samples to an asset writer input.",
          "name" : "AVAssetWriterInputPixelBufferAdaptor",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVAssetWriterInputPixelBufferAdaptor"
        },
        {
          "description" : "An object that appends tagged buffer groups to an asset writer input.",
          "name" : "AVAssetWriterInputTaggedPixelBufferGroupAdaptor",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVAssetWriterInputTaggedPixelBufferGroupAdaptor"
        },
        {
          "description" : "An object that appends timed metadata groups to an asset writer input.",
          "name" : "AVAssetWriterInputMetadataAdaptor",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVAssetWriterInputMetadataAdaptor"
        },
        {
          "description" : "A group of inputs with tracks that are mutually exclusive to each other for playback or processing.",
          "name" : "AVAssetWriterInputGroup",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVAssetWriterInputGroup"
        }
      ],
      "title" : "Media writing"
    }
  ],
  "source" : "appleJSON",
  "title" : "Converting side-by-side 3D video to multiview HEVC and spatial video",
  "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video"
}