{
  "abstract" : "Get a depth map with a photo to create effects like the system camera’s Portrait mode (on compatible devices).",
  "codeExamples" : [
    {
      "code" : "\/\/ Select a depth-capable capture device.\nguard let videoDevice = AVCaptureDevice.default(.builtInWideAngleCamera,\n    for: .video, position: .unspecified)\n    else { fatalError(\"No dual camera.\") }\nguard let videoDeviceInput = try? AVCaptureDeviceInput(device: videoDevice),\n    self.captureSession.canAddInput(videoDeviceInput)\n    else { fatalError(\"Can't add video input.\") }\nself.captureSession.beginConfiguration()\nself.captureSession.addInput(videoDeviceInput)\n\n\/\/ Set up photo output for depth data capture.\nlet photoOutput = AVCapturePhotoOutput()\nguard self.captureSession.canAddOutput(photoOutput)\n    else { fatalError(\"Can't add photo output.\") }\nself.captureSession.addOutput(photoOutput)\nself.captureSession.sessionPreset = .photo\n\/\/ Enable delivery of depth data after adding the output to the capture session.\nphotoOutput.isDepthDataDeliveryEnabled = photoOutput.isDepthDataDeliverySupported\nself.captureSession.commitConfiguration()",
      "language" : "swift"
    },
    {
      "code" : "let photoSettings = AVCapturePhotoSettings(format: [AVVideoCodecKey: AVVideoCodecType.hevc])\nphotoSettings.isDepthDataDeliveryEnabled = photoOutput.isDepthDataDeliverySupported\n\n\/\/ Shoot the photo, using a custom class to handle capture delegate callbacks.\nlet captureProcessor = PhotoCaptureProcessor()\nphotoOutput.capturePhoto(with: photoSettings, delegate: captureProcessor)",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Select a depth (not disparity) format that works with the active color format.\nlet availableFormats = captureDevice.activeFormat.supportedDepthDataFormats\n\nlet depthFormat = availableFormats.filter { format in\n    let pixelFormatType =\n        CMFormatDescriptionGetMediaSubType(format.formatDescription)\n    \n    return (pixelFormatType == kCVPixelFormatType_DepthFloat16 ||\n            pixelFormatType == kCVPixelFormatType_DepthFloat32)\n}.first\n\n\/\/ Set the capture device to use that depth format.\ncaptureSession.beginConfiguration()\ncaptureDevice.activeDepthDataFormat = depthFormat\ncaptureSession.commitConfiguration()",
      "language" : "swift"
    }
  ],
  "contentHash" : "a803799f0361f75e70622114e84d4cf6fa1eae7435f2cbe07de077fc36403bf9",
  "crawledAt" : "2025-12-02T16:09:48Z",
  "id" : "9D975AF7-7C49-443E-BFEF-EEE0215C9F8C",
  "kind" : "article",
  "language" : "swift",
  "module" : "AVFoundation",
  "overview" : "## Overview\n\nOn iOS devices with a back-facing dual camera or a front-facing TrueDepth camera, the capture system can record depth information. A depth map is like an image; however, instead of each pixel providing a color, it indicates distance from the camera to that part of the image (either in absolute terms, or relative to other pixels in the depth map).\n\nYou can use a depth map together with a photo to create image-processing effects that treat foreground and background elements of a photo differently, like the Portrait mode in the iOS Camera app. By saving color and depth data separately, you can even apply and change these effects long after a photo has been captured.\n\n\n\nYou can add depth capture to many of the other photography workflows covered in [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/capturing-still-and-live-photos] by adding the following steps.\n\n### Prepare for depth photo capture\n\nTo capture depth maps, you’ll need to first select a [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDevice\/DeviceType-swift.struct\/builtInDualCamera] or [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDevice\/DeviceType-swift.struct\/builtInTrueDepthCamera] capture device as your session’s video input. Even if an iOS device has a dual camera or TrueDepth camera, selecting the default back- or front-facing camera doesn’t enable depth capture.\n\nCapturing depth also requires an internal reconfiguration of the capture pipeline, briefly delaying capture and interrupting any in-progress captures. Before shooting your first depth photo, make sure you configure the pipeline appropriately by enabling depth capture on your [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput] object.\n\n### Choose settings\n\nOnce your photo output is ready for depth capture, you can request that any individual photos capture a depth map along with the color image. Create an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoSettings] object, choosing the format for the color image. Then, enable depth capture and depth output (and any other settings you’d like for that photo) and call the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput\/capturePhoto(with:delegate:)] method.\n\n### Handle results\n\nAfter a capture, the photo output calls your delegate’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoCaptureDelegate\/photoOutput(_:didFinishProcessingPhoto:error:)] method, providing the photo and captured depth data as an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhoto] object.\n\nIf you plan to use the captured depth data immediately—for example, to display a preview of a depth-based image processing effect—you can find it in the photo object’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhoto\/depthData] property.\n\nOtherwise, the capture output embeds depth data and depth-related metadata when you use the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhoto\/fileDataRepresentation()] method to produce file data for saving the photo. If you add the resulting file to the Photos library, other apps (including the system Photos app) automatically recognize the depth data within and can apply depth-based image processing effects. (If you need to disable this option, see the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoSettings\/embedsDepthDataInPhoto] setting).\n\n### About disparity, depth, and accuracy\n\nWhen you enable depth capture with the back-facing dual camera on compatible devices (see [https:\/\/developer.apple.com\/library\/archive\/documentation\/DeviceInformation\/Reference\/iOSDeviceCompatibility\/Introduction\/Introduction.html#\/\/apple_ref\/doc\/uid\/TP40013599]), the system captures imagery using both cameras. Because the two parallel cameras are a small distance apart on the back of the device, similar features found in both images show a parallax shift: objects that are closer to the camera shift by a greater distance between the two images. The capture system uses this difference, or *disparity*, to infer the relative distances from the camera to objects in the image, as shown below.\n\n\n\nEach point in a depth map captured by a dual camera device measures disparity units of 1\/meters, and offers [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVDepthData\/Accuracy\/relative] accuracy. An individual point isn’t a good estimate of real-world distance, but the variation between points is consistent enough to use for depth-based image processing effects.\n\nThe TrueDepth camera projects an infrared light pattern in front of the camera and images that pattern with an infrared camera. By observing how objects in the scene distorts the pattern, the capture system can calculate the distance rom the camera to each point in the image.\n\nThe TrueDepth camera produces disparity maps by default so that the resulting depth data is similar to that produced by a dual camera device. However, unlike a dual camera device, the TrueDepth camera can directly measure depth (in meters) with [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVDepthData\/Accuracy\/absolute] accuracy. To capture depth instead of disparity, set the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDevice\/activeDepthDataFormat] of the capture device before starting your capture session:",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/AVFoundation\/capturing-photos-with-depth\ncrawled: 2025-12-02T16:09:48Z\n---\n\n# Capturing photos with depth\n\n**Article**\n\nGet a depth map with a photo to create effects like the system camera’s Portrait mode (on compatible devices).\n\n## Overview\n\nOn iOS devices with a back-facing dual camera or a front-facing TrueDepth camera, the capture system can record depth information. A depth map is like an image; however, instead of each pixel providing a color, it indicates distance from the camera to that part of the image (either in absolute terms, or relative to other pixels in the depth map).\n\nYou can use a depth map together with a photo to create image-processing effects that treat foreground and background elements of a photo differently, like the Portrait mode in the iOS Camera app. By saving color and depth data separately, you can even apply and change these effects long after a photo has been captured.\n\n\n\nYou can add depth capture to many of the other photography workflows covered in [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/capturing-still-and-live-photos] by adding the following steps.\n\n### Prepare for depth photo capture\n\nTo capture depth maps, you’ll need to first select a [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDevice\/DeviceType-swift.struct\/builtInDualCamera] or [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDevice\/DeviceType-swift.struct\/builtInTrueDepthCamera] capture device as your session’s video input. Even if an iOS device has a dual camera or TrueDepth camera, selecting the default back- or front-facing camera doesn’t enable depth capture.\n\nCapturing depth also requires an internal reconfiguration of the capture pipeline, briefly delaying capture and interrupting any in-progress captures. Before shooting your first depth photo, make sure you configure the pipeline appropriately by enabling depth capture on your [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput] object.\n\n```swift\n\/\/ Select a depth-capable capture device.\nguard let videoDevice = AVCaptureDevice.default(.builtInWideAngleCamera,\n    for: .video, position: .unspecified)\n    else { fatalError(\"No dual camera.\") }\nguard let videoDeviceInput = try? AVCaptureDeviceInput(device: videoDevice),\n    self.captureSession.canAddInput(videoDeviceInput)\n    else { fatalError(\"Can't add video input.\") }\nself.captureSession.beginConfiguration()\nself.captureSession.addInput(videoDeviceInput)\n\n\/\/ Set up photo output for depth data capture.\nlet photoOutput = AVCapturePhotoOutput()\nguard self.captureSession.canAddOutput(photoOutput)\n    else { fatalError(\"Can't add photo output.\") }\nself.captureSession.addOutput(photoOutput)\nself.captureSession.sessionPreset = .photo\n\/\/ Enable delivery of depth data after adding the output to the capture session.\nphotoOutput.isDepthDataDeliveryEnabled = photoOutput.isDepthDataDeliverySupported\nself.captureSession.commitConfiguration()\n```\n\n\n\n### Choose settings\n\nOnce your photo output is ready for depth capture, you can request that any individual photos capture a depth map along with the color image. Create an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoSettings] object, choosing the format for the color image. Then, enable depth capture and depth output (and any other settings you’d like for that photo) and call the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput\/capturePhoto(with:delegate:)] method.\n\n```swift\nlet photoSettings = AVCapturePhotoSettings(format: [AVVideoCodecKey: AVVideoCodecType.hevc])\nphotoSettings.isDepthDataDeliveryEnabled = photoOutput.isDepthDataDeliverySupported\n\n\/\/ Shoot the photo, using a custom class to handle capture delegate callbacks.\nlet captureProcessor = PhotoCaptureProcessor()\nphotoOutput.capturePhoto(with: photoSettings, delegate: captureProcessor)\n```\n\n### Handle results\n\nAfter a capture, the photo output calls your delegate’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoCaptureDelegate\/photoOutput(_:didFinishProcessingPhoto:error:)] method, providing the photo and captured depth data as an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhoto] object.\n\nIf you plan to use the captured depth data immediately—for example, to display a preview of a depth-based image processing effect—you can find it in the photo object’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhoto\/depthData] property.\n\nOtherwise, the capture output embeds depth data and depth-related metadata when you use the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhoto\/fileDataRepresentation()] method to produce file data for saving the photo. If you add the resulting file to the Photos library, other apps (including the system Photos app) automatically recognize the depth data within and can apply depth-based image processing effects. (If you need to disable this option, see the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoSettings\/embedsDepthDataInPhoto] setting).\n\n### About disparity, depth, and accuracy\n\nWhen you enable depth capture with the back-facing dual camera on compatible devices (see [https:\/\/developer.apple.com\/library\/archive\/documentation\/DeviceInformation\/Reference\/iOSDeviceCompatibility\/Introduction\/Introduction.html#\/\/apple_ref\/doc\/uid\/TP40013599]), the system captures imagery using both cameras. Because the two parallel cameras are a small distance apart on the back of the device, similar features found in both images show a parallax shift: objects that are closer to the camera shift by a greater distance between the two images. The capture system uses this difference, or *disparity*, to infer the relative distances from the camera to objects in the image, as shown below.\n\n\n\nEach point in a depth map captured by a dual camera device measures disparity units of 1\/meters, and offers [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVDepthData\/Accuracy\/relative] accuracy. An individual point isn’t a good estimate of real-world distance, but the variation between points is consistent enough to use for depth-based image processing effects.\n\nThe TrueDepth camera projects an infrared light pattern in front of the camera and images that pattern with an infrared camera. By observing how objects in the scene distorts the pattern, the capture system can calculate the distance rom the camera to each point in the image.\n\nThe TrueDepth camera produces disparity maps by default so that the resulting depth data is similar to that produced by a dual camera device. However, unlike a dual camera device, the TrueDepth camera can directly measure depth (in meters) with [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVDepthData\/Accuracy\/absolute] accuracy. To capture depth instead of disparity, set the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDevice\/activeDepthDataFormat] of the capture device before starting your capture session:\n\n```swift\n\/\/ Select a depth (not disparity) format that works with the active color format.\nlet availableFormats = captureDevice.activeFormat.supportedDepthDataFormats\n\nlet depthFormat = availableFormats.filter { format in\n    let pixelFormatType =\n        CMFormatDescriptionGetMediaSubType(format.formatDescription)\n    \n    return (pixelFormatType == kCVPixelFormatType_DepthFloat16 ||\n            pixelFormatType == kCVPixelFormatType_DepthFloat32)\n}.first\n\n\/\/ Set the capture device to use that depth format.\ncaptureSession.beginConfiguration()\ncaptureDevice.activeDepthDataFormat = depthFormat\ncaptureSession.commitConfiguration()\n```\n\n## Depth data capture\n\n- **Creating auxiliary depth data manually**: Generate a depth image and attach it to your own image.\n- **Capturing depth using the LiDAR camera**: Access the LiDAR camera on supporting devices to capture precise depth data.\n- **AVCamFilter: Applying filters to a capture stream**: Render a capture stream with rose-colored filtering and depth effects.\n- **Streaming depth data from the TrueDepth camera**: Visualize depth data in 2D and 3D from the TrueDepth camera.\n- **Enhancing live video by leveraging TrueDepth camera data**: Apply your own background to a live capture feed streamed from the front-facing TrueDepth camera.\n- **AVCaptureDepthDataOutput**: A capture output that records scene depth information on compatible camera devices.\n- **AVDepthData**: A container for per-pixel distance or disparity information captured by compatible camera devices.\n- **AVCameraCalibrationData**: Information about the camera characteristics used to capture images and depth data.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Generate a depth image and attach it to your own image.",
          "name" : "Creating auxiliary depth data manually",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/creating-auxiliary-depth-data-manually"
        },
        {
          "description" : "Access the LiDAR camera on supporting devices to capture precise depth data.",
          "name" : "Capturing depth using the LiDAR camera",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/capturing-depth-using-the-lidar-camera"
        },
        {
          "description" : "Render a capture stream with rose-colored filtering and depth effects.",
          "name" : "AVCamFilter: Applying filters to a capture stream",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/avcamfilter-applying-filters-to-a-capture-stream"
        },
        {
          "description" : "Visualize depth data in 2D and 3D from the TrueDepth camera.",
          "name" : "Streaming depth data from the TrueDepth camera",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/streaming-depth-data-from-the-truedepth-camera"
        },
        {
          "description" : "Apply your own background to a live capture feed streamed from the front-facing TrueDepth camera.",
          "name" : "Enhancing live video by leveraging TrueDepth camera data",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/enhancing-live-video-by-leveraging-truedepth-camera-data"
        },
        {
          "description" : "A capture output that records scene depth information on compatible camera devices.",
          "name" : "AVCaptureDepthDataOutput",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVCaptureDepthDataOutput"
        },
        {
          "description" : "A container for per-pixel distance or disparity information captured by compatible camera devices.",
          "name" : "AVDepthData",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVDepthData"
        },
        {
          "description" : "Information about the camera characteristics used to capture images and depth data.",
          "name" : "AVCameraCalibrationData",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVCameraCalibrationData"
        }
      ],
      "title" : "Depth data capture"
    }
  ],
  "source" : "appleJSON",
  "title" : "Capturing photos with depth",
  "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/capturing-photos-with-depth"
}