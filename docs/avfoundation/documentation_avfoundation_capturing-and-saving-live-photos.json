{
  "abstract" : "Capture Live Photos like those created in the system Camera app and save them to the Photos library.",
  "codeExamples" : [
    {
      "code" : "enum CameraError: Error {\n    case configurationFailed\n    \/\/ ... additional error cases ...\n}\n\nfunc configureSession() throws {\n    captureSession.beginConfiguration()\n    \n    \/\/ ... add camera input and photo output ...\n    \n    guard let audioDevice = AVCaptureDevice.default(for: .audio),\n          let audioDeviceInput = try? AVCaptureDeviceInput(device: audioDevice) else {\n              throw CameraError.configurationFailed\n    }\n    \n    if captureSession.canAddInput(audioDeviceInput) {\n        captureSession.addInput(audioDeviceInput)\n    } else {\n        throw CameraError.configurationFailed\n    }\n    \n    \/\/ ... configure photo output and start running ...\n    \n    captureSession.commitConfiguration()\n}",
      "language" : "swift"
    },
    {
      "code" : "let photoOutput = AVCapturePhotoOutput()\n\n\/\/ Attempt to add the photo output to the session.\nif captureSession.canAddOutput(photoOutput) {\n    captureSession.sessionPreset = .photo\n    captureSession.addOutput(photoOutput)\n} else {\n    throw CameraError.configurationFailed\n}\n\n\/\/ Configure the photo output's behavior.\nphotoOutput.isHighResolutionCaptureEnabled = true\nphotoOutput.isLivePhotoCaptureEnabled = photoOutput.isLivePhotoCaptureSupported\n\n\/\/ Start the capture session.\ncaptureSession.startRunning()",
      "language" : "swift"
    },
    {
      "code" : "let photoSettings = AVCapturePhotoSettings(format: [AVVideoCodecKey: AVVideoCodecType.hevc])\nphotoSettings.livePhotoMovieFileURL = \/\/ output url\n\n\/\/ Shoot the Live Photo, using a custom class to handle capture delegate callbacks.\nlet captureProcessor = LivePhotoCaptureProcessor()\nphotoOutput.capturePhoto(with: photoSettings, delegate: captureProcessor)",
      "language" : "swift"
    },
    {
      "code" : "func photoOutput(_ output: AVCapturePhotoOutput,\n                 didFinishProcessingPhoto photo: AVCapturePhoto,\n                 error: Error?) {\n    guard error != nil else {\n        print(\"Error capturing Live Photo still: \\(error!)\");\n        return\n    }\n    \n    \/\/ Get and process the captured image data.\n    processImage(photo.fileDataRepresentation())\n}",
      "language" : "swift"
    },
    {
      "code" : "func photoOutput(_ output: AVCapturePhotoOutput,\n                 didFinishProcessingLivePhotoToMovieFileAt outputFileURL: URL,\n                 duration: CMTime,\n                 photoDisplayTime: CMTime,\n                 resolvedSettings: AVCaptureResolvedPhotoSettings,\n                 error: Error?) {\n    \n    guard error != nil else {\n        print(\"Error capturing Live Photo movie: \\(error!)\");\n        return\n    }\n    \n    guard let stillImageData = stillImageData else { return }\n    \n    \/\/ Save Live Photo.\n    saveLivePhotoToPhotosLibrary(stillImageData: stillImageData,\n                                 livePhotoMovieURL: outputFileURL)\n}",
      "language" : "swift"
    },
    {
      "code" : "func saveLivePhotoToPhotosLibrary(stillImageData: Data, livePhotoMovieURL: URL) {    PHPhotoLibrary.requestAuthorization { status in\n        guard status == .authorized else { return }\n        \n        PHPhotoLibrary.shared().performChanges({\n            \/\/ Add the captured photo's file data as the main resource for the Photos asset.\n            let creationRequest = PHAssetCreationRequest.forAsset()\n            creationRequest.addResource(with: .photo, data: stillImageData, options: nil)\n            \n            \/\/ Add the movie file URL as the Live Photo's paired video resource.\n            let options = PHAssetResourceCreationOptions()\n            options.shouldMoveFile = true\n            creationRequest.addResource(with: .pairedVideo, fileURL: livePhotoMovieURL, options: options)\n        }) { success, error in\n            \/\/ Handle completion.\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "class LivePhotoCaptureProcessor: NSObject, AVCapturePhotoCaptureDelegate {\n    \/\/ ... other PhotoCaptureDelegate methods and supporting properties ...\n    \n    \/\/ A handler to call when Live Photo capture begins and ends.\n    var livePhotoStatusHandler: (Bool) -> () = { _ in }\n    \n    \/\/ A property for tracking in-progress captures and updating UI accordingly.\n    var livePhotosInProgress = 0 {\n        didSet {\n            \/\/ Update the UI accordingly based on the value of this property\n        }\n    }\n    \n    \/\/ Call the handler when PhotoCaptureDelegate methods indicate Live Photo capture is in progress.\n    func photoOutput(_ output: AVCapturePhotoOutput,\n                     willBeginCaptureFor resolvedSettings: AVCaptureResolvedPhotoSettings) {\n        let capturingLivePhoto = (resolvedSettings.livePhotoMovieDimensions.width > 0 && resolvedSettings.livePhotoMovieDimensions.height > 0)\n        livePhotoStatusHandler(capturingLivePhoto)\n    }\n    \n    func photoOutput(_ output: AVCapturePhotoOutput,\n                     didFinishRecordingLivePhotoMovieForEventualFileAt outputFileURL: URL,\n                     resolvedSettings: AVCaptureResolvedPhotoSettings) {\n        livePhotoStatusHandler(false)\n    }\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "a61c74c4799aaeaa3a9d1b10f4bbb4ed09de8757a3eafdc73cedd1f80a06a8ee",
  "crawledAt" : "2025-12-02T19:05:01Z",
  "id" : "AFAD21D6-D7CF-410A-A2A5-0C15E64807B5",
  "kind" : "article",
  "language" : "swift",
  "module" : "AVFoundation",
  "overview" : "## Overview\n\nA Live Photo is a picture that includes motion and sound from the moments just before and after its capture. Your app can capture and record Live Photos using the AVFoundation capture system and the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput] class.\n\n### Enable Live Photo capture\n\nFor a still photo your capture session needs only a video input, but a Live Photo includes sound, so you’ll need to also connect an audio capture device to your session:\n\nBecause you’re already using a built-in camera device for video (see [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/setting-up-a-capture-session]), you can simply use the default audio capture device—the system automatically uses the best microphone configuration for the camera position.\n\nCapturing Live Photos requires an internal reconfiguration of the capture pipeline, which takes time and interrupts any in-progress captures. Before shooting your first Live Photo, make sure you’ve configured the pipeline appropriately by enabling Live Photo capture on your [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput] object:\n\n### Capture a Live Photo\n\nOnce your photo output is ready for Live Photos, you can choose still image or Live Photo capture for each shot. To capture a Live Photo, create an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoSettings] object, choosing the format for the still image portion of the Live Photo and providing a URL for writing the movie portion of the Live Photo. Then, call [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput\/capturePhoto(with:delegate:)] to trigger capture:\n\n### Handle Live Photo results\n\nA Live Photo appears to users in the Photos app as a single asset, but it’s actually composed of separate files: the primary still image, and a movie file containing motion and sound from the moments before and after. The capture system delivers these results separately, as soon as each becomes available.\n\nThe [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoCaptureDelegate\/photoOutput(_:didFinishProcessingPhoto:error:)] method delivers the still image portion of the Live Photo as an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhoto] object. Because you’ll need to save the still image and movie files together, it’s best to extract the image file data from the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhoto] and keep it until the movie file finishes recording, as shown below. (You can also use this method to indicate in your UI that the still image has been captured.)\n\nThe [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoCaptureDelegate\/photoOutput(_:didFinishProcessingLivePhotoToMovieFileAt:duration:photoDisplayTime:resolvedSettings:error:)] method fires later, indicating that the URL you specified when triggering the capture now contains a complete movie file. Once you have both the still image and movie portions of your Live Photo, you can save them together:\n\n### Save a Live Photo to the photos library\n\nUse the [doc:\/\/com.apple.documentation\/documentation\/Photos\/PHAssetCreationRequest] class to create a single Photos asset consisting of media from multiple files—in the case of a Live Photo, the still image and its paired video. As in [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/saving-captured-photos], you’ll need to wrap that request in a [doc:\/\/com.apple.documentation\/documentation\/Photos\/PHPhotoLibrary] change block, and first make sure that your app has the user’s permission to access Photos.\n\n### Track Live Photo progress\n\nCapturing Live Photos adds two additional steps to the process shown in [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/tracking-photo-capture-progress]: after delivery of the still photo result (step 4), the photo output notifies you of movie capture status (step 5) and delivers the movie result (step 6). (Final cleanup becomes step 7.)\n\n\n\nWhen the user captures a Live Photo in the system Camera app, a “Live” indicator appears for a few seconds to let the user know that video and audio are still being recorded. To provide a similar interface in your app, implement these methods in your photo capture delegate:\n\nYou can have multiple Live Photo captures running at the same time, so it’s best to use these methods to keep track of the number of captures “in flight” and hide your indicator only when that number reaches zero:",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/AVFoundation\/capturing-and-saving-live-photos\ncrawled: 2025-12-02T19:05:01Z\n---\n\n# Capturing and saving Live Photos\n\n**Article**\n\nCapture Live Photos like those created in the system Camera app and save them to the Photos library.\n\n## Overview\n\nA Live Photo is a picture that includes motion and sound from the moments just before and after its capture. Your app can capture and record Live Photos using the AVFoundation capture system and the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput] class.\n\n\n\n### Enable Live Photo capture\n\nFor a still photo your capture session needs only a video input, but a Live Photo includes sound, so you’ll need to also connect an audio capture device to your session:\n\n```swift\nenum CameraError: Error {\n    case configurationFailed\n    \/\/ ... additional error cases ...\n}\n\nfunc configureSession() throws {\n    captureSession.beginConfiguration()\n    \n    \/\/ ... add camera input and photo output ...\n    \n    guard let audioDevice = AVCaptureDevice.default(for: .audio),\n          let audioDeviceInput = try? AVCaptureDeviceInput(device: audioDevice) else {\n              throw CameraError.configurationFailed\n    }\n    \n    if captureSession.canAddInput(audioDeviceInput) {\n        captureSession.addInput(audioDeviceInput)\n    } else {\n        throw CameraError.configurationFailed\n    }\n    \n    \/\/ ... configure photo output and start running ...\n    \n    captureSession.commitConfiguration()\n}\n```\n\nBecause you’re already using a built-in camera device for video (see [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/setting-up-a-capture-session]), you can simply use the default audio capture device—the system automatically uses the best microphone configuration for the camera position.\n\nCapturing Live Photos requires an internal reconfiguration of the capture pipeline, which takes time and interrupts any in-progress captures. Before shooting your first Live Photo, make sure you’ve configured the pipeline appropriately by enabling Live Photo capture on your [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput] object:\n\n```swift\nlet photoOutput = AVCapturePhotoOutput()\n\n\/\/ Attempt to add the photo output to the session.\nif captureSession.canAddOutput(photoOutput) {\n    captureSession.sessionPreset = .photo\n    captureSession.addOutput(photoOutput)\n} else {\n    throw CameraError.configurationFailed\n}\n\n\/\/ Configure the photo output's behavior.\nphotoOutput.isHighResolutionCaptureEnabled = true\nphotoOutput.isLivePhotoCaptureEnabled = photoOutput.isLivePhotoCaptureSupported\n\n\/\/ Start the capture session.\ncaptureSession.startRunning()\n```\n\n### Capture a Live Photo\n\nOnce your photo output is ready for Live Photos, you can choose still image or Live Photo capture for each shot. To capture a Live Photo, create an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoSettings] object, choosing the format for the still image portion of the Live Photo and providing a URL for writing the movie portion of the Live Photo. Then, call [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput\/capturePhoto(with:delegate:)] to trigger capture:\n\n```swift\nlet photoSettings = AVCapturePhotoSettings(format: [AVVideoCodecKey: AVVideoCodecType.hevc])\nphotoSettings.livePhotoMovieFileURL = \/\/ output url\n\n\/\/ Shoot the Live Photo, using a custom class to handle capture delegate callbacks.\nlet captureProcessor = LivePhotoCaptureProcessor()\nphotoOutput.capturePhoto(with: photoSettings, delegate: captureProcessor)\n```\n\n### Handle Live Photo results\n\nA Live Photo appears to users in the Photos app as a single asset, but it’s actually composed of separate files: the primary still image, and a movie file containing motion and sound from the moments before and after. The capture system delivers these results separately, as soon as each becomes available.\n\nThe [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoCaptureDelegate\/photoOutput(_:didFinishProcessingPhoto:error:)] method delivers the still image portion of the Live Photo as an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhoto] object. Because you’ll need to save the still image and movie files together, it’s best to extract the image file data from the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhoto] and keep it until the movie file finishes recording, as shown below. (You can also use this method to indicate in your UI that the still image has been captured.)\n\n```swift\nfunc photoOutput(_ output: AVCapturePhotoOutput,\n                 didFinishProcessingPhoto photo: AVCapturePhoto,\n                 error: Error?) {\n    guard error != nil else {\n        print(\"Error capturing Live Photo still: \\(error!)\");\n        return\n    }\n    \n    \/\/ Get and process the captured image data.\n    processImage(photo.fileDataRepresentation())\n}\n```\n\nThe [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoCaptureDelegate\/photoOutput(_:didFinishProcessingLivePhotoToMovieFileAt:duration:photoDisplayTime:resolvedSettings:error:)] method fires later, indicating that the URL you specified when triggering the capture now contains a complete movie file. Once you have both the still image and movie portions of your Live Photo, you can save them together:\n\n```swift\nfunc photoOutput(_ output: AVCapturePhotoOutput,\n                 didFinishProcessingLivePhotoToMovieFileAt outputFileURL: URL,\n                 duration: CMTime,\n                 photoDisplayTime: CMTime,\n                 resolvedSettings: AVCaptureResolvedPhotoSettings,\n                 error: Error?) {\n    \n    guard error != nil else {\n        print(\"Error capturing Live Photo movie: \\(error!)\");\n        return\n    }\n    \n    guard let stillImageData = stillImageData else { return }\n    \n    \/\/ Save Live Photo.\n    saveLivePhotoToPhotosLibrary(stillImageData: stillImageData,\n                                 livePhotoMovieURL: outputFileURL)\n}\n```\n\n\n\n### Save a Live Photo to the photos library\n\nUse the [doc:\/\/com.apple.documentation\/documentation\/Photos\/PHAssetCreationRequest] class to create a single Photos asset consisting of media from multiple files—in the case of a Live Photo, the still image and its paired video. As in [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/saving-captured-photos], you’ll need to wrap that request in a [doc:\/\/com.apple.documentation\/documentation\/Photos\/PHPhotoLibrary] change block, and first make sure that your app has the user’s permission to access Photos.\n\n```swift\nfunc saveLivePhotoToPhotosLibrary(stillImageData: Data, livePhotoMovieURL: URL) {    PHPhotoLibrary.requestAuthorization { status in\n        guard status == .authorized else { return }\n        \n        PHPhotoLibrary.shared().performChanges({\n            \/\/ Add the captured photo's file data as the main resource for the Photos asset.\n            let creationRequest = PHAssetCreationRequest.forAsset()\n            creationRequest.addResource(with: .photo, data: stillImageData, options: nil)\n            \n            \/\/ Add the movie file URL as the Live Photo's paired video resource.\n            let options = PHAssetResourceCreationOptions()\n            options.shouldMoveFile = true\n            creationRequest.addResource(with: .pairedVideo, fileURL: livePhotoMovieURL, options: options)\n        }) { success, error in\n            \/\/ Handle completion.\n        }\n    }\n}\n```\n\n\n\n### Track Live Photo progress\n\nCapturing Live Photos adds two additional steps to the process shown in [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/tracking-photo-capture-progress]: after delivery of the still photo result (step 4), the photo output notifies you of movie capture status (step 5) and delivers the movie result (step 6). (Final cleanup becomes step 7.)\n\n\n\nWhen the user captures a Live Photo in the system Camera app, a “Live” indicator appears for a few seconds to let the user know that video and audio are still being recorded. To provide a similar interface in your app, implement these methods in your photo capture delegate:\n\n- The [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoCaptureDelegate\/photoOutput(_:willBeginCaptureFor:)] method tells you that capture has started: implement this method to show a recording indicator.\n- The [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoCaptureDelegate\/photoOutput(_:didFinishRecordingLivePhotoMovieForEventualFileAt:resolvedSettings:)] method tells you that a Live Photo movie is no longer recording: implement this method to hide the indicator. (Note that the movie file is not yet available at this time.)\n\nYou can have multiple Live Photo captures running at the same time, so it’s best to use these methods to keep track of the number of captures “in flight” and hide your indicator only when that number reaches zero:\n\n```swift\nclass LivePhotoCaptureProcessor: NSObject, AVCapturePhotoCaptureDelegate {\n    \/\/ ... other PhotoCaptureDelegate methods and supporting properties ...\n    \n    \/\/ A handler to call when Live Photo capture begins and ends.\n    var livePhotoStatusHandler: (Bool) -> () = { _ in }\n    \n    \/\/ A property for tracking in-progress captures and updating UI accordingly.\n    var livePhotosInProgress = 0 {\n        didSet {\n            \/\/ Update the UI accordingly based on the value of this property\n        }\n    }\n    \n    \/\/ Call the handler when PhotoCaptureDelegate methods indicate Live Photo capture is in progress.\n    func photoOutput(_ output: AVCapturePhotoOutput,\n                     willBeginCaptureFor resolvedSettings: AVCaptureResolvedPhotoSettings) {\n        let capturingLivePhoto = (resolvedSettings.livePhotoMovieDimensions.width > 0 && resolvedSettings.livePhotoMovieDimensions.height > 0)\n        livePhotoStatusHandler(capturingLivePhoto)\n    }\n    \n    func photoOutput(_ output: AVCapturePhotoOutput,\n                     didFinishRecordingLivePhotoMovieForEventualFileAt outputFileURL: URL,\n                     resolvedSettings: AVCaptureResolvedPhotoSettings) {\n        livePhotoStatusHandler(false)\n    }\n}\n```\n\n## Next steps\n\n- **Saving captured photos**: Add an image and other data from a photo capture to the photo library.\n- **Tracking photo capture progress**: Monitor key events during capture to provide feedback in your camera UI.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Add an image and other data from a photo capture to the photo library.",
          "name" : "Saving captured photos",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/saving-captured-photos"
        },
        {
          "description" : "Monitor key events during capture to provide feedback in your camera UI.",
          "name" : "Tracking photo capture progress",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/tracking-photo-capture-progress"
        }
      ],
      "title" : "Next steps"
    }
  ],
  "source" : "appleJSON",
  "title" : "Capturing and saving Live Photos",
  "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/capturing-and-saving-live-photos"
}