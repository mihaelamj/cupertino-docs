{
  "abstract" : "Monitor key events during capture to provide feedback in your camera UI.",
  "codeExamples" : [
    {
      "code" : "class PhotoCaptureProcessor: NSObject, AVCapturePhotoCaptureDelegate {\n    var completionHandler: () -> () = {}\n    func photoOutput(_ output: AVCapturePhotoOutput, didFinishCaptureFor resolvedSettings: AVCaptureResolvedPhotoSettings, error: Error?) {\n        completionHandler()\n    }\n    \/\/ ... other delegate methods to handle capture results...\n}\n\n\/\/ Keep a set of in-progress capture delegates.\nvar capturesInProgress = Set<PhotoCaptureProcessor>()\n\nfunc shootPhoto() {    \n    \/\/ Make a new capture delegate for each capture and add it to the set.\n    let captureProcessor = PhotoCaptureProcessor()\n    capturesInProgress.insert(captureProcessor)\n    \n    \/\/ Schedule for the capture delegate to be removed from the set after capture.\n    captureProcessor.completionHandler = { [weak self] in\n        self?.capturesInProgress.remove(captureProcessor); return\n    }\n    \n    self.photoOutput.capturePhoto(with: self.settingsForNextPhoto(), delegate: captureProcessor)\n}\n",
      "language" : "swift"
    }
  ],
  "contentHash" : "6ad1de83895dbb847c7fd6b303b91b2b06c8e2cb54fdc98e48e337372daf8a79",
  "crawledAt" : "2025-12-02T19:23:55Z",
  "id" : "28EC215C-D268-40F8-8549-FCB8D12DBA39",
  "kind" : "article",
  "language" : "swift",
  "module" : "AVFoundation",
  "overview" : "## Overview\n\nCapturing a photo with an iOS device camera is a complex process involving physical camera mechanisms, image signal processing, the operating system, and your app. While it’s possible for your app to ignore many stages of this process and simply wait for a final result, you can create a more responsive camera interface by monitoring each step.\n\nAfter you call [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput\/capturePhoto(with:delegate:)], your delegate object can follow along with five major steps in the process (or more, depending on your photo settings). Depending on your capture workflow and the capture UI you want to create, your delegate can handle some or all of these steps:\n\n\n\nThe capture system provides an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureResolvedPhotoSettings] object at each step in this process. Because multiple captures can be in progress at the same time, each resolved photo settings object has a [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureResolvedPhotoSettings\/uniqueID] whose value matches the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoSettings\/uniqueID] of the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoSettings] you used to take the photo.\n\n### Get resolved capture settings\n\nWhen you specify the settings for a photo, some of the settings you choose can be automatic, left for the capture system to decide at precisely the moment of capture. For example, you can choose [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDevice\/FlashMode-swift.enum\/auto] flash mode, allowing the camera itself to determine based on scene lighting whether to fire the flash when exposing the photo.\n\nJust before starting the exposure, the photo output calls your delegate’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoCaptureDelegate\/photoOutput(_:willBeginCaptureFor:)] method, whose `resolvedSettings` parameter tells you the actual settings for that capture. For example, if you chose [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDevice\/FlashMode-swift.enum\/auto] flash mode, the resolved settings object tells you whether the flash is in use for the current capture—you could use this information to show in your UI that the flash was used.\n\n### Handle exposure start\n\nWhen the exposure time begins, the photo output calls your delegate’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoCaptureDelegate\/photoOutput(_:willCapturePhotoFor:)] method. In traditional photography, this moment is equivalent to the opening of the camera shutter. The system also automatically plays a shutter sound at this time.\n\nIn your UI, you can respond to this method to display a shutter animation or some other indicator that the requested photo is being taken.\n\n### Handle exposure end\n\nThe photo output calls your delegate’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoCaptureDelegate\/photoOutput(_:didCapturePhotoFor:)] method as soon as the exposure time completes. The system still needs time to process camera data before providing an image to your app, but you can use this moment to display in your UI that the exposure is complete. For example, you can simulate a shutter effect by hiding the camera preview in the `willCapturePhoto` method and showing it again in the `didCapturePhoto` method.\n\n### Handle photo results\n\nWhen the photo output has image data available for your app, it calls your delegate’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoCaptureDelegate\/photoOutput(_:didFinishProcessingPhoto:error:)] method. Depending on your photo settings, the photo output may call this method multiple times:\n\nFor example, if you request RAW+HEIF capture in a three-exposure bracket, the photo output calls your delegate’s `didFinishProcessingPhoto` method six times (2 formats × 3 exposures), providing six [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhoto] objects. To keep track of multiple results, compare the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhoto\/photoCount] from each photo to the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureResolvedPhotoSettings\/expectedPhotoCount] of your resolved settings.\n\n### Clean up when capture is complete\n\nWhen all of the system’s work for a capture is complete, the photo output calls your delegate’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoCaptureDelegate\/photoOutput(_:didFinishCaptureFor:error:)] method. You can use this moment to finish your app’s part of the capture process:\n\nThe code below shows one way to manage multiple photo capture delegate objects:",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/AVFoundation\/tracking-photo-capture-progress\ncrawled: 2025-12-02T19:23:55Z\n---\n\n# Tracking photo capture progress\n\n**Article**\n\nMonitor key events during capture to provide feedback in your camera UI.\n\n## Overview\n\nCapturing a photo with an iOS device camera is a complex process involving physical camera mechanisms, image signal processing, the operating system, and your app. While it’s possible for your app to ignore many stages of this process and simply wait for a final result, you can create a more responsive camera interface by monitoring each step.\n\nAfter you call [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput\/capturePhoto(with:delegate:)], your delegate object can follow along with five major steps in the process (or more, depending on your photo settings). Depending on your capture workflow and the capture UI you want to create, your delegate can handle some or all of these steps:\n\n\n\n1. Settings resolved\n2. Exposure started\n3. Exposure complete\n4. Result data delivery\n5. Capture complete\n\nThe capture system provides an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureResolvedPhotoSettings] object at each step in this process. Because multiple captures can be in progress at the same time, each resolved photo settings object has a [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureResolvedPhotoSettings\/uniqueID] whose value matches the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoSettings\/uniqueID] of the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoSettings] you used to take the photo.\n\n### Get resolved capture settings\n\nWhen you specify the settings for a photo, some of the settings you choose can be automatic, left for the capture system to decide at precisely the moment of capture. For example, you can choose [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDevice\/FlashMode-swift.enum\/auto] flash mode, allowing the camera itself to determine based on scene lighting whether to fire the flash when exposing the photo.\n\nJust before starting the exposure, the photo output calls your delegate’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoCaptureDelegate\/photoOutput(_:willBeginCaptureFor:)] method, whose `resolvedSettings` parameter tells you the actual settings for that capture. For example, if you chose [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDevice\/FlashMode-swift.enum\/auto] flash mode, the resolved settings object tells you whether the flash is in use for the current capture—you could use this information to show in your UI that the flash was used.\n\n### Handle exposure start\n\nWhen the exposure time begins, the photo output calls your delegate’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoCaptureDelegate\/photoOutput(_:willCapturePhotoFor:)] method. In traditional photography, this moment is equivalent to the opening of the camera shutter. The system also automatically plays a shutter sound at this time.\n\nIn your UI, you can respond to this method to display a shutter animation or some other indicator that the requested photo is being taken.\n\n### Handle exposure end\n\nThe photo output calls your delegate’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoCaptureDelegate\/photoOutput(_:didCapturePhotoFor:)] method as soon as the exposure time completes. The system still needs time to process camera data before providing an image to your app, but you can use this moment to display in your UI that the exposure is complete. For example, you can simulate a shutter effect by hiding the camera preview in the `willCapturePhoto` method and showing it again in the `didCapturePhoto` method.\n\n### Handle photo results\n\nWhen the photo output has image data available for your app, it calls your delegate’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoCaptureDelegate\/photoOutput(_:didFinishProcessingPhoto:error:)] method. Depending on your photo settings, the photo output may call this method multiple times:\n\n- If you request bracketed capture, this method fires (at least) once for each exposure in the bracket, providing the image for that exposure.\n- If you request capture in both RAW and processed formats (such as HEIF\/HEVC or JPEG), this method fires (at least) once for each format.\n\nFor example, if you request RAW+HEIF capture in a three-exposure bracket, the photo output calls your delegate’s `didFinishProcessingPhoto` method six times (2 formats × 3 exposures), providing six [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhoto] objects. To keep track of multiple results, compare the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhoto\/photoCount] from each photo to the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureResolvedPhotoSettings\/expectedPhotoCount] of your resolved settings.\n\n\n\n### Clean up when capture is complete\n\nWhen all of the system’s work for a capture is complete, the photo output calls your delegate’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoCaptureDelegate\/photoOutput(_:didFinishCaptureFor:error:)] method. You can use this moment to finish your app’s part of the capture process:\n\n- If your capture expects multiple results, cache those in your [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoCaptureDelegate\/photoOutput(_:didFinishProcessingPhoto:error:)] (and [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoCaptureDelegate\/photoOutput(_:didFinishProcessingLivePhotoToMovieFileAt:duration:photoDisplayTime:resolvedSettings:error:)]) methods, then save results to local storage or add them to the Photos library in your `didFinishCapture` method.\n- If your capture process manages other resources, clean up those resources in your `didFinishCapture` method. If you use a separate photo capture delegate object for each capture, this is a good time to remove any strong references to such objects.\n\nThe code below shows one way to manage multiple photo capture delegate objects:\n\n```swift\nclass PhotoCaptureProcessor: NSObject, AVCapturePhotoCaptureDelegate {\n    var completionHandler: () -> () = {}\n    func photoOutput(_ output: AVCapturePhotoOutput, didFinishCaptureFor resolvedSettings: AVCaptureResolvedPhotoSettings, error: Error?) {\n        completionHandler()\n    }\n    \/\/ ... other delegate methods to handle capture results...\n}\n\n\/\/ Keep a set of in-progress capture delegates.\nvar capturesInProgress = Set<PhotoCaptureProcessor>()\n\nfunc shootPhoto() {    \n    \/\/ Make a new capture delegate for each capture and add it to the set.\n    let captureProcessor = PhotoCaptureProcessor()\n    capturesInProgress.insert(captureProcessor)\n    \n    \/\/ Schedule for the capture delegate to be removed from the set after capture.\n    captureProcessor.completionHandler = { [weak self] in\n        self?.capturesInProgress.remove(captureProcessor); return\n    }\n    \n    self.photoOutput.capturePhoto(with: self.settingsForNextPhoto(), delegate: captureProcessor)\n}\n\n```\n\n## Next steps\n\n- **Saving captured photos**: Add an image and other data from a photo capture to the photo library.\n- **Capturing and saving Live Photos**: Capture Live Photos like those created in the system Camera app and save them to the Photos library.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Add an image and other data from a photo capture to the photo library.",
          "name" : "Saving captured photos",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/saving-captured-photos"
        },
        {
          "description" : "Capture Live Photos like those created in the system Camera app and save them to the Photos library.",
          "name" : "Capturing and saving Live Photos",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/capturing-and-saving-live-photos"
        }
      ],
      "title" : "Next steps"
    }
  ],
  "source" : "appleJSON",
  "title" : "Tracking photo capture progress",
  "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/tracking-photo-capture-progress"
}