{
  "abstract" : "Create a custom video compositor to edit spatial video for playback and export.",
  "codeExamples" : [
    {
      "code" : "\/\/\/ A Boolean value that indicates whether the custom compositor supports source tagged buffers.\nlet supportsSourceTaggedBuffers = true",
      "language" : "swift"
    },
    {
      "code" : "func startRequest(_ request: AVAsynchronousVideoCompositionRequest) {\n\n    \/\/ If no track identifier is found, cancel the request and return.\n    guard let firstTrackIDNumber = request.sourceTrackIDs.first else {\n        request.finishCancelledRequest()\n        return\n    }\n\n    let firstTrackID = CMPersistentTrackID(truncating: firstTrackIDNumber)\n\n    \/\/ Attempt to retrieve the tagged buffers in the source track.\n    if let taggedBuffers = request.sourceTaggedDynamicBuffers(byTrackID: firstTrackID) {\n        \/\/ Process the tagged buffers from stereoscopic source track.\n        processTaggedBuffers(taggedBuffers: taggedBuffers, request: request)\n    }\n    \/\/ Attempt to retrieve the monoscopic video frame in the source track.\n    else if let pixelBuffer = request.sourceFrame(byTrackID: firstTrackID) {\n        \/\/ Process pixel buffer from monoscopic source track.\n        processMonoscopicBuffer(sourcePixelBuffer: pixelBuffer, request: request)\n    }\n    \/\/ No source frames were found. Finish with an error.\n    else {\n        request.finish(with: CompositorError.invalidSource)\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ Builds a video composition object.\nfunc build() async throws -> AVVideoComposition {\n\n    \/\/ Create the video composition configuration object for the asset.\n    var configuration = try await AVVideoComposition.Configuration(for: asset)\n    \/\/ Specify the custom compositor implementation class to use.\n    configuration.customVideoCompositorClass = compositorConfiguration.class\n\n    ...\n\n    return AVVideoComposition(configuration: configuration)\n}",
      "language" : "swift"
    },
    {
      "code" : "if compositorConfiguration.outputsStereo {\n    \/\/ Wrap the instructions in the app's custom instruction type.\n    configuration.instructions = configuration.instructions.compactMap {\n        SpatialVideoCompositionInstruction(\n            instruction: $0,\n            spatialConfiguration: spatialConfiguration,\n            projectionTag: projectionTag\n        )\n    }\n    configuration.outputBufferDescription = [\n        [.stereoView(.leftEye), projectionTag, .mediaType(.video)],\n        [.stereoView(.rightEye), projectionTag, .mediaType(.video)]\n    ]\n} else {\n    configuration.outputBufferDescription = nil\n    configuration.spatialVideoConfigurations = []\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ Creates a new player to play the movie at the specified URL.\n\/\/\/ - Parameter url: The URL of the movie file to play.\nprivate func makePlayer(url: URL) async throws -> AVPlayer {\n    let asset = AVURLAsset(url: url)\n    let playerItem = AVPlayerItem(asset: asset)\n    \/\/ Create a video composition for the currently user-selected custom compositor.\n    if let videoComposition = try await makeVideoComposition(for: asset) {\n        playerItem.videoComposition = videoComposition\n        playerItem.seekingWaitsForVideoCompositionRendering = true\n    }\n    return AVPlayer(playerItem: playerItem)\n}",
      "language" : "swift"
    },
    {
      "code" : "func export(asset: AVAsset, videoComposition: AVVideoComposition? = nil) async throws {\n\n    \/\/ If this method receives a video composition that produces stereo output, use an MV-HEVC preset.\n    \/\/ Note: the `outputsStereo` property is an app-specific extension.\n    let preset = if let videoComposition, videoComposition.outputsStereo {\n        AVAssetExportPresetMVHEVC4320x4320\n    }\n    \/\/ Otherwise, use a standard HEVC preset.\n    else {\n        AVAssetExportPresetHEVCHighestQuality\n    }\n\n    \/\/ Attempt to create an export session with the selected preset.\n    guard let exportSession = AVAssetExportSession(asset: asset, presetName: preset) else {\n        throw ExportError.noExportSession\n    }\n\n    \/\/ If a valid video composition was passed to this method, set it on the export session.\n    if let videoComposition {\n        exportSession.videoComposition = videoComposition\n    }\n\n    ...\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ The read-write loop is executed in a dedicated dispatch queue.\nwriterInput.requestMediaDataWhenReady(on: DispatchQueue(label: \"com.apple.spatialcompositor.reader\")) {\n    while writerInput.isReadyForMoreMediaData {\n\n        guard let sampleBuffer = readerOutput.copyNextSampleBuffer() else {\n            \/\/ A nil sample buffer indicates the end of the input.\n            finishWritingAndResume()\n            return\n        }\n\n        if let taggedBuffers = sampleBuffer.taggedBuffers, let taggedPixelBufferGroupReceiver {\n            \/\/ Send tagged buffers to writer input via tagged pixel buffer group receiver.\n            \/\/ Make sure the tagged buffers are `CMTaggedDynamicBuffers` with `layerID` tags.\n            let wellFormedTaggedBuffers = taggedBuffers.ensureLayerIDTagsAndMakeDynamic(leftEyeLayer: 0, rightEyeLayer: 1)\n            do {\n                let pts = sampleBuffer.presentationTimeStamp\n                if try !taggedPixelBufferGroupReceiver.appendImmediately(wellFormedTaggedBuffers, with: pts) {\n                    finishWritingAndResume(error: .appendTaggedBuffersFailed)\n                    return\n                }\n            } catch {\n                finishWritingAndResume(error: .appendTaggedBuffersFailed)\n                return\n            }\n        } else {\n            \/\/ The reader output is a normal sample buffer. Send to writer input directly.\n            writerInput.append(sampleBuffer)\n        }\n        \/\/ Send async notification for progress update.\n        let percent = (Double) (sampleBuffer.presentationTimeStamp.seconds \/ assetDuration)\n        statusContinuation.yield(ExporterStatus.exporting(progress: percent))\n    }\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "ef68634bf57b8d20bd6cf9cab49fc33363627fcb399d9ca8a35997700c9c8bfb",
  "crawledAt" : "2025-12-02T15:30:06Z",
  "id" : "EC3B2B55-F45D-4E5F-8663-388AB14289D0",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "AVFoundation",
  "overview" : "## Overview\n\nSpatial video content requires specialized processing to maintain its stereoscopic presentation or convert it to monoscopic output for different viewing contexts. While AVFoundation provides built-in video compositing capabilities for basic effects like transforms and fades, processing spatial video’s dual-eye data streams demands more sophisticated control.\n\nAVFoundation’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVVideoCompositing] protocol provides the framework for building custom video compositors with full control over video processing. This sample demonstrates how to implement custom compositors that handle spatial video’s tagged buffer data, enabling you to process stereoscopic content for both playback and export scenarios.\n\n## Configure the sample code project\n\nTo run the sample app, place one or more spatial video files on your device to process. The project includes a sample spatial video file for testing, and you can capture your own content on Apple Vision Pro or iPhone 16.\n\nWhen you run the app, it prompts you to select a spatial video file through the system file picker on your platform.\n\n## Create a custom compositor\n\nThe sample provides two [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVVideoCompositing] implementations: `MonoOutputCompositor` and `StereoOutputCompositor`. Both process mono or stereo input, but output either monoscopic or stereoscopic video respectively.\n\nTo indicate that they support processing spatial video, both implementations override the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVVideoCompositing\/supportsSourceTaggedBuffers] property and provide a value of `true`.\n\nThe framework calls the compositor’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVVideoCompositing\/startRequest(_:)] method to process each video frame, passing an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAsynchronousVideoCompositionRequest] that provides access to the source frame data. The implementation handles both spatial and mono video sources:\n\nThe method first attempts to retrieve tagged buffers, which indicates the video data is spatial. If it finds no tagged buffers, it processes the source as monoscopic video. If neither attempt succeeds, it completes the request with an error.\n\n## Build a video composition object\n\nThe sample creates a video composition using the app’s `VideoCompositionBuilder` structure. This type’s `build()` method creates an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVVideoComposition\/Configuration] for the selected asset and indicates to use the custom compositor:\n\nThe configuration includes two key properties for spatial video:\n\nThe builder configures the output based on whether the compositor produces stereo or mono output:\n\n## Use a custom compositor for playback\n\nIntegrating the custom compositor for playback is straightforward. The sample’s `SampleModel` class creates an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVPlayer] with the video composition:\n\nThe method creates a video composition using the selected compositor configuration. If the method creates a valid composition, it assigns it to the player item and enables rendering synchronization.\n\n## Export with an asset export session\n\nThe sample’s `ExportSessionExporter` class demonstrates export using [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetExportSession]. The export method selects the appropriate preset based on the video composition’s output type:\n\nFor stereo output, the method uses the MV-HEVC preset to maintain spatial video format. For mono output, it uses the standard HEVC preset. After creating the export session, it assigns the video composition and performs the export operation.\n\n## Export with an asset reader and writer\n\nThe sample’s `ReaderWriterExporter` class provides fine-grained export control using [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetReader] and [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriter]. This approach requires more setup but offers greater flexibility over the export process.\n\nThe core export operation uses a read-write loop that processes each video frame:\n\nThe loop reads sample buffers from the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetReaderVideoCompositionOutput], which applies the custom video composition. For spatial video output, it uses tagged pixel buffer groups; for mono output, it appends sample buffers directly to the writer input.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/AVFoundation\/processing-spatial-video-with-a-custom-video-compositor\ncrawled: 2025-12-02T15:30:06Z\n---\n\n# Processing spatial video with a custom video compositor\n\n**Sample Code**\n\nCreate a custom video compositor to edit spatial video for playback and export.\n\n## Overview\n\nSpatial video content requires specialized processing to maintain its stereoscopic presentation or convert it to monoscopic output for different viewing contexts. While AVFoundation provides built-in video compositing capabilities for basic effects like transforms and fades, processing spatial video’s dual-eye data streams demands more sophisticated control.\n\nAVFoundation’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVVideoCompositing] protocol provides the framework for building custom video compositors with full control over video processing. This sample demonstrates how to implement custom compositors that handle spatial video’s tagged buffer data, enabling you to process stereoscopic content for both playback and export scenarios.\n\n## Configure the sample code project\n\nTo run the sample app, place one or more spatial video files on your device to process. The project includes a sample spatial video file for testing, and you can capture your own content on Apple Vision Pro or iPhone 16.\n\nWhen you run the app, it prompts you to select a spatial video file through the system file picker on your platform.\n\n## Create a custom compositor\n\nThe sample provides two [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVVideoCompositing] implementations: `MonoOutputCompositor` and `StereoOutputCompositor`. Both process mono or stereo input, but output either monoscopic or stereoscopic video respectively.\n\nTo indicate that they support processing spatial video, both implementations override the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVVideoCompositing\/supportsSourceTaggedBuffers] property and provide a value of `true`.\n\n```swift\n\/\/\/ A Boolean value that indicates whether the custom compositor supports source tagged buffers.\nlet supportsSourceTaggedBuffers = true\n```\n\nThe framework calls the compositor’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVVideoCompositing\/startRequest(_:)] method to process each video frame, passing an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAsynchronousVideoCompositionRequest] that provides access to the source frame data. The implementation handles both spatial and mono video sources:\n\n```swift\nfunc startRequest(_ request: AVAsynchronousVideoCompositionRequest) {\n\n    \/\/ If no track identifier is found, cancel the request and return.\n    guard let firstTrackIDNumber = request.sourceTrackIDs.first else {\n        request.finishCancelledRequest()\n        return\n    }\n\n    let firstTrackID = CMPersistentTrackID(truncating: firstTrackIDNumber)\n\n    \/\/ Attempt to retrieve the tagged buffers in the source track.\n    if let taggedBuffers = request.sourceTaggedDynamicBuffers(byTrackID: firstTrackID) {\n        \/\/ Process the tagged buffers from stereoscopic source track.\n        processTaggedBuffers(taggedBuffers: taggedBuffers, request: request)\n    }\n    \/\/ Attempt to retrieve the monoscopic video frame in the source track.\n    else if let pixelBuffer = request.sourceFrame(byTrackID: firstTrackID) {\n        \/\/ Process pixel buffer from monoscopic source track.\n        processMonoscopicBuffer(sourcePixelBuffer: pixelBuffer, request: request)\n    }\n    \/\/ No source frames were found. Finish with an error.\n    else {\n        request.finish(with: CompositorError.invalidSource)\n    }\n}\n```\n\nThe method first attempts to retrieve tagged buffers, which indicates the video data is spatial. If it finds no tagged buffers, it processes the source as monoscopic video. If neither attempt succeeds, it completes the request with an error.\n\n\n\n## Build a video composition object\n\nThe sample creates a video composition using the app’s `VideoCompositionBuilder` structure. This type’s `build()` method creates an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVVideoComposition\/Configuration] for the selected asset and indicates to use the custom compositor:\n\n```swift\n\/\/\/ Builds a video composition object.\nfunc build() async throws -> AVVideoComposition {\n\n    \/\/ Create the video composition configuration object for the asset.\n    var configuration = try await AVVideoComposition.Configuration(for: asset)\n    \/\/ Specify the custom compositor implementation class to use.\n    configuration.customVideoCompositorClass = compositorConfiguration.class\n\n    ...\n\n    return AVVideoComposition(configuration: configuration)\n}\n```\n\nThe configuration includes two key properties for spatial video:\n\n\n\nThe builder configures the output based on whether the compositor produces stereo or mono output:\n\n```swift\nif compositorConfiguration.outputsStereo {\n    \/\/ Wrap the instructions in the app's custom instruction type.\n    configuration.instructions = configuration.instructions.compactMap {\n        SpatialVideoCompositionInstruction(\n            instruction: $0,\n            spatialConfiguration: spatialConfiguration,\n            projectionTag: projectionTag\n        )\n    }\n    configuration.outputBufferDescription = [\n        [.stereoView(.leftEye), projectionTag, .mediaType(.video)],\n        [.stereoView(.rightEye), projectionTag, .mediaType(.video)]\n    ]\n} else {\n    configuration.outputBufferDescription = nil\n    configuration.spatialVideoConfigurations = []\n}\n```\n\n## Use a custom compositor for playback\n\nIntegrating the custom compositor for playback is straightforward. The sample’s `SampleModel` class creates an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVPlayer] with the video composition:\n\n```swift\n\/\/\/ Creates a new player to play the movie at the specified URL.\n\/\/\/ - Parameter url: The URL of the movie file to play.\nprivate func makePlayer(url: URL) async throws -> AVPlayer {\n    let asset = AVURLAsset(url: url)\n    let playerItem = AVPlayerItem(asset: asset)\n    \/\/ Create a video composition for the currently user-selected custom compositor.\n    if let videoComposition = try await makeVideoComposition(for: asset) {\n        playerItem.videoComposition = videoComposition\n        playerItem.seekingWaitsForVideoCompositionRendering = true\n    }\n    return AVPlayer(playerItem: playerItem)\n}\n```\n\nThe method creates a video composition using the selected compositor configuration. If the method creates a valid composition, it assigns it to the player item and enables rendering synchronization.\n\n## Export with an asset export session\n\nThe sample’s `ExportSessionExporter` class demonstrates export using [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetExportSession]. The export method selects the appropriate preset based on the video composition’s output type:\n\n```swift\nfunc export(asset: AVAsset, videoComposition: AVVideoComposition? = nil) async throws {\n\n    \/\/ If this method receives a video composition that produces stereo output, use an MV-HEVC preset.\n    \/\/ Note: the `outputsStereo` property is an app-specific extension.\n    let preset = if let videoComposition, videoComposition.outputsStereo {\n        AVAssetExportPresetMVHEVC4320x4320\n    }\n    \/\/ Otherwise, use a standard HEVC preset.\n    else {\n        AVAssetExportPresetHEVCHighestQuality\n    }\n\n    \/\/ Attempt to create an export session with the selected preset.\n    guard let exportSession = AVAssetExportSession(asset: asset, presetName: preset) else {\n        throw ExportError.noExportSession\n    }\n\n    \/\/ If a valid video composition was passed to this method, set it on the export session.\n    if let videoComposition {\n        exportSession.videoComposition = videoComposition\n    }\n\n    ...\n}\n```\n\nFor stereo output, the method uses the MV-HEVC preset to maintain spatial video format. For mono output, it uses the standard HEVC preset. After creating the export session, it assigns the video composition and performs the export operation.\n\n## Export with an asset reader and writer\n\nThe sample’s `ReaderWriterExporter` class provides fine-grained export control using [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetReader] and [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetWriter]. This approach requires more setup but offers greater flexibility over the export process.\n\nThe core export operation uses a read-write loop that processes each video frame:\n\n```swift\n\/\/ The read-write loop is executed in a dedicated dispatch queue.\nwriterInput.requestMediaDataWhenReady(on: DispatchQueue(label: \"com.apple.spatialcompositor.reader\")) {\n    while writerInput.isReadyForMoreMediaData {\n\n        guard let sampleBuffer = readerOutput.copyNextSampleBuffer() else {\n            \/\/ A nil sample buffer indicates the end of the input.\n            finishWritingAndResume()\n            return\n        }\n\n        if let taggedBuffers = sampleBuffer.taggedBuffers, let taggedPixelBufferGroupReceiver {\n            \/\/ Send tagged buffers to writer input via tagged pixel buffer group receiver.\n            \/\/ Make sure the tagged buffers are `CMTaggedDynamicBuffers` with `layerID` tags.\n            let wellFormedTaggedBuffers = taggedBuffers.ensureLayerIDTagsAndMakeDynamic(leftEyeLayer: 0, rightEyeLayer: 1)\n            do {\n                let pts = sampleBuffer.presentationTimeStamp\n                if try !taggedPixelBufferGroupReceiver.appendImmediately(wellFormedTaggedBuffers, with: pts) {\n                    finishWritingAndResume(error: .appendTaggedBuffersFailed)\n                    return\n                }\n            } catch {\n                finishWritingAndResume(error: .appendTaggedBuffersFailed)\n                return\n            }\n        } else {\n            \/\/ The reader output is a normal sample buffer. Send to writer input directly.\n            writerInput.append(sampleBuffer)\n        }\n        \/\/ Send async notification for progress update.\n        let percent = (Double) (sampleBuffer.presentationTimeStamp.seconds \/ assetDuration)\n        statusContinuation.yield(ExporterStatus.exporting(progress: percent))\n    }\n}\n```\n\nThe loop reads sample buffers from the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetReaderVideoCompositionOutput], which applies the custom video composition. For spatial video output, it uses tagged pixel buffer groups; for mono output, it appends sample buffers directly to the writer input.\n\n## Custom video compositing\n\n- **AVVideoCompositing**: A protocol that defines the methods custom video compositors must implement.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "A protocol that defines the methods custom video compositors must implement.",
          "name" : "AVVideoCompositing",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVVideoCompositing"
        }
      ],
      "title" : "Custom video compositing"
    }
  ],
  "source" : "appleJSON",
  "title" : "Processing spatial video with a custom video compositor",
  "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/processing-spatial-video-with-a-custom-video-compositor"
}