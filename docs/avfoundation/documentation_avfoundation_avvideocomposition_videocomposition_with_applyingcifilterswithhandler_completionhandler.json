{
  "abstract" : "Returns a new video composition that’s configured to apply Core Image filters to each video frame of the specified asset.",
  "codeExamples" : [
    {
      "code" : "guard let filter = CIFilter(name: \"CIGaussianBlur\") else { return }\nlet composition = try await AVVideoComposition.videoComposition(with: asset) { request in\n    \/\/ Clamp to avoid blurring transparent pixels at the image edges.\n    let source = request.sourceImage.clampedToExtent()\n    filter.setValue(source, forKey: kCIInputImageKey)\n            \n    \/\/ Vary filter parameters based on the video timing.\n    let seconds = CMTimeGetSeconds(request.compositionTime)\n    filter.setValue(seconds * 10.0, forKey: kCIInputRadiusKey)\n            \n    \/\/ Crop the blurred output to the bounds of the original image.\n    if let output = filter.outputImage?.cropped(to: request.sourceImage.extent) {\n        request.finish(with: output, context: nil)\n    } else {\n        request.finish(with: AppError.customError)\n    }\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "5a93d8bf0ed1db130caeb5f9096077700b181635de086b897768bef90624c3cb",
  "crawledAt" : "2025-12-01T16:50:27Z",
  "declaration" : {
    "code" : "class func videoComposition(with asset: AVAsset, applyingCIFiltersWithHandler applier: @escaping (AVAsynchronousCIImageFilteringRequest) -> Void, completionHandler: @escaping (AVVideoComposition?, (any Error)?) -> Void)",
    "language" : "swift"
  },
  "id" : "4A166018-DA32-4E67-AD85-C3644BC2775D",
  "kind" : "method",
  "module" : "AVFoundation",
  "overview" : "## Discussion\n\nThe composition calls the specified handler one time for each frame to display (or processed for export) from the asset’s first enabled video track. In that block, you access the video frame and return a filtered result using the provided [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAsynchronousCIImageFilteringRequest] object. Use that object’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAsynchronousCIImageFilteringRequest\/sourceImage] property to get the video frame in the form of a [doc:\/\/com.apple.documentation\/documentation\/CoreImage\/CIImage] object you can apply filters to. Pass the result of your filters to the `request` object’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAsynchronousCIImageFilteringRequest\/finish(with:context:)] method. (If your filter rendering fails, call the `request` object’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAsynchronousCIImageFilteringRequest\/finish(with:)] method if you can’t apply filters).\n\nCreating a composition with this method sets values for the following properties:",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst",
    "macOS",
    "tvOS",
    "visionOS"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/avfoundation\/avvideocomposition\/videocomposition(with:applyingcifilterswithhandler:completionhandler:)\ncrawled: 2025-12-01T16:50:27Z\n---\n\n# videoComposition(with:applyingCIFiltersWithHandler:completionHandler:)\n\n**Type Method**\n\nReturns a new video composition that’s configured to apply Core Image filters to each video frame of the specified asset.\n\n## Declaration\n\n```swift\nclass func videoComposition(with asset: AVAsset, applyingCIFiltersWithHandler applier: @escaping (AVAsynchronousCIImageFilteringRequest) -> Void, completionHandler: @escaping (AVVideoComposition?, (any Error)?) -> Void)\n```\n\n```swift\nclass func videoComposition(with asset: AVAsset, applyingCIFiltersWithHandler applier: @escaping (AVAsynchronousCIImageFilteringRequest) -> Void) async throws -> AVVideoComposition\n```\n\n## Parameters\n\n- **asset**: The asset whose configuration matches the intended use of the video composition.\n- **applier**: A block that AVFoundation calls when processing each video frame.\n\nThe block takes a single parameter and has no return value:\n\n\n- **completionHandler**: A block the system calls when it finishes creating the new video composition.\n\n## Discussion\n\nThe composition calls the specified handler one time for each frame to display (or processed for export) from the asset’s first enabled video track. In that block, you access the video frame and return a filtered result using the provided [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAsynchronousCIImageFilteringRequest] object. Use that object’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAsynchronousCIImageFilteringRequest\/sourceImage] property to get the video frame in the form of a [doc:\/\/com.apple.documentation\/documentation\/CoreImage\/CIImage] object you can apply filters to. Pass the result of your filters to the `request` object’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAsynchronousCIImageFilteringRequest\/finish(with:context:)] method. (If your filter rendering fails, call the `request` object’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAsynchronousCIImageFilteringRequest\/finish(with:)] method if you can’t apply filters).\n\n```swift\nguard let filter = CIFilter(name: \"CIGaussianBlur\") else { return }\nlet composition = try await AVVideoComposition.videoComposition(with: asset) { request in\n    \/\/ Clamp to avoid blurring transparent pixels at the image edges.\n    let source = request.sourceImage.clampedToExtent()\n    filter.setValue(source, forKey: kCIInputImageKey)\n            \n    \/\/ Vary filter parameters based on the video timing.\n    let seconds = CMTimeGetSeconds(request.compositionTime)\n    filter.setValue(seconds * 10.0, forKey: kCIInputRadiusKey)\n            \n    \/\/ Crop the blurred output to the bounds of the original image.\n    if let output = filter.outputImage?.cropped(to: request.sourceImage.extent) {\n        request.finish(with: output, context: nil)\n    } else {\n        request.finish(with: AppError.customError)\n    }\n}\n```\n\nCreating a composition with this method sets values for the following properties:\n\n- The value of the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVVideoComposition\/frameDuration] property accommodates the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetTrack\/nominalFrameRate] value for the asset’s first enabled video track. If the nominal frame rate is zero, AVFoundation uses a default frame rate of 30 fps.\n- The [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVVideoComposition\/renderSize] property value a size that encompasses the asset’s first enabled video track, respecting the track’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetTrack\/preferredTransform] property.\n- The [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVVideoComposition\/renderScale] property value is `1.0`.\n\n## Creating a video composition\n\n- **init(configuration:)**: Initialize an AVVideoComposition with a configuration.\n- **AVVideoComposition.Configuration**: Configurable properties for initializing a new AVVideoComposition instance.\n- **init(applyingFiltersTo:applier:)**: Creates a video composition configured to apply Core Image filters to each video frame of the specified asset.\n- **AVAsynchronousCIImageFilteringRequest**: An object that supports using Core Image filters to process an individual video frame in a video composition.\n- **AVCIImageFilteringParameters**\n- **AVCIImageFilteringResult**: An output video frame processed with Core Image filtering.\n- **videoComposition(withPropertiesOf:completionHandler:)**: Returns a new video composition that’s configured to present the video tracks of the specified asset.\n- **init(propertiesOf:)**: Creates a video composition object configured to present the video tracks of the specified asset.\n- **init(asset:applyingCIFiltersWithHandler:)**: Creates a video composition configured to apply Core Image filters to each video frame of the specified asset.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Initialize an AVVideoComposition with a configuration.",
          "name" : "init(configuration:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVVideoComposition\/init(configuration:)"
        },
        {
          "description" : "Configurable properties for initializing a new AVVideoComposition instance.",
          "name" : "AVVideoComposition.Configuration",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVVideoComposition\/Configuration"
        },
        {
          "description" : "Creates a video composition configured to apply Core Image filters to each video frame of the specified asset.",
          "name" : "init(applyingFiltersTo:applier:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVVideoComposition\/init(applyingFiltersTo:applier:)"
        },
        {
          "description" : "An object that supports using Core Image filters to process an individual video frame in a video composition.",
          "name" : "AVAsynchronousCIImageFilteringRequest",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVAsynchronousCIImageFilteringRequest"
        },
        {
          "description" : "",
          "name" : "AVCIImageFilteringParameters",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVCIImageFilteringParameters"
        },
        {
          "description" : "An output video frame processed with Core Image filtering.",
          "name" : "AVCIImageFilteringResult",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVCIImageFilteringResult"
        },
        {
          "description" : "Returns a new video composition that’s configured to present the video tracks of the specified asset.",
          "name" : "videoComposition(withPropertiesOf:completionHandler:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVVideoComposition\/videoComposition(withPropertiesOf:completionHandler:)"
        },
        {
          "description" : "Creates a video composition object configured to present the video tracks of the specified asset.",
          "name" : "init(propertiesOf:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVVideoComposition\/init(propertiesOf:)"
        },
        {
          "description" : "Creates a video composition configured to apply Core Image filters to each video frame of the specified asset.",
          "name" : "init(asset:applyingCIFiltersWithHandler:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVVideoComposition\/init(asset:applyingCIFiltersWithHandler:)"
        }
      ],
      "title" : "Creating a video composition"
    }
  ],
  "source" : "appleJSON",
  "title" : "videoComposition(with:applyingCIFiltersWithHandler:completionHandler:)",
  "url" : "https:\/\/developer.apple.com\/documentation\/avfoundation\/avvideocomposition\/videocomposition(with:applyingcifilterswithhandler:completionhandler:)"
}