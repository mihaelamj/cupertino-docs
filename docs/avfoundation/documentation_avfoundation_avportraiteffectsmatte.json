{
  "abstract" : "An auxiliary image used to separate foreground from background with high resolution.",
  "codeExamples" : [

  ],
  "conformsTo" : [
    "CVarArg",
    "CustomDebugStringConvertible",
    "CustomStringConvertible",
    "Equatable",
    "Hashable",
    "NSObjectProtocol"
  ],
  "contentHash" : "b2abe2407dbe12a56d1f0be3265d34f10ff4a6ae10e882da1e9e39720bbc905c",
  "crawledAt" : "2025-12-03T20:08:50Z",
  "declaration" : {
    "code" : "class AVPortraitEffectsMatte",
    "language" : "swift"
  },
  "id" : "F401359B-3C80-4945-ABAA-D014962772CA",
  "kind" : "class",
  "language" : "swift",
  "module" : "AVFoundation",
  "overview" : "## Overview\n\nBefore iOS 11, the iPhone camera software used depth maps to render a shallow depth of field (the *bokeh* effect) into still images taken in Portrait Mode before discarding the maps. Because the effect was part of the photo, you couldn’t access the maps separately, as metadata, for photos taken by devices running iOS 10 or earlier.\n\nStarting in iOS 11, apps accessing the photo library can use images containing embedded auxiliary depth maps to render creative depth effects, such as forced perspective, or image projection from 2D to 3D space. These depth maps are low-resolution compared to the full-resolution RGB image. As such, the depth effects you can render are limited by the resolution and accuracy of the maps. Fine detail, such as hair, is challenging to preserve faithfully at the resolution of these depth maps.\n\nStarting in iOS 12, the portrait effects matte helps achieve this fine-grain level of detail.\n\n\n\nUsing the auxiliary matte image, you can improve the quality of rendered portrait effects, such as Natural Light, Studio Light, Contour Light, Stage Light, and Stage Light Mono.\n\nUnlike the depth map, the portrait effects matte isn’t intended to faithfully preserve all gradations of depth in the scene. It’s a depth-guided, people-focused segmentation mask generated from a proprietary Apple neural network trained to detect people. It separates an individual in the foreground from whatever is in the background, with greater detail and clarity than with the depth map alone. It achieves this clarity in part because the matte image has higher resolution than the depth map.\n\nFor information about capturing the portrait effects matte, see [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/configuring-camera-capture-to-collect-a-portrait-effects-matte]. To learn how to extract a portrait effects matte from photos previously captured in portrait mode on a device running iOS 12, see [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/extracting-portrait-effects-matte-image-data-from-a-photo].",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst",
    "macOS",
    "tvOS",
    "visionOS",
    "watchOS"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVPortraitEffectsMatte\ncrawled: 2025-12-03T20:08:50Z\n---\n\n# AVPortraitEffectsMatte\n\n**Class**\n\nAn auxiliary image used to separate foreground from background with high resolution.\n\n## Declaration\n\n```swift\nclass AVPortraitEffectsMatte\n```\n\n## Overview\n\nBefore iOS 11, the iPhone camera software used depth maps to render a shallow depth of field (the *bokeh* effect) into still images taken in Portrait Mode before discarding the maps. Because the effect was part of the photo, you couldn’t access the maps separately, as metadata, for photos taken by devices running iOS 10 or earlier.\n\nStarting in iOS 11, apps accessing the photo library can use images containing embedded auxiliary depth maps to render creative depth effects, such as forced perspective, or image projection from 2D to 3D space. These depth maps are low-resolution compared to the full-resolution RGB image. As such, the depth effects you can render are limited by the resolution and accuracy of the maps. Fine detail, such as hair, is challenging to preserve faithfully at the resolution of these depth maps.\n\nStarting in iOS 12, the portrait effects matte helps achieve this fine-grain level of detail.\n\n\n\n\n\nUsing the auxiliary matte image, you can improve the quality of rendered portrait effects, such as Natural Light, Studio Light, Contour Light, Stage Light, and Stage Light Mono.\n\nUnlike the depth map, the portrait effects matte isn’t intended to faithfully preserve all gradations of depth in the scene. It’s a depth-guided, people-focused segmentation mask generated from a proprietary Apple neural network trained to detect people. It separates an individual in the foreground from whatever is in the background, with greater detail and clarity than with the depth map alone. It achieves this clarity in part because the matte image has higher resolution than the depth map.\n\nFor information about capturing the portrait effects matte, see [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/configuring-camera-capture-to-collect-a-portrait-effects-matte]. To learn how to extract a portrait effects matte from photos previously captured in portrait mode on a device running iOS 12, see [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/extracting-portrait-effects-matte-image-data-from-a-photo].\n\n## Creating a Portrait Effects matte\n\n- **Configuring camera capture to collect a Portrait Effects matte**: Prepare your app to capture a portrait effects matte when taking photos.\n- **init(fromDictionaryRepresentation:)**: Initializes a portrait effects matte instance from auxiliary image information in an image file.\n- **applyingExifOrientation(_:)**: Returns a derivative portrait effects matte after applying the specified EXIF orientation.\n- **replacingPortraitEffectsMatte(with:)**: Returns a portrait effects matte by wrapping the replacement pixel buffer.\n\n## Examining a Portrait Effects matte\n\n- **Extracting Portrait Effects matte image data from a photo**: Check for portrait effects matte metadata in existing images.\n- **mattingImage**: The portrait effects matte’s internal image, formatted as a pixel buffer.\n- **pixelFormatType**: The pixel format type of this portrait effects matte’s internal image.\n- **dictionaryRepresentation(forAuxiliaryDataType:)**: A dictionary of primitive map information used for writing an image file with a portrait effects matte.\n\n## Matte data\n\n- **AVSemanticSegmentationMatte**: An object that wraps a matting image for a particular semantic segmentation.\n\n## Inherits From\n\n- NSObject\n\n## Conforms To\n\n- CVarArg\n- CustomDebugStringConvertible\n- CustomStringConvertible\n- Equatable\n- Hashable\n- NSObjectProtocol\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Prepare your app to capture a portrait effects matte when taking photos.",
          "name" : "Configuring camera capture to collect a Portrait Effects matte",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/configuring-camera-capture-to-collect-a-portrait-effects-matte"
        },
        {
          "description" : "Initializes a portrait effects matte instance from auxiliary image information in an image file.",
          "name" : "init(fromDictionaryRepresentation:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVPortraitEffectsMatte\/init(fromDictionaryRepresentation:)"
        },
        {
          "description" : "Returns a derivative portrait effects matte after applying the specified EXIF orientation.",
          "name" : "applyingExifOrientation(_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVPortraitEffectsMatte\/applyingExifOrientation(_:)"
        },
        {
          "description" : "Returns a portrait effects matte by wrapping the replacement pixel buffer.",
          "name" : "replacingPortraitEffectsMatte(with:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVPortraitEffectsMatte\/replacingPortraitEffectsMatte(with:)"
        }
      ],
      "title" : "Creating a Portrait Effects matte"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "Check for portrait effects matte metadata in existing images.",
          "name" : "Extracting Portrait Effects matte image data from a photo",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/extracting-portrait-effects-matte-image-data-from-a-photo"
        },
        {
          "description" : "The portrait effects matte’s internal image, formatted as a pixel buffer.",
          "name" : "mattingImage",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVPortraitEffectsMatte\/mattingImage"
        },
        {
          "description" : "The pixel format type of this portrait effects matte’s internal image.",
          "name" : "pixelFormatType",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVPortraitEffectsMatte\/pixelFormatType"
        },
        {
          "description" : "A dictionary of primitive map information used for writing an image file with a portrait effects matte.",
          "name" : "dictionaryRepresentation(forAuxiliaryDataType:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVPortraitEffectsMatte\/dictionaryRepresentation(forAuxiliaryDataType:)"
        }
      ],
      "title" : "Examining a Portrait Effects matte"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "An object that wraps a matting image for a particular semantic segmentation.",
          "name" : "AVSemanticSegmentationMatte",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVSemanticSegmentationMatte"
        }
      ],
      "title" : "Matte data"
    },
    {
      "content" : "",
      "items" : [
        {
          "name" : "NSObject"
        }
      ],
      "title" : "Inherits From"
    }
  ],
  "source" : "appleJSON",
  "title" : "AVPortraitEffectsMatte",
  "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVPortraitEffectsMatte"
}