{
  "abstract" : "Configure input devices, output media, preview views, and basic settings before capturing photos or video.",
  "codeExamples" : [
    {
      "code" : "captureSession.beginConfiguration()\nlet videoDevice = AVCaptureDevice.default(.builtInWideAngleCamera,\n                                          for: .video, position: .unspecified)\nguard\n    let videoDeviceInput = try? AVCaptureDeviceInput(device: videoDevice!),\n    captureSession.canAddInput(videoDeviceInput)\n    else { return }\ncaptureSession.addInput(videoDeviceInput)",
      "language" : "swift"
    },
    {
      "code" : "let photoOutput = AVCapturePhotoOutput()\nguard captureSession.canAddOutput(photoOutput) else { return }\ncaptureSession.sessionPreset = .photo\ncaptureSession.addOutput(photoOutput)\ncaptureSession.commitConfiguration()",
      "language" : "swift"
    },
    {
      "code" : "class PreviewView: UIView {\n    override class var layerClass: AnyClass {\n        return AVCaptureVideoPreviewLayer.self\n    }\n    \n    \/\/\/ Convenience wrapper to get layer as its statically known type.\n    var videoPreviewLayer: AVCaptureVideoPreviewLayer {\n        return layer as! AVCaptureVideoPreviewLayer\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "self.previewView.videoPreviewLayer.session = self.captureSession",
      "language" : "swift"
    }
  ],
  "contentHash" : "050c12991a9e3b24fe82fffb480e43a55929f65d94033a25f250055c05555923",
  "crawledAt" : "2025-12-02T16:03:07Z",
  "id" : "9C173300-B806-49B5-9D43-F0D11FF4847A",
  "kind" : "article",
  "language" : "swift",
  "module" : "AVFoundation",
  "overview" : "## Overview\n\nAn [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureSession] is the basis for all media capture in iOS and macOS. It manages your app’s exclusive access to the OS capture infrastructure and capture devices, as well as the flow of data from input devices to media outputs. How you configure connections between inputs and outputs defines the capabilities of your capture session. For example, the diagram below shows a capture session that can capture both photos and movies and provides a camera preview, using the iPhone back camera and microphone.\n\n\n\n### Connect inputs and outputs to the session\n\nAll capture sessions need at least one capture input and capture output. Capture inputs ([doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureInput] subclasses) are media sources—typically recording devices like the cameras and microphone built into an iOS device or Mac. Capture outputs ([doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureOutput] subclasses) use data provided by capture inputs to produce media, like image and movie files.\n\nTo use a camera for video input (to capture photos or movies), select an appropriate [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDevice], create a corresponding [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDeviceInput], and add it to the session:\n\nNext, add outputs for the kinds of media you plan to capture from the camera you’ve selected. For example, to enable capturing photos, add an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput] to the session:\n\nA session can have multiple inputs and outputs. For example:\n\n### Display a camera preview\n\nIt’s important to let the user see input from the camera before choosing to snap a photo or start video recording, as in the viewfinder of a traditional camera. You can provide such a preview by connecting an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureVideoPreviewLayer] to your capture session, which displays a live video feed from the camera whenever the session is running.\n\n[doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureVideoPreviewLayer] is a Core Animation layer, so you can display and style it in your interface as you would any other [doc:\/\/com.apple.documentation\/documentation\/QuartzCore\/CALayer] subclass. The simplest way to add a preview layer to a UIKit app is to define a [doc:\/\/com.apple.documentation\/documentation\/UIKit\/UIView] subclass whose [doc:\/\/com.apple.documentation\/documentation\/UIKit\/UIView\/layerClass] is [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureVideoPreviewLayer], as shown below.\n\nThen, to use the preview layer with a capture session, set the layer’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureVideoPreviewLayer\/session] property:\n\n### Run the capture session\n\nAfter you’ve configured inputs, outputs, and previews, call [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureSession\/startRunning()] to let data flow from inputs to outputs.\n\nWith some capture outputs, running the session is all you need to begin media capture. For example, if your session contains an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureVideoDataOutput], you start receiving delivering video frames as soon as the session is running.\n\nWith other capture outputs, you first start the session running, then use the capture output class itself to initiate capture. In a photography app, for example, running the session enables a viewfinder-style preview, but you use the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput] [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput\/capturePhoto(with:delegate:)] method to snap a picture.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/AVFoundation\/setting-up-a-capture-session\ncrawled: 2025-12-02T16:03:07Z\n---\n\n# Setting up a capture session\n\n**Article**\n\nConfigure input devices, output media, preview views, and basic settings before capturing photos or video.\n\n## Overview\n\nAn [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureSession] is the basis for all media capture in iOS and macOS. It manages your app’s exclusive access to the OS capture infrastructure and capture devices, as well as the flow of data from input devices to media outputs. How you configure connections between inputs and outputs defines the capabilities of your capture session. For example, the diagram below shows a capture session that can capture both photos and movies and provides a camera preview, using the iPhone back camera and microphone.\n\n\n\n### Connect inputs and outputs to the session\n\nAll capture sessions need at least one capture input and capture output. Capture inputs ([doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureInput] subclasses) are media sources—typically recording devices like the cameras and microphone built into an iOS device or Mac. Capture outputs ([doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureOutput] subclasses) use data provided by capture inputs to produce media, like image and movie files.\n\nTo use a camera for video input (to capture photos or movies), select an appropriate [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDevice], create a corresponding [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDeviceInput], and add it to the session:\n\n```swift\ncaptureSession.beginConfiguration()\nlet videoDevice = AVCaptureDevice.default(.builtInWideAngleCamera,\n                                          for: .video, position: .unspecified)\nguard\n    let videoDeviceInput = try? AVCaptureDeviceInput(device: videoDevice!),\n    captureSession.canAddInput(videoDeviceInput)\n    else { return }\ncaptureSession.addInput(videoDeviceInput)\n```\n\n\n\nNext, add outputs for the kinds of media you plan to capture from the camera you’ve selected. For example, to enable capturing photos, add an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput] to the session:\n\n```swift\nlet photoOutput = AVCapturePhotoOutput()\nguard captureSession.canAddOutput(photoOutput) else { return }\ncaptureSession.sessionPreset = .photo\ncaptureSession.addOutput(photoOutput)\ncaptureSession.commitConfiguration()\n```\n\nA session can have multiple inputs and outputs. For example:\n\n- To record both video and audio in a movie, add inputs for both camera and microphone devices.\n- To capture both photos and movies from the same camera, add both [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput] and [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureMovieFileOutput] to your session.\n\n\n\n### Display a camera preview\n\nIt’s important to let the user see input from the camera before choosing to snap a photo or start video recording, as in the viewfinder of a traditional camera. You can provide such a preview by connecting an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureVideoPreviewLayer] to your capture session, which displays a live video feed from the camera whenever the session is running.\n\n[doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureVideoPreviewLayer] is a Core Animation layer, so you can display and style it in your interface as you would any other [doc:\/\/com.apple.documentation\/documentation\/QuartzCore\/CALayer] subclass. The simplest way to add a preview layer to a UIKit app is to define a [doc:\/\/com.apple.documentation\/documentation\/UIKit\/UIView] subclass whose [doc:\/\/com.apple.documentation\/documentation\/UIKit\/UIView\/layerClass] is [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureVideoPreviewLayer], as shown below.\n\n```swift\nclass PreviewView: UIView {\n    override class var layerClass: AnyClass {\n        return AVCaptureVideoPreviewLayer.self\n    }\n    \n    \/\/\/ Convenience wrapper to get layer as its statically known type.\n    var videoPreviewLayer: AVCaptureVideoPreviewLayer {\n        return layer as! AVCaptureVideoPreviewLayer\n    }\n}\n```\n\nThen, to use the preview layer with a capture session, set the layer’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureVideoPreviewLayer\/session] property:\n\n```swift\nself.previewView.videoPreviewLayer.session = self.captureSession\n```\n\n\n\n### Run the capture session\n\nAfter you’ve configured inputs, outputs, and previews, call [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureSession\/startRunning()] to let data flow from inputs to outputs.\n\nWith some capture outputs, running the session is all you need to begin media capture. For example, if your session contains an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureVideoDataOutput], you start receiving delivering video frames as soon as the session is running.\n\nWith other capture outputs, you first start the session running, then use the capture output class itself to initiate capture. In a photography app, for example, running the session enables a viewfinder-style preview, but you use the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput] [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput\/capturePhoto(with:delegate:)] method to snap a picture.\n\n## Capture sessions\n\n- **Accessing the camera while multitasking on iPad**: Operate the camera in Split View, Slide Over, Picture in Picture, and Stage Manager modes.\n- **AVCam: Building a camera app**: Capture photos and record video using the front and rear iPhone and iPad cameras.\n- **Capturing Cinematic video**: Capture video with an adjustable depth of field and focus points.\n- **AVMultiCamPiP: Capturing from Multiple Cameras**: Simultaneously record the output from the front and back cameras into a single movie file by using a multi-camera capture session.\n- **AVCamBarcode: detecting barcodes and faces**: Identify machine readable codes or faces by using the camera.\n- **AVCaptureSession**: An object that configures capture behavior and coordinates the flow of data from input devices to capture outputs.\n- **AVCaptureMultiCamSession**: A capture session that supports simultaneous capture from multiple inputs of the same media type.\n- **AVCaptureInput**: An abstract superclass for objects that provide input data to a capture session.\n- **AVCaptureOutput**: An abstract superclass for objects that provide media output destinations for a capture session.\n- **AVCaptureConnection**: An object that represents a connection from a capture input to a capture output.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Operate the camera in Split View, Slide Over, Picture in Picture, and Stage Manager modes.",
          "name" : "Accessing the camera while multitasking on iPad",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVKit\/accessing-the-camera-while-multitasking-on-ipad"
        },
        {
          "description" : "Capture photos and record video using the front and rear iPhone and iPad cameras.",
          "name" : "AVCam: Building a camera app",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/avcam-building-a-camera-app"
        },
        {
          "description" : "Capture video with an adjustable depth of field and focus points.",
          "name" : "Capturing Cinematic video",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/capturing-cinematic-video"
        },
        {
          "description" : "Simultaneously record the output from the front and back cameras into a single movie file by using a multi-camera capture session.",
          "name" : "AVMultiCamPiP: Capturing from Multiple Cameras",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/avmulticampip-capturing-from-multiple-cameras"
        },
        {
          "description" : "Identify machine readable codes or faces by using the camera.",
          "name" : "AVCamBarcode: detecting barcodes and faces",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/avcambarcode-detecting-barcodes-and-faces"
        },
        {
          "description" : "An object that configures capture behavior and coordinates the flow of data from input devices to capture outputs.",
          "name" : "AVCaptureSession",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVCaptureSession"
        },
        {
          "description" : "A capture session that supports simultaneous capture from multiple inputs of the same media type.",
          "name" : "AVCaptureMultiCamSession",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVCaptureMultiCamSession"
        },
        {
          "description" : "An abstract superclass for objects that provide input data to a capture session.",
          "name" : "AVCaptureInput",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVCaptureInput"
        },
        {
          "description" : "An abstract superclass for objects that provide media output destinations for a capture session.",
          "name" : "AVCaptureOutput",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVCaptureOutput"
        },
        {
          "description" : "An object that represents a connection from a capture input to a capture output.",
          "name" : "AVCaptureConnection",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVCaptureConnection"
        }
      ],
      "title" : "Capture sessions"
    }
  ],
  "source" : "appleJSON",
  "title" : "Setting up a capture session",
  "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/setting-up-a-capture-session"
}