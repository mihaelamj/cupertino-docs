{
  "abstract" : "Access the LiDAR camera on supporting devices to capture precise depth data.",
  "codeExamples" : [
    {
      "code" : "\/\/ Look up the LiDAR camera.\nguard let device = AVCaptureDevice.default(.builtInLiDARDepthCamera, for: .video, position: .back) else {\n    throw ConfigurationError.lidarDeviceUnavailable\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Find a match that outputs video data in the format the app's custom Metal views require.\nguard let format = (device.formats.last { format in\n    format.formatDescription.dimensions.width == preferredWidthResolution &&\n    format.formatDescription.mediaSubType.rawValue == kCVPixelFormatType_420YpCbCr8BiPlanarFullRange &&\n    !format.isVideoBinned &&\n    !format.supportedDepthDataFormats.isEmpty\n}) else {\n    throw ConfigurationError.requiredFormatUnavailable\n}\n\n\/\/ Find a match that outputs depth data in the format the app's custom Metal views require.\nguard let depthFormat = (format.supportedDepthDataFormats.last { depthFormat in\n    depthFormat.formatDescription.mediaSubType.rawValue == kCVPixelFormatType_DepthFloat16\n}) else {\n    throw ConfigurationError.requiredFormatUnavailable\n}\n\n\/\/ Begin the device configuration.\ntry device.lockForConfiguration()\n\n\/\/ Configure the device and depth formats.\ndevice.activeFormat = format\ndevice.activeDepthDataFormat = depthFormat\n\n\/\/ Finish the device configuration.\ndevice.unlockForConfiguration()",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Create an object to output video sample buffers.\nvideoDataOutput = AVCaptureVideoDataOutput()\ncaptureSession.addOutput(videoDataOutput)\n\n\/\/ Create an object to output depth data.\ndepthDataOutput = AVCaptureDepthDataOutput()\ndepthDataOutput.isFilteringEnabled = isFilteringEnabled\ncaptureSession.addOutput(depthDataOutput)\n\n\/\/ Create an object to synchronize the delivery of depth and video data.\noutputVideoSync = AVCaptureDataOutputSynchronizer(dataOutputs: [depthDataOutput, videoDataOutput])\noutputVideoSync.setDelegate(self, queue: videoQueue)",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Create an object to output photos.\nphotoOutput = AVCapturePhotoOutput()\nphotoOutput.maxPhotoQualityPrioritization = .quality\ncaptureSession.addOutput(photoOutput)\n\n\/\/ Enable delivery of depth data after adding the output to the capture session.\nphotoOutput.isDepthDataDeliveryEnabled = true",
      "language" : "swift"
    },
    {
      "code" : "func dataOutputSynchronizer(_ synchronizer: AVCaptureDataOutputSynchronizer,\n                            didOutput synchronizedDataCollection: AVCaptureSynchronizedDataCollection) {\n    \/\/ Retrieve the synchronized depth and sample buffer container objects.\n    guard let syncedDepthData = synchronizedDataCollection.synchronizedData(for: depthDataOutput) as? AVCaptureSynchronizedDepthData,\n          let syncedVideoData = synchronizedDataCollection.synchronizedData(for: videoDataOutput) as? AVCaptureSynchronizedSampleBufferData else { return }\n    \n    guard let pixelBuffer = syncedVideoData.sampleBuffer.imageBuffer,\n          let cameraCalibrationData = syncedDepthData.depthData.cameraCalibrationData else { return }\n    \n    \/\/ Package the captured data.\n    let data = CameraCapturedData(depth: syncedDepthData.depthData.depthDataMap.texture(withFormat: .r16Float, planeIndex: 0, addToCache: textureCache),\n                                  colorY: pixelBuffer.texture(withFormat: .r8Unorm, planeIndex: 0, addToCache: textureCache),\n                                  colorCbCr: pixelBuffer.texture(withFormat: .rg8Unorm, planeIndex: 1, addToCache: textureCache),\n                                  cameraIntrinsics: cameraCalibrationData.intrinsicMatrix,\n                                  cameraReferenceDimensions: cameraCalibrationData.intrinsicMatrixReferenceDimensions)\n    \n    delegate?.onNewData(capturedData: data)\n}",
      "language" : "swift"
    },
    {
      "code" : "func capturePhoto() {\n    var photoSettings: AVCapturePhotoSettings\n    if  photoOutput.availablePhotoPixelFormatTypes.contains(kCVPixelFormatType_420YpCbCr8BiPlanarFullRange) {\n        photoSettings = AVCapturePhotoSettings(format: [\n            kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_420YpCbCr8BiPlanarFullRange\n        ])\n    } else {\n        photoSettings = AVCapturePhotoSettings()\n    }\n    \n    \/\/ Capture depth data with this photo capture.\n    photoSettings.isDepthDataDeliveryEnabled = true\n    photoOutput.capturePhoto(with: photoSettings, delegate: self)\n}",
      "language" : "swift"
    },
    {
      "code" : "func photoOutput(_ output: AVCapturePhotoOutput, didFinishProcessingPhoto photo: AVCapturePhoto, error: Error?) {\n    \n    \/\/ Retrieve the image and depth data.\n    guard let pixelBuffer = photo.pixelBuffer,\n          let depthData = photo.depthData,\n          let cameraCalibrationData = depthData.cameraCalibrationData else { return }\n    \n    \/\/ Stop the stream until the user returns to streaming mode.\n    stopStream()\n    \n    \/\/ Convert the depth data to the expected format.\n    let convertedDepth = depthData.converting(toDepthDataType: kCVPixelFormatType_DepthFloat16)\n    \n    \/\/ Package the captured data.\n    let data = CameraCapturedData(depth: convertedDepth.depthDataMap.texture(withFormat: .r16Float, planeIndex: 0, addToCache: textureCache),\n                                  colorY: pixelBuffer.texture(withFormat: .r8Unorm, planeIndex: 0, addToCache: textureCache),\n                                  colorCbCr: pixelBuffer.texture(withFormat: .rg8Unorm, planeIndex: 1, addToCache: textureCache),\n                                  cameraIntrinsics: cameraCalibrationData.intrinsicMatrix,\n                                  cameraReferenceDimensions: cameraCalibrationData.intrinsicMatrixReferenceDimensions)\n    \n    delegate?.onNewPhotoData(capturedData: data)\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "3bf960b4cf306292d96039ec5b96d264d3c72d3166f4c4ba07d5b741cd4fe9a8",
  "crawledAt" : "2025-12-02T15:48:09Z",
  "id" : "FA2F3EBC-0B61-4037-A3D9-28B430872B87",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "AVFoundation",
  "overview" : "## Overview\n\nAVFoundation introduced depth data capture for photos and video in iOS 11. The data it provides is suitable for many apps, but may not meet the needs of those that require greater precision depth. Starting in iOS 15.4, you can access the LiDAR camera on supported hardware, which offers high-precision depth data suitable for use cases like room scanning and measurement.\n\nThis sample code project shows how to capture and render depth data from the LiDAR camera. It starts in streaming mode, which demonstrates how to capture synchronized video and depth data. When you tap the Camera button in the upper-left corner of the screen, the app switches to photo mode, which illustrates how to capture photos with depth data. In both modes, the app provides several Metal-based visualizations of the depth and image data.\n\n### Configure the sample code project\n\nRun this sample code on a device that provides a LiDAR camera, such as:\n\n### Configure the LiDAR camera\n\nThe sample app’s `CameraController` class provides the code that configures and manages the capture session, and handles the delivery of new video and depth data. It begins its configuration by retrieving the LiDAR camera. It calls the capture device’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDevice\/default(_:for:position:)] class method, passing it the new [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDevice\/DeviceType-swift.struct\/builtInLiDARDepthCamera] device type available in iOS 15.4 and later.\n\nAfter retrieving the device, the app configures it with a specific video and depth format. It asks the device for its supported formats and finds the best nonbinned, full-range YUV color format that matches the sample app’s preferred width and supports depth capture. Finally, it sets the active formats on the device as in the following code example:\n\n### Configure the capture outputs\n\nThe app operates in streaming or photo mode. To enable streaming output, it creates an instance of [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureVideoDataOutput] and [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDepthDataOutput] to capture video sample buffers and depth data, respectively. It configures them as follows:\n\nBecause the video and depth data stream from separate output objects, the sample uses an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDataOutputSynchronizer] to synchronize the delivery from both outputs to a single callback. The `CameraController` class adopts the synchronizer’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDataOutputSynchronizerDelegate] protocol and responds to the delivery of new video and depth data.\n\nTo handle photo capture, the app also creates an instance of [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput]. It optimizes the output for high-quality capture and adds the output to the capture session.\n\nAfter it adds the output to the session, it enables the delivery of depth data, which configures the capture pipeline appropriately. It can only enable depth delivery after adding the output to the capture session because the output needs to determine whether the pipeline configuration can deliver it.\n\n### Capture synchronized video and depth\n\nAfter configuring the capture session’s inputs and outputs as required, the app is ready to start capturing data. The app starts in streaming mode, which uses the video data and depth data outputs and an `AVCaptureDataOutputSynchronizer` to synchronize the delivery of their data. The app adopts the synchronizer’s delegate protocol and implements its [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDataOutputSynchronizerDelegate\/dataOutputSynchronizer(_:didOutput:)] method to handle the delivery, as the following example shows:\n\nThe app retrieves the container objects that store the synchronized data from the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureSynchronizedDataCollection]. It then unwraps the underlying video pixel buffer and depth data, and packages them for the app’s Metal views to display.\n\n### Capture photos and depth\n\nWhen you tap the app’s Camera button in the upper-left corner of the user interface, the app switches to photo mode. When this occurs, the app calls its `capturePhoto()` method, which creates a photo settings object, requests depth delivery on it, and initiates a photo capture.\n\nWhen the framework finishes the photo capture, it calls the photo output’s delegate method and passes it the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhoto] object that contains the image and depth data. The sample retrieves the data from the photo, stops the stream until the user returns to streaming mode, and, similarly to the video case, packages the captured data for delivery to the app’s user interface layer.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/AVFoundation\/capturing-depth-using-the-lidar-camera\ncrawled: 2025-12-02T15:48:09Z\n---\n\n# Capturing depth using the LiDAR camera\n\n**Sample Code**\n\nAccess the LiDAR camera on supporting devices to capture precise depth data.\n\n## Overview\n\nAVFoundation introduced depth data capture for photos and video in iOS 11. The data it provides is suitable for many apps, but may not meet the needs of those that require greater precision depth. Starting in iOS 15.4, you can access the LiDAR camera on supported hardware, which offers high-precision depth data suitable for use cases like room scanning and measurement.\n\nThis sample code project shows how to capture and render depth data from the LiDAR camera. It starts in streaming mode, which demonstrates how to capture synchronized video and depth data. When you tap the Camera button in the upper-left corner of the screen, the app switches to photo mode, which illustrates how to capture photos with depth data. In both modes, the app provides several Metal-based visualizations of the depth and image data.\n\n### Configure the sample code project\n\nRun this sample code on a device that provides a LiDAR camera, such as:\n\n- iPhone 12 Pro or later\n- iPad Pro 11-inch (3rd generation) or later\n- iPad Pro 12.9-inch (5th generation) or later\n\n### Configure the LiDAR camera\n\nThe sample app’s `CameraController` class provides the code that configures and manages the capture session, and handles the delivery of new video and depth data. It begins its configuration by retrieving the LiDAR camera. It calls the capture device’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDevice\/default(_:for:position:)] class method, passing it the new [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDevice\/DeviceType-swift.struct\/builtInLiDARDepthCamera] device type available in iOS 15.4 and later.\n\n```swift\n\/\/ Look up the LiDAR camera.\nguard let device = AVCaptureDevice.default(.builtInLiDARDepthCamera, for: .video, position: .back) else {\n    throw ConfigurationError.lidarDeviceUnavailable\n}\n```\n\nAfter retrieving the device, the app configures it with a specific video and depth format. It asks the device for its supported formats and finds the best nonbinned, full-range YUV color format that matches the sample app’s preferred width and supports depth capture. Finally, it sets the active formats on the device as in the following code example:\n\n```swift\n\/\/ Find a match that outputs video data in the format the app's custom Metal views require.\nguard let format = (device.formats.last { format in\n    format.formatDescription.dimensions.width == preferredWidthResolution &&\n    format.formatDescription.mediaSubType.rawValue == kCVPixelFormatType_420YpCbCr8BiPlanarFullRange &&\n    !format.isVideoBinned &&\n    !format.supportedDepthDataFormats.isEmpty\n}) else {\n    throw ConfigurationError.requiredFormatUnavailable\n}\n\n\/\/ Find a match that outputs depth data in the format the app's custom Metal views require.\nguard let depthFormat = (format.supportedDepthDataFormats.last { depthFormat in\n    depthFormat.formatDescription.mediaSubType.rawValue == kCVPixelFormatType_DepthFloat16\n}) else {\n    throw ConfigurationError.requiredFormatUnavailable\n}\n\n\/\/ Begin the device configuration.\ntry device.lockForConfiguration()\n\n\/\/ Configure the device and depth formats.\ndevice.activeFormat = format\ndevice.activeDepthDataFormat = depthFormat\n\n\/\/ Finish the device configuration.\ndevice.unlockForConfiguration()\n```\n\n### Configure the capture outputs\n\nThe app operates in streaming or photo mode. To enable streaming output, it creates an instance of [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureVideoDataOutput] and [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDepthDataOutput] to capture video sample buffers and depth data, respectively. It configures them as follows:\n\n```swift\n\/\/ Create an object to output video sample buffers.\nvideoDataOutput = AVCaptureVideoDataOutput()\ncaptureSession.addOutput(videoDataOutput)\n\n\/\/ Create an object to output depth data.\ndepthDataOutput = AVCaptureDepthDataOutput()\ndepthDataOutput.isFilteringEnabled = isFilteringEnabled\ncaptureSession.addOutput(depthDataOutput)\n\n\/\/ Create an object to synchronize the delivery of depth and video data.\noutputVideoSync = AVCaptureDataOutputSynchronizer(dataOutputs: [depthDataOutput, videoDataOutput])\noutputVideoSync.setDelegate(self, queue: videoQueue)\n```\n\nBecause the video and depth data stream from separate output objects, the sample uses an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDataOutputSynchronizer] to synchronize the delivery from both outputs to a single callback. The `CameraController` class adopts the synchronizer’s [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDataOutputSynchronizerDelegate] protocol and responds to the delivery of new video and depth data.\n\nTo handle photo capture, the app also creates an instance of [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhotoOutput]. It optimizes the output for high-quality capture and adds the output to the capture session.\n\n```swift\n\/\/ Create an object to output photos.\nphotoOutput = AVCapturePhotoOutput()\nphotoOutput.maxPhotoQualityPrioritization = .quality\ncaptureSession.addOutput(photoOutput)\n\n\/\/ Enable delivery of depth data after adding the output to the capture session.\nphotoOutput.isDepthDataDeliveryEnabled = true\n```\n\nAfter it adds the output to the session, it enables the delivery of depth data, which configures the capture pipeline appropriately. It can only enable depth delivery after adding the output to the capture session because the output needs to determine whether the pipeline configuration can deliver it.\n\n### Capture synchronized video and depth\n\nAfter configuring the capture session’s inputs and outputs as required, the app is ready to start capturing data. The app starts in streaming mode, which uses the video data and depth data outputs and an `AVCaptureDataOutputSynchronizer` to synchronize the delivery of their data. The app adopts the synchronizer’s delegate protocol and implements its [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureDataOutputSynchronizerDelegate\/dataOutputSynchronizer(_:didOutput:)] method to handle the delivery, as the following example shows:\n\n```swift\nfunc dataOutputSynchronizer(_ synchronizer: AVCaptureDataOutputSynchronizer,\n                            didOutput synchronizedDataCollection: AVCaptureSynchronizedDataCollection) {\n    \/\/ Retrieve the synchronized depth and sample buffer container objects.\n    guard let syncedDepthData = synchronizedDataCollection.synchronizedData(for: depthDataOutput) as? AVCaptureSynchronizedDepthData,\n          let syncedVideoData = synchronizedDataCollection.synchronizedData(for: videoDataOutput) as? AVCaptureSynchronizedSampleBufferData else { return }\n    \n    guard let pixelBuffer = syncedVideoData.sampleBuffer.imageBuffer,\n          let cameraCalibrationData = syncedDepthData.depthData.cameraCalibrationData else { return }\n    \n    \/\/ Package the captured data.\n    let data = CameraCapturedData(depth: syncedDepthData.depthData.depthDataMap.texture(withFormat: .r16Float, planeIndex: 0, addToCache: textureCache),\n                                  colorY: pixelBuffer.texture(withFormat: .r8Unorm, planeIndex: 0, addToCache: textureCache),\n                                  colorCbCr: pixelBuffer.texture(withFormat: .rg8Unorm, planeIndex: 1, addToCache: textureCache),\n                                  cameraIntrinsics: cameraCalibrationData.intrinsicMatrix,\n                                  cameraReferenceDimensions: cameraCalibrationData.intrinsicMatrixReferenceDimensions)\n    \n    delegate?.onNewData(capturedData: data)\n}\n```\n\nThe app retrieves the container objects that store the synchronized data from the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureSynchronizedDataCollection]. It then unwraps the underlying video pixel buffer and depth data, and packages them for the app’s Metal views to display.\n\n### Capture photos and depth\n\nWhen you tap the app’s Camera button in the upper-left corner of the user interface, the app switches to photo mode. When this occurs, the app calls its `capturePhoto()` method, which creates a photo settings object, requests depth delivery on it, and initiates a photo capture.\n\n```swift\nfunc capturePhoto() {\n    var photoSettings: AVCapturePhotoSettings\n    if  photoOutput.availablePhotoPixelFormatTypes.contains(kCVPixelFormatType_420YpCbCr8BiPlanarFullRange) {\n        photoSettings = AVCapturePhotoSettings(format: [\n            kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_420YpCbCr8BiPlanarFullRange\n        ])\n    } else {\n        photoSettings = AVCapturePhotoSettings()\n    }\n    \n    \/\/ Capture depth data with this photo capture.\n    photoSettings.isDepthDataDeliveryEnabled = true\n    photoOutput.capturePhoto(with: photoSettings, delegate: self)\n}\n```\n\nWhen the framework finishes the photo capture, it calls the photo output’s delegate method and passes it the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCapturePhoto] object that contains the image and depth data. The sample retrieves the data from the photo, stops the stream until the user returns to streaming mode, and, similarly to the video case, packages the captured data for delivery to the app’s user interface layer.\n\n```swift\nfunc photoOutput(_ output: AVCapturePhotoOutput, didFinishProcessingPhoto photo: AVCapturePhoto, error: Error?) {\n    \n    \/\/ Retrieve the image and depth data.\n    guard let pixelBuffer = photo.pixelBuffer,\n          let depthData = photo.depthData,\n          let cameraCalibrationData = depthData.cameraCalibrationData else { return }\n    \n    \/\/ Stop the stream until the user returns to streaming mode.\n    stopStream()\n    \n    \/\/ Convert the depth data to the expected format.\n    let convertedDepth = depthData.converting(toDepthDataType: kCVPixelFormatType_DepthFloat16)\n    \n    \/\/ Package the captured data.\n    let data = CameraCapturedData(depth: convertedDepth.depthDataMap.texture(withFormat: .r16Float, planeIndex: 0, addToCache: textureCache),\n                                  colorY: pixelBuffer.texture(withFormat: .r8Unorm, planeIndex: 0, addToCache: textureCache),\n                                  colorCbCr: pixelBuffer.texture(withFormat: .rg8Unorm, planeIndex: 1, addToCache: textureCache),\n                                  cameraIntrinsics: cameraCalibrationData.intrinsicMatrix,\n                                  cameraReferenceDimensions: cameraCalibrationData.intrinsicMatrixReferenceDimensions)\n    \n    delegate?.onNewPhotoData(capturedData: data)\n}\n```\n\n## Depth data capture\n\n- **Capturing photos with depth**: Get a depth map with a photo to create effects like the system camera’s Portrait mode (on compatible devices).\n- **Creating auxiliary depth data manually**: Generate a depth image and attach it to your own image.\n- **AVCamFilter: Applying filters to a capture stream**: Render a capture stream with rose-colored filtering and depth effects.\n- **Streaming depth data from the TrueDepth camera**: Visualize depth data in 2D and 3D from the TrueDepth camera.\n- **Enhancing live video by leveraging TrueDepth camera data**: Apply your own background to a live capture feed streamed from the front-facing TrueDepth camera.\n- **AVCaptureDepthDataOutput**: A capture output that records scene depth information on compatible camera devices.\n- **AVDepthData**: A container for per-pixel distance or disparity information captured by compatible camera devices.\n- **AVCameraCalibrationData**: Information about the camera characteristics used to capture images and depth data.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Get a depth map with a photo to create effects like the system camera’s Portrait mode (on compatible devices).",
          "name" : "Capturing photos with depth",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/capturing-photos-with-depth"
        },
        {
          "description" : "Generate a depth image and attach it to your own image.",
          "name" : "Creating auxiliary depth data manually",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/creating-auxiliary-depth-data-manually"
        },
        {
          "description" : "Render a capture stream with rose-colored filtering and depth effects.",
          "name" : "AVCamFilter: Applying filters to a capture stream",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/avcamfilter-applying-filters-to-a-capture-stream"
        },
        {
          "description" : "Visualize depth data in 2D and 3D from the TrueDepth camera.",
          "name" : "Streaming depth data from the TrueDepth camera",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/streaming-depth-data-from-the-truedepth-camera"
        },
        {
          "description" : "Apply your own background to a live capture feed streamed from the front-facing TrueDepth camera.",
          "name" : "Enhancing live video by leveraging TrueDepth camera data",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/enhancing-live-video-by-leveraging-truedepth-camera-data"
        },
        {
          "description" : "A capture output that records scene depth information on compatible camera devices.",
          "name" : "AVCaptureDepthDataOutput",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVCaptureDepthDataOutput"
        },
        {
          "description" : "A container for per-pixel distance or disparity information captured by compatible camera devices.",
          "name" : "AVDepthData",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVDepthData"
        },
        {
          "description" : "Information about the camera characteristics used to capture images and depth data.",
          "name" : "AVCameraCalibrationData",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVCameraCalibrationData"
        }
      ],
      "title" : "Depth data capture"
    }
  ],
  "source" : "appleJSON",
  "title" : "Capturing depth using the LiDAR camera",
  "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/capturing-depth-using-the-lidar-camera"
}