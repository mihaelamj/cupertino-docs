{
  "abstract" : "Apply your own background to a live capture feed streamed from the front-facing TrueDepth camera.",
  "codeExamples" : [
    {
      "code" : "\/\/ Communicate with the session and other session objects on this queue.\nprivate let sessionQueue = DispatchQueue(label: \"session queue\", attributes: [], autoreleaseFrequency: .workItem)",
      "language" : "swift"
    },
    {
      "code" : "sessionQueue.async {\n    self.configureSession()\n}",
      "language" : "swift"
    },
    {
      "code" : "private let depthDataOutput = AVCaptureDepthDataOutput()",
      "language" : "swift"
    },
    {
      "code" : "if session.canAddOutput(depthDataOutput) {\n    session.addOutput(depthDataOutput)\n    depthDataOutput.isFilteringEnabled = true\n    if let connection = depthDataOutput.connection(with: .depthData) {\n        connection.isEnabled = true\n    } else {\n        print(\"No AVCaptureConnection\")\n    }\n} else {\n    print(\"Could not add depth data output to the session\")\n    setupResult = .configurationFailed\n    session.commitConfiguration()\n    return\n}",
      "language" : "swift"
    },
    {
      "code" : "let depthFormats = videoDevice.activeFormat.supportedDepthDataFormats\nlet depth32formats = depthFormats.filter({\n    CMFormatDescriptionGetMediaSubType($0.formatDescription) == kCVPixelFormatType_DepthFloat32\n})\nif depth32formats.isEmpty {\n    print(\"Device does not support Float32 depth format\")\n    setupResult = .configurationFailed\n    session.commitConfiguration()\n    return\n}\n\nlet selectedFormat = depth32formats.max(by: { first, second in\n    CMVideoFormatDescriptionGetDimensions(first.formatDescription).width <\n        CMVideoFormatDescriptionGetDimensions(second.formatDescription).width })",
      "language" : "swift"
    },
    {
      "code" : "outputSynchronizer = AVCaptureDataOutputSynchronizer(dataOutputs: [videoDataOutput, depthDataOutput, metadataOutput])\noutputSynchronizer!.setDelegate(self, queue: dataOutputQueue)",
      "language" : "swift"
    },
    {
      "code" : "self.session.addOutput(metadataOutput)\nif metadataOutput.availableMetadataObjectTypes.contains(.face) {\n    metadataOutput.metadataObjectTypes = [.face]\n}",
      "language" : "swift"
    },
    {
      "code" : "if let syncedMetaData: AVCaptureSynchronizedMetadataObjectData =\n    synchronizedDataCollection.synchronizedData(for: metadataOutput) as? AVCaptureSynchronizedMetadataObjectData,\n    let firstFace = syncedMetaData.metadataObjects.first,\n    let connection = self.videoDataOutput.connection(with: AVMediaType.video),\n    let face = videoDataOutput.transformedMetadataObject(for: firstFace, connection: connection) {\n    let faceCenter = CGPoint(x: face.bounds.midX, y: face.bounds.midY)",
      "language" : "swift"
    },
    {
      "code" : "let scaleFactor = CGFloat(CVPixelBufferGetWidth(depthPixelBuffer)) \/ CGFloat(CVPixelBufferGetWidth(videoPixelBuffer))\nlet pixelX = Int((faceCenter.x * scaleFactor).rounded())\nlet pixelY = Int((faceCenter.y * scaleFactor).rounded())",
      "language" : "swift"
    },
    {
      "code" : "let depthWidth = CVPixelBufferGetWidth(depthPixelBuffer)\nlet depthHeight = CVPixelBufferGetHeight(depthPixelBuffer)\n\nCVPixelBufferLockBaseAddress(depthPixelBuffer, CVPixelBufferLockFlags(rawValue: 0))\n\nfor yMap in 0 ..< depthHeight {\n    let rowData = CVPixelBufferGetBaseAddress(depthPixelBuffer)! + yMap * CVPixelBufferGetBytesPerRow(depthPixelBuffer)\n    let data = UnsafeMutableBufferPointer<Float32>(start: rowData.assumingMemoryBound(to: Float32.self), count: depthWidth)\n    for index in 0 ..< depthWidth {\n        if data[index] > 0 && data[index] <= depthCutOff {\n            data[index] = 1.0\n        } else {\n            data[index] = 0.0\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "let depthMaskImage = CIImage(cvPixelBuffer: depthPixelBuffer, options: [:])\n\n\/\/ Smooth edges to create an alpha matte, then upscale it to the RGB resolution.\nlet alphaUpscaleFactor = Float(CVPixelBufferGetWidth(videoPixelBuffer)) \/ Float(depthWidth)\nlet alphaMatte = depthMaskImage.clampedToExtent()\n    .applyingFilter(\"CIGaussianBlur\", parameters: [\"inputRadius\": blurRadius])\n    .applyingFilter(\"CIGammaAdjust\", parameters: [\"inputPower\": gamma])\n    .cropped(to: depthMaskImage.extent)\n    .applyingFilter(\"CIBicubicScaleTransform\", parameters: [\"inputScale\": alphaUpscaleFactor])",
      "language" : "swift"
    },
    {
      "code" : "let image = CIImage(cvPixelBuffer: videoPixelBuffer)\n\n\/\/ Apply alpha matte to the video.\nvar parameters = [\"inputMaskImage\": alphaMatte]\nif let background = self.backgroundImage {\n    parameters[\"inputBackgroundImage\"] = background\n}\n\nlet output = image.applyingFilter(\"CIBlendWithMask\", parameters: parameters)",
      "language" : "swift"
    },
    {
      "code" : "previewView.image = output",
      "language" : "swift"
    }
  ],
  "contentHash" : "c07e367a420c9612424348a7df8152c0528e26f82be4b646f8f0763e5b018c0a",
  "crawledAt" : "2025-12-02T15:48:13Z",
  "id" : "6D39D445-4F0C-48A5-B771-002D6A10E86C",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "AVFoundation",
  "overview" : "## Overview\n\nThe TrueDepth camera provides real-time depth data that allows you to segment foreground from background in a video feed.\n\nThis sample app leverages depth data to dynamically replace the entire background with a custom image. It then performs Gaussian filtering and other image processing operations to remove holes and smooth the effect.\n\n### Preview the sample app\n\nTo see this sample app in action, build and run the project in Xcode on a device running iOS 11 or later. Because Xcode doesn’t have access to the TrueDepth camera, this sample won’t work in the Xcode simulator.\n\nThe sample app begins by removing the background, replacing it with black. Apply your own image from the camera roll by swiping down anywhere on the video feed.\n\n### Set up live capture from the TrueDepth camera\n\nSet up an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureSession] on a separate thread via the session queue. Initialize this session queue before configuring the camera for capture.\n\nThe [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureSession\/startRunning()] method is a blocking call which can take a long time. Dispatch session setup to the sessionQueue so the main queue isn’t blocked, allowing the app’s UI to stay responsive.\n\nSetting up the camera for video capture follows many of the same steps as normal video capture. See [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/setting-up-a-capture-session] for details on configuring streaming setup.\n\nOn top of normal setup, request depth data by declaring a separate output:\n\nExplicitly add this output type to your capture session:\n\nSearch for the highest resolution available with floating-point depth values, and lock the configuration to the format.\n\nSynchronize the normal RGB video data with depth data output. The first output in the dataOutputs array is the master output.\n\n### Create a binary foreground mask\n\nAssume the foreground to be a human face. You can accomplish face detection through the Vision framework’s [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNDetectFaceRectanglesRequest], but this sample doesn’t need anything else from Vision, so it’s simpler to consult the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVMetadataObject] for [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVMetadataObject\/ObjectType\/face].\n\nUsing the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVMetadataObject], locate the face’s bounding box and center. Assume there is only one face and take the first one in the metadata object.\n\nDepth maps differ from their normal camera image counterparts in resolution; as a result, normal image coordinates differ from depth map coordinates by a scale factor. Compute the scale factor and transform the face’s center to depth map coordinates.\n\nOnce you have the face in depth map coordinates, threshold the image to create a binary mask image, where the foreground pixels are `1`, and the background pixels are `0`.\n\n### Smooth the depth mask with Core Image filters\n\nThe depth map doesn’t share the RGB image’s sharp resolution, so the mask may contain holes along the interface between foreground and background. Once you have a downsampled mask image, use a Gaussian filter to smooth out the holes, so the interface doesn’t look jagged or pixelated. Clamp your image before filtering it, and crop it afterward, so it retains the proper size when applied with the original image.\n\nThe parameters of your `CIGaussianBlur` and `CIGammaAdjust` filters directly affect the smoothness of the edge pixels. You can tune the blur and smoothness by adjusting the Gaussian blur filter’s input radius, as well as the gamma adjustment filter’s input power.\n\n\n\n### Blend foreground and background with the alpha matte\n\nThe final step is applying your filtered smooth binary mask to the input video frame.\n\nBecause you’ve performed image processing in Core Image using the `CIGaussianBlur` and `CIGammaAdjust` filters, it’s most computationally efficient to apply the resulting mask in Core Image, as well. That means converting your video from [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/cvpixelbuffer-q2e] format to [doc:\/\/com.apple.documentation\/documentation\/CoreImage\/CIImage] format, allowing you to apply the alpha matte to the original image, and blend in your custom background image with the `CIBlendWithMask` filter.\n\nUpdate your preview to display the final composited image onscreen.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/AVFoundation\/enhancing-live-video-by-leveraging-truedepth-camera-data\ncrawled: 2025-12-02T15:48:13Z\n---\n\n# Enhancing live video by leveraging TrueDepth camera data\n\n**Sample Code**\n\nApply your own background to a live capture feed streamed from the front-facing TrueDepth camera.\n\n## Overview\n\nThe TrueDepth camera provides real-time depth data that allows you to segment foreground from background in a video feed.\n\nThis sample app leverages depth data to dynamically replace the entire background with a custom image. It then performs Gaussian filtering and other image processing operations to remove holes and smooth the effect.\n\n### Preview the sample app\n\nTo see this sample app in action, build and run the project in Xcode on a device running iOS 11 or later. Because Xcode doesn’t have access to the TrueDepth camera, this sample won’t work in the Xcode simulator.\n\nThe sample app begins by removing the background, replacing it with black. Apply your own image from the camera roll by swiping down anywhere on the video feed.\n\n### Set up live capture from the TrueDepth camera\n\nSet up an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureSession] on a separate thread via the session queue. Initialize this session queue before configuring the camera for capture.\n\n```swift\n\/\/ Communicate with the session and other session objects on this queue.\nprivate let sessionQueue = DispatchQueue(label: \"session queue\", attributes: [], autoreleaseFrequency: .workItem)\n```\n\nThe [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVCaptureSession\/startRunning()] method is a blocking call which can take a long time. Dispatch session setup to the sessionQueue so the main queue isn’t blocked, allowing the app’s UI to stay responsive.\n\n```swift\nsessionQueue.async {\n    self.configureSession()\n}\n```\n\nSetting up the camera for video capture follows many of the same steps as normal video capture. See [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/setting-up-a-capture-session] for details on configuring streaming setup.\n\nOn top of normal setup, request depth data by declaring a separate output:\n\n```swift\nprivate let depthDataOutput = AVCaptureDepthDataOutput()\n```\n\nExplicitly add this output type to your capture session:\n\n```swift\nif session.canAddOutput(depthDataOutput) {\n    session.addOutput(depthDataOutput)\n    depthDataOutput.isFilteringEnabled = true\n    if let connection = depthDataOutput.connection(with: .depthData) {\n        connection.isEnabled = true\n    } else {\n        print(\"No AVCaptureConnection\")\n    }\n} else {\n    print(\"Could not add depth data output to the session\")\n    setupResult = .configurationFailed\n    session.commitConfiguration()\n    return\n}\n```\n\nSearch for the highest resolution available with floating-point depth values, and lock the configuration to the format.\n\n```swift\nlet depthFormats = videoDevice.activeFormat.supportedDepthDataFormats\nlet depth32formats = depthFormats.filter({\n    CMFormatDescriptionGetMediaSubType($0.formatDescription) == kCVPixelFormatType_DepthFloat32\n})\nif depth32formats.isEmpty {\n    print(\"Device does not support Float32 depth format\")\n    setupResult = .configurationFailed\n    session.commitConfiguration()\n    return\n}\n\nlet selectedFormat = depth32formats.max(by: { first, second in\n    CMVideoFormatDescriptionGetDimensions(first.formatDescription).width <\n        CMVideoFormatDescriptionGetDimensions(second.formatDescription).width })\n```\n\nSynchronize the normal RGB video data with depth data output. The first output in the dataOutputs array is the master output.\n\n```swift\noutputSynchronizer = AVCaptureDataOutputSynchronizer(dataOutputs: [videoDataOutput, depthDataOutput, metadataOutput])\noutputSynchronizer!.setDelegate(self, queue: dataOutputQueue)\n```\n\n### Create a binary foreground mask\n\nAssume the foreground to be a human face. You can accomplish face detection through the Vision framework’s [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNDetectFaceRectanglesRequest], but this sample doesn’t need anything else from Vision, so it’s simpler to consult the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVMetadataObject] for [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVMetadataObject\/ObjectType\/face].\n\n```swift\nself.session.addOutput(metadataOutput)\nif metadataOutput.availableMetadataObjectTypes.contains(.face) {\n    metadataOutput.metadataObjectTypes = [.face]\n}\n```\n\nUsing the [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVMetadataObject], locate the face’s bounding box and center. Assume there is only one face and take the first one in the metadata object.\n\n```swift\nif let syncedMetaData: AVCaptureSynchronizedMetadataObjectData =\n    synchronizedDataCollection.synchronizedData(for: metadataOutput) as? AVCaptureSynchronizedMetadataObjectData,\n    let firstFace = syncedMetaData.metadataObjects.first,\n    let connection = self.videoDataOutput.connection(with: AVMediaType.video),\n    let face = videoDataOutput.transformedMetadataObject(for: firstFace, connection: connection) {\n    let faceCenter = CGPoint(x: face.bounds.midX, y: face.bounds.midY)\n```\n\nDepth maps differ from their normal camera image counterparts in resolution; as a result, normal image coordinates differ from depth map coordinates by a scale factor. Compute the scale factor and transform the face’s center to depth map coordinates.\n\n```swift\nlet scaleFactor = CGFloat(CVPixelBufferGetWidth(depthPixelBuffer)) \/ CGFloat(CVPixelBufferGetWidth(videoPixelBuffer))\nlet pixelX = Int((faceCenter.x * scaleFactor).rounded())\nlet pixelY = Int((faceCenter.y * scaleFactor).rounded())\n```\n\nOnce you have the face in depth map coordinates, threshold the image to create a binary mask image, where the foreground pixels are `1`, and the background pixels are `0`.\n\n```swift\nlet depthWidth = CVPixelBufferGetWidth(depthPixelBuffer)\nlet depthHeight = CVPixelBufferGetHeight(depthPixelBuffer)\n\nCVPixelBufferLockBaseAddress(depthPixelBuffer, CVPixelBufferLockFlags(rawValue: 0))\n\nfor yMap in 0 ..< depthHeight {\n    let rowData = CVPixelBufferGetBaseAddress(depthPixelBuffer)! + yMap * CVPixelBufferGetBytesPerRow(depthPixelBuffer)\n    let data = UnsafeMutableBufferPointer<Float32>(start: rowData.assumingMemoryBound(to: Float32.self), count: depthWidth)\n    for index in 0 ..< depthWidth {\n        if data[index] > 0 && data[index] <= depthCutOff {\n            data[index] = 1.0\n        } else {\n            data[index] = 0.0\n        }\n    }\n}\n```\n\n### Smooth the depth mask with Core Image filters\n\nThe depth map doesn’t share the RGB image’s sharp resolution, so the mask may contain holes along the interface between foreground and background. Once you have a downsampled mask image, use a Gaussian filter to smooth out the holes, so the interface doesn’t look jagged or pixelated. Clamp your image before filtering it, and crop it afterward, so it retains the proper size when applied with the original image.\n\n```swift\nlet depthMaskImage = CIImage(cvPixelBuffer: depthPixelBuffer, options: [:])\n\n\/\/ Smooth edges to create an alpha matte, then upscale it to the RGB resolution.\nlet alphaUpscaleFactor = Float(CVPixelBufferGetWidth(videoPixelBuffer)) \/ Float(depthWidth)\nlet alphaMatte = depthMaskImage.clampedToExtent()\n    .applyingFilter(\"CIGaussianBlur\", parameters: [\"inputRadius\": blurRadius])\n    .applyingFilter(\"CIGammaAdjust\", parameters: [\"inputPower\": gamma])\n    .cropped(to: depthMaskImage.extent)\n    .applyingFilter(\"CIBicubicScaleTransform\", parameters: [\"inputScale\": alphaUpscaleFactor])\n```\n\nThe parameters of your `CIGaussianBlur` and `CIGammaAdjust` filters directly affect the smoothness of the edge pixels. You can tune the blur and smoothness by adjusting the Gaussian blur filter’s input radius, as well as the gamma adjustment filter’s input power.\n\n\n\n### Blend foreground and background with the alpha matte\n\nThe final step is applying your filtered smooth binary mask to the input video frame.\n\nBecause you’ve performed image processing in Core Image using the `CIGaussianBlur` and `CIGammaAdjust` filters, it’s most computationally efficient to apply the resulting mask in Core Image, as well. That means converting your video from [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/cvpixelbuffer-q2e] format to [doc:\/\/com.apple.documentation\/documentation\/CoreImage\/CIImage] format, allowing you to apply the alpha matte to the original image, and blend in your custom background image with the `CIBlendWithMask` filter.\n\n```swift\nlet image = CIImage(cvPixelBuffer: videoPixelBuffer)\n\n\/\/ Apply alpha matte to the video.\nvar parameters = [\"inputMaskImage\": alphaMatte]\nif let background = self.backgroundImage {\n    parameters[\"inputBackgroundImage\"] = background\n}\n\nlet output = image.applyingFilter(\"CIBlendWithMask\", parameters: parameters)\n```\n\nUpdate your preview to display the final composited image onscreen.\n\n```swift\npreviewView.image = output\n```\n\n## Depth data capture\n\n- **Capturing photos with depth**: Get a depth map with a photo to create effects like the system camera’s Portrait mode (on compatible devices).\n- **Creating auxiliary depth data manually**: Generate a depth image and attach it to your own image.\n- **Capturing depth using the LiDAR camera**: Access the LiDAR camera on supporting devices to capture precise depth data.\n- **AVCamFilter: Applying filters to a capture stream**: Render a capture stream with rose-colored filtering and depth effects.\n- **Streaming depth data from the TrueDepth camera**: Visualize depth data in 2D and 3D from the TrueDepth camera.\n- **AVCaptureDepthDataOutput**: A capture output that records scene depth information on compatible camera devices.\n- **AVDepthData**: A container for per-pixel distance or disparity information captured by compatible camera devices.\n- **AVCameraCalibrationData**: Information about the camera characteristics used to capture images and depth data.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Get a depth map with a photo to create effects like the system camera’s Portrait mode (on compatible devices).",
          "name" : "Capturing photos with depth",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/capturing-photos-with-depth"
        },
        {
          "description" : "Generate a depth image and attach it to your own image.",
          "name" : "Creating auxiliary depth data manually",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/creating-auxiliary-depth-data-manually"
        },
        {
          "description" : "Access the LiDAR camera on supporting devices to capture precise depth data.",
          "name" : "Capturing depth using the LiDAR camera",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/capturing-depth-using-the-lidar-camera"
        },
        {
          "description" : "Render a capture stream with rose-colored filtering and depth effects.",
          "name" : "AVCamFilter: Applying filters to a capture stream",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/avcamfilter-applying-filters-to-a-capture-stream"
        },
        {
          "description" : "Visualize depth data in 2D and 3D from the TrueDepth camera.",
          "name" : "Streaming depth data from the TrueDepth camera",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/streaming-depth-data-from-the-truedepth-camera"
        },
        {
          "description" : "A capture output that records scene depth information on compatible camera devices.",
          "name" : "AVCaptureDepthDataOutput",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVCaptureDepthDataOutput"
        },
        {
          "description" : "A container for per-pixel distance or disparity information captured by compatible camera devices.",
          "name" : "AVDepthData",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVDepthData"
        },
        {
          "description" : "Information about the camera characteristics used to capture images and depth data.",
          "name" : "AVCameraCalibrationData",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVCameraCalibrationData"
        }
      ],
      "title" : "Depth data capture"
    }
  ],
  "source" : "appleJSON",
  "title" : "Enhancing live video by leveraging TrueDepth camera data",
  "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/enhancing-live-video-by-leveraging-truedepth-camera-data"
}