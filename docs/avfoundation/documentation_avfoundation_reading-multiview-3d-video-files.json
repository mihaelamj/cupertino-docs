{
  "abstract" : "Render single images for the left eye and right eye from a multiview High Efficiency Video Coding format file by reading individual video frames.",
  "codeExamples" : [
    {
      "code" : "init(filename: URL) {\n    asset = AVURLAsset(url: filename)\n    Task { @MainActor in\n        do {\n            let (duration, isPlayable, isReadable) = try await asset.load(.duration, .isPlayable, .isReadable)\n            self.duration = duration.seconds\n            self.isPlayable = isPlayable\n            self.isReadable = isReadable\n        } catch {\n            self.error = error\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "if let track = try await asset.loadTracks(withMediaCharacteristic: .containsStereoMultiviewVideo).first {\n    self.track = track",
      "language" : "swift"
    },
    {
      "code" : "guard let cursor = track.makeSampleCursorAtFirstSampleInDecodeOrder() else {\n    return []\n}\nlet sampleBufferGenerator = AVSampleBufferGenerator(asset: asset, timebase: nil)\nvar presentationTimes = [CMTime]()\nlet request = AVSampleBufferRequest(start: cursor)\nvar numSamples: Int64 = 0",
      "language" : "swift"
    },
    {
      "code" : "repeat {\n    let buf = try sampleBufferGenerator.makeSampleBuffer(for: request)\n    presentationTimes.append(buf.presentationTimeStamp)\n    numSamples = cursor.stepInDecodeOrder(byCount: 1)\n} while numSamples == 1",
      "language" : "swift"
    },
    {
      "code" : "private func loadVideoLayerIdsForTrack(_ videoTrack: AVAssetTrack) async throws -> [Int64]? {\n    let formatDescriptions = try await videoTrack.load(.formatDescriptions)\n    var tags = [Int64]()\n    if let tagCollections = formatDescriptions.first?.tagCollections {\n        tags = tagCollections.flatMap({ $0 }).compactMap { tag in\n            tag.value(onlyIfMatching: .videoLayerID)\n        }\n    }\n    return tags\n}",
      "language" : "swift"
    },
    {
      "code" : "guard let assetReader, let trackOutput else {\n    return\n}\nguard assetReader.status == .reading else {\n    publishState(.error(message: \"UNEXPECTED STATUS \\(assetReader.status)\"))\n    return\n}\nguard let sampleBuffer = trackOutput.copyNextSampleBuffer() else {\n    publishState(.error(message: \"READING SAMPLE BUFFER, STATUS \\(assetReader.status), ERROR \\(String(describing: assetReader.error))\"))\n    return\n}\nguard let taggedBuffers = sampleBuffer.taggedBuffers else {\n    publishState(.error(message: \"SAMPLE BUFFER CONTAINS NO TAGGED BUFFERS: \\(sampleBuffer)\"))\n    return\n}\nguard taggedBuffers.count == 2 else {\n    publishState(.error(message: \"EXPECTED 2 TAGGED BUFFERS, GOT \\(taggedBuffers.count)\"))\n    return\n}",
      "language" : "swift"
    },
    {
      "code" : "taggedBuffers.forEach { taggedBuffer in\n    switch taggedBuffer.buffer {\n    case let .pixelBuffer(pixelBuffer):\n        let ciimage = CIImage(cvPixelBuffer: pixelBuffer)\n        let context: CIContext = CIContext(options: nil)\n        let cgImage: CGImage = context.createCGImage(ciimage, from: ciimage.extent)!\n        let tags = taggedBuffer.tags\n        Task {\n            await MainActor.run {\n                let nsImage = NSImage(cgImage: cgImage, size: NSSize(width: 320, height: 240))\n                if tags.contains(.stereoView(.leftEye)) {\n                    leftEye = nsImage\n                } else if tags.contains(.stereoView(.rightEye)) {\n                    rightEye = nsImage\n                }\n            }\n        }\n    case .sampleBuffer(let samp):\n        publishState(.error(message: \"EXPECTED PIXEL BUFFER, GOT SAMPLE BUFFER \\(samp)\"))\n    @unknown default:\n        publishState(.error(message: \"EXPECTED PIXEL BUFFER TYPE, GOT \\(taggedBuffer.buffer)\"))\n    }\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "52b25ce36b3cde93dec8b32d2c1b28ca7e6bc878bf1631d30805867856c02a85",
  "crawledAt" : "2025-12-02T15:48:16Z",
  "id" : "FAC437D9-66EE-4698-9CBB-031931EB36E9",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "AVFoundation",
  "overview" : "## Overview\n\nMultiview High Efficiency Video Coding (MV-HEVC) media files contain information to produce stereoscopic frames, one for the left eye and one for the right, to create an effect of depth and allow for 3D video. This is the standard format for presenting 3D video in visionOS, encoded as MPEG-4 or QuickTime files.\n\nPreviewing and testing MV-HEVC files without hardware requires the ability to load, view, and step through the video data on a timeline. This sample app opens a media file, checking for the MV-HEVC format, then presents a view containing the individual frames at the timestamp. Step through the timeline by dragging the slider to a specific timestamp, or advance to the next frame by pressing the Space bar.\n\nFor the full details of the MV-HEVC format, see [https:\/\/developer.apple.com\/av-foundation\/HEVC-Stereo-Video-Profile.pdf] and [https:\/\/developer.apple.com\/av-foundation\/Stereo-Video-ISOBMFF-Extensions.pdf].\n\n### Load and inspect the media asset\n\nThe app first displays a button labeled Open MVHEVC File. When selected, the button presents an [doc:\/\/com.apple.documentation\/documentation\/AppKit\/NSOpenPanel] for choosing video media. Next, the app initializes a `MediaDetailViewModel`, loading this file as an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVURLAsset]. Before opening the file to present any elements for a stereo video frame, the app ensures a playable, readable file, and gets its total length in time. This is all performed in the initializer.\n\n### Load track data and timestamps\n\nAfter confirming the track is readable video data, the app initializes a `StereoViewModel`by calling [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAsset\/loadTracks(withMediaCharacteristic:completionHandler:)] requesting a [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVMediaCharacteristic\/containsStereoMultiviewVideo] track. This check confirms that the file meets the MV-HEVC specification and has valid stereo data.\n\nNext, the app pulls available timestamps for each frame in the track by calling `presentationTimesFor(track:asset:)`. The app places a video sample cursor at the start of the track with [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetTrack\/makeSampleCursorAtFirstSampleInDecodeOrder()], then creates a new [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVSampleBufferGenerator] and [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVSampleBufferRequest].\n\nTo read the timestamps, obtain the sample buffer for the current cursor from [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVSampleBufferGenerator\/makeSampleBuffer(for:)], then add the [doc:\/\/com.apple.documentation\/documentation\/CoreMedia\/CMSampleBuffer\/presentationTimeStamp] for the frame. The cursor steps forward by calling [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVSampleCursor\/stepInDecodeOrder(byCount:)], reading and caching timestamps for each frame in the buffer. When `stepInDecodeOrder(byCount:)` returns no next frame, sample times are in the cache and reading the video track completes.\n\n### Load video layer information\n\nAfter preparing timestamps, the app calls `loadVideoLayerIdsForTrack()` to get the layer IDs for the two tracks associated with the left and right eyes. The app calls [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAsynchronousKeyValueLoading\/load(_:isolation:)]to retrieve metadata, then filters the layer data out of the first available track’s [doc:\/\/com.apple.documentation\/documentation\/CoreMedia\/CMFormatDescription\/tagCollections]. The filter predicate is [doc:\/\/com.apple.documentation\/documentation\/CoreMedia\/CMTag-swift.class\/value(onlyIfMatching:)], extracting only video layer IDs.\n\n### Load video frames from buffers\n\nWith the timestamp and left eye and right eye video layers identified, `readBufferFromAsset(at:)` calls the `readNextBufferFromAsset()` method of the app to retrieve and display the frame data. The method starts with a series of `guard` checks to ensure read access to the track, creates a local copy of the sample buffer by calling [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetReaderOutput\/copyNextSampleBuffer()], and retrieves the tagged video buffers from the track.\n\nThe app parses each [doc:\/\/com.apple.documentation\/documentation\/CoreMedia\/CMTaggedBuffer\/Buffer-swift.enum\/pixelBuffer(_:)] from the returned sample buffers into an image for display using [doc:\/\/com.apple.documentation\/documentation\/CoreImage\/CIImage\/init(cvPixelBuffer:)]. The app creates an [doc:\/\/com.apple.documentation\/documentation\/AppKit\/NSImage] and sets it to the view content as either `leftEye` or `rightEye` depending on whether the view contains a [doc:\/\/com.apple.documentation\/documentation\/CoreMedia\/CMTag-swift.class\/stereoView(_:)] for the left or right eye.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/AVFoundation\/reading-multiview-3d-video-files\ncrawled: 2025-12-02T15:48:16Z\n---\n\n# Reading multiview 3D video files\n\n**Sample Code**\n\nRender single images for the left eye and right eye from a multiview High Efficiency Video Coding format file by reading individual video frames.\n\n## Overview\n\nMultiview High Efficiency Video Coding (MV-HEVC) media files contain information to produce stereoscopic frames, one for the left eye and one for the right, to create an effect of depth and allow for 3D video. This is the standard format for presenting 3D video in visionOS, encoded as MPEG-4 or QuickTime files.\n\nPreviewing and testing MV-HEVC files without hardware requires the ability to load, view, and step through the video data on a timeline. This sample app opens a media file, checking for the MV-HEVC format, then presents a view containing the individual frames at the timestamp. Step through the timeline by dragging the slider to a specific timestamp, or advance to the next frame by pressing the Space bar.\n\nFor the full details of the MV-HEVC format, see [https:\/\/developer.apple.com\/av-foundation\/HEVC-Stereo-Video-Profile.pdf] and [https:\/\/developer.apple.com\/av-foundation\/Stereo-Video-ISOBMFF-Extensions.pdf].\n\n### Load and inspect the media asset\n\nThe app first displays a button labeled Open MVHEVC File. When selected, the button presents an [doc:\/\/com.apple.documentation\/documentation\/AppKit\/NSOpenPanel] for choosing video media. Next, the app initializes a `MediaDetailViewModel`, loading this file as an [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVURLAsset]. Before opening the file to present any elements for a stereo video frame, the app ensures a playable, readable file, and gets its total length in time. This is all performed in the initializer.\n\n```swift\ninit(filename: URL) {\n    asset = AVURLAsset(url: filename)\n    Task { @MainActor in\n        do {\n            let (duration, isPlayable, isReadable) = try await asset.load(.duration, .isPlayable, .isReadable)\n            self.duration = duration.seconds\n            self.isPlayable = isPlayable\n            self.isReadable = isReadable\n        } catch {\n            self.error = error\n        }\n    }\n}\n```\n\n### Load track data and timestamps\n\nAfter confirming the track is readable video data, the app initializes a `StereoViewModel`by calling [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAsset\/loadTracks(withMediaCharacteristic:completionHandler:)] requesting a [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVMediaCharacteristic\/containsStereoMultiviewVideo] track. This check confirms that the file meets the MV-HEVC specification and has valid stereo data.\n\n```swift\nif let track = try await asset.loadTracks(withMediaCharacteristic: .containsStereoMultiviewVideo).first {\n    self.track = track\n```\n\nNext, the app pulls available timestamps for each frame in the track by calling `presentationTimesFor(track:asset:)`. The app places a video sample cursor at the start of the track with [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetTrack\/makeSampleCursorAtFirstSampleInDecodeOrder()], then creates a new [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVSampleBufferGenerator] and [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVSampleBufferRequest].\n\n```swift\nguard let cursor = track.makeSampleCursorAtFirstSampleInDecodeOrder() else {\n    return []\n}\nlet sampleBufferGenerator = AVSampleBufferGenerator(asset: asset, timebase: nil)\nvar presentationTimes = [CMTime]()\nlet request = AVSampleBufferRequest(start: cursor)\nvar numSamples: Int64 = 0\n```\n\nTo read the timestamps, obtain the sample buffer for the current cursor from [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVSampleBufferGenerator\/makeSampleBuffer(for:)], then add the [doc:\/\/com.apple.documentation\/documentation\/CoreMedia\/CMSampleBuffer\/presentationTimeStamp] for the frame. The cursor steps forward by calling [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVSampleCursor\/stepInDecodeOrder(byCount:)], reading and caching timestamps for each frame in the buffer. When `stepInDecodeOrder(byCount:)` returns no next frame, sample times are in the cache and reading the video track completes.\n\n```swift\nrepeat {\n    let buf = try sampleBufferGenerator.makeSampleBuffer(for: request)\n    presentationTimes.append(buf.presentationTimeStamp)\n    numSamples = cursor.stepInDecodeOrder(byCount: 1)\n} while numSamples == 1\n```\n\n### Load video layer information\n\nAfter preparing timestamps, the app calls `loadVideoLayerIdsForTrack()` to get the layer IDs for the two tracks associated with the left and right eyes. The app calls [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAsynchronousKeyValueLoading\/load(_:isolation:)]to retrieve metadata, then filters the layer data out of the first available track’s [doc:\/\/com.apple.documentation\/documentation\/CoreMedia\/CMFormatDescription\/tagCollections]. The filter predicate is [doc:\/\/com.apple.documentation\/documentation\/CoreMedia\/CMTag-swift.class\/value(onlyIfMatching:)], extracting only video layer IDs.\n\n```swift\nprivate func loadVideoLayerIdsForTrack(_ videoTrack: AVAssetTrack) async throws -> [Int64]? {\n    let formatDescriptions = try await videoTrack.load(.formatDescriptions)\n    var tags = [Int64]()\n    if let tagCollections = formatDescriptions.first?.tagCollections {\n        tags = tagCollections.flatMap({ $0 }).compactMap { tag in\n            tag.value(onlyIfMatching: .videoLayerID)\n        }\n    }\n    return tags\n}\n```\n\n### Load video frames from buffers\n\nWith the timestamp and left eye and right eye video layers identified, `readBufferFromAsset(at:)` calls the `readNextBufferFromAsset()` method of the app to retrieve and display the frame data. The method starts with a series of `guard` checks to ensure read access to the track, creates a local copy of the sample buffer by calling [doc:\/\/com.apple.avfoundation\/documentation\/AVFoundation\/AVAssetReaderOutput\/copyNextSampleBuffer()], and retrieves the tagged video buffers from the track.\n\n```swift\nguard let assetReader, let trackOutput else {\n    return\n}\nguard assetReader.status == .reading else {\n    publishState(.error(message: \"UNEXPECTED STATUS \\(assetReader.status)\"))\n    return\n}\nguard let sampleBuffer = trackOutput.copyNextSampleBuffer() else {\n    publishState(.error(message: \"READING SAMPLE BUFFER, STATUS \\(assetReader.status), ERROR \\(String(describing: assetReader.error))\"))\n    return\n}\nguard let taggedBuffers = sampleBuffer.taggedBuffers else {\n    publishState(.error(message: \"SAMPLE BUFFER CONTAINS NO TAGGED BUFFERS: \\(sampleBuffer)\"))\n    return\n}\nguard taggedBuffers.count == 2 else {\n    publishState(.error(message: \"EXPECTED 2 TAGGED BUFFERS, GOT \\(taggedBuffers.count)\"))\n    return\n}\n```\n\nThe app parses each [doc:\/\/com.apple.documentation\/documentation\/CoreMedia\/CMTaggedBuffer\/Buffer-swift.enum\/pixelBuffer(_:)] from the returned sample buffers into an image for display using [doc:\/\/com.apple.documentation\/documentation\/CoreImage\/CIImage\/init(cvPixelBuffer:)]. The app creates an [doc:\/\/com.apple.documentation\/documentation\/AppKit\/NSImage] and sets it to the view content as either `leftEye` or `rightEye` depending on whether the view contains a [doc:\/\/com.apple.documentation\/documentation\/CoreMedia\/CMTag-swift.class\/stereoView(_:)] for the left or right eye.\n\n```swift\ntaggedBuffers.forEach { taggedBuffer in\n    switch taggedBuffer.buffer {\n    case let .pixelBuffer(pixelBuffer):\n        let ciimage = CIImage(cvPixelBuffer: pixelBuffer)\n        let context: CIContext = CIContext(options: nil)\n        let cgImage: CGImage = context.createCGImage(ciimage, from: ciimage.extent)!\n        let tags = taggedBuffer.tags\n        Task {\n            await MainActor.run {\n                let nsImage = NSImage(cgImage: cgImage, size: NSSize(width: 320, height: 240))\n                if tags.contains(.stereoView(.leftEye)) {\n                    leftEye = nsImage\n                } else if tags.contains(.stereoView(.rightEye)) {\n                    rightEye = nsImage\n                }\n            }\n        }\n    case .sampleBuffer(let samp):\n        publishState(.error(message: \"EXPECTED PIXEL BUFFER, GOT SAMPLE BUFFER \\(samp)\"))\n    @unknown default:\n        publishState(.error(message: \"EXPECTED PIXEL BUFFER TYPE, GOT \\(taggedBuffer.buffer)\"))\n    }\n}\n```\n\n## Media reading\n\n- **AVAssetReader**: An object that reads media data from an asset.\n- **AVAssetReaderOutput**: An abstract class that defines the interface to read media samples from an asset reader.\n- **AVAssetReaderTrackOutput**: An object that reads media data from a single track of an asset.\n- **AVAssetReaderAudioMixOutput**: An object that reads audio samples that result from mixing audio from one or more tracks.\n- **AVAssetReaderVideoCompositionOutput**: An object that reads composited video frames from one or more tracks of an asset.\n- **AVAssetReaderSampleReferenceOutput**: An object that reads sample references from an asset track.\n- **AVAssetReaderOutputMetadataAdaptor**: An object that creates timed metadata group objects for an asset track.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "An object that reads media data from an asset.",
          "name" : "AVAssetReader",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVAssetReader"
        },
        {
          "description" : "An abstract class that defines the interface to read media samples from an asset reader.",
          "name" : "AVAssetReaderOutput",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVAssetReaderOutput"
        },
        {
          "description" : "An object that reads media data from a single track of an asset.",
          "name" : "AVAssetReaderTrackOutput",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVAssetReaderTrackOutput"
        },
        {
          "description" : "An object that reads audio samples that result from mixing audio from one or more tracks.",
          "name" : "AVAssetReaderAudioMixOutput",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVAssetReaderAudioMixOutput"
        },
        {
          "description" : "An object that reads composited video frames from one or more tracks of an asset.",
          "name" : "AVAssetReaderVideoCompositionOutput",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVAssetReaderVideoCompositionOutput"
        },
        {
          "description" : "An object that reads sample references from an asset track.",
          "name" : "AVAssetReaderSampleReferenceOutput",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVAssetReaderSampleReferenceOutput"
        },
        {
          "description" : "An object that creates timed metadata group objects for an asset track.",
          "name" : "AVAssetReaderOutputMetadataAdaptor",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/AVAssetReaderOutputMetadataAdaptor"
        }
      ],
      "title" : "Media reading"
    }
  ],
  "source" : "appleJSON",
  "title" : "Reading multiview 3D video files",
  "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/reading-multiview-3d-video-files"
}