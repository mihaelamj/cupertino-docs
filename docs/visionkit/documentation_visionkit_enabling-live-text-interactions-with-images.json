{
  "abstract" : "Add a Live Text interface that enables users to perform actions with text and QR codes that appear in images.",
  "codeExamples" : [
    {
      "code" : "let interaction = ImageAnalysisInteraction()\nimageView.addInteraction(interaction)",
      "language" : "swift"
    },
    {
      "code" : "interaction.delegate = self",
      "language" : "swift"
    },
    {
      "code" : "let overlayView = ImageAnalysisOverlayView()",
      "language" : "swift"
    },
    {
      "code" : "overlayView.autoresizingMask = [.width, .height]\noverlayView.frame = imageView.bounds\noverlayView.trackingImageView = imageView",
      "language" : "swift"
    },
    {
      "code" : "imageView.addSubview(overlayView)",
      "language" : "swift"
    },
    {
      "code" : "overlayView.delegate = self",
      "language" : "swift"
    },
    {
      "code" : "func overlayView(_ overlayView: ImageAnalysisOverlayView, \n           shouldBeginAt point: CGPoint, forAnalysisType \n           analysisType: ImageAnalysisOverlayView.InteractionTypes) -> Bool { \n    overlayView.hasInteractiveItem(at: point) || \n    overlayView.hasActiveTextSelection \n}",
      "language" : "swift"
    },
    {
      "code" : "let configuration = ImageAnalyzer.Configuration([.text, .machineReadableCode])",
      "language" : "swift"
    },
    {
      "code" : "let analyzer = ImageAnalyzer()\nlet analysis = try? await analyzer.analyze(image, configuration: configuration)",
      "language" : "swift"
    },
    {
      "code" : "interaction.analysis = analysis",
      "language" : "swift"
    },
    {
      "code" : "overlayView.analysis = analysis",
      "language" : "swift"
    },
    {
      "code" : "interaction.preferredInteractionTypes = .automatic",
      "language" : "swift"
    }
  ],
  "contentHash" : "56a96fc4a8baf6ad2ebc12e56cda502b35e3bfc8822e878aa8336c0d74108ff8",
  "crawledAt" : "2025-12-02T16:45:25Z",
  "id" : "E94DA64E-7EE2-470B-9DDA-4B1B6EEC583D",
  "kind" : "article",
  "language" : "swift",
  "module" : "VisionKit",
  "overview" : "## Overview\n\nEnhance the user experience with images in your app by adding standard Live Text interactions. Live Text lets users tap text in an image to copy it, make a call, send an email, translate it, or look up directions. VisionKit provides this familiar Live Text interface to your images and across platforms.\n\nFor iOS apps, users can use a long-press gesture (touch and hold) a QR code to follow a link or take another action, depending on the payload. The payload is the data that a QR code contains, such as a URL or email address.\n\nBefore interacting with items in an image, the user can tap the Live Text button in the lower-right corner to highlight the recognized items. Then a data-specific action menu appears when the user double-taps on text in an image, and uses a long-press gesture on an email address or QR code.\n\n\n\nIn your app, you decide whether Live Text recognizes text and data within text, and for iOS apps, QR codes in the image. You choose the types of items to look for and run the analysis on the image, and then VisionKit provides the entire Live Text interface with the breadth of actions for specific types.\n\n### Check whether the device supports Live Text\n\nBefore showing a Live Text interface in your app, check whether the device supports Live Text. For iOS apps, the image analyzer is available only on devices with the A12 Bionic chip and later. If the `ImageAnalyzer` [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalyzer\/isSupported] property is `true`, show the Live Text interface. Otherwise, disable any feature in your app that relies on Live Text. If you attempt to start the Live Text interface when this property is `false`, the interface doesn’t appear.\n\n### Add a Live Text interaction object to your view in iOS\n\nFor iOS apps, you add the Live Text interface by adding an interaction object to the view containing the image. If you use a [doc:\/\/com.apple.documentation\/documentation\/UIKit\/UIImageView] to display the image, it handles the image content area calculations for you.\n\nAdd an [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction] object directly on the view containing the image.\n\nOptionally, customize the interface by setting the interaction’s delegate to an object that implements the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteractionDelegate] protocol methods.\n\nIf you don’t use a [doc:\/\/com.apple.documentation\/documentation\/UIKit\/UIImageView] object, inform the interaction object when the content area of the image changes while the interaction bounds don’t change. Implement the `ImageAnalysisInteractionDelegate` [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteractionDelegate\/contentsRect(for:)] protocol method to return the content area of the image. This keeps the Live Text highlights within the bounds of the image. Then use the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction\/setContentsRectNeedsUpdate()] method to notify the interaction if the content area changes.\n\n### Add a Live Text view to your image view in macOS\n\nFor macOS apps, add the Live Text interface by adding a view above the view containing the image. If you use an [doc:\/\/com.apple.documentation\/documentation\/AppKit\/NSImageView] to display the image, it handles the image content area calculations for you.\n\nFirst create an instance of the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisOverlayView] class.\n\nThen configure the overlay view to fit the bounds of the image. If your view is an image view, set the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisOverlayView\/trackingImageView] property so that the dimensions adjust when the scaling and alignment properties change.\n\nAdd the overlay view above the image in the hierarchy. For example, add it as a subview of the view containing the image.\n\nTo customize the interface, set the overlay view’s delegate to an object that implements the optional [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisOverlayViewDelegate] protocol methods.\n\nImplement the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisOverlayViewDelegate\/overlayView(_:shouldBeginAt:forAnalysisType:)-35czq] protocol method to return `true` to begin the interaction.\n\nIf you aren’t using an [doc:\/\/com.apple.documentation\/documentation\/AppKit\/NSImageView] object, implement the `ImageAnalysisOverlayViewDelegate` [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisOverlayViewDelegate\/contentsRect(for:)-34yzu] protocol method to return the content area of the image. Then use the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisOverlayView\/setContentsRectNeedsUpdate()] method to notify the overlay view if the content area of the image changes while the view bounds don’t change.\n\n\n\n### Find items and start the interaction with an image\n\nProcess the image that your view displays using an [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalyzer] object. First, specify the types of items in the image you want to find when you create an [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalyzer\/Configuration] object. For iOS apps, the analyzer recognizes both text and machine-readable QR codes in an image; for macOS apps, the analyzer recognizes text in an image.\n\nThen analyze the image by sending [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalyzer\/analyze(_:configuration:)] to an [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalyzer] object, passing the image and the configuration. To improve performance, use a single shared instance of the analyzer throughout the app.\n\nFor iOS apps, start the Live Text interface by setting the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction\/analysis] property of the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction] object to the results of the analyze method. For example, set the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction\/analysis] property in the action method of a control that starts Live Text.\n\nFor macOS apps, start the Live Text interface by setting the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisOverlayView\/analysis] property of the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisOverlayView] object.\n\nThe standard Live Text menu appears when users click and hold items in the image. The user chooses actions from this menu, depending on the type of item.\n\n### Customize behavior using interaction types\n\nYou can change the behavior of the interface by enabling types of interactions with items the analyzer finds in the image. If you set the interaction or overlay view [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction\/preferredInteractionTypes] property to [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction\/InteractionTypes\/automatic], users can interact with all types of items that the analyzer finds.\n\nIf you set the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction\/preferredInteractionTypes] property to just [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction\/InteractionTypes\/textSelection], users can only select text in the image and then perform a basic text action, such as copying, translating, or sharing the text. For iOS apps, if the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction] object’s [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction\/allowLongPressForDataDetectorsInTextMode] property is `true`, users can also touch and hold text that contains data (URLs, phone numbers, and email addresses) to perform a data-specific action.\n\nFor iOS apps, users can tap the Live Text button in the lower-right corner to switch to highlight mode. Then users can touch and hold data that the analyzer detects in text to perform a data-specific action, such as opening a URL, calling a phone number, or sending an email.\n\n\n\nTo let users interact with data that the analyzer detects in text in macOS, set the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisOverlayView\/preferredInteractionTypes] property to [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisOverlayView\/InteractionTypes\/dataDetectors]. Then users can hover over data in text to perform a data-specific action.\n\nFor iOS apps only, users can touch and hold QR codes and perform an action, depending on the payload, such as following an embedded link. To enable QR code interactions, set the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalyzer\/Configuration] structure’s [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalyzer\/Configuration\/analysisTypes] property to [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalyzer\/AnalysisTypes\/machineReadableCode].\n\n### Change the supplementary interface\n\nYou can modify the Live Text supplementary interface to conform better to the look of your app. The supplementary interface consists of the quick actions in the lower-left corner and the Live Text button in the lower-right corner of the interface.\n\nTo hide the supplementary interface, set the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction\/isSupplementaryInterfaceHidden] to `false`. For iOS apps, if the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction\/allowLongPressForDataDetectorsInTextMode] property is `true`, users can still touch and hold text to perform data-specific actions.\n\nIf your interface overlaps the supplementary interface, set the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction\/supplementaryInterfaceContentInsets] property appropriately to move the quick actions and Live Text button.\n\nIf you want the supplementary interface to use a custom font, set the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction\/supplementaryInterfaceFont] property. Quick actions use the specified font for text and font weight for symbols. For button size consistency, the Live Text interface ignores the point size.\n\n### Specify recognized languages\n\nBy default the image analyzer attempts to recognize the user’s preferred languages. If you want the analyzer to consider other languages, set the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalyzer\/Configuration\/locales] property of the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalyzer\/Configuration] object.\n\nTo determine whether the image analyzer supports a language, check whether the array that the `ImageAnalyzer` [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalyzer\/supportedTextRecognitionLanguages] class property returns includes the language ID.\n\nFor more information on language IDs, see [doc:\/\/com.apple.documentation\/documentation\/Xcode\/choosing-localization-regions-and-scripts].",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/VisionKit\/enabling-live-text-interactions-with-images\ncrawled: 2025-12-02T16:45:25Z\n---\n\n# Enabling Live Text interactions with images\n\n**Article**\n\nAdd a Live Text interface that enables users to perform actions with text and QR codes that appear in images.\n\n## Overview\n\nEnhance the user experience with images in your app by adding standard Live Text interactions. Live Text lets users tap text in an image to copy it, make a call, send an email, translate it, or look up directions. VisionKit provides this familiar Live Text interface to your images and across platforms.\n\nFor iOS apps, users can use a long-press gesture (touch and hold) a QR code to follow a link or take another action, depending on the payload. The payload is the data that a QR code contains, such as a URL or email address.\n\nBefore interacting with items in an image, the user can tap the Live Text button in the lower-right corner to highlight the recognized items. Then a data-specific action menu appears when the user double-taps on text in an image, and uses a long-press gesture on an email address or QR code.\n\n\n\nIn your app, you decide whether Live Text recognizes text and data within text, and for iOS apps, QR codes in the image. You choose the types of items to look for and run the analysis on the image, and then VisionKit provides the entire Live Text interface with the breadth of actions for specific types.\n\n\n\n### Check whether the device supports Live Text\n\nBefore showing a Live Text interface in your app, check whether the device supports Live Text. For iOS apps, the image analyzer is available only on devices with the A12 Bionic chip and later. If the `ImageAnalyzer` [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalyzer\/isSupported] property is `true`, show the Live Text interface. Otherwise, disable any feature in your app that relies on Live Text. If you attempt to start the Live Text interface when this property is `false`, the interface doesn’t appear.\n\n### Add a Live Text interaction object to your view in iOS\n\nFor iOS apps, you add the Live Text interface by adding an interaction object to the view containing the image. If you use a [doc:\/\/com.apple.documentation\/documentation\/UIKit\/UIImageView] to display the image, it handles the image content area calculations for you.\n\nAdd an [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction] object directly on the view containing the image.\n\n```swift\nlet interaction = ImageAnalysisInteraction()\nimageView.addInteraction(interaction)\n```\n\nOptionally, customize the interface by setting the interaction’s delegate to an object that implements the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteractionDelegate] protocol methods.\n\n```swift\ninteraction.delegate = self\n```\n\nIf you don’t use a [doc:\/\/com.apple.documentation\/documentation\/UIKit\/UIImageView] object, inform the interaction object when the content area of the image changes while the interaction bounds don’t change. Implement the `ImageAnalysisInteractionDelegate` [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteractionDelegate\/contentsRect(for:)] protocol method to return the content area of the image. This keeps the Live Text highlights within the bounds of the image. Then use the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction\/setContentsRectNeedsUpdate()] method to notify the interaction if the content area changes.\n\n### Add a Live Text view to your image view in macOS\n\nFor macOS apps, add the Live Text interface by adding a view above the view containing the image. If you use an [doc:\/\/com.apple.documentation\/documentation\/AppKit\/NSImageView] to display the image, it handles the image content area calculations for you.\n\nFirst create an instance of the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisOverlayView] class.\n\n```swift\nlet overlayView = ImageAnalysisOverlayView()\n```\n\nThen configure the overlay view to fit the bounds of the image. If your view is an image view, set the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisOverlayView\/trackingImageView] property so that the dimensions adjust when the scaling and alignment properties change.\n\n```swift\noverlayView.autoresizingMask = [.width, .height]\noverlayView.frame = imageView.bounds\noverlayView.trackingImageView = imageView\n```\n\nAdd the overlay view above the image in the hierarchy. For example, add it as a subview of the view containing the image.\n\n```swift\nimageView.addSubview(overlayView)\n```\n\nTo customize the interface, set the overlay view’s delegate to an object that implements the optional [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisOverlayViewDelegate] protocol methods.\n\n```swift\noverlayView.delegate = self\n```\n\nImplement the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisOverlayViewDelegate\/overlayView(_:shouldBeginAt:forAnalysisType:)-35czq] protocol method to return `true` to begin the interaction.\n\n```swift\nfunc overlayView(_ overlayView: ImageAnalysisOverlayView, \n           shouldBeginAt point: CGPoint, forAnalysisType \n           analysisType: ImageAnalysisOverlayView.InteractionTypes) -> Bool { \n    overlayView.hasInteractiveItem(at: point) || \n    overlayView.hasActiveTextSelection \n}\n```\n\nIf you aren’t using an [doc:\/\/com.apple.documentation\/documentation\/AppKit\/NSImageView] object, implement the `ImageAnalysisOverlayViewDelegate` [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisOverlayViewDelegate\/contentsRect(for:)-34yzu] protocol method to return the content area of the image. Then use the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisOverlayView\/setContentsRectNeedsUpdate()] method to notify the overlay view if the content area of the image changes while the view bounds don’t change.\n\n\n\n### Find items and start the interaction with an image\n\nProcess the image that your view displays using an [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalyzer] object. First, specify the types of items in the image you want to find when you create an [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalyzer\/Configuration] object. For iOS apps, the analyzer recognizes both text and machine-readable QR codes in an image; for macOS apps, the analyzer recognizes text in an image.\n\n```swift\nlet configuration = ImageAnalyzer.Configuration([.text, .machineReadableCode])\n```\n\nThen analyze the image by sending [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalyzer\/analyze(_:configuration:)] to an [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalyzer] object, passing the image and the configuration. To improve performance, use a single shared instance of the analyzer throughout the app.\n\n```swift\nlet analyzer = ImageAnalyzer()\nlet analysis = try? await analyzer.analyze(image, configuration: configuration)\n```\n\nFor iOS apps, start the Live Text interface by setting the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction\/analysis] property of the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction] object to the results of the analyze method. For example, set the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction\/analysis] property in the action method of a control that starts Live Text.\n\n```swift\ninteraction.analysis = analysis\n```\n\nFor macOS apps, start the Live Text interface by setting the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisOverlayView\/analysis] property of the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisOverlayView] object.\n\n```swift\noverlayView.analysis = analysis\n```\n\nThe standard Live Text menu appears when users click and hold items in the image. The user chooses actions from this menu, depending on the type of item.\n\n### Customize behavior using interaction types\n\nYou can change the behavior of the interface by enabling types of interactions with items the analyzer finds in the image. If you set the interaction or overlay view [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction\/preferredInteractionTypes] property to [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction\/InteractionTypes\/automatic], users can interact with all types of items that the analyzer finds.\n\n```swift\ninteraction.preferredInteractionTypes = .automatic\n```\n\nIf you set the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction\/preferredInteractionTypes] property to just [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction\/InteractionTypes\/textSelection], users can only select text in the image and then perform a basic text action, such as copying, translating, or sharing the text. For iOS apps, if the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction] object’s [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction\/allowLongPressForDataDetectorsInTextMode] property is `true`, users can also touch and hold text that contains data (URLs, phone numbers, and email addresses) to perform a data-specific action.\n\nFor iOS apps, users can tap the Live Text button in the lower-right corner to switch to highlight mode. Then users can touch and hold data that the analyzer detects in text to perform a data-specific action, such as opening a URL, calling a phone number, or sending an email.\n\n\n\nTo let users interact with data that the analyzer detects in text in macOS, set the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisOverlayView\/preferredInteractionTypes] property to [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisOverlayView\/InteractionTypes\/dataDetectors]. Then users can hover over data in text to perform a data-specific action.\n\nFor iOS apps only, users can touch and hold QR codes and perform an action, depending on the payload, such as following an embedded link. To enable QR code interactions, set the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalyzer\/Configuration] structure’s [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalyzer\/Configuration\/analysisTypes] property to [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalyzer\/AnalysisTypes\/machineReadableCode].\n\n### Change the supplementary interface\n\nYou can modify the Live Text supplementary interface to conform better to the look of your app. The supplementary interface consists of the quick actions in the lower-left corner and the Live Text button in the lower-right corner of the interface.\n\nTo hide the supplementary interface, set the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction\/isSupplementaryInterfaceHidden] to `false`. For iOS apps, if the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction\/allowLongPressForDataDetectorsInTextMode] property is `true`, users can still touch and hold text to perform data-specific actions.\n\nIf your interface overlaps the supplementary interface, set the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction\/supplementaryInterfaceContentInsets] property appropriately to move the quick actions and Live Text button.\n\nIf you want the supplementary interface to use a custom font, set the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalysisInteraction\/supplementaryInterfaceFont] property. Quick actions use the specified font for text and font weight for symbols. For button size consistency, the Live Text interface ignores the point size.\n\n### Specify recognized languages\n\nBy default the image analyzer attempts to recognize the user’s preferred languages. If you want the analyzer to consider other languages, set the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalyzer\/Configuration\/locales] property of the [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalyzer\/Configuration] object.\n\nTo determine whether the image analyzer supports a language, check whether the array that the `ImageAnalyzer` [doc:\/\/com.apple.VisionKit\/documentation\/VisionKit\/ImageAnalyzer\/supportedTextRecognitionLanguages] class property returns includes the language ID.\n\nFor more information on language IDs, see [doc:\/\/com.apple.documentation\/documentation\/Xcode\/choosing-localization-regions-and-scripts].\n\n## Content recognition and interaction in images\n\n- **ImageAnalyzer**: An object that finds items in images that people can interact with, such as subjects, text, and QR codes.\n- **ImageAnalysis**: An object that represents the results of analyzing an image, and provides the input for the Live Text interface object.\n- **ImageAnalysisInteraction**: An interface that enables people to interact with recognized text, barcodes, and other objects in an image.\n- **ImageAnalysisInteractionDelegate**: A delegate that handles image-analysis and user-interaction callbacks for an interaction object.\n- **ImageAnalysisOverlayView**: A view that enables people to interact with recognized text, barcodes, and other objects in an image.\n- **ImageAnalysisOverlayViewDelegate**: A delegate that handles image-analysis and user-interaction callbacks for an overlay view.\n- **CameraRegionView**: This view displays a stabilized region of interest within a person’s view and provides passthrough camera feed for that selected region.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "An object that finds items in images that people can interact with, such as subjects, text, and QR codes.",
          "name" : "ImageAnalyzer",
          "url" : "https:\/\/developer.apple.com\/documentation\/VisionKit\/ImageAnalyzer"
        },
        {
          "description" : "An object that represents the results of analyzing an image, and provides the input for the Live Text interface object.",
          "name" : "ImageAnalysis",
          "url" : "https:\/\/developer.apple.com\/documentation\/VisionKit\/ImageAnalysis"
        },
        {
          "description" : "An interface that enables people to interact with recognized text, barcodes, and other objects in an image.",
          "name" : "ImageAnalysisInteraction",
          "url" : "https:\/\/developer.apple.com\/documentation\/VisionKit\/ImageAnalysisInteraction"
        },
        {
          "description" : "A delegate that handles image-analysis and user-interaction callbacks for an interaction object.",
          "name" : "ImageAnalysisInteractionDelegate",
          "url" : "https:\/\/developer.apple.com\/documentation\/VisionKit\/ImageAnalysisInteractionDelegate"
        },
        {
          "description" : "A view that enables people to interact with recognized text, barcodes, and other objects in an image.",
          "name" : "ImageAnalysisOverlayView",
          "url" : "https:\/\/developer.apple.com\/documentation\/VisionKit\/ImageAnalysisOverlayView"
        },
        {
          "description" : "A delegate that handles image-analysis and user-interaction callbacks for an overlay view.",
          "name" : "ImageAnalysisOverlayViewDelegate",
          "url" : "https:\/\/developer.apple.com\/documentation\/VisionKit\/ImageAnalysisOverlayViewDelegate"
        },
        {
          "description" : "This view displays a stabilized region of interest within a person’s view and provides passthrough camera feed for that selected region.",
          "name" : "CameraRegionView",
          "url" : "https:\/\/developer.apple.com\/documentation\/VisionKit\/CameraRegionView"
        }
      ],
      "title" : "Content recognition and interaction in images"
    }
  ],
  "source" : "appleJSON",
  "title" : "Enabling Live Text interactions with images",
  "url" : "https:\/\/developer.apple.com\/documentation\/VisionKit\/enabling-live-text-interactions-with-images"
}