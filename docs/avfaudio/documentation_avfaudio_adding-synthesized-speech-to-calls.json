{
  "abstract" : "Provide a more accessible experience by adding your app’s audio to a call.",
  "codeExamples" : [
    {
      "code" : "\/\/ Retrieve the current microphone injection permission.\nlet permission = AVAudioApplication.shared.microphoneInjectionPermission",
      "language" : "swift"
    },
    {
      "code" : "@ViewBuilder\nvar alertButtons: some View {\n    Button(\"Open Settings\") {\n        Task {\n            do {\n                \/\/ Open the configuration screen for this feature in the Settings app.\n                try await AccessibilitySettings.openSettings(for: .allowAppsToAddAudioToCalls)\n            } catch {\n                print(\"Unable to open Settings app: \\(error)\")\n            }\n        }\n    }\n    Button(\"Cancel\", role: .cancel) {}\n}",
      "language" : "swift"
    },
    {
      "code" : "<key>NSMicrophoneInjectionUsageDescription<\/key>\n<string>This app adds its audio to calls to support augmentative and alternative communication.<\/string>",
      "language" : "swift"
    },
    {
      "code" : "\/\/ If undetermined, prompt the person to grant the app access, and turn on the feature, if allowed.\nif await AVAudioApplication.requestMicrophoneInjectionPermission() == .granted {\n    \/\/ If the person grants access, turn on adding app audio.\n}",
      "language" : "swift"
    },
    {
      "code" : "let mode: AVAudioSession.MicrophoneInjectionMode = \/\/ spoken audio or none\ntry AVAudioSession.sharedInstance().setPreferredMicrophoneInjectionMode(mode)",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ Monitor the active state of phone and FaceTime calls.\nprivate func observeCallState() async {\n    \/\/ Await notification of changes to the audio session's microphone injection capabilities.\n    for await notification in NotificationCenter.default.notifications(named: AVAudioSession.microphoneInjectionCapabilitiesChangeNotification) {\n        \/\/ Inspect the user information dictionary to determine whether microphone injection is available.\n        isCallActive = notification.userInfo?[AVAudioSessionMicrophoneInjectionIsAvailableKey] as? Bool ?? false\n    }\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "fb8f02b76e6151caa0703bf1ed904dc2ace378076f4c931dcf5a2b06e836a486",
  "crawledAt" : "2025-12-03T10:14:10Z",
  "id" : "7B2C9968-181A-433E-96D9-E2D8146738AD",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "AVFAudio",
  "overview" : "## Overview\n\nThis sample shows how to create an accessibility app that supports augmentative and alternative communication (AAC) by adding synthesized speech to a call. This feature is available in iOS 18.2 and visionOS 2.2 and later, and is available to use with calling apps that capture microphone input using Apple’s voice processing like Phone, FaceTime, and most VoIP apps.\n\nThe sample app provides a basic user interface with a button to toggle the feature state and a text field. When you enter text into the field and press enter, the app speaks the phrase. If you have an active call in progress and you enable adding the app’s audio to it, you’ll hear the synthesized speech on the originating and receiving ends of the call.\n\n## Configure the sample code project\n\nThe sample requires running on an iOS device with iOS 18.2 or later. To test the sample, establish a phone or FaceTime call with another device.\n\n## Enable the accessibility service\n\nBefore an app can add its audio to calls, a person must turn on a system-level service in the Settings app by choosing Accessibility > Audio & Visual > Add Audio in Calls. This setting is global to the device and influences the availability of the service for all apps.\n\n\n\nThe sample determines the state of this setting by querying the shared [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioApplication] object for its microphone injection permission:\n\nA permission value of [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioApplication\/MicrophoneInjectionPermission-swift.enum\/serviceDisabled] indicates the person hasn’t turned on the service, which means apps can’t add audio to calls. When the app retrieves this value, it presents an alert dialog that indicates the current state and provides the person an opportunity to update their setting. When you press the dialog’s Open Settings button, the app uses the Accessibility framework to directly open the Add Audio in Calls screen in the Settings app like shown below.\n\n## Request permission\n\nTurning on Add Audio in Calls makes the feature available to apps on the system, but apps must explicitly request and be granted permission to use the feature. The sample determines its permission by querying for the current microphone injection permission. A value of [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioApplication\/MicrophoneInjectionPermission-swift.enum\/undetermined] indicates the app hasn’t yet requested permission and needs to before it can use the feature.\n\nFor an app to request a person’s permission, it needs to provide an `NSMicrophoneInjectionUsageDescription` key in its `Info.plist` file with a description of why the app requests microphone access. The system displays this string when an app requests user permission. The sample app defines this entry as follows.\n\nAttempting to request permission without this usage string present results in the system quitting the app.\n\nThe app requests permission by calling the  [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioApplication\/requestRecordPermission(completionHandler:)] method and awaiting a response:\n\nCalling this method causes the system to present a dialog that requests user permission. If a person grants the app permission, the call returns a value of [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioApplication\/MicrophoneInjectionPermission-swift.enum\/granted] and the app updates its state accordingly.\n\nIf a person denies the app permission, the system sets the microphone injection permission state to [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioApplication\/MicrophoneInjectionPermission-swift.enum\/denied], and the app is unable to use the feature. The app remains in this state until a person explicitly changes the permission from the Add Audio in Calls screen in the Settings app. If you attempt to turn on the feature in the app while in this state, the app presents an alert similar to the one shown in the previous section to update the app’s permission.\n\n## Enable adding audio to calls\n\nWhen the app has permission to add audio and a person toggles the feature state in the user interface, the app responds by calling the shared [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioSession] object’s [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioSession\/setPreferredMicrophoneInjectionMode(_:)] method. To turn on the feature, the app passes the method a value of [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioSession\/MicrophoneInjectionMode\/spokenAudio]; to turn off the feature, it passes a value of [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioSession\/MicrophoneInjectionMode\/none].\n\nWhen turned on during an active call, the system plays the app’s audio locally and adds it to the microphone’s input stream.\n\n## Monitor the availability of calls\n\nTo determine whether a call can use this feature, the sample awaits notification of changes to the state of the audio session’s microphone injection capabilities:\n\nWhen a call begins or ends, the system posts a notification of the change. The app queries the notification’s user information dictionary for its [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioSessionMicrophoneInjectionIsAvailableKey] value to determine whether there’s an active call. When the value is `true`, the app updates its UI to show a pulsing phone icon in the toolbar to indicate the call is active.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/avfaudio\/adding-synthesized-speech-to-calls\ncrawled: 2025-12-03T10:14:10Z\n---\n\n# Adding synthesized speech to calls\n\n**Sample Code**\n\nProvide a more accessible experience by adding your app’s audio to a call.\n\n## Overview\n\nThis sample shows how to create an accessibility app that supports augmentative and alternative communication (AAC) by adding synthesized speech to a call. This feature is available in iOS 18.2 and visionOS 2.2 and later, and is available to use with calling apps that capture microphone input using Apple’s voice processing like Phone, FaceTime, and most VoIP apps.\n\nThe sample app provides a basic user interface with a button to toggle the feature state and a text field. When you enter text into the field and press enter, the app speaks the phrase. If you have an active call in progress and you enable adding the app’s audio to it, you’ll hear the synthesized speech on the originating and receiving ends of the call.\n\n## Configure the sample code project\n\nThe sample requires running on an iOS device with iOS 18.2 or later. To test the sample, establish a phone or FaceTime call with another device.\n\n## Enable the accessibility service\n\nBefore an app can add its audio to calls, a person must turn on a system-level service in the Settings app by choosing Accessibility > Audio & Visual > Add Audio in Calls. This setting is global to the device and influences the availability of the service for all apps.\n\n\n\nThe sample determines the state of this setting by querying the shared [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioApplication] object for its microphone injection permission:\n\n```swift\n\/\/ Retrieve the current microphone injection permission.\nlet permission = AVAudioApplication.shared.microphoneInjectionPermission\n```\n\nA permission value of [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioApplication\/MicrophoneInjectionPermission-swift.enum\/serviceDisabled] indicates the person hasn’t turned on the service, which means apps can’t add audio to calls. When the app retrieves this value, it presents an alert dialog that indicates the current state and provides the person an opportunity to update their setting. When you press the dialog’s Open Settings button, the app uses the Accessibility framework to directly open the Add Audio in Calls screen in the Settings app like shown below.\n\n```swift\n@ViewBuilder\nvar alertButtons: some View {\n    Button(\"Open Settings\") {\n        Task {\n            do {\n                \/\/ Open the configuration screen for this feature in the Settings app.\n                try await AccessibilitySettings.openSettings(for: .allowAppsToAddAudioToCalls)\n            } catch {\n                print(\"Unable to open Settings app: \\(error)\")\n            }\n        }\n    }\n    Button(\"Cancel\", role: .cancel) {}\n}\n```\n\n## Request permission\n\nTurning on Add Audio in Calls makes the feature available to apps on the system, but apps must explicitly request and be granted permission to use the feature. The sample determines its permission by querying for the current microphone injection permission. A value of [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioApplication\/MicrophoneInjectionPermission-swift.enum\/undetermined] indicates the app hasn’t yet requested permission and needs to before it can use the feature.\n\nFor an app to request a person’s permission, it needs to provide an `NSMicrophoneInjectionUsageDescription` key in its `Info.plist` file with a description of why the app requests microphone access. The system displays this string when an app requests user permission. The sample app defines this entry as follows.\n\n```swift\n<key>NSMicrophoneInjectionUsageDescription<\/key>\n<string>This app adds its audio to calls to support augmentative and alternative communication.<\/string>\n```\n\nAttempting to request permission without this usage string present results in the system quitting the app.\n\nThe app requests permission by calling the  [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioApplication\/requestRecordPermission(completionHandler:)] method and awaiting a response:\n\n```swift\n\/\/ If undetermined, prompt the person to grant the app access, and turn on the feature, if allowed.\nif await AVAudioApplication.requestMicrophoneInjectionPermission() == .granted {\n    \/\/ If the person grants access, turn on adding app audio.\n}\n```\n\nCalling this method causes the system to present a dialog that requests user permission. If a person grants the app permission, the call returns a value of [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioApplication\/MicrophoneInjectionPermission-swift.enum\/granted] and the app updates its state accordingly.\n\nIf a person denies the app permission, the system sets the microphone injection permission state to [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioApplication\/MicrophoneInjectionPermission-swift.enum\/denied], and the app is unable to use the feature. The app remains in this state until a person explicitly changes the permission from the Add Audio in Calls screen in the Settings app. If you attempt to turn on the feature in the app while in this state, the app presents an alert similar to the one shown in the previous section to update the app’s permission.\n\n## Enable adding audio to calls\n\nWhen the app has permission to add audio and a person toggles the feature state in the user interface, the app responds by calling the shared [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioSession] object’s [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioSession\/setPreferredMicrophoneInjectionMode(_:)] method. To turn on the feature, the app passes the method a value of [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioSession\/MicrophoneInjectionMode\/spokenAudio]; to turn off the feature, it passes a value of [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioSession\/MicrophoneInjectionMode\/none].\n\n```swift\nlet mode: AVAudioSession.MicrophoneInjectionMode = \/\/ spoken audio or none\ntry AVAudioSession.sharedInstance().setPreferredMicrophoneInjectionMode(mode)\n```\n\nWhen turned on during an active call, the system plays the app’s audio locally and adds it to the microphone’s input stream.\n\n\n\n## Monitor the availability of calls\n\nTo determine whether a call can use this feature, the sample awaits notification of changes to the state of the audio session’s microphone injection capabilities:\n\n```swift\n\/\/\/ Monitor the active state of phone and FaceTime calls.\nprivate func observeCallState() async {\n    \/\/ Await notification of changes to the audio session's microphone injection capabilities.\n    for await notification in NotificationCenter.default.notifications(named: AVAudioSession.microphoneInjectionCapabilitiesChangeNotification) {\n        \/\/ Inspect the user information dictionary to determine whether microphone injection is available.\n        isCallActive = notification.userInfo?[AVAudioSessionMicrophoneInjectionIsAvailableKey] as? Bool ?? false\n    }\n}\n```\n\nWhen a call begins or ends, the system posts a notification of the change. The app queries the notification’s user information dictionary for its [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioSessionMicrophoneInjectionIsAvailableKey] value to determine whether there’s an active call. When the value is `true`, the app updates its UI to show a pulsing phone icon in the toolbar to indicate the call is active.\n\n## System audio\n\n- **Handling audio interruptions**: Observe audio session notifications to ensure that your app responds appropriately to interruptions.\n- **Responding to audio route changes**: Observe audio session notifications to ensure that your app responds appropriately to route changes.\n- **Routing audio to specific devices in multidevice sessions**: Map audio channels to specific devices in multiroute sessions for recording and playback.\n- **Capturing stereo audio from built-In microphones**: Configure an iOS device’s built-in microphones to add stereo recording capabilities to your app.\n- **AVAudioSession**: An object that communicates to the system how you intend to use audio in your app.\n- **AVAudioApplication**: An object that manages one or more audio sessions that belong to an app.\n- **AVAudioRoutingArbiter**: An object for configuring macOS apps to participate in AirPods Automatic Switching.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Observe audio session notifications to ensure that your app responds appropriately to interruptions.",
          "name" : "Handling audio interruptions",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFAudio\/handling-audio-interruptions"
        },
        {
          "description" : "Observe audio session notifications to ensure that your app responds appropriately to route changes.",
          "name" : "Responding to audio route changes",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFAudio\/responding-to-audio-route-changes"
        },
        {
          "description" : "Map audio channels to specific devices in multiroute sessions for recording and playback.",
          "name" : "Routing audio to specific devices in multidevice sessions",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFAudio\/routing-audio-to-specific-devices-in-multidevice-sessions"
        },
        {
          "description" : "Configure an iOS device’s built-in microphones to add stereo recording capabilities to your app.",
          "name" : "Capturing stereo audio from built-In microphones",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFAudio\/capturing-stereo-audio-from-built-in-microphones"
        },
        {
          "description" : "An object that communicates to the system how you intend to use audio in your app.",
          "name" : "AVAudioSession",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFAudio\/AVAudioSession"
        },
        {
          "description" : "An object that manages one or more audio sessions that belong to an app.",
          "name" : "AVAudioApplication",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFAudio\/AVAudioApplication"
        },
        {
          "description" : "An object for configuring macOS apps to participate in AirPods Automatic Switching.",
          "name" : "AVAudioRoutingArbiter",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFAudio\/AVAudioRoutingArbiter"
        }
      ],
      "title" : "System audio"
    }
  ],
  "source" : "appleJSON",
  "title" : "Adding synthesized speech to calls",
  "url" : "https:\/\/developer.apple.com\/documentation\/avfaudio\/adding-synthesized-speech-to-calls"
}