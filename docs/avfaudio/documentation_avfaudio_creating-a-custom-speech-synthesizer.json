{
  "abstract" : "Use your custom voices to synthesize speech by building a speech synthesis provider.",
  "codeExamples" : [
    {
      "code" : "let groupDefaults = UserDefaults(suiteName: \"group.com.example.apple.samplecode.CustomSpeechSynthesizerExample\")",
      "language" : "swift"
    },
    {
      "code" : "private func saveVoicesToGroupDefaults() {\n    \n    \/\/ Update the list of voices in the shared group defaults.\n    groupDefaults?.set(voices, forKey: \"voices\")\n    \n    \/\/ Inform the system that the available voices changed.\n    AVSpeechSynthesisProviderVoice.updateSpeechVoices()\n    \n}",
      "language" : "swift"
    },
    {
      "code" : "public override var speechVoices: [AVSpeechSynthesisProviderVoice] {\n    get {\n        let voices: [String] = (groupDefaults?.value(forKey: \"voices\") as? [String]) ?? []\n        return voices.map { voice in\n            return AVSpeechSynthesisProviderVoice(name: voice,\n                                                  identifier: \"com.identifier.\\(voice)\",\n                                                  primaryLanguages: [\"en-US\"],\n                                                  supportedLanguages: [\"en-US\"])\n        }\n    }\n    set { }\n}",
      "language" : "swift"
    },
    {
      "code" : "public override func synthesizeSpeechRequest(_ speechRequest: AVSpeechSynthesisProviderRequest) {\n    request = speechRequest\n    currentBuffer = getAudioBufferForSSML(speechRequest.ssmlRepresentation)\n    framePosition = 0\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Iterate through the requested number of frames.\nfor frame in 0..<frameCount {\n    \/\/ Copy the source frames into the target buffer.\n    frames[Int(frame)] = sourceFrames[Int(self.framePosition)]\n    self.framePosition += 1\n    \/\/ Complete the request if the frame position exceeds the available buffer.\n    if self.framePosition >= self.currentBuffer!.frameLength {\n        actionFlags.pointee = .offlineUnitRenderAction_Complete\n        break\n    }\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "c526b9569db2e1100e67c1f8d5cfa3dc6b8487673aaf981370f2d7e421a46fb6",
  "crawledAt" : "2025-12-02T15:47:41Z",
  "id" : "1811355D-7B42-4C88-884E-1BD313F32A39",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "AVFAudio",
  "overview" : "## Overview\n\nA speech synthesis provider allows you to bring your custom voices to iOS and macOS for system use with text-to-speech features like VoiceOver. A speech synthesizer receives text and information about speech properties, and provides an audio representation of the speech. To generate audio, you create an audio unit extension.\n\nThe sample app shows you how to provide a list of voices to the system, and how to create a basic speech synthesizer to represent the voices you specify. It explores how to create an audio unit that’s responsible for handling text-to-speech requests to synthesize speech by using Speech Synthesis Markup Language (SSML). The sample inspects a request’s SSML for two strings — *hello* and *goodbye* — and plays the associated audio file.\n\n### Configure the sample code project\n\nTo run this sample app, you’ll need the following:\n\nTo create the audio files that the extension uses, perform the following steps:\n\nAfter you run the project, provide a name for your voice and choose Add Voice. The system takes up to 30 seconds to refresh the list of available voices. Verify that the voice is available in macOS by opening System Settings and choosing Accessibility > Spoken Content. Select the system voice drop-down menu to view the voices you add.\n\nIn iOS, open the Settings app and choose Accessibility > Spoken Content > Voices.\n\n### Create an app to provide voices\n\nThe sample creates a host app to customize the list of voices available to the speech synthesis extension, using an App Group to share information between the host app and the audio unit extension. For more information about App Groups, see [doc:\/\/com.apple.documentation\/documentation\/Xcode\/configuring-app-groups].\n\nWhen the sample changes the list of available voices, it informs the system that they’re available for use by calling [doc:\/\/com.apple.documentation\/documentation\/AVFAudio\/AVSpeechSynthesisProviderVoice\/updateSpeechVoices()].\n\n### Get the list of available voices\n\nThe extension is an [doc:\/\/com.apple.documentation\/documentation\/AVFAudio\/AVSpeechSynthesisProviderAudioUnit] that’s responsbile for handling speech synthesis tasks. The audio unit provides a list of available voices to the system, inspects a request’s SSML, and provides audio buffers to the system. To provide a list of voices, the sample retrieves the list of voices the host app provides and initializes an [doc:\/\/com.apple.documentation\/documentation\/AVFAudio\/AVSpeechSynthesisProviderVoice] for each one.\n\n### Handle the speech request\n\nWhen there’s text available to synthesize, the system calls [doc:\/\/com.apple.documentation\/documentation\/AVFAudio\/AVSpeechSynthesisProviderAudioUnit\/synthesizeSpeechRequest(_:)] with an [doc:\/\/com.apple.documentation\/documentation\/AVFAudio\/AVSpeechSynthesisProviderRequest]. The request contains the SSML representation that describes the text to synthesize and the corresponding attributes for customizing pitch, rate, intonation, and more.\n\nThe sample calls a helper method with the SSML to retrieve a buffer that contains the audio to play back in a render block. The method [doc:\/\/com.apple.documentation\/documentation\/AVFAudio\/AVSpeechSynthesisProviderAudioUnit\/cancelSpeechRequest()] discards the current speech request.\n\n### Provide audio buffers to the system\n\nThe system calls an audio unit’s rendering block to get a list of audio buffers for playback. The block receives the number of frames the system requests. The sample copies the current buffer’s frames into the target buffer for rendering. When the sample exhausts the available audio buffer, it sets the render action to [doc:\/\/com.apple.documentation\/documentation\/AudioToolbox\/AudioUnitRenderActionFlags\/offlineUnitRenderAction_Complete].",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/AVFAudio\/creating-a-custom-speech-synthesizer\ncrawled: 2025-12-02T15:47:41Z\n---\n\n# Creating a custom speech synthesizer\n\n**Sample Code**\n\nUse your custom voices to synthesize speech by building a speech synthesis provider.\n\n## Overview\n\nA speech synthesis provider allows you to bring your custom voices to iOS and macOS for system use with text-to-speech features like VoiceOver. A speech synthesizer receives text and information about speech properties, and provides an audio representation of the speech. To generate audio, you create an audio unit extension.\n\nThe sample app shows you how to provide a list of voices to the system, and how to create a basic speech synthesizer to represent the voices you specify. It explores how to create an audio unit that’s responsible for handling text-to-speech requests to synthesize speech by using Speech Synthesis Markup Language (SSML). The sample inspects a request’s SSML for two strings — *hello* and *goodbye* — and plays the associated audio file.\n\n### Configure the sample code project\n\nTo run this sample app, you’ll need the following:\n\n- A Mac with macOS 13 or later, or an iPhone with iOS 16 or later\n- Xcode 14.1 beta or later\n\nTo create the audio files that the extension uses, perform the following steps:\n\n1. Open Terminal.\n2. Run the command `say -v Zarvox hello -o hello.aiff`.\n3. Run the command `say -v Zarvox goodbye -o goodbye.aiff`.\n4. In the project navigator, expand the `CustomSpeechSynthesizerExampleExtension` group.\n5. Drag and drop the audio files you generate into the Audio group.\n\nAfter you run the project, provide a name for your voice and choose Add Voice. The system takes up to 30 seconds to refresh the list of available voices. Verify that the voice is available in macOS by opening System Settings and choosing Accessibility > Spoken Content. Select the system voice drop-down menu to view the voices you add.\n\nIn iOS, open the Settings app and choose Accessibility > Spoken Content > Voices.\n\n### Create an app to provide voices\n\nThe sample creates a host app to customize the list of voices available to the speech synthesis extension, using an App Group to share information between the host app and the audio unit extension. For more information about App Groups, see [doc:\/\/com.apple.documentation\/documentation\/Xcode\/configuring-app-groups].\n\n```swift\nlet groupDefaults = UserDefaults(suiteName: \"group.com.example.apple.samplecode.CustomSpeechSynthesizerExample\")\n```\n\nWhen the sample changes the list of available voices, it informs the system that they’re available for use by calling [doc:\/\/com.apple.documentation\/documentation\/AVFAudio\/AVSpeechSynthesisProviderVoice\/updateSpeechVoices()].\n\n```swift\nprivate func saveVoicesToGroupDefaults() {\n    \n    \/\/ Update the list of voices in the shared group defaults.\n    groupDefaults?.set(voices, forKey: \"voices\")\n    \n    \/\/ Inform the system that the available voices changed.\n    AVSpeechSynthesisProviderVoice.updateSpeechVoices()\n    \n}\n```\n\n### Get the list of available voices\n\nThe extension is an [doc:\/\/com.apple.documentation\/documentation\/AVFAudio\/AVSpeechSynthesisProviderAudioUnit] that’s responsbile for handling speech synthesis tasks. The audio unit provides a list of available voices to the system, inspects a request’s SSML, and provides audio buffers to the system. To provide a list of voices, the sample retrieves the list of voices the host app provides and initializes an [doc:\/\/com.apple.documentation\/documentation\/AVFAudio\/AVSpeechSynthesisProviderVoice] for each one.\n\n```swift\npublic override var speechVoices: [AVSpeechSynthesisProviderVoice] {\n    get {\n        let voices: [String] = (groupDefaults?.value(forKey: \"voices\") as? [String]) ?? []\n        return voices.map { voice in\n            return AVSpeechSynthesisProviderVoice(name: voice,\n                                                  identifier: \"com.identifier.\\(voice)\",\n                                                  primaryLanguages: [\"en-US\"],\n                                                  supportedLanguages: [\"en-US\"])\n        }\n    }\n    set { }\n}\n```\n\n### Handle the speech request\n\nWhen there’s text available to synthesize, the system calls [doc:\/\/com.apple.documentation\/documentation\/AVFAudio\/AVSpeechSynthesisProviderAudioUnit\/synthesizeSpeechRequest(_:)] with an [doc:\/\/com.apple.documentation\/documentation\/AVFAudio\/AVSpeechSynthesisProviderRequest]. The request contains the SSML representation that describes the text to synthesize and the corresponding attributes for customizing pitch, rate, intonation, and more.\n\n```swift\npublic override func synthesizeSpeechRequest(_ speechRequest: AVSpeechSynthesisProviderRequest) {\n    request = speechRequest\n    currentBuffer = getAudioBufferForSSML(speechRequest.ssmlRepresentation)\n    framePosition = 0\n}\n```\n\nThe sample calls a helper method with the SSML to retrieve a buffer that contains the audio to play back in a render block. The method [doc:\/\/com.apple.documentation\/documentation\/AVFAudio\/AVSpeechSynthesisProviderAudioUnit\/cancelSpeechRequest()] discards the current speech request.\n\n### Provide audio buffers to the system\n\nThe system calls an audio unit’s rendering block to get a list of audio buffers for playback. The block receives the number of frames the system requests. The sample copies the current buffer’s frames into the target buffer for rendering. When the sample exhausts the available audio buffer, it sets the render action to [doc:\/\/com.apple.documentation\/documentation\/AudioToolbox\/AudioUnitRenderActionFlags\/offlineUnitRenderAction_Complete].\n\n```swift\n\/\/ Iterate through the requested number of frames.\nfor frame in 0..<frameCount {\n    \/\/ Copy the source frames into the target buffer.\n    frames[Int(frame)] = sourceFrames[Int(self.framePosition)]\n    self.framePosition += 1\n    \/\/ Complete the request if the frame position exceeds the available buffer.\n    if self.framePosition >= self.currentBuffer!.frameLength {\n        actionFlags.pointee = .offlineUnitRenderAction_Complete\n        break\n    }\n}\n```\n\n## Speech synthesis\n\n- **AVSpeechSynthesisProviderAudioUnit**: An object that generates speech from text.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "An object that generates speech from text.",
          "name" : "AVSpeechSynthesisProviderAudioUnit",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFAudio\/AVSpeechSynthesisProviderAudioUnit"
        }
      ],
      "title" : "Speech synthesis"
    }
  ],
  "source" : "appleJSON",
  "title" : "Creating a custom speech synthesizer",
  "url" : "https:\/\/developer.apple.com\/documentation\/AVFAudio\/creating-a-custom-speech-synthesizer"
}