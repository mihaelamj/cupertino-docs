{
  "abstract" : "Add offline audio processing features to your app by enabling offline manual rendering mode.",
  "codeExamples" : [
    {
      "code" : "let sourceFile: AVAudioFile\nlet format: AVAudioFormat\ndo {\n    let sourceFileURL = Bundle.main.url(forResource: \"Rhythm\", withExtension: \"caf\")!\n    sourceFile = try AVAudioFile(forReading: sourceFileURL)\n    format = sourceFile.processingFormat\n} catch {\n    fatalError(\"Unable to load the source audio file: \\(error.localizedDescription).\")\n}",
      "language" : "swift"
    },
    {
      "code" : "let engine = AVAudioEngine()\nlet player = AVAudioPlayerNode()\nlet reverb = AVAudioUnitReverb()\n\nengine.attach(player)\nengine.attach(reverb)\n\n\/\/ Set the desired reverb parameters.\nreverb.loadFactoryPreset(.mediumHall)\nreverb.wetDryMix = 50\n\n\/\/ Connect the nodes.\nengine.connect(player, to: reverb, format: format)\nengine.connect(reverb, to: engine.mainMixerNode, format: format)\n\n\/\/ Schedule the source file.\nplayer.scheduleFile(sourceFile, at: nil)",
      "language" : "swift"
    },
    {
      "code" : "do {\n    \/\/ The maximum number of frames the engine renders in any single render call.\n    let maxFrames: AVAudioFrameCount = 4096\n    try engine.enableManualRenderingMode(.offline, format: format,\n                                         maximumFrameCount: maxFrames)\n} catch {\n    fatalError(\"Enabling manual rendering mode failed: \\(error).\")\n}",
      "language" : "swift"
    },
    {
      "code" : "do {\n    try engine.start()\n    player.play()\n} catch {\n    fatalError(\"Unable to start audio engine: \\(error).\")\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ The output buffer to which the engine renders the processed data.\nlet buffer = AVAudioPCMBuffer(pcmFormat: engine.manualRenderingFormat,\n                              frameCapacity: engine.manualRenderingMaximumFrameCount)!\n\nlet outputFile: AVAudioFile\ndo {\n    let documentsURL = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]\n    let outputURL = documentsURL.appendingPathComponent(\"Rhythm-processed.caf\")\n    outputFile = try AVAudioFile(forWriting: outputURL, settings: sourceFile.fileFormat.settings)\n} catch {\n    fatalError(\"Unable to open output audio file: \\(error).\")\n}",
      "language" : "swift"
    },
    {
      "code" : "while engine.manualRenderingSampleTime < sourceFile.length {\n    do {\n        let frameCount = sourceFile.length - engine.manualRenderingSampleTime\n        let framesToRender = min(AVAudioFrameCount(frameCount), buffer.frameCapacity)\n        \n        let status = try engine.renderOffline(framesToRender, to: buffer)\n        \n        switch status {\n            \n        case .success:\n            \/\/ The data rendered successfully. Write it to the output file.\n            try outputFile.write(from: buffer)\n            \n        case .insufficientDataFromInputNode:\n            \/\/ Applicable only when using the input node as one of the sources.\n            break\n            \n        case .cannotDoInCurrentContext:\n            \/\/ The engine couldn't render in the current render call.\n            \/\/ Retry in the next iteration.\n            break\n            \n        case .error:\n            \/\/ An error occurred while rendering the audio.\n            fatalError(\"The manual rendering failed.\")\n        }\n    } catch {\n        fatalError(\"The manual rendering failed: \\(error).\")\n    }\n}\n\n\/\/ Stop the player node and engine.\nplayer.stop()\nengine.stop()",
      "language" : "swift"
    }
  ],
  "contentHash" : "5d9d618caf6d7d684542617e313e1049515dd0ca8bcc2175d5aea55d0dfedb1b",
  "crawledAt" : "2025-12-02T15:47:51Z",
  "id" : "41218BA8-47D9-4FF6-8832-86BA1F429CE9",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "AVFAudio",
  "overview" : "## Overview\n\nYou commonly use [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioEngine] to add advanced real-time audio playback features to your app. In a real-time scenario, the audio hardware drives the engine’s I\/O and renders the data to the output hardware, such as the device’s built-in speaker or connected headphones.\n\n\n\nYou can also use `AVAudioEngine` to perform *offline* audio processing by enabling the engine’s offline manual rendering mode.  In this mode, the engine’s input and output nodes are disconnected from the audio hardware and the rendering is driven by your app. You use offline manual rendering mode to perform advanced postprocessing tasks, such as applying effects or performing audio analysis, usually much faster than you can do in real time.\n\n\n\nThis sample playground shows you how to enable the audio engine’s manual rendering mode and drive the rendering process from your app.\n\n### Prepare the Source Audio\n\nThe sample loads its audio data from a file contained within the playground. It creates an [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioFile] object to wrap the on-disk file and retrieves its input format, which is used to configure later stages of the audio pipeline.\n\n### Create and Configure the Audio Engine\n\nThe sample creates the engine and configures it to perform the desired processing. It creates a player node to drive the input, feeds its output into a reverb node, and connects the output of the reverb node to the main mixer node (which is implicitly connected to the output node). Finally, it schedules the audio file for playback.\n\n### Enable Offline Manual Rendering Mode\n\nIf you started the engine at this point, you’d hear the audio playing through the active output device. To change this default behavior, you need to explicitly configure the engine to use manual rendering mode. The sample enables this mode by calling the [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioEngine\/enableManualRenderingMode(_:format:maximumFrameCount:)] method, passing it the offline rendering mode value, the audio format, and the maximum number of frames to render in a given pass of the render thread.\n\nWith the configuration complete, the sample starts the engine and tells the player to play.\n\n### Prepare the Output Destinations\n\nUnlike in a real-time playback scenario, the output from the audio engine isn’t rendered to the output hardware, but is instead rendered to an output buffer and ultimately written to a file on disk. The sample creates the buffer object to render into, and an output file in your `~\/Documents` directory to save the processed audio.\n\n### Manually Render the Audio\n\nThe sample sequentially loops over the full duration of the input audio file, and in each pass, asks the engine to render the next batch of frames to the output buffer. If the audio engine successfully renders the data, the sample writes the buffer to the output file on disk. When the sample finishes processing the audio data, it stops the player and engine.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/AVFAudio\/performing-offline-audio-processing\ncrawled: 2025-12-02T15:47:51Z\n---\n\n# Performing offline audio processing\n\n**Sample Code**\n\nAdd offline audio processing features to your app by enabling offline manual rendering mode.\n\n## Overview\n\nYou commonly use [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioEngine] to add advanced real-time audio playback features to your app. In a real-time scenario, the audio hardware drives the engine’s I\/O and renders the data to the output hardware, such as the device’s built-in speaker or connected headphones.\n\n\n\nYou can also use `AVAudioEngine` to perform *offline* audio processing by enabling the engine’s offline manual rendering mode.  In this mode, the engine’s input and output nodes are disconnected from the audio hardware and the rendering is driven by your app. You use offline manual rendering mode to perform advanced postprocessing tasks, such as applying effects or performing audio analysis, usually much faster than you can do in real time.\n\n\n\nThis sample playground shows you how to enable the audio engine’s manual rendering mode and drive the rendering process from your app.\n\n### Prepare the Source Audio\n\nThe sample loads its audio data from a file contained within the playground. It creates an [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioFile] object to wrap the on-disk file and retrieves its input format, which is used to configure later stages of the audio pipeline.\n\n```swift\nlet sourceFile: AVAudioFile\nlet format: AVAudioFormat\ndo {\n    let sourceFileURL = Bundle.main.url(forResource: \"Rhythm\", withExtension: \"caf\")!\n    sourceFile = try AVAudioFile(forReading: sourceFileURL)\n    format = sourceFile.processingFormat\n} catch {\n    fatalError(\"Unable to load the source audio file: \\(error.localizedDescription).\")\n}\n```\n\n### Create and Configure the Audio Engine\n\nThe sample creates the engine and configures it to perform the desired processing. It creates a player node to drive the input, feeds its output into a reverb node, and connects the output of the reverb node to the main mixer node (which is implicitly connected to the output node). Finally, it schedules the audio file for playback.\n\n```swift\nlet engine = AVAudioEngine()\nlet player = AVAudioPlayerNode()\nlet reverb = AVAudioUnitReverb()\n\nengine.attach(player)\nengine.attach(reverb)\n\n\/\/ Set the desired reverb parameters.\nreverb.loadFactoryPreset(.mediumHall)\nreverb.wetDryMix = 50\n\n\/\/ Connect the nodes.\nengine.connect(player, to: reverb, format: format)\nengine.connect(reverb, to: engine.mainMixerNode, format: format)\n\n\/\/ Schedule the source file.\nplayer.scheduleFile(sourceFile, at: nil)\n```\n\n### Enable Offline Manual Rendering Mode\n\nIf you started the engine at this point, you’d hear the audio playing through the active output device. To change this default behavior, you need to explicitly configure the engine to use manual rendering mode. The sample enables this mode by calling the [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioEngine\/enableManualRenderingMode(_:format:maximumFrameCount:)] method, passing it the offline rendering mode value, the audio format, and the maximum number of frames to render in a given pass of the render thread.\n\n```swift\ndo {\n    \/\/ The maximum number of frames the engine renders in any single render call.\n    let maxFrames: AVAudioFrameCount = 4096\n    try engine.enableManualRenderingMode(.offline, format: format,\n                                         maximumFrameCount: maxFrames)\n} catch {\n    fatalError(\"Enabling manual rendering mode failed: \\(error).\")\n}\n```\n\nWith the configuration complete, the sample starts the engine and tells the player to play.\n\n```swift\ndo {\n    try engine.start()\n    player.play()\n} catch {\n    fatalError(\"Unable to start audio engine: \\(error).\")\n}\n```\n\n### Prepare the Output Destinations\n\nUnlike in a real-time playback scenario, the output from the audio engine isn’t rendered to the output hardware, but is instead rendered to an output buffer and ultimately written to a file on disk. The sample creates the buffer object to render into, and an output file in your `~\/Documents` directory to save the processed audio.\n\n```swift\n\/\/ The output buffer to which the engine renders the processed data.\nlet buffer = AVAudioPCMBuffer(pcmFormat: engine.manualRenderingFormat,\n                              frameCapacity: engine.manualRenderingMaximumFrameCount)!\n\nlet outputFile: AVAudioFile\ndo {\n    let documentsURL = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]\n    let outputURL = documentsURL.appendingPathComponent(\"Rhythm-processed.caf\")\n    outputFile = try AVAudioFile(forWriting: outputURL, settings: sourceFile.fileFormat.settings)\n} catch {\n    fatalError(\"Unable to open output audio file: \\(error).\")\n}\n```\n\n### Manually Render the Audio\n\nThe sample sequentially loops over the full duration of the input audio file, and in each pass, asks the engine to render the next batch of frames to the output buffer. If the audio engine successfully renders the data, the sample writes the buffer to the output file on disk. When the sample finishes processing the audio data, it stops the player and engine.\n\n```swift\nwhile engine.manualRenderingSampleTime < sourceFile.length {\n    do {\n        let frameCount = sourceFile.length - engine.manualRenderingSampleTime\n        let framesToRender = min(AVAudioFrameCount(frameCount), buffer.frameCapacity)\n        \n        let status = try engine.renderOffline(framesToRender, to: buffer)\n        \n        switch status {\n            \n        case .success:\n            \/\/ The data rendered successfully. Write it to the output file.\n            try outputFile.write(from: buffer)\n            \n        case .insufficientDataFromInputNode:\n            \/\/ Applicable only when using the input node as one of the sources.\n            break\n            \n        case .cannotDoInCurrentContext:\n            \/\/ The engine couldn't render in the current render call.\n            \/\/ Retry in the next iteration.\n            break\n            \n        case .error:\n            \/\/ An error occurred while rendering the audio.\n            fatalError(\"The manual rendering failed.\")\n        }\n    } catch {\n        fatalError(\"The manual rendering failed: \\(error).\")\n    }\n}\n\n\/\/ Stop the player node and engine.\nplayer.stop()\nengine.stop()\n```\n\n## Rendering\n\n- **Building a signal generator**: Generate audio signals using an audio source node and a custom render callback.\n- **AVAudioSourceNode**: An object that supplies audio data.\n- **AVAudioSinkNode**: An object that receives audio data.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Generate audio signals using an audio source node and a custom render callback.",
          "name" : "Building a signal generator",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFAudio\/building-a-signal-generator"
        },
        {
          "description" : "An object that supplies audio data.",
          "name" : "AVAudioSourceNode",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFAudio\/AVAudioSourceNode"
        },
        {
          "description" : "An object that receives audio data.",
          "name" : "AVAudioSinkNode",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFAudio\/AVAudioSinkNode"
        }
      ],
      "title" : "Rendering"
    }
  ],
  "source" : "appleJSON",
  "title" : "Performing offline audio processing",
  "url" : "https:\/\/developer.apple.com\/documentation\/AVFAudio\/performing-offline-audio-processing"
}