{
  "abstract" : "Map audio channels to specific devices in multiroute sessions for recording and playback.",
  "codeExamples" : [
    {
      "code" : "Device 1 (AirPods): 2 channels > Global positions [0, 1]\nDevice 2 (Built-in): 2 channels > Global positions [2, 3]\nResult: Global channel array [0, 1, 2, 3]",
      "language" : "swift"
    },
    {
      "code" : "var channelMap: [Int32] = [-1, -1, 0, 1]\n\/\/ channelMap[0] = -1: AirPods left is silent.\n\/\/ channelMap[1] = -1: AirPods right is silent.\n\/\/ channelMap[2] = 0: Built-in left plays the file's left channel.\n\/\/ channelMap[3] = 1: Built-in right plays the file's right channel.",
      "language" : "swift"
    },
    {
      "code" : "let audioSession = AVAudioSession.sharedInstance()\n\n\/\/ Configure and activate the audio session for multiroute setup.\n\nlet currentRoute = audioSession.currentRoute\nvar outputChannelOffset: UInt32 = 0\n\nfor (portIndex, outputPort) in currentRoute.outputs.enumerated() {\n    guard let channels = outputPort.channels else { continue }\n\n    \/\/ The `outputChannelOffset` is the starting global channel index \n    \/\/ for this device.\n\n    \/\/ Increase the `outputChannelOffset` by the number of \n    \/\/ channels in this port.\n    outputChannelOffset += UInt32(channels.count)\n}",
      "language" : "swift"
    },
    {
      "code" : "let audioSession = AVAudioSession.sharedInstance()\n\n\/\/ Configure and activate the audio session for multiroute setup.\n\n\/\/ Configure audio IO.\nlet engine = AVAudioEngine()\nlet player = AVAudioPlayerNode()\n\nengine.attach(player)\nengine.connect(player, to: engine.mainMixerNode, format: nil)\n\nguard let outputAudioUnit = engine.outputNode.audioUnit else {\n    fatalError(\"Failed to get output AudioUnit\")\n}\n\n\/\/ Get the total output channels from the \n\/\/ audio session.\nlet totalOutputChannels = audioSession.maximumOutputNumberOfChannels\n\n\/\/ Create the channel map: \n\/\/ Silence all channels except target device.\nvar channelMap = Array<Int32>(repeating: -1, count: totalOutputChannels)\n\n\/\/ Route stereo audio to built-in speaker at channels 2 and 3.\nchannelMap[2] = 0  \/\/ Route stream channel 0 (left) to output channel 2.\nchannelMap[3] = 1  \/\/ Route stream channel 1 (right) to output channel 3.\n\n\/\/ Apply the channel map to `AudioUnit`.\nlet result = AudioUnitSetProperty(\n    outputAudioUnit,\n    kAudioOutputUnitProperty_ChannelMap,\n    kAudioUnitScope_Output,\n    0,\n    channelMap,\n    UInt32(channelMap.count * MemoryLayout<Int32>.size)\n)\n\nguard result == noErr else {\n    fatalError(\"Failed to set channel map: \\(result)\")\n}\n\n\/\/ Start the audio engine and play audio.\ntry? engine.start()\n\nguard let audioFileURL = Bundle.main.url(forResource: \"sample\", withExtension: \"mp3\"),\n      let audioFile = try? AVAudioFile(forReading: audioFileURL) else {\n    fatalError(\"Failed to load audio file\")\n}\n\nplayer.scheduleFile(audioFile, at: nil)\nplayer.play()",
      "language" : "swift"
    },
    {
      "code" : "Device 1 (Built-in Mic): 2 channels > Hardware channels [0, 1] (front, back)\nDevice 2 (AirPods Mic): 1 channel > Hardware channel [2]\nResult: Hardware channel array [0, 1, 2]",
      "language" : "swift"
    },
    {
      "code" : "var channelMap: [Int32] = [0, 2]\n\/\/ channelMap[0] = 0: File channel 0 records from Built-in front mic (hardware channel 0).\n\/\/ channelMap[1] = 2: File channel 1 records from AirPods mic (hardware channel 2).",
      "language" : "swift"
    },
    {
      "code" : "let audioSession = AVAudioSession.sharedInstance()\n\n\/\/ Configure and activate the audio session for multiroute setup.\n\nlet currentRoute = audioSession.currentRoute\n\nfor (portIndex, inputPort) in currentRoute.inputs.enumerated() {\n    guard let channels = inputPort.channels else { continue }\n\n    print(\"Port \\(portIndex): \\(inputPort.portName)\")\n    for (channelIndex, channel) in channels.enumerated() {\n        print(\"  Hardware channel \\(channelIndex): \\(channel.channelName)\")\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Configure audio IO.\nlet engine = AVAudioEngine()\nlet inputNode = engine.inputNode\n\n\/\/ Get the hardware input format to match the sample rate.\nlet hwFormat = inputNode.inputFormat(forBus: 0)\n\n\/\/ Create the recording format with the desired channel count\n\/\/ and hardware sample rate.\nlet recordingFormat = AVAudioFormat(\n    standardFormatWithSampleRate: hwFormat.sampleRate,\n    channels: 2\n)!\n\n\/\/ Configure the channel map using the underlying `AudioUnit`.\nguard let inputAudioUnit = inputNode.audioUnit else {\n    fatalError(\"Failed to get input AudioUnit\")\n}\n\nvar channelMap: [Int32] = [\n    0, \/\/ File channel 0 < Hardware channel 0.\n    2 \/\/ File channel 1 < Hardware channel 2.\n]\n\nlet result = AudioUnitSetProperty(\n    inputAudioUnit,\n    kAudioOutputUnitProperty_ChannelMap,\n    kAudioUnitScope_Input,\n    1,\n    channelMap,\n    UInt32(channelMap.count * MemoryLayout<Int32>.size)\n)\n\nguard result == noErr else {\n    fatalError(\"Failed to set input channel map: \\(result)\")\n}\n\n\/\/ Install tap with recording format to start capturing audio.\ninputNode.installTap(onBus: 0, bufferSize: 4096, format: recordingFormat) { buffer, time in\n    \/\/ Process the recorded audio buffer here.\n    \/\/ Example: Write to file, analyze audio, and so on.\n}\n\n\/\/ Start the engine.\ntry engine.start()",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Get the device UID from `AVAudioSession`.\nlet audioSession = AVAudioSession.sharedInstance()\n\n\/\/ Configure and activate the audio session for multiroute setup.\n\nlet outputPort = audioSession.currentRoute.outputs.first!\n\n\/\/ Create the channel assignment for device channel 0.\nvar channelAssignment = AudioQueueChannelAssignment(\n    mDeviceUID: outputPort.uid as CFString,\n    mChannelNumber: 1  \/\/ 1-based indexing.\n)\n\n\/\/ Apply the channel assignments to `AudioQueue`\nAudioQueueSetProperty(\n    audioQueue,\n    kAudioQueueProperty_ChannelAssignments,\n    &channelAssignment,\n    UInt32(MemoryLayout<AudioQueueChannelAssignment>.size)\n)",
      "language" : "swift"
    },
    {
      "code" : "let audioSession = AVAudioSession.sharedInstance()\n\n\/\/ Configure and activate the audio session for multiroute setup.\n\nlet outputPort = audioSession.currentRoute.outputs.first!\nlet channelDescription = outputPort.channels!.first!\n\nlet player = try AVAudioPlayer(contentsOf: audioFileURL)\nplayer.channelAssignments = [channelDescription]\nplayer.play()",
      "language" : "swift"
    },
    {
      "code" : "let audioSession = AVAudioSession.sharedInstance()\n\n\/\/ Configure and activate the audio session for multiroute setup.\n\nlet inputPort = audioSession.currentRoute.inputs.first!\nlet channelDescription = inputPort.channels!.first!\n\nlet recorder = try AVAudioRecorder(url: outputURL, settings: settings)\nrecorder.channelAssignments = [channelDescription]\nrecorder.record()",
      "language" : "swift"
    }
  ],
  "contentHash" : "e17cd8c9844690894351575b209ec425a0cee8c0824ccc79328ceec5366440ee",
  "crawledAt" : "2025-12-03T10:14:09Z",
  "id" : "93C7C8DD-49E2-4720-95E0-A7157DADC9A4",
  "kind" : "article",
  "language" : "swift",
  "module" : "AVFAudio",
  "overview" : "## Overview\n\nWhen working with multiple audio devices simultaneously, such as recording from multiple microphones or routing playback to different speakers, you need precise control over which audio reaches which device.\n\n[doc:\/\/com.apple.documentation\/documentation\/CoreAudio] provides channel mapping to bind specific audio channels to targeted devices. Use the input and output nodes from [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioEngine] for position-based routing with global channel indices, [doc:\/\/com.apple.documentation\/documentation\/AudioToolbox\/audio-queue-services] for device-based routing using device identifiers, or [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioPlayer] and [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioRecorder] for high-level routing with audio session channel descriptions.\n\n## Route audio outputs using audio engine’s output node\n\nUse [doc:\/\/com.apple.documentation\/documentation\/AudioUnit] channel maps to route playback audio to specific devices by mapping to global channel positions. A channel map is an array where the index represents the destination channel and the value specifies which source channel to route to that destination. Use `-1` to specify silence for unused channels.\n\nIn multidevice configurations, [doc:\/\/com.apple.documentation\/documentation\/CoreAudio] flattens all device channels into a sequential global channel space. When multiple audio devices are active, Core Audio assigns channels sequentially based on port order. For example, with AirPods (2 channels) and built-in speaker (2 channels) connected:\n\nTo route a stereo audio file to the built-in speaker at channels 2 and 3, create a channel map sized to match the total output channels:\n\nBecause port order can vary and channel counts differ between devices, first discover the actual port-to-channel mapping via [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioSession\/currentRoute]. Calculate channel indices dynamically based on the current audio route configuration:\n\nThis iterates through output ports, calculating their position in the global flattened channel array. Use these calculated indices to target specific devices in your channel mapping.\n\nAfter discovering channel positions, configure the [doc:\/\/com.apple.documentation\/documentation\/AudioUnit] with a channel map. This example routes a stereo audio file to the built-in speaker, assuming it occupies channels 2 and 3 in the global channel space:\n\n## Route audio inputs using audio engine’s input node\n\nWhen recording from multiple input devices, channel map semantics differ from playback. The channel map array size must match your desired recording channel count (not the total hardware channel count), and each array element specifies which hardware channel to pull from.\n\nIn multidevice input configurations, similar to output, hardware channels are available sequentially. For example, with a device’s stereo built-in microphone (2 channels: front and back) and AirPods microphone (1 channel):\n\nTo record a two-channel file capturing audio from the device’s built-in front mic (channel 0) and AirPods (channel 2), set the input client format to 2 channels, then create a channel map sized to match the desired recording channel count:\n\nDiscover the available input channels:\n\nConfigure the input channel map to select specific hardware channels for recording. This example records a two-channel file from hardware channels 0 and 2:\n\n## Route audio with audio queue channel assignments\n\nUse [doc:\/\/com.apple.documentation\/documentation\/AudioToolbox\/audio-queue-services] channel assignments to route audio to specific device channels by device UID rather than global channel position. This approach provides direct device targeting without needing to calculate global channel indices.\n\nSet the [doc:\/\/com.apple.documentation\/documentation\/AudioToolbox\/kAudioQueueProperty_ChannelAssignments] property with an [doc:\/\/com.apple.documentation\/documentation\/AudioToolbox\/AudioQueueChannelAssignment] structure specifying the target device UID and channel number:\n\n## Route high-level audio an audio player or recorder\n\n[doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioPlayer] and [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioRecorder] provide a high-level approach using the audio player’s [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioPlayer\/channelAssignments], or the audio recorder’s [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioRecorder\/channelAssignments] property with [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioSessionChannelDescription] objects directly from the audio session.\n\nFor output [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioPlayer]:\n\nFor input with [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioRecorder]:",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/avfaudio\/routing-audio-to-specific-devices-in-multidevice-sessions\ncrawled: 2025-12-03T10:14:09Z\n---\n\n# Routing audio to specific devices in multidevice sessions\n\n**Article**\n\nMap audio channels to specific devices in multiroute sessions for recording and playback.\n\n## Overview\n\nWhen working with multiple audio devices simultaneously, such as recording from multiple microphones or routing playback to different speakers, you need precise control over which audio reaches which device.\n\n[doc:\/\/com.apple.documentation\/documentation\/CoreAudio] provides channel mapping to bind specific audio channels to targeted devices. Use the input and output nodes from [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioEngine] for position-based routing with global channel indices, [doc:\/\/com.apple.documentation\/documentation\/AudioToolbox\/audio-queue-services] for device-based routing using device identifiers, or [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioPlayer] and [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioRecorder] for high-level routing with audio session channel descriptions.\n\n## Route audio outputs using audio engine’s output node\n\nUse [doc:\/\/com.apple.documentation\/documentation\/AudioUnit] channel maps to route playback audio to specific devices by mapping to global channel positions. A channel map is an array where the index represents the destination channel and the value specifies which source channel to route to that destination. Use `-1` to specify silence for unused channels.\n\nIn multidevice configurations, [doc:\/\/com.apple.documentation\/documentation\/CoreAudio] flattens all device channels into a sequential global channel space. When multiple audio devices are active, Core Audio assigns channels sequentially based on port order. For example, with AirPods (2 channels) and built-in speaker (2 channels) connected:\n\n```swift\nDevice 1 (AirPods): 2 channels > Global positions [0, 1]\nDevice 2 (Built-in): 2 channels > Global positions [2, 3]\nResult: Global channel array [0, 1, 2, 3]\n```\n\nTo route a stereo audio file to the built-in speaker at channels 2 and 3, create a channel map sized to match the total output channels:\n\n```swift\nvar channelMap: [Int32] = [-1, -1, 0, 1]\n\/\/ channelMap[0] = -1: AirPods left is silent.\n\/\/ channelMap[1] = -1: AirPods right is silent.\n\/\/ channelMap[2] = 0: Built-in left plays the file's left channel.\n\/\/ channelMap[3] = 1: Built-in right plays the file's right channel.\n```\n\nBecause port order can vary and channel counts differ between devices, first discover the actual port-to-channel mapping via [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioSession\/currentRoute]. Calculate channel indices dynamically based on the current audio route configuration:\n\n```swift\nlet audioSession = AVAudioSession.sharedInstance()\n\n\/\/ Configure and activate the audio session for multiroute setup.\n\nlet currentRoute = audioSession.currentRoute\nvar outputChannelOffset: UInt32 = 0\n\nfor (portIndex, outputPort) in currentRoute.outputs.enumerated() {\n    guard let channels = outputPort.channels else { continue }\n\n    \/\/ The `outputChannelOffset` is the starting global channel index \n    \/\/ for this device.\n\n    \/\/ Increase the `outputChannelOffset` by the number of \n    \/\/ channels in this port.\n    outputChannelOffset += UInt32(channels.count)\n}\n```\n\nThis iterates through output ports, calculating their position in the global flattened channel array. Use these calculated indices to target specific devices in your channel mapping.\n\nAfter discovering channel positions, configure the [doc:\/\/com.apple.documentation\/documentation\/AudioUnit] with a channel map. This example routes a stereo audio file to the built-in speaker, assuming it occupies channels 2 and 3 in the global channel space:\n\n```swift\nlet audioSession = AVAudioSession.sharedInstance()\n\n\/\/ Configure and activate the audio session for multiroute setup.\n\n\/\/ Configure audio IO.\nlet engine = AVAudioEngine()\nlet player = AVAudioPlayerNode()\n\nengine.attach(player)\nengine.connect(player, to: engine.mainMixerNode, format: nil)\n\nguard let outputAudioUnit = engine.outputNode.audioUnit else {\n    fatalError(\"Failed to get output AudioUnit\")\n}\n\n\/\/ Get the total output channels from the \n\/\/ audio session.\nlet totalOutputChannels = audioSession.maximumOutputNumberOfChannels\n\n\/\/ Create the channel map: \n\/\/ Silence all channels except target device.\nvar channelMap = Array<Int32>(repeating: -1, count: totalOutputChannels)\n\n\/\/ Route stereo audio to built-in speaker at channels 2 and 3.\nchannelMap[2] = 0  \/\/ Route stream channel 0 (left) to output channel 2.\nchannelMap[3] = 1  \/\/ Route stream channel 1 (right) to output channel 3.\n\n\/\/ Apply the channel map to `AudioUnit`.\nlet result = AudioUnitSetProperty(\n    outputAudioUnit,\n    kAudioOutputUnitProperty_ChannelMap,\n    kAudioUnitScope_Output,\n    0,\n    channelMap,\n    UInt32(channelMap.count * MemoryLayout<Int32>.size)\n)\n\nguard result == noErr else {\n    fatalError(\"Failed to set channel map: \\(result)\")\n}\n\n\/\/ Start the audio engine and play audio.\ntry? engine.start()\n\nguard let audioFileURL = Bundle.main.url(forResource: \"sample\", withExtension: \"mp3\"),\n      let audioFile = try? AVAudioFile(forReading: audioFileURL) else {\n    fatalError(\"Failed to load audio file\")\n}\n\nplayer.scheduleFile(audioFile, at: nil)\nplayer.play()\n```\n\n\n\n## Route audio inputs using audio engine’s input node\n\nWhen recording from multiple input devices, channel map semantics differ from playback. The channel map array size must match your desired recording channel count (not the total hardware channel count), and each array element specifies which hardware channel to pull from.\n\nIn multidevice input configurations, similar to output, hardware channels are available sequentially. For example, with a device’s stereo built-in microphone (2 channels: front and back) and AirPods microphone (1 channel):\n\n```swift\nDevice 1 (Built-in Mic): 2 channels > Hardware channels [0, 1] (front, back)\nDevice 2 (AirPods Mic): 1 channel > Hardware channel [2]\nResult: Hardware channel array [0, 1, 2]\n```\n\nTo record a two-channel file capturing audio from the device’s built-in front mic (channel 0) and AirPods (channel 2), set the input client format to 2 channels, then create a channel map sized to match the desired recording channel count:\n\n```swift\nvar channelMap: [Int32] = [0, 2]\n\/\/ channelMap[0] = 0: File channel 0 records from Built-in front mic (hardware channel 0).\n\/\/ channelMap[1] = 2: File channel 1 records from AirPods mic (hardware channel 2).\n```\n\nDiscover the available input channels:\n\n```swift\nlet audioSession = AVAudioSession.sharedInstance()\n\n\/\/ Configure and activate the audio session for multiroute setup.\n\nlet currentRoute = audioSession.currentRoute\n\nfor (portIndex, inputPort) in currentRoute.inputs.enumerated() {\n    guard let channels = inputPort.channels else { continue }\n\n    print(\"Port \\(portIndex): \\(inputPort.portName)\")\n    for (channelIndex, channel) in channels.enumerated() {\n        print(\"  Hardware channel \\(channelIndex): \\(channel.channelName)\")\n    }\n}\n```\n\nConfigure the input channel map to select specific hardware channels for recording. This example records a two-channel file from hardware channels 0 and 2:\n\n```swift\n\/\/ Configure audio IO.\nlet engine = AVAudioEngine()\nlet inputNode = engine.inputNode\n\n\/\/ Get the hardware input format to match the sample rate.\nlet hwFormat = inputNode.inputFormat(forBus: 0)\n\n\/\/ Create the recording format with the desired channel count\n\/\/ and hardware sample rate.\nlet recordingFormat = AVAudioFormat(\n    standardFormatWithSampleRate: hwFormat.sampleRate,\n    channels: 2\n)!\n\n\/\/ Configure the channel map using the underlying `AudioUnit`.\nguard let inputAudioUnit = inputNode.audioUnit else {\n    fatalError(\"Failed to get input AudioUnit\")\n}\n\nvar channelMap: [Int32] = [\n    0, \/\/ File channel 0 < Hardware channel 0.\n    2 \/\/ File channel 1 < Hardware channel 2.\n]\n\nlet result = AudioUnitSetProperty(\n    inputAudioUnit,\n    kAudioOutputUnitProperty_ChannelMap,\n    kAudioUnitScope_Input,\n    1,\n    channelMap,\n    UInt32(channelMap.count * MemoryLayout<Int32>.size)\n)\n\nguard result == noErr else {\n    fatalError(\"Failed to set input channel map: \\(result)\")\n}\n\n\/\/ Install tap with recording format to start capturing audio.\ninputNode.installTap(onBus: 0, bufferSize: 4096, format: recordingFormat) { buffer, time in\n    \/\/ Process the recorded audio buffer here.\n    \/\/ Example: Write to file, analyze audio, and so on.\n}\n\n\/\/ Start the engine.\ntry engine.start()\n```\n\n\n\n## Route audio with audio queue channel assignments\n\nUse [doc:\/\/com.apple.documentation\/documentation\/AudioToolbox\/audio-queue-services] channel assignments to route audio to specific device channels by device UID rather than global channel position. This approach provides direct device targeting without needing to calculate global channel indices.\n\nSet the [doc:\/\/com.apple.documentation\/documentation\/AudioToolbox\/kAudioQueueProperty_ChannelAssignments] property with an [doc:\/\/com.apple.documentation\/documentation\/AudioToolbox\/AudioQueueChannelAssignment] structure specifying the target device UID and channel number:\n\n```swift\n\/\/ Get the device UID from `AVAudioSession`.\nlet audioSession = AVAudioSession.sharedInstance()\n\n\/\/ Configure and activate the audio session for multiroute setup.\n\nlet outputPort = audioSession.currentRoute.outputs.first!\n\n\/\/ Create the channel assignment for device channel 0.\nvar channelAssignment = AudioQueueChannelAssignment(\n    mDeviceUID: outputPort.uid as CFString,\n    mChannelNumber: 1  \/\/ 1-based indexing.\n)\n\n\/\/ Apply the channel assignments to `AudioQueue`\nAudioQueueSetProperty(\n    audioQueue,\n    kAudioQueueProperty_ChannelAssignments,\n    &channelAssignment,\n    UInt32(MemoryLayout<AudioQueueChannelAssignment>.size)\n)\n```\n\n## Route high-level audio an audio player or recorder\n\n[doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioPlayer] and [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioRecorder] provide a high-level approach using the audio player’s [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioPlayer\/channelAssignments], or the audio recorder’s [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioRecorder\/channelAssignments] property with [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioSessionChannelDescription] objects directly from the audio session.\n\n\n\nFor output [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioPlayer]:\n\n```swift\nlet audioSession = AVAudioSession.sharedInstance()\n\n\/\/ Configure and activate the audio session for multiroute setup.\n\nlet outputPort = audioSession.currentRoute.outputs.first!\nlet channelDescription = outputPort.channels!.first!\n\nlet player = try AVAudioPlayer(contentsOf: audioFileURL)\nplayer.channelAssignments = [channelDescription]\nplayer.play()\n```\n\nFor input with [doc:\/\/com.apple.avfaudio\/documentation\/AVFAudio\/AVAudioRecorder]:\n\n```swift\nlet audioSession = AVAudioSession.sharedInstance()\n\n\/\/ Configure and activate the audio session for multiroute setup.\n\nlet inputPort = audioSession.currentRoute.inputs.first!\nlet channelDescription = inputPort.channels!.first!\n\nlet recorder = try AVAudioRecorder(url: outputURL, settings: settings)\nrecorder.channelAssignments = [channelDescription]\nrecorder.record()\n```\n\n## System audio\n\n- **Handling audio interruptions**: Observe audio session notifications to ensure that your app responds appropriately to interruptions.\n- **Responding to audio route changes**: Observe audio session notifications to ensure that your app responds appropriately to route changes.\n- **Adding synthesized speech to calls**: Provide a more accessible experience by adding your app’s audio to a call.\n- **Capturing stereo audio from built-In microphones**: Configure an iOS device’s built-in microphones to add stereo recording capabilities to your app.\n- **AVAudioSession**: An object that communicates to the system how you intend to use audio in your app.\n- **AVAudioApplication**: An object that manages one or more audio sessions that belong to an app.\n- **AVAudioRoutingArbiter**: An object for configuring macOS apps to participate in AirPods Automatic Switching.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Observe audio session notifications to ensure that your app responds appropriately to interruptions.",
          "name" : "Handling audio interruptions",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFAudio\/handling-audio-interruptions"
        },
        {
          "description" : "Observe audio session notifications to ensure that your app responds appropriately to route changes.",
          "name" : "Responding to audio route changes",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFAudio\/responding-to-audio-route-changes"
        },
        {
          "description" : "Provide a more accessible experience by adding your app’s audio to a call.",
          "name" : "Adding synthesized speech to calls",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFAudio\/Adding-synthesized-speech-to-calls"
        },
        {
          "description" : "Configure an iOS device’s built-in microphones to add stereo recording capabilities to your app.",
          "name" : "Capturing stereo audio from built-In microphones",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFAudio\/capturing-stereo-audio-from-built-in-microphones"
        },
        {
          "description" : "An object that communicates to the system how you intend to use audio in your app.",
          "name" : "AVAudioSession",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFAudio\/AVAudioSession"
        },
        {
          "description" : "An object that manages one or more audio sessions that belong to an app.",
          "name" : "AVAudioApplication",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFAudio\/AVAudioApplication"
        },
        {
          "description" : "An object for configuring macOS apps to participate in AirPods Automatic Switching.",
          "name" : "AVAudioRoutingArbiter",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFAudio\/AVAudioRoutingArbiter"
        }
      ],
      "title" : "System audio"
    }
  ],
  "source" : "appleJSON",
  "title" : "Routing audio to specific devices in multidevice sessions",
  "url" : "https:\/\/developer.apple.com\/documentation\/avfaudio\/routing-audio-to-specific-devices-in-multidevice-sessions"
}