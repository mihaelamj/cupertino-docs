{
  "abstract" : "A block that subclasses use to send marker information to the host.",
  "codeExamples" : [

  ],
  "contentHash" : "8da670197335223932e8996f9a8ff6e18f45d44b1300db3f6c86802cb192500f",
  "crawledAt" : "2025-12-02T18:56:38Z",
  "declaration" : {
    "code" : "var speechSynthesisOutputMetadataBlock: AVSpeechSynthesisProviderOutputBlock? { get set }",
    "language" : "swift"
  },
  "id" : "13B97F14-B0C5-4EFE-88E9-0C62F502E331",
  "kind" : "property",
  "language" : "swift",
  "module" : "AVFAudio",
  "overview" : "## Discussion\n\nA host sets this block to retrieve metadata for a request.\n\nA synthesizer calls this method when it produces data relevant to the audio buffers it’s sending back to a host. In some cases, the system may delay speech output until it delivers these markers. For example, word highlighting depends on marker data from synthesizers to time what word to highlight. The array of markers can reference audio buffers that the system delivers at a later time.\n\nThere may be cases where a subclass doesn’t have marker data until it completes extra audio processing. If marker data changes, this block replaces that audio buffer range’s marker data.",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst",
    "macOS",
    "tvOS",
    "visionOS"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/AVFAudio\/AVSpeechSynthesisProviderAudioUnit\/speechSynthesisOutputMetadataBlock\ncrawled: 2025-12-02T18:56:38Z\n---\n\n# speechSynthesisOutputMetadataBlock\n\n**Instance Property**\n\nA block that subclasses use to send marker information to the host.\n\n## Declaration\n\n```swift\nvar speechSynthesisOutputMetadataBlock: AVSpeechSynthesisProviderOutputBlock? { get set }\n```\n\n## Discussion\n\nA host sets this block to retrieve metadata for a request.\n\nA synthesizer calls this method when it produces data relevant to the audio buffers it’s sending back to a host. In some cases, the system may delay speech output until it delivers these markers. For example, word highlighting depends on marker data from synthesizers to time what word to highlight. The array of markers can reference audio buffers that the system delivers at a later time.\n\nThere may be cases where a subclass doesn’t have marker data until it completes extra audio processing. If marker data changes, this block replaces that audio buffer range’s marker data.\n\n## Supplying metadata\n\n- **AVSpeechSynthesisProviderOutputBlock**: A type that represents the method for sending marker information to the host.\n- **AVSpeechSynthesisMarker**: An object that contains information about the synthesized audio.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "A type that represents the method for sending marker information to the host.",
          "name" : "AVSpeechSynthesisProviderOutputBlock",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFAudio\/AVSpeechSynthesisProviderOutputBlock"
        },
        {
          "description" : "An object that contains information about the synthesized audio.",
          "name" : "AVSpeechSynthesisMarker",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFAudio\/AVSpeechSynthesisMarker"
        }
      ],
      "title" : "Supplying metadata"
    }
  ],
  "source" : "appleJSON",
  "title" : "speechSynthesisOutputMetadataBlock",
  "url" : "https:\/\/developer.apple.com\/documentation\/AVFAudio\/AVSpeechSynthesisProviderAudioUnit\/speechSynthesisOutputMetadataBlock"
}