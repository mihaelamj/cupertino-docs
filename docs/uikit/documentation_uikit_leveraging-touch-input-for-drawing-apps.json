{
  "abstract" : "Capture touches as a series of strokes and render them efficiently on a drawing canvas.",
  "codeExamples" : [
    {
      "code" : "override func touchesBegan(_ touches: Set<UITouch>, with event: UIEvent?) {\n    if trackedTouch == nil {\n        trackedTouch = touches.first\n        initialTimestamp = trackedTouch?.timestamp\n        collectForce = trackedTouch!.type == .pencil || view?.traitCollection.forceTouchCapability == .available\n        if !isForPencil {\n            \/\/ Give other gestures, such as pan and pinch, a chance by\n            \/\/ slightly delaying the `.begin.\n            fingerStartTimer = Timer.scheduledTimer(\n                withTimeInterval: cancellationTimeInterval,\n                repeats: false,\n                block: { [weak self] (timer) in\n                    guard let strongSelf = self else { return }\n                    if strongSelf.state == .possible {\n                        strongSelf.state = .began\n                    }\n                }\n            )\n        }\n    }\n    if append(touches: touches, event: event) {\n        if isForPencil {\n            state = .began\n        }\n    }\n}\n\noverride func touchesMoved(_ touches: Set<UITouch>, with event: UIEvent?) {\n    if append(touches: touches, event: event) {\n        if state == .began {\n            state = .changed\n        }\n    }\n}\n\noverride func touchesEnded(_ touches: Set<UITouch>, with event: UIEvent?) {\n    if append(touches: touches, event: event) {\n        stroke.state = .done\n        state = .ended\n    }\n}\n\noverride func touchesCancelled(_ touches: Set<UITouch>, with event: UIEvent?) {\n    if append(touches: touches, event: event) {\n        stroke.state = .cancelled\n        state = .failed\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "self.fingerStrokeRecognizer = setupStrokeGestureRecognizer(isForPencil: false)\nself.pencilStrokeRecognizer = setupStrokeGestureRecognizer(isForPencil: true)",
      "language" : "swift"
    },
    {
      "code" : "func setupStrokeGestureRecognizer(isForPencil: Bool) -> StrokeGestureRecognizer {\n    let recognizer = StrokeGestureRecognizer(target: self, action: #selector(strokeUpdated(_:)))\n    recognizer.delegate = self\n    recognizer.cancelsTouchesInView = false\n    scrollView.addGestureRecognizer(recognizer)\n    recognizer.coordinateSpaceView = cgView\n    recognizer.isForPencil = isForPencil\n    return recognizer\n}",
      "language" : "swift"
    },
    {
      "code" : "var isForPencil: Bool = false {\n    didSet {\n        if isForPencil {\n            allowedTouchTypes = [UITouch.TouchType.pencil.rawValue as NSNumber]\n        } else {\n            allowedTouchTypes = [UITouch.TouchType.direct.rawValue as NSNumber]\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "var pencilMode = false {\n    didSet {\n        if pencilMode {\n            scrollView.panGestureRecognizer.minimumNumberOfTouches = 1\n            pencilButton.isHidden = false\n            if let view = fingerStrokeRecognizer.view {\n                view.removeGestureRecognizer(fingerStrokeRecognizer)\n            }\n        } else {\n            scrollView.panGestureRecognizer.minimumNumberOfTouches = 2\n            pencilButton.isHidden = true\n            if fingerStrokeRecognizer.view == nil {\n                scrollView.addGestureRecognizer(fingerStrokeRecognizer)\n            }\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "if collectsCoalescedTouches {\n    if let event = event {\n        let coalescedTouches = event.coalescedTouches(for: touchToAppend)!\n        let lastIndex = coalescedTouches.count - 1\n        for index in 0..<lastIndex {\n            saveStrokeSample(stroke: stroke, touch: coalescedTouches[index], view: view, coalesced: true, predicted: false)\n        }\n        saveStrokeSample(stroke: stroke, touch: coalescedTouches[lastIndex], view: view, coalesced: false, predicted: false)\n    }\n} else {\n    saveStrokeSample(stroke: stroke, touch: touchToAppend, view: view, coalesced: false, predicted: false)\n}",
      "language" : "swift"
    },
    {
      "code" : "if usesPredictedSamples && stroke.state == .active {\n    if let predictedTouches = event?.predictedTouches(for: touchToAppend) {\n        for touch in predictedTouches {\n            saveStrokeSample(stroke: stroke, touch: touch, view: view, coalesced: false, predicted: true)\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "override func touchesEstimatedPropertiesUpdated(_ touches: Set<UITouch>) {\n    for touch in touches {\n        guard let index = touch.estimationUpdateIndex else {\n            continue\n        }\n        if let (stroke, sampleIndex) = outstandingUpdateIndexes[Int(index.intValue)] {\n            var sample = stroke.samples[sampleIndex]\n            let expectedUpdates = sample.estimatedPropertiesExpectingUpdates\n            if expectedUpdates.contains(.force) {\n                sample.force = touch.force\n                if !touch.estimatedProperties.contains(.force) {\n                    \/\/ Only remove the estimate flag if the new value isn't estimated as well.\n                    sample.estimatedProperties.remove(.force)\n                }\n            }\n            sample.estimatedPropertiesExpectingUpdates = touch.estimatedPropertiesExpectingUpdates\n            if touch.estimatedPropertiesExpectingUpdates == [] {\n                outstandingUpdateIndexes.removeValue(forKey: sampleIndex)\n            }\n            stroke.update(sample: sample, at: sampleIndex)\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "func drawCalligraphy(in context: CGContext,\n                     toSample: StrokeSample,\n                     fromSample: StrokeSample,\n                     forceAccessBlock: (_ sample: StrokeSample) -> CGFloat) {\n\n    var fromAzimuthUnitVector = Stroke.calligraphyFallbackAzimuthUnitVector\n    var toAzimuthUnitVector = Stroke.calligraphyFallbackAzimuthUnitVector\n\n    if fromSample.azimuth != nil {\n\n        if lockedAzimuthUnitVector == nil {\n            lockedAzimuthUnitVector = fromSample.azimuthUnitVector\n        }\n        fromAzimuthUnitVector = fromSample.azimuthUnitVector\n        toAzimuthUnitVector = toSample.azimuthUnitVector\n        if fromSample.altitude! > azimuthLockAltitudeThreshold {\n            fromAzimuthUnitVector = lockedAzimuthUnitVector!\n        }\n        if toSample.altitude! > azimuthLockAltitudeThreshold {\n            toAzimuthUnitVector = lockedAzimuthUnitVector!\n        } else {\n            lockedAzimuthUnitVector = toAzimuthUnitVector\n        }\n\n    }\n    \/\/ Rotate 90 degrees\n    let calligraphyTransform = CGAffineTransform(rotationAngle: CGFloat.pi \/ 2.0)\n    fromAzimuthUnitVector = fromAzimuthUnitVector.applying(calligraphyTransform)\n    toAzimuthUnitVector = toAzimuthUnitVector.applying(calligraphyTransform)\n\n    let fromUnitVector = fromAzimuthUnitVector * forceAccessBlock(fromSample)\n    let toUnitVector = toAzimuthUnitVector * forceAccessBlock(toSample)\n\n    context.beginPath()\n    context.addLines(between: [\n        fromSample.location + fromUnitVector,\n        toSample.location + toUnitVector,\n        toSample.location - toUnitVector,\n        fromSample.location - fromUnitVector\n        ])\n    context.closePath()\n\n    context.drawPath(using: .fillStroke)\n\n}",
      "language" : "swift"
    },
    {
      "code" : "func drawDebugMarkings(in context: CGContext, fromSample: StrokeSample) {\n\n    let isEstimated = fromSample.estimatedProperties.contains(.azimuth)\n    guard displayOptions == .debug,\n        fromSample.predicted == false,\n        fromSample.azimuth != nil,\n        (!fromSample.coalesced || isEstimated) else {\n            return\n    }\n\n    let length = CGFloat(20.0)\n    let azimuthUnitVector = fromSample.azimuthUnitVector\n    let azimuthTarget = fromSample.location + azimuthUnitVector * length\n    let altitudeStart = azimuthTarget + (azimuthUnitVector * (length \/ -2.0))\n    let transformToApply = CGAffineTransform(rotationAngle: fromSample.altitude!)\n    let altitudeTarget = altitudeStart + (azimuthUnitVector * (length \/ 2.0)).applying(transformToApply)\n\n    \/\/ Draw altitude as black line coming from the center of the azimuth.\n    altitudeSettings(in: context)\n    context.beginPath()\n    context.move(to: altitudeStart)\n    context.addLine(to: altitudeTarget)\n    context.strokePath()\n\n    \/\/ Draw azimuth as blue (or orange if estimated) line.\n    azimuthSettings(in: context)\n    if isEstimated {\n        context.setStrokeColor(UIColor.orange.cgColor)\n    }\n    context.beginPath()\n    context.move(to: fromSample.location)\n    context.addLine(to: azimuthTarget)\n    context.strokePath()\n\n}",
      "language" : "swift"
    },
    {
      "code" : "func pencilInteractionDidTap(_ interaction: UIPencilInteraction) {\n    if UIPencilInteraction.preferredTapAction == .switchPrevious {\n        leftRingControl.switchToPreviousTool()\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "let pencilInteraction = UIPencilInteraction()\npencilInteraction.delegate = self\nview.addInteraction(pencilInteraction)",
      "language" : "swift"
    }
  ],
  "contentHash" : "0994e9aa0d91cb1daf25aebefa9354e58a6f36ed3d1357bc08c27083aa5658bb",
  "crawledAt" : "2025-12-02T15:52:23Z",
  "id" : "F9E97C5D-3ED3-4FB9-829B-838AD1C366C0",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "UIKit",
  "overview" : "## Overview\n\nThis sample project, Speed Sketch, shows how to render a series of strokes a user makes by moving their finger or Apple Pencil across the screen. The project also demonstrates how to:\n\n### Distinguish between finger and Apple Pencil touches\n\nWith Speed Sketch, users can draw by moving their finger or Apple Pencil across the screen, and the system reports the touches to the app. Speed Sketch captures these touches using the custom gesture recognizer, `StrokeGestureRecognizer`.\n\nThe stroke gesture recognizer receives each touch event, and appends data from the touches to an array of data samples.\n\nThe [doc:\/\/com.apple.uikit\/documentation\/UIKit\/UIGestureRecognizer\/touchesBegan(_:with:)] method sets a timer to give other gesture recognizers, such as pan and pinch, time to handle touches that the user makes with their finger. This behavior is unique to drawing apps such as Speed Sketch that start capturing and drawing a stroke the moment the user touches the screen.\n\nIn order to track finger and Apple Pencil touches separately, Speed Sketch uses two instances of `StrokeGestureRecognizer`: one for finger touches and the other to track touches coming from Apple Pencil.\n\nTo simplify the code, the app uses a helper method to create the gesture recognizers.\n\nThe gesture recognizer distinguishes between the touch types by setting the [doc:\/\/com.apple.uikit\/documentation\/UIKit\/UIGestureRecognizer\/allowedTouchTypes] property to [doc:\/\/com.apple.uikit\/documentation\/UIKit\/UITouch\/TouchType\/pencil] when tracking Apple Pencil touches, and to [doc:\/\/com.apple.uikit\/documentation\/UIKit\/UITouch\/TouchType\/direct] when tracking touches from a finger.\n\nEach time the user draws a stroke on the screen, the stroke gesture recognizer calls the `strokeUpdate(_:)` method. This method checks to see if the recognizer is for Apple Pencil, and if so, puts the app into pencil mode. While the app is in pencil mode, the user can use one finger to pan the drawing canvas. When the app isn’t in pencil mode—that is, when the user is drawing with their finger—the user uses two fingers to pan the drawing canvas.\n\n### Improve drawing performance\n\nThe stroke gesture recognizer receives touch events from the system. For most apps, the time span between each touch event is enough to handle the gesture. However, users of drawing apps expect greater precision, which requires the app to gather all the touches reported since the last delivered touch event.\n\nTo gather the additional touches, Speed Sketch calls the [doc:\/\/com.apple.uikit\/documentation\/UIKit\/UIEvent\/coalescedTouches(for:)] method. This method returns an array of [doc:\/\/com.apple.uikit\/documentation\/UIKit\/UITouch] objects representing the touches received by the system but not delivered in the last event. The additional touches allow Speed Sketch to provide a smoother rendering of the stroke by storing the coalesced touches as part of the stroke’s data sample.\n\nIn addition to coalesced touches, Speed Sketch uses touch data predicted by the system as a way to enhance the perceived accuracy of the app’s drawing capabilities. Speed Sketch retrieves the predicted touches by calling the [doc:\/\/com.apple.uikit\/documentation\/UIKit\/UIEvent\/predictedTouches(for:)] method for the event, then saves the touches as part of the data sample for the stroke.\n\nThe app replaces the predicted touches with actual touches as they become available.\n\n### Enhance for Apple Pencil\n\nApple Pencil can sense tilt (altitude), force (pressure), and orientation (azimuth), which drawing apps can use to affect the appearance of strokes. For instance, Speed Sketch uses azimuth to enhance the strokes when using the app’s Calligraphy drawing tool.\n\nSpeed Sketch also displays the altitude and azimuth data as part of the stroke when the user selects the Debug drawing tool.\n\nStarting with the second generation Apple Pencil, users can double-tap their finger on Apple Pencil to request an action from the app (see [doc:\/\/com.apple.uikit\/documentation\/UIKit\/apple-pencil-interactions] for more information). When possible, apps should honor the system settings for double-tap actions on Apple Pencil, which include:\n\nSpeed Sketch doesn’t have an eraser tool or color pallet, but it does have different drawing tools: Calligraphy, Ink, and Debug. When the user’s preferred double-tap action is [doc:\/\/com.apple.uikit\/documentation\/UIKit\/UIPencilPreferredAction\/switchPrevious] and the user double-taps their finger on Apple Pencil, Speed Sketch switches to the tool last used by the user. The sample app ignores the other preferred actions.\n\nIn order for Speed Sketch to receive the double-tap event, it adds a [doc:\/\/com.apple.uikit\/documentation\/UIKit\/UIPencilInteraction] object to the canvas view.\n\nFor additional information on supporting touch input in drawing apps and Apple Pencil, watch the WWDC 2016 session video [https:\/\/developer.apple.com\/videos\/play\/wwdc2016\/220] and the Tech Talks session video [https:\/\/developer.apple.com\/videos\/play\/tech-talks\/804\/].",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/UIKit\/leveraging-touch-input-for-drawing-apps\ncrawled: 2025-12-02T15:52:23Z\n---\n\n# Leveraging touch input for drawing apps\n\n**Sample Code**\n\nCapture touches as a series of strokes and render them efficiently on a drawing canvas.\n\n## Overview\n\nThis sample project, Speed Sketch, shows how to render a series of strokes a user makes by moving their finger or Apple Pencil across the screen. The project also demonstrates how to:\n\n- Distinguish between finger and Apple Pencil touches.\n- Improve drawing performance.\n- Enhance the app for Apple Pencil.\n\n### Distinguish between finger and Apple Pencil touches\n\nWith Speed Sketch, users can draw by moving their finger or Apple Pencil across the screen, and the system reports the touches to the app. Speed Sketch captures these touches using the custom gesture recognizer, `StrokeGestureRecognizer`.\n\nThe stroke gesture recognizer receives each touch event, and appends data from the touches to an array of data samples.\n\n```swift\noverride func touchesBegan(_ touches: Set<UITouch>, with event: UIEvent?) {\n    if trackedTouch == nil {\n        trackedTouch = touches.first\n        initialTimestamp = trackedTouch?.timestamp\n        collectForce = trackedTouch!.type == .pencil || view?.traitCollection.forceTouchCapability == .available\n        if !isForPencil {\n            \/\/ Give other gestures, such as pan and pinch, a chance by\n            \/\/ slightly delaying the `.begin.\n            fingerStartTimer = Timer.scheduledTimer(\n                withTimeInterval: cancellationTimeInterval,\n                repeats: false,\n                block: { [weak self] (timer) in\n                    guard let strongSelf = self else { return }\n                    if strongSelf.state == .possible {\n                        strongSelf.state = .began\n                    }\n                }\n            )\n        }\n    }\n    if append(touches: touches, event: event) {\n        if isForPencil {\n            state = .began\n        }\n    }\n}\n\noverride func touchesMoved(_ touches: Set<UITouch>, with event: UIEvent?) {\n    if append(touches: touches, event: event) {\n        if state == .began {\n            state = .changed\n        }\n    }\n}\n\noverride func touchesEnded(_ touches: Set<UITouch>, with event: UIEvent?) {\n    if append(touches: touches, event: event) {\n        stroke.state = .done\n        state = .ended\n    }\n}\n\noverride func touchesCancelled(_ touches: Set<UITouch>, with event: UIEvent?) {\n    if append(touches: touches, event: event) {\n        stroke.state = .cancelled\n        state = .failed\n    }\n}\n```\n\nThe [doc:\/\/com.apple.uikit\/documentation\/UIKit\/UIGestureRecognizer\/touchesBegan(_:with:)] method sets a timer to give other gesture recognizers, such as pan and pinch, time to handle touches that the user makes with their finger. This behavior is unique to drawing apps such as Speed Sketch that start capturing and drawing a stroke the moment the user touches the screen.\n\nIn order to track finger and Apple Pencil touches separately, Speed Sketch uses two instances of `StrokeGestureRecognizer`: one for finger touches and the other to track touches coming from Apple Pencil.\n\n```swift\nself.fingerStrokeRecognizer = setupStrokeGestureRecognizer(isForPencil: false)\nself.pencilStrokeRecognizer = setupStrokeGestureRecognizer(isForPencil: true)\n```\n\nTo simplify the code, the app uses a helper method to create the gesture recognizers.\n\n```swift\nfunc setupStrokeGestureRecognizer(isForPencil: Bool) -> StrokeGestureRecognizer {\n    let recognizer = StrokeGestureRecognizer(target: self, action: #selector(strokeUpdated(_:)))\n    recognizer.delegate = self\n    recognizer.cancelsTouchesInView = false\n    scrollView.addGestureRecognizer(recognizer)\n    recognizer.coordinateSpaceView = cgView\n    recognizer.isForPencil = isForPencil\n    return recognizer\n}\n```\n\nThe gesture recognizer distinguishes between the touch types by setting the [doc:\/\/com.apple.uikit\/documentation\/UIKit\/UIGestureRecognizer\/allowedTouchTypes] property to [doc:\/\/com.apple.uikit\/documentation\/UIKit\/UITouch\/TouchType\/pencil] when tracking Apple Pencil touches, and to [doc:\/\/com.apple.uikit\/documentation\/UIKit\/UITouch\/TouchType\/direct] when tracking touches from a finger.\n\n```swift\nvar isForPencil: Bool = false {\n    didSet {\n        if isForPencil {\n            allowedTouchTypes = [UITouch.TouchType.pencil.rawValue as NSNumber]\n        } else {\n            allowedTouchTypes = [UITouch.TouchType.direct.rawValue as NSNumber]\n        }\n    }\n}\n```\n\nEach time the user draws a stroke on the screen, the stroke gesture recognizer calls the `strokeUpdate(_:)` method. This method checks to see if the recognizer is for Apple Pencil, and if so, puts the app into pencil mode. While the app is in pencil mode, the user can use one finger to pan the drawing canvas. When the app isn’t in pencil mode—that is, when the user is drawing with their finger—the user uses two fingers to pan the drawing canvas.\n\n```swift\nvar pencilMode = false {\n    didSet {\n        if pencilMode {\n            scrollView.panGestureRecognizer.minimumNumberOfTouches = 1\n            pencilButton.isHidden = false\n            if let view = fingerStrokeRecognizer.view {\n                view.removeGestureRecognizer(fingerStrokeRecognizer)\n            }\n        } else {\n            scrollView.panGestureRecognizer.minimumNumberOfTouches = 2\n            pencilButton.isHidden = true\n            if fingerStrokeRecognizer.view == nil {\n                scrollView.addGestureRecognizer(fingerStrokeRecognizer)\n            }\n        }\n    }\n}\n```\n\n### Improve drawing performance\n\nThe stroke gesture recognizer receives touch events from the system. For most apps, the time span between each touch event is enough to handle the gesture. However, users of drawing apps expect greater precision, which requires the app to gather all the touches reported since the last delivered touch event.\n\nTo gather the additional touches, Speed Sketch calls the [doc:\/\/com.apple.uikit\/documentation\/UIKit\/UIEvent\/coalescedTouches(for:)] method. This method returns an array of [doc:\/\/com.apple.uikit\/documentation\/UIKit\/UITouch] objects representing the touches received by the system but not delivered in the last event. The additional touches allow Speed Sketch to provide a smoother rendering of the stroke by storing the coalesced touches as part of the stroke’s data sample.\n\n```swift\nif collectsCoalescedTouches {\n    if let event = event {\n        let coalescedTouches = event.coalescedTouches(for: touchToAppend)!\n        let lastIndex = coalescedTouches.count - 1\n        for index in 0..<lastIndex {\n            saveStrokeSample(stroke: stroke, touch: coalescedTouches[index], view: view, coalesced: true, predicted: false)\n        }\n        saveStrokeSample(stroke: stroke, touch: coalescedTouches[lastIndex], view: view, coalesced: false, predicted: false)\n    }\n} else {\n    saveStrokeSample(stroke: stroke, touch: touchToAppend, view: view, coalesced: false, predicted: false)\n}\n```\n\nIn addition to coalesced touches, Speed Sketch uses touch data predicted by the system as a way to enhance the perceived accuracy of the app’s drawing capabilities. Speed Sketch retrieves the predicted touches by calling the [doc:\/\/com.apple.uikit\/documentation\/UIKit\/UIEvent\/predictedTouches(for:)] method for the event, then saves the touches as part of the data sample for the stroke.\n\n```swift\nif usesPredictedSamples && stroke.state == .active {\n    if let predictedTouches = event?.predictedTouches(for: touchToAppend) {\n        for touch in predictedTouches {\n            saveStrokeSample(stroke: stroke, touch: touch, view: view, coalesced: false, predicted: true)\n        }\n    }\n}\n```\n\nThe app replaces the predicted touches with actual touches as they become available.\n\n```swift\noverride func touchesEstimatedPropertiesUpdated(_ touches: Set<UITouch>) {\n    for touch in touches {\n        guard let index = touch.estimationUpdateIndex else {\n            continue\n        }\n        if let (stroke, sampleIndex) = outstandingUpdateIndexes[Int(index.intValue)] {\n            var sample = stroke.samples[sampleIndex]\n            let expectedUpdates = sample.estimatedPropertiesExpectingUpdates\n            if expectedUpdates.contains(.force) {\n                sample.force = touch.force\n                if !touch.estimatedProperties.contains(.force) {\n                    \/\/ Only remove the estimate flag if the new value isn't estimated as well.\n                    sample.estimatedProperties.remove(.force)\n                }\n            }\n            sample.estimatedPropertiesExpectingUpdates = touch.estimatedPropertiesExpectingUpdates\n            if touch.estimatedPropertiesExpectingUpdates == [] {\n                outstandingUpdateIndexes.removeValue(forKey: sampleIndex)\n            }\n            stroke.update(sample: sample, at: sampleIndex)\n        }\n    }\n}\n```\n\n### Enhance for Apple Pencil\n\nApple Pencil can sense tilt (altitude), force (pressure), and orientation (azimuth), which drawing apps can use to affect the appearance of strokes. For instance, Speed Sketch uses azimuth to enhance the strokes when using the app’s Calligraphy drawing tool.\n\n```swift\nfunc drawCalligraphy(in context: CGContext,\n                     toSample: StrokeSample,\n                     fromSample: StrokeSample,\n                     forceAccessBlock: (_ sample: StrokeSample) -> CGFloat) {\n\n    var fromAzimuthUnitVector = Stroke.calligraphyFallbackAzimuthUnitVector\n    var toAzimuthUnitVector = Stroke.calligraphyFallbackAzimuthUnitVector\n\n    if fromSample.azimuth != nil {\n\n        if lockedAzimuthUnitVector == nil {\n            lockedAzimuthUnitVector = fromSample.azimuthUnitVector\n        }\n        fromAzimuthUnitVector = fromSample.azimuthUnitVector\n        toAzimuthUnitVector = toSample.azimuthUnitVector\n        if fromSample.altitude! > azimuthLockAltitudeThreshold {\n            fromAzimuthUnitVector = lockedAzimuthUnitVector!\n        }\n        if toSample.altitude! > azimuthLockAltitudeThreshold {\n            toAzimuthUnitVector = lockedAzimuthUnitVector!\n        } else {\n            lockedAzimuthUnitVector = toAzimuthUnitVector\n        }\n\n    }\n    \/\/ Rotate 90 degrees\n    let calligraphyTransform = CGAffineTransform(rotationAngle: CGFloat.pi \/ 2.0)\n    fromAzimuthUnitVector = fromAzimuthUnitVector.applying(calligraphyTransform)\n    toAzimuthUnitVector = toAzimuthUnitVector.applying(calligraphyTransform)\n\n    let fromUnitVector = fromAzimuthUnitVector * forceAccessBlock(fromSample)\n    let toUnitVector = toAzimuthUnitVector * forceAccessBlock(toSample)\n\n    context.beginPath()\n    context.addLines(between: [\n        fromSample.location + fromUnitVector,\n        toSample.location + toUnitVector,\n        toSample.location - toUnitVector,\n        fromSample.location - fromUnitVector\n        ])\n    context.closePath()\n\n    context.drawPath(using: .fillStroke)\n\n}\n```\n\nSpeed Sketch also displays the altitude and azimuth data as part of the stroke when the user selects the Debug drawing tool.\n\n```swift\nfunc drawDebugMarkings(in context: CGContext, fromSample: StrokeSample) {\n\n    let isEstimated = fromSample.estimatedProperties.contains(.azimuth)\n    guard displayOptions == .debug,\n        fromSample.predicted == false,\n        fromSample.azimuth != nil,\n        (!fromSample.coalesced || isEstimated) else {\n            return\n    }\n\n    let length = CGFloat(20.0)\n    let azimuthUnitVector = fromSample.azimuthUnitVector\n    let azimuthTarget = fromSample.location + azimuthUnitVector * length\n    let altitudeStart = azimuthTarget + (azimuthUnitVector * (length \/ -2.0))\n    let transformToApply = CGAffineTransform(rotationAngle: fromSample.altitude!)\n    let altitudeTarget = altitudeStart + (azimuthUnitVector * (length \/ 2.0)).applying(transformToApply)\n\n    \/\/ Draw altitude as black line coming from the center of the azimuth.\n    altitudeSettings(in: context)\n    context.beginPath()\n    context.move(to: altitudeStart)\n    context.addLine(to: altitudeTarget)\n    context.strokePath()\n\n    \/\/ Draw azimuth as blue (or orange if estimated) line.\n    azimuthSettings(in: context)\n    if isEstimated {\n        context.setStrokeColor(UIColor.orange.cgColor)\n    }\n    context.beginPath()\n    context.move(to: fromSample.location)\n    context.addLine(to: azimuthTarget)\n    context.strokePath()\n\n}\n```\n\nStarting with the second generation Apple Pencil, users can double-tap their finger on Apple Pencil to request an action from the app (see [doc:\/\/com.apple.uikit\/documentation\/UIKit\/apple-pencil-interactions] for more information). When possible, apps should honor the system settings for double-tap actions on Apple Pencil, which include:\n\n- Switching to the eraser or the last used tool\n- Displaying a color pallet\n- Ignoring double tap\n\nSpeed Sketch doesn’t have an eraser tool or color pallet, but it does have different drawing tools: Calligraphy, Ink, and Debug. When the user’s preferred double-tap action is [doc:\/\/com.apple.uikit\/documentation\/UIKit\/UIPencilPreferredAction\/switchPrevious] and the user double-taps their finger on Apple Pencil, Speed Sketch switches to the tool last used by the user. The sample app ignores the other preferred actions.\n\n```swift\nfunc pencilInteractionDidTap(_ interaction: UIPencilInteraction) {\n    if UIPencilInteraction.preferredTapAction == .switchPrevious {\n        leftRingControl.switchToPreviousTool()\n    }\n}\n```\n\nIn order for Speed Sketch to receive the double-tap event, it adds a [doc:\/\/com.apple.uikit\/documentation\/UIKit\/UIPencilInteraction] object to the canvas view.\n\n```swift\nlet pencilInteraction = UIPencilInteraction()\npencilInteraction.delegate = self\nview.addInteraction(pencilInteraction)\n```\n\nFor additional information on supporting touch input in drawing apps and Apple Pencil, watch the WWDC 2016 session video [https:\/\/developer.apple.com\/videos\/play\/wwdc2016\/220] and the Tech Talks session video [https:\/\/developer.apple.com\/videos\/play\/tech-talks\/804\/].\n\n## Touches\n\n- **Handling touches in your view**: Use touch events directly on a view subclass if touch handling is intricately linked to the view’s content.\n- **Handling input from Apple Pencil**: Learn how to detect and respond to touches from Apple Pencil.\n- **Tracking the force of 3D Touch events**: Manipulate your content based on the force of touches.\n- **Illustrating the force, altitude, and azimuth properties of touch input**: Capture Apple Pencil and touch input in views.\n- **UITouch**: An object representing the location, size, movement, and force of a touch occurring on the screen.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Use touch events directly on a view subclass if touch handling is intricately linked to the view’s content.",
          "name" : "Handling touches in your view",
          "url" : "https:\/\/developer.apple.com\/documentation\/UIKit\/handling-touches-in-your-view"
        },
        {
          "description" : "Learn how to detect and respond to touches from Apple Pencil.",
          "name" : "Handling input from Apple Pencil",
          "url" : "https:\/\/developer.apple.com\/documentation\/UIKit\/handling-input-from-apple-pencil"
        },
        {
          "description" : "Manipulate your content based on the force of touches.",
          "name" : "Tracking the force of 3D Touch events",
          "url" : "https:\/\/developer.apple.com\/documentation\/UIKit\/tracking-the-force-of-3d-touch-events"
        },
        {
          "description" : "Capture Apple Pencil and touch input in views.",
          "name" : "Illustrating the force, altitude, and azimuth properties of touch input",
          "url" : "https:\/\/developer.apple.com\/documentation\/UIKit\/illustrating-the-force-altitude-and-azimuth-properties-of-touch-input"
        },
        {
          "description" : "An object representing the location, size, movement, and force of a touch occurring on the screen.",
          "name" : "UITouch",
          "url" : "https:\/\/developer.apple.com\/documentation\/UIKit\/UITouch"
        }
      ],
      "title" : "Touches"
    }
  ],
  "source" : "appleJSON",
  "title" : "Leveraging touch input for drawing apps",
  "url" : "https:\/\/developer.apple.com\/documentation\/UIKit\/leveraging-touch-input-for-drawing-apps"
}