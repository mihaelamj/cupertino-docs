{
  "abstract" : "Access texture data from the CPU by copying it to a buffer.",
  "codeExamples" : [
    {
      "code" : "_view.framebufferOnly = NO;\n((CAMetalLayer*)_view.layer).allowsNextDrawableTimeout = NO;\n_view.colorPixelFormat = MTLPixelFormatBGRA8Unorm;",
      "language" : "objective-c"
    },
    {
      "code" : "CGPoint bottomUpPixelPosition = [_view convertPointToBacking:event.locationInWindow];\nCGPoint topDownPixelPosition = CGPointMake(bottomUpPixelPosition.x,\n                                           _view.drawableSize.height - bottomUpPixelPosition.y);",
      "language" : "objective-c"
    },
    {
      "code" : "- (CGPoint)pointToBacking:(CGPoint)point\n{\n    CGFloat scale = _view.contentScaleFactor;\n\n    CGPoint pixel;\n\n    pixel.x = point.x * scale;\n    pixel.y = point.y * scale;\n\n    \/\/ Round the pixel values down to put them on a well-defined grid.\n    pixel.x = (int64_t)pixel.x;\n    pixel.y = (int64_t)pixel.y;\n\n    \/\/ Add .5 to move to the center of the pixel.\n    pixel.x += 0.5f;\n    pixel.y += 0.5f;\n\n    return pixel;\n}",
      "language" : "objective-c"
    },
    {
      "code" : "\nid<MTLCommandBuffer> commandBuffer = [_commandQueue commandBuffer];\n\n\/\/ Encode a render pass to render the image to the drawable texture.\n[self drawScene:view withCommandBuffer:commandBuffer];\n",
      "language" : "objective-c"
    },
    {
      "code" : "id<MTLTexture> readTexture = view.currentDrawable.texture;\n\nMTLOrigin readOrigin = MTLOriginMake(region.origin.x, region.origin.y, 0);\nMTLSize readSize = MTLSizeMake(region.size.width, region.size.height, 1);\n\nconst id<MTLBuffer> pixelBuffer = [self readPixelsWithCommandBuffer:commandBuffer\n                                                        fromTexture:readTexture\n                                                           atOrigin:readOrigin\n                                                           withSize:readSize];",
      "language" : "objective-c"
    },
    {
      "code" : "NSUInteger bytesPerPixel = sizeofPixelFormat(texture.pixelFormat);\nNSUInteger bytesPerRow   = size.width * bytesPerPixel;\nNSUInteger bytesPerImage = size.height * bytesPerRow;\n\n_readBuffer = [texture.device newBufferWithLength:bytesPerImage options:MTLResourceStorageModeShared];",
      "language" : "objective-c"
    },
    {
      "code" : "id <MTLBlitCommandEncoder> blitEncoder = [commandBuffer blitCommandEncoder];\n\n[blitEncoder copyFromTexture:texture\n                 sourceSlice:0\n                 sourceLevel:0\n                sourceOrigin:origin\n                  sourceSize:size\n                    toBuffer:_readBuffer\n           destinationOffset:0\n      destinationBytesPerRow:bytesPerRow\n    destinationBytesPerImage:bytesPerImage];\n\n[blitEncoder endEncoding];\n",
      "language" : "objective-c"
    },
    {
      "code" : "[commandBuffer commit];\n\n\/\/ The app needs to wait for the GPU to complete the blit pass before it can\n\/\/ read data from _readBuffer.\n[commandBuffer waitUntilCompleted];",
      "language" : "objective-c"
    },
    {
      "code" : "AAPLPixelBGRA8Unorm *pixels = (AAPLPixelBGRA8Unorm *)pixelBuffer.contents;",
      "language" : "objective-c"
    },
    {
      "code" : "\/\/ Create an `NSData` object and initialize it with the pixel data.\n\/\/ Use the CPU to copy the pixel data from the `pixelBuffer.contents`\n\/\/ pointer to `data`.\nNSData *data = [[NSData alloc] initWithBytes:pixels length:pixelBuffer.length];\n\n\/\/ Create a new image from the pixel data.\nAAPLImage *image = [[AAPLImage alloc] initWithBGRA8UnormData:data\n                                                       width:readSize.width\n                                                      height:readSize.height];",
      "language" : "objective-c"
    }
  ],
  "contentHash" : "b60d8886d6ca271eb0d9aed6090cfbf6bc2ec56ca0584bbdfada871078f30d00",
  "crawledAt" : "2025-12-02T15:49:35Z",
  "id" : "253CAAB8-7590-4B82-9B44-7D789E70EE14",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "Metal",
  "overview" : "## Overview\n\nMetal optimizes textures for fast access by the GPU, but it doesn’t allow you to directly access a texture’s contents from the CPU. When your app code needs to change or read a texture’s contents, you use Metal to copy data between textures and CPU-accessible memory — either system memory or a Metal buffer allocated using shared storage. This sample configures drawable textures for read access and copies rendered pixel data from those textures to a Metal buffer.\n\nRun the sample, then tap or click on a single point to read the pixel data stored at that point. Alternatively, drag out a rectangle to capture pixel data for a region on the screen. The sample converts your selection to a rectangle in the drawable texture’s coordinate system. Next, it renders an image to the texture. Finally, it copies the pixel data from the selected rectangle into a buffer for the sample to process further.\n\n### Configure the drawable texture for read access\n\nBy default, MetalKit views create drawable textures for rendering only, so other Metal commands can’t access the texture. The code below creates a view whose textures include read access. Because the sample needs to get a texture whenever the user selects part of the view, the code configures the view’s Metal layer to wait indefinitely for a new drawable.\n\nBecause configuring the drawable textures for read access means that Metal may not apply some optimizations, only change the drawable configuration when necessary. For similar reasons, don’t configure the view to wait indefinitely in performance-sensitive apps.\n\n### Determine which pixels to copy\n\nThe `AAPLViewController` class manages user interaction. When a user interacts with a view, AppKit and UIKit send events with positions specified in the view’s coordinate system. To determine which pixels to copy from the Metal drawable texture, the app transforms these view coordinates into the Metal texture coordinate system.\n\nBecause of differences in graphics coordinate systems and APIs, the code to convert between view coordinates and texture coordinates varies by platform.\n\nIn macOS, the code calls the `pointToBacking:` method on the view to convert a position into a pixel location in the backing store, and then applies a coordinate transformation to adjust the origin and the y-axis.\n\nIn iOS, the app reads the view’s `contentScaleFactor` and applies a scaling transform to the view coordinate. iOS views and Metal textures use the same coordinate conventions, so the code doesn’t move the origin or change the y-axis orientation.\n\n### Render the pixel data\n\nWhen the user selects a rectangle in the view, the view controller calls the `renderAndReadPixelsFromView:withRegion` method to render the drawable’s contents and copy them to a Metal buffer.\n\nIt creates a new command buffer and calls a utility method to encode a render pass. The specific rendered image isn’t important to this sample.\n\nAfter encoding the render pass, it calls another method to encode commands to copy a section of the rendered texture. The sample encodes the commands to copy the pixel data before presenting the drawable texture because the system discards the texture’s contents after presenting it.\n\n### Copy pixel data to a buffer\n\nThe renderer’s `readPixelsWithCommandBuffer:fromTexture:atOrigin:withSize:` method encodes the commands to copy the texture. Because the sample passes the same command buffer into this method, Metal encodes these new commands after the render pass. Metal automatically manages the dependencies on the destination texture, and ensures that rendering completes before copying the texture data.\n\nFirst, the method allocates a Metal buffer to hold the pixel data. It calculates the size of the buffer by multiplying the size of one pixel in bytes by the region’s width and height. Similarly, the code calculates the number of bytes per row, which the code needs later when copying the data. The sample doesn’t add any padding at the end of rows. Then, it calls the Metal device object to create the new Metal buffer, specifying a shared storage mode so that the app can read the buffer’s contents afterwards.\n\nNext, the method creates an [doc:\/\/com.apple.metal\/documentation\/Metal\/MTLBlitCommandEncoder], which provides commands that copy data between Metal resources, fill resources with data, and perform other similar resource-related tasks that don’t directly involve computation or rendering. The sample encodes a blit command to copy the texture data to the beginning of the new buffer. It then ends the blit pass.\n\nFinally, it commits the command buffer and calls [doc:\/\/com.apple.metal\/documentation\/Metal\/MTLCommandBuffer\/waitUntilCompleted()] to immediately wait for the GPU to finish executing the rendering and blit commands. After this call returns control to the method, the buffer contains the requested pixel data. In a real-time app, synchronizing commands unnecessarily reduces parallelism between the CPU and GPU; this sample synchronizes in this way to simplify the code.\n\n### Read the pixels from the buffer\n\nThe app calls the buffer’s [doc:\/\/com.apple.metal\/documentation\/Metal\/MTLBuffer\/contents()] method to get a pointer to the pixel data.\n\nThe sample copies the buffer’s data into an [doc:\/\/com.apple.documentation\/documentation\/Foundation\/NSData] object and passes it to another method to initialize an `AAPLImage` object. For more information on `AAPLImage`, see [doc:\/\/com.apple.metal\/documentation\/Metal\/creating-and-sampling-textures].\n\nThe renderer returns this image object to the view controller for further processing. The view controller’s behavior varies depending on the operating system. In MacOS, the sample writes the image to the file  `~\/Desktop\/ReadPixelsImage.tga`, while in iOS, the sample adds the image to the Photos library. The view controller performs this processing without using Metal, so the steps it takes aren’t important to this sample.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/Metal\/reading-pixel-data-from-a-drawable-texture\ncrawled: 2025-12-02T15:49:35Z\n---\n\n# Reading pixel data from a drawable texture\n\n**Sample Code**\n\nAccess texture data from the CPU by copying it to a buffer.\n\n## Overview\n\nMetal optimizes textures for fast access by the GPU, but it doesn’t allow you to directly access a texture’s contents from the CPU. When your app code needs to change or read a texture’s contents, you use Metal to copy data between textures and CPU-accessible memory — either system memory or a Metal buffer allocated using shared storage. This sample configures drawable textures for read access and copies rendered pixel data from those textures to a Metal buffer.\n\nRun the sample, then tap or click on a single point to read the pixel data stored at that point. Alternatively, drag out a rectangle to capture pixel data for a region on the screen. The sample converts your selection to a rectangle in the drawable texture’s coordinate system. Next, it renders an image to the texture. Finally, it copies the pixel data from the selected rectangle into a buffer for the sample to process further.\n\n### Configure the drawable texture for read access\n\nBy default, MetalKit views create drawable textures for rendering only, so other Metal commands can’t access the texture. The code below creates a view whose textures include read access. Because the sample needs to get a texture whenever the user selects part of the view, the code configures the view’s Metal layer to wait indefinitely for a new drawable.\n\n```objective-c\n_view.framebufferOnly = NO;\n((CAMetalLayer*)_view.layer).allowsNextDrawableTimeout = NO;\n_view.colorPixelFormat = MTLPixelFormatBGRA8Unorm;\n```\n\nBecause configuring the drawable textures for read access means that Metal may not apply some optimizations, only change the drawable configuration when necessary. For similar reasons, don’t configure the view to wait indefinitely in performance-sensitive apps.\n\n### Determine which pixels to copy\n\nThe `AAPLViewController` class manages user interaction. When a user interacts with a view, AppKit and UIKit send events with positions specified in the view’s coordinate system. To determine which pixels to copy from the Metal drawable texture, the app transforms these view coordinates into the Metal texture coordinate system.\n\nBecause of differences in graphics coordinate systems and APIs, the code to convert between view coordinates and texture coordinates varies by platform.\n\nIn macOS, the code calls the `pointToBacking:` method on the view to convert a position into a pixel location in the backing store, and then applies a coordinate transformation to adjust the origin and the y-axis.\n\n```objective-c\nCGPoint bottomUpPixelPosition = [_view convertPointToBacking:event.locationInWindow];\nCGPoint topDownPixelPosition = CGPointMake(bottomUpPixelPosition.x,\n                                           _view.drawableSize.height - bottomUpPixelPosition.y);\n```\n\nIn iOS, the app reads the view’s `contentScaleFactor` and applies a scaling transform to the view coordinate. iOS views and Metal textures use the same coordinate conventions, so the code doesn’t move the origin or change the y-axis orientation.\n\n```objective-c\n- (CGPoint)pointToBacking:(CGPoint)point\n{\n    CGFloat scale = _view.contentScaleFactor;\n\n    CGPoint pixel;\n\n    pixel.x = point.x * scale;\n    pixel.y = point.y * scale;\n\n    \/\/ Round the pixel values down to put them on a well-defined grid.\n    pixel.x = (int64_t)pixel.x;\n    pixel.y = (int64_t)pixel.y;\n\n    \/\/ Add .5 to move to the center of the pixel.\n    pixel.x += 0.5f;\n    pixel.y += 0.5f;\n\n    return pixel;\n}\n```\n\n### Render the pixel data\n\nWhen the user selects a rectangle in the view, the view controller calls the `renderAndReadPixelsFromView:withRegion` method to render the drawable’s contents and copy them to a Metal buffer.\n\nIt creates a new command buffer and calls a utility method to encode a render pass. The specific rendered image isn’t important to this sample.\n\n```objective-c\n\nid<MTLCommandBuffer> commandBuffer = [_commandQueue commandBuffer];\n\n\/\/ Encode a render pass to render the image to the drawable texture.\n[self drawScene:view withCommandBuffer:commandBuffer];\n\n```\n\nAfter encoding the render pass, it calls another method to encode commands to copy a section of the rendered texture. The sample encodes the commands to copy the pixel data before presenting the drawable texture because the system discards the texture’s contents after presenting it.\n\n```objective-c\nid<MTLTexture> readTexture = view.currentDrawable.texture;\n\nMTLOrigin readOrigin = MTLOriginMake(region.origin.x, region.origin.y, 0);\nMTLSize readSize = MTLSizeMake(region.size.width, region.size.height, 1);\n\nconst id<MTLBuffer> pixelBuffer = [self readPixelsWithCommandBuffer:commandBuffer\n                                                        fromTexture:readTexture\n                                                           atOrigin:readOrigin\n                                                           withSize:readSize];\n```\n\n### Copy pixel data to a buffer\n\nThe renderer’s `readPixelsWithCommandBuffer:fromTexture:atOrigin:withSize:` method encodes the commands to copy the texture. Because the sample passes the same command buffer into this method, Metal encodes these new commands after the render pass. Metal automatically manages the dependencies on the destination texture, and ensures that rendering completes before copying the texture data.\n\nFirst, the method allocates a Metal buffer to hold the pixel data. It calculates the size of the buffer by multiplying the size of one pixel in bytes by the region’s width and height. Similarly, the code calculates the number of bytes per row, which the code needs later when copying the data. The sample doesn’t add any padding at the end of rows. Then, it calls the Metal device object to create the new Metal buffer, specifying a shared storage mode so that the app can read the buffer’s contents afterwards.\n\n```objective-c\nNSUInteger bytesPerPixel = sizeofPixelFormat(texture.pixelFormat);\nNSUInteger bytesPerRow   = size.width * bytesPerPixel;\nNSUInteger bytesPerImage = size.height * bytesPerRow;\n\n_readBuffer = [texture.device newBufferWithLength:bytesPerImage options:MTLResourceStorageModeShared];\n```\n\nNext, the method creates an [doc:\/\/com.apple.metal\/documentation\/Metal\/MTLBlitCommandEncoder], which provides commands that copy data between Metal resources, fill resources with data, and perform other similar resource-related tasks that don’t directly involve computation or rendering. The sample encodes a blit command to copy the texture data to the beginning of the new buffer. It then ends the blit pass.\n\n```objective-c\nid <MTLBlitCommandEncoder> blitEncoder = [commandBuffer blitCommandEncoder];\n\n[blitEncoder copyFromTexture:texture\n                 sourceSlice:0\n                 sourceLevel:0\n                sourceOrigin:origin\n                  sourceSize:size\n                    toBuffer:_readBuffer\n           destinationOffset:0\n      destinationBytesPerRow:bytesPerRow\n    destinationBytesPerImage:bytesPerImage];\n\n[blitEncoder endEncoding];\n\n```\n\nFinally, it commits the command buffer and calls [doc:\/\/com.apple.metal\/documentation\/Metal\/MTLCommandBuffer\/waitUntilCompleted()] to immediately wait for the GPU to finish executing the rendering and blit commands. After this call returns control to the method, the buffer contains the requested pixel data. In a real-time app, synchronizing commands unnecessarily reduces parallelism between the CPU and GPU; this sample synchronizes in this way to simplify the code.\n\n```objective-c\n[commandBuffer commit];\n\n\/\/ The app needs to wait for the GPU to complete the blit pass before it can\n\/\/ read data from _readBuffer.\n[commandBuffer waitUntilCompleted];\n```\n\n### Read the pixels from the buffer\n\nThe app calls the buffer’s [doc:\/\/com.apple.metal\/documentation\/Metal\/MTLBuffer\/contents()] method to get a pointer to the pixel data.\n\n```objective-c\nAAPLPixelBGRA8Unorm *pixels = (AAPLPixelBGRA8Unorm *)pixelBuffer.contents;\n```\n\nThe sample copies the buffer’s data into an [doc:\/\/com.apple.documentation\/documentation\/Foundation\/NSData] object and passes it to another method to initialize an `AAPLImage` object. For more information on `AAPLImage`, see [doc:\/\/com.apple.metal\/documentation\/Metal\/creating-and-sampling-textures].\n\n```objective-c\n\/\/ Create an `NSData` object and initialize it with the pixel data.\n\/\/ Use the CPU to copy the pixel data from the `pixelBuffer.contents`\n\/\/ pointer to `data`.\nNSData *data = [[NSData alloc] initWithBytes:pixels length:pixelBuffer.length];\n\n\/\/ Create a new image from the pixel data.\nAAPLImage *image = [[AAPLImage alloc] initWithBGRA8UnormData:data\n                                                       width:readSize.width\n                                                      height:readSize.height];\n```\n\nThe renderer returns this image object to the view controller for further processing. The view controller’s behavior varies depending on the operating system. In MacOS, the sample writes the image to the file  `~\/Desktop\/ReadPixelsImage.tga`, while in iOS, the sample adds the image to the Photos library. The view controller performs this processing without using Metal, so the steps it takes aren’t important to this sample.\n\n## Textures\n\n- **Processing a texture in a compute function**: Create textures by running copy and dispatch commands in a compute pass on a GPU.\n- **Creating and sampling textures**: Load image data into a texture and apply it to a quadrangle.\n- **Streaming large images with Metal sparse textures**: Limit texture memory usage for large textures by loading or unloading image detail on the basis of MIP and tile region.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Create textures by running copy and dispatch commands in a compute pass on a GPU.",
          "name" : "Processing a texture in a compute function",
          "url" : "https:\/\/developer.apple.com\/documentation\/Metal\/processing-a-texture-in-a-compute-function"
        },
        {
          "description" : "Load image data into a texture and apply it to a quadrangle.",
          "name" : "Creating and sampling textures",
          "url" : "https:\/\/developer.apple.com\/documentation\/Metal\/creating-and-sampling-textures"
        },
        {
          "description" : "Limit texture memory usage for large textures by loading or unloading image detail on the basis of MIP and tile region.",
          "name" : "Streaming large images with Metal sparse textures",
          "url" : "https:\/\/developer.apple.com\/documentation\/Metal\/streaming-large-images-with-metal-sparse-textures"
        }
      ],
      "title" : "Textures"
    }
  ],
  "source" : "appleJSON",
  "title" : "Reading pixel data from a drawable texture",
  "url" : "https:\/\/developer.apple.com\/documentation\/Metal\/reading-pixel-data-from-a-drawable-texture"
}