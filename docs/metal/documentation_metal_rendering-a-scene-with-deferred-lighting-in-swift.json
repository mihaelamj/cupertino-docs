{
  "abstract" : "Avoid expensive lighting calculations by implementing a deferred lighting renderer optimized for immediate mode and tile-based deferred renderer GPUs.",
  "codeExamples" : [
    {
      "code" : "#define USE_EYE_DEPTH              1\n#define LIGHT_STENCIL_CULLING      1"
    },
    {
      "code" : "encodePass(into: commandBuffer, using: gBufferAndLightingPassDescriptor, label: \"GBuffer & Lighting Pass\") { renderEncoder in\n\n    encodeGBufferStage(using: renderEncoder)\n    encodeDirectionalLightingStage(using: renderEncoder)\n    encodeLightMaskStage(using: renderEncoder)\n    encodePointLightStage(using: renderEncoder)\n    encodeSkyboxStage(using: renderEncoder)\n    encodeFairyBillboardStage(using: renderEncoder)\n}",
      "language" : "swift"
    },
    {
      "code" : "encodePass(into: commandBuffer,\n           using: gBufferPassDescriptor,\n           label: \"GBuffer Generation Pass\") { renderEncoder in\n\n            encodeGBufferStage(using: renderEncoder)\n}",
      "language" : "swift"
    },
    {
      "code" : "encodePass(into: commandBuffer,\n           using: lightingPassDescriptor,\n           label: \"Lighting Pass\") { (renderEncoder) in\n\n            encodeDirectionalLightingStage(using: renderEncoder)\n            encodeLightMaskStage(using: renderEncoder)\n            encodePointLightStage(using: renderEncoder)\n            encodeSkyboxStage(using: renderEncoder)\n            encodeFairyBillboardStage(using: renderEncoder)\n}",
      "language" : "swift"
    },
    {
      "code" : "lazy var shadowGeneration = makeRenderPipelineState(label: \"Shadow Generation Stage\") { descriptor in\n    descriptor.vertexFunction = library.makeFunction(name: \"shadow_vertex\")\n    descriptor.depthAttachmentPixelFormat = .depth32Float\n}",
      "language" : "swift"
    },
    {
      "code" : "renderEncoder.setDepthBias(0.015, slopeScale: 7, clamp: 0.02)",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Compare the depth value in the shadow map to the depth value of the fragment in the sun's.\n\/\/ frame of reference.  If the sample is occluded, it will be zero.\nfloat shadow_sample = shadowMap.sample_compare(shadowSampler, in.shadow_coord.xy, in.shadow_coord.z);",
      "language" : "metal"
    },
    {
      "code" : "gBuffer.normal_shadow = half4(eye_normal.xyz, shadow_sample);",
      "language" : "metal"
    },
    {
      "code" : "var storageMode = MTLStorageMode.private",
      "language" : "swift"
    },
    {
      "code" : "fragment half4\ndeferred_directional_lighting_fragment_traditional(\n    QuadInOut                in                      [[ stage_in ]],\n    constant AAPLFrameData & frameData               [[ buffer(AAPLBufferFrameData) ]],\n    texture2d<half>          albedo_specular_GBuffer [[ texture(AAPLRenderTargetAlbedo) ]],\n    texture2d<half>          normal_shadow_GBuffer   [[ texture(AAPLRenderTargetNormal) ]],\n    texture2d<float>         depth_GBuffer           [[ texture(AAPLRenderTargetDepth)  ]])",
      "language" : "metal"
    },
    {
      "code" : "struct GBufferData\n{\n    half4 lighting        [[color(AAPLRenderTargetLighting), raster_order_group(AAPLLightingROG)]];\n    half4 albedo_specular [[color(AAPLRenderTargetAlbedo),   raster_order_group(AAPLGBufferROG)]];\n    half4 normal_shadow   [[color(AAPLRenderTargetNormal),   raster_order_group(AAPLGBufferROG)]];\n    float depth           [[color(AAPLRenderTargetDepth),    raster_order_group(AAPLGBufferROG)]];\n};",
      "language" : "metal"
    },
    {
      "code" : "deferred_directional_lighting_fragment_single_pass(\n    QuadInOut                in        [[ stage_in ]],\n    constant AAPLFrameData & frameData [[ buffer(AAPLBufferFrameData) ]],\n    GBufferData              GBuffer)",
      "language" : "metal"
    },
    {
      "code" : "renderEncoder.setRenderPipelineState(lightMaskPipelineState)\nrenderEncoder.setDepthStencilState(lightMaskDepthStencilState)\n\nrenderEncoder.setStencilReferenceValue(128)\nrenderEncoder.setCullMode(.front)\n\nrenderEncoder.setVertexBuffer(scene.frameData,\n                              offset: 0,\n                              index: Int(AAPLBufferFrameData.rawValue))\n\nrenderEncoder.setVertexBuffer(scene.pointLights,\n                              offset: 0,\n                              index: Int(AAPLBufferIndexLightsData.rawValue))\n\nrenderEncoder.setVertexBuffer(scene.lightPositions,\n                              offset: 0,\n                              index: Int(AAPLBufferIndexLightsPosition.rawValue))\n\nrenderEncoder.setFragmentBuffer(scene.frameData,\n                                offset: 0,\n                                index: Int(AAPLBufferFrameData.rawValue))\n\nrenderEncoder.draw(meshes: [scene.icosahedron],\n                   instanceCount: scene.numberOfLights,\n                   requiresMaterials: false)",
      "language" : "swift"
    },
    {
      "code" : "renderEncoder.setRenderPipelineState(pipelineStates.pointLighting)\nrenderEncoder.setDepthStencilState(depthStencilStates.pointLighting)\n\nif !device.supportsFamily(.apple1) {\n    scene.setGBufferTextures(renderEncoder: renderEncoder)\n}\n\nrenderEncoder.setStencilReferenceValue(128)\nrenderEncoder.setCullMode(.back)\n\nrenderEncoder.setVertexBuffer(scene.frameData,\n                              offset: 0,\n                              index: Int(AAPLBufferFrameData.rawValue))\n\nrenderEncoder.setVertexBuffer(scene.pointLights,\n                              offset: 0,\n                              index: Int(AAPLBufferIndexLightsData.rawValue))\n\nrenderEncoder.setVertexBuffer(scene.lightPositions,\n                              offset: 0,\n                              index: Int(AAPLBufferIndexLightsPosition.rawValue))\n\nrenderEncoder.setFragmentBuffer(scene.frameData,\n                                offset: 0,\n                                index: Int(AAPLBufferFrameData.rawValue))\n\nrenderEncoder.setFragmentBuffer(scene.pointLights,\n                                offset: 0,\n                                index: Int(AAPLBufferIndexLightsData.rawValue))\n\nrenderEncoder.setFragmentBuffer(scene.lightPositions,\n                                offset: 0,\n                                index: Int(AAPLBufferIndexLightsPosition.rawValue))\n\nrenderEncoder.draw(meshes: [scene.icosahedron],\n                   instanceCount: scene.numberOfLights,\n                   requiresMaterials: false)",
      "language" : "swift"
    },
    {
      "code" : "renderEncoder.setRenderPipelineState(pipelineStates.skybox)\nrenderEncoder.setDepthStencilState(depthStencilStates.skybox)\n\nrenderEncoder.setCullMode(.front)\n\nrenderEncoder.setVertexBuffer(scene.frameData, offset: 0, index: Int(AAPLBufferFrameData.rawValue))\nrenderEncoder.setFragmentTexture(scene.skyMap, index: Int(AAPLTextureIndexBaseColor.rawValue))\n\nrenderEncoder.draw(meshes: [scene.skyMesh],\n                   requiresMaterials: false)",
      "language" : "swift"
    },
    {
      "code" : "half4 c = colorMap.sample(linearSampler, float2(in.tex_coord));\n\nhalf3 fragColor = in.color * c.x;\n\nreturn half4(fragColor, c.x);",
      "language" : "metal"
    }
  ],
  "contentHash" : "bc5804447bda1b0180c0b98bf01374ebc8b6ab69b6b5b135b71702cd60e3dbc2",
  "crawledAt" : "2025-12-02T15:49:38Z",
  "id" : "21BF9B61-0454-4578-9FE8-A8750D5AD59C",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "Metal",
  "overview" : "## Overview\n\nThis sample demonstrates a deferred lighting renderer that implements shadows using a shadow map, and culls light volumes using the stencil buffer.\n\n\n\nDeferred lighting can render a large number of lights more easily than forward lighting. For example, with forward lighting, in a scene with many lights, it’s infeasible for every fragment to calculate the contribution of every light. Complex sorting and binning algorithms need to be implemented to limit the calculation of light contributions to only those lights affecting each fragment. With deferred lighting, multiple lights can be applied to the scene with ease.\n\n### Configure the sample code project\n\nThe Xcode project contains schemes for running the sample on macOS, iOS, or tvOS.  The default scheme is macOS, which runs the sample as is on your Mac.\n\nThe sample contains the following preprocessor conditionals that you can modify to control the configuration of the app.\n\nHere’s what they modify in the app’s behavior:\n\n### Review important concepts\n\nBefore you get started with the sample app, review these concepts to better understand key details of a deferred lighting renderer and some unique Metal features.\n\n**Traditional Deferred Lighting Renderer**\n\nA traditional deferred lighting renderer is typically separated into two render passes:\n\n\n\n**Single-Pass Deferred Lighting on Apple silicon GPUs**\n\nApple silicon GPUs, found on all iOS and tvOS device and now certain macOS devices, use a tile-based deferred rendering (TBDR) architecture, which allows them to render data to tile memory within the GPU. By rendering to tile memory, the device avoids potentially expensive round trips between the GPU and system memory (via a bandwidth-constrained memory bus). Whether a GPU writes tile memory to system memory depends on these configurations:\n\nWhen `MTLStoreAction.store` is set as a store action, output data for the render targets of a render pass is written from tile memory to system memory, where the render targets are backed by textures. If this data is then used for a subsequent render pass, input data from these textures is read from system memory into a texture cache in the GPU. Therefore, a traditional deferred lighting renderer that accesses system memory requires geometry buffer data to be stored in system memory between the first and second render passes.\n\n\n\nHowever, because of the TBDR architecture, Apple silicon GPUs can also read data from tile memory at any given time. This allows fragment shaders to read from and perform calculations on render targets in tile memory, before this data is written to tile memory again. This feature allows the sample to avoid storing geometry buffer data in system memory between the first and second render passes; thus, a deferred lighting renderer can be implemented with a single render pass.\n\nGeometry buffer data is produced and consumed exclusively by the GPU, not the CPU, within the single render pass. Therefore, this data isn’t loaded from system memory before the render pass begins, nor is it stored in system memory after the render pass finishes. Instead of reading geometry buffer data from a texture in system memory, the lighting fragment functions read data from the geometry buffer while it’s still attached to the render pass as a render target. Thus, system memory doesn’t need to be allocated for geometry buffer textures, and each of these textures can be declared with a `MTLStorageMode.memoryless` storage mode.\n\n\n\n**Deferred Lighting with Raster Order Groups**\n\nBy default, when a fragment shader writes data to a pixel, the GPU waits until the shader has completely finished writing to that pixel before beginning the execution of another fragment shader for that same pixel.\n\n\n\nRaster order groups allow apps to increase the parallelization of the GPU’s fragment shaders. With raster order groups, a fragment function can separate render targets into different execution groups. This separation allows the GPU to read from and perform calculations on render targets in one group, before a previous instance of a fragment shader has finished writing data to pixels in another group.\n\n\n\nIn this sample, some lighting fragment functions use these raster order groups:\n\nThese raster order groups allow the GPU to read the geometry buffer in a fragment shader and execute the lighting calculations, before the lighting calculations from a previous instance of a fragment shader have finished writing their output data.\n\n### Render a deferred lighting frame\n\nThe sample renders each full frame by rendering these stages, in this order:\n\nThe sample’s single pass deferred renderer produces the geometry buffer and performs all subsequent stages in a single render pass. This single-pass implementation is possible due to the TBDR architecture of iOS and tvOS GPUs, which allows a device to read geometry buffer data from render targets in tile memory.\n\nThe sample’s traditional deferred renderer produces the geometry buffer in one render pass and then performs all subsequent stages in another render pass. This two-pass implementation is necessary with GPUs using an IMR architecture, which don’t support reading render target color data in a fragment function.\n\n### Render the shadow map\n\nThe sample renders a shadow map for the single directional light in the scene (the sun) by rendering the model from the light’s perspective.\n\n\n\nThe render pipeline for the shadow map has a vertex function but not a fragment function; therefore, the sample can determine the screen-space depth value written to the shadow map without executing further stages of the render pipeline. (Additionally, the render executes quickly because it doesn’t have a fragment function.)\n\nBefore drawing geometry for the shadow map, the sample sets a depth bias value to reduce shadow artifacts:\n\nThen, in the fragment function of the geometry buffer stage, the sample tests whether the fragment is occluded and shadowed:\n\nThe sample stores the result of the `sample_compare` function in the `w` component of the `normal_shadow` render target:\n\nIn the directional light and point light composition stages, the sample reads the shadow value from the geometry buffer and applies it to the fragment.\n\n### Render the geometry buffer\n\nThe sample’s geometry buffer contains these textures:\n\n\n\nWhen the sample renders the geometry buffer, both the traditional and single pass deferred renderers attach all the geometry buffer textures as render targets for the render pass. However, because devices using a TBDR architecture can both render the geometry buffer and read from it in a single render pass, the sample creates the geometry buffer textures with a memoryless storage mode, which indicates that system memory isn’t allocated for these textures. Instead, these textures are allocated and populated only in tile memory for the duration of the render pass.\n\nThe sample creates the geometry buffer textures in the implmentation of the common `drawableSizeWillChange` computed property, but the single-pass deferred renderer sets the `storageMode` variable to `MTLStorageMode.memoryless` while the traditional deferred renderer sets it to `MTLStorageMode.private`.\n\nFor the traditional deferred renderer, after the sample finishes writing data to the geometry buffer textures, it calls the `endEncoding` method to finalize the geometry buffer render pass. Because the store action for the render command encoder is set to `MTLStoreAction.store`, the GPU writes each of the render target textures to video memory when the encoder completes its execution. This allows the sample to read these textures from video memory in the subsequent deferred lighting and composition render pass.\n\nFor the single pass deferred renderer, after the sample finishes writing data to the geometry buffer textures, the sample doesn’t finalize the render command encoder and instead continues to use it for subsequent stages.\n\n### Apply the directional lighting and shadows\n\nThe sample applies directional lighting and shadows to the drawable that’s destined for the display.\n\nThe traditional deferred renderer reads geometry buffer data from textures set as arguments to a fragment function:\n\nThe single pass deferred renderer reads geometry buffer data from render targets attached to the render pass:\n\nAlthough these fragment functions have different inputs, they share a common implementation in the `deferred_directional_lighting_fragment_common` fragment function. This function performs these operations:\n\nBecause this is the first stage that renders to the drawable, the iOS and tvOS renderer obtains a drawable before the earlier geometry buffer stage so that the drawable can be merged with the output of later stages. The traditional deferred renderer, however, delays obtaining a drawable until after the geometry buffer stage is completed and before the directional light stage begins. This delay reduces the amount of time that the app holds onto the drawable and thus improves performance.\n\n### Cull the light volumes\n\nThe sample creates a stencil mask that’s used to avoid executing expensive lighting calculations for many fragments. It creates this stencil mask by using the depth buffer from the geometry buffer pass, and the stencil buffer, to track whether a light volume intersects any geometry. (If not, then it isn’t casting light on anything.)\n\nIn the `encodeLightMaskStage` implementation, the sample sets the `lightMask` object of the `PipelineStates` class and encodes an instanced draw call to draw only the back faces of icosahedrons, which encompass the volumes of the point lights. If a fragment within this draw call fails the depth test, this result indicates that the back face of the icosahedron is behind some geometry.\n\nThe `lightMask` pipeline object doesn’t have a fragment function, so no color data is written from this render pipeline. However, due to the set `lightMask` depth and stencil state, any fragment that fails the depth test increments the stencil buffer for that fragment. Fragments that contain geometry have a starting depth value of `128`, which the sample set in the geometry buffer stage. Therefore, any fragment that fails the depth test while `lightMask` depth and stencil state is set increments the depth value to greater than `128`. (Because front face culling is enabled, a fragment that fails the depth test and has a value greater than `128` indicates that at least the back half of the icosahedron is behind all geometry.)\n\nIn the next draw call, in the `encodePointLightStage` implementation, the sample applies the contribution of the point lights to the drawable. The sample tests whether the front half of the icosahedron is in front of all geometry, which determines if the volume intersects some geometry and thus if the fragment should be lit. The depth and stencil state,  `pointLight`, set for this draw call only executes the fragment function if the stencil value for the fragment is greater than the reference value of `128`. (Because the stencil test value is set to `MTLCompareFunction.less`, the sample passes the test only if the reference value of `128` is less than the value in the stencil buffer.)\n\nBecause the draw call in `encodeLightMaskStage` increments the stencil values for fragments that are behind any geometry, the only fragments for which the sample executes the fragment function are those that meet both of these conditions:\n\nThe following diagrams show the difference in fragment coverage between a rendered frame that uses this stencil mask algorithm and another that doesn’t. When the algorithm is enabled, pixels in green are pixels for which the point light fragment function was executed.\n\n\n\nWhen the algorithm is disabled, pixels in green and red are pixels for which the point light fragment function was executed.\n\n\n\n### Render the skybox and fairy lights\n\nIn the final lighting stages, the sample applies much simpler lighting techniques to the scene.\n\nThe sample applies depth testing to the skybox, against the temple’s geometry, so the renderer only renders to areas of the drawable that have not been filled by some geometry.\n\nThe sample renders fairy lights onto the drawable as 2D circles and uses a texture to determine the alpha blending factors for their fragments.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/Metal\/rendering-a-scene-with-deferred-lighting-in-swift\ncrawled: 2025-12-02T15:49:38Z\n---\n\n# Rendering a scene with deferred lighting in Swift\n\n**Sample Code**\n\nAvoid expensive lighting calculations by implementing a deferred lighting renderer optimized for immediate mode and tile-based deferred renderer GPUs.\n\n## Overview\n\nThis sample demonstrates a deferred lighting renderer that implements shadows using a shadow map, and culls light volumes using the stencil buffer.\n\n\n\nDeferred lighting can render a large number of lights more easily than forward lighting. For example, with forward lighting, in a scene with many lights, it’s infeasible for every fragment to calculate the contribution of every light. Complex sorting and binning algorithms need to be implemented to limit the calculation of light contributions to only those lights affecting each fragment. With deferred lighting, multiple lights can be applied to the scene with ease.\n\n### Configure the sample code project\n\nThe Xcode project contains schemes for running the sample on macOS, iOS, or tvOS.  The default scheme is macOS, which runs the sample as is on your Mac.\n\n\n\nThe sample contains the following preprocessor conditionals that you can modify to control the configuration of the app.\n\n```\n#define USE_EYE_DEPTH              1\n#define LIGHT_STENCIL_CULLING      1\n```\n\nHere’s what they modify in the app’s behavior:\n\n- `USE_EYE_DEPTH` — When enabled, writes depth values in eye space to the geometry buffer depth component. This allows the deferred pass to calculate the eye space fragment position more easily to apply lighting. When disabled, the screen depth is written to the geometry buffer depth component and an extra inverse transform from screen space to eye space is necessary to calculate lighting contributions in the deferred pass.\n- `LIGHT_STENCIL_CULLING` — When enabled, uses the stencil buffer to avoid execution of lighting calculations on fragments that don’t intersect with a 3D light volume. When disabled, the GPU calculates lighting for all fragments covered by a light in screen space. This means that considerably more fragments need expensive lighting calculations than is actually necessary.\n\n### Review important concepts\n\nBefore you get started with the sample app, review these concepts to better understand key details of a deferred lighting renderer and some unique Metal features.\n\n**Traditional Deferred Lighting Renderer**\n\nA traditional deferred lighting renderer is typically separated into two render passes:\n\n- **First pass: Geometry buffer rendering.** The renderer draws and transforms the scene’s models, and the fragment function renders the results to a collection of textures known as the *geometry buffer* or *g-buffer*. The geometry buffer contains material colors from the models, as well as per-fragment normal, shadow, and depth values.\n- **Second pass: Deferred lighting and composition.** The renderer draws each light volume, using the geometry buffer data to reconstruct the position of each fragment and apply the lighting calculations. As the lights are drawn, the output of each light is blended on top of the previous light outputs. Finally, the renderer composites other data, such as shadows and directional lighting, onto the scene by executing a full-screen quad or a compute kernel.\n\n\n\n\n\n**Single-Pass Deferred Lighting on Apple silicon GPUs**\n\nApple silicon GPUs, found on all iOS and tvOS device and now certain macOS devices, use a tile-based deferred rendering (TBDR) architecture, which allows them to render data to tile memory within the GPU. By rendering to tile memory, the device avoids potentially expensive round trips between the GPU and system memory (via a bandwidth-constrained memory bus). Whether a GPU writes tile memory to system memory depends on these configurations:\n\n- The store action of the app’s render command encoders.\n- The storage mode of the app’s textures.\n\nWhen `MTLStoreAction.store` is set as a store action, output data for the render targets of a render pass is written from tile memory to system memory, where the render targets are backed by textures. If this data is then used for a subsequent render pass, input data from these textures is read from system memory into a texture cache in the GPU. Therefore, a traditional deferred lighting renderer that accesses system memory requires geometry buffer data to be stored in system memory between the first and second render passes.\n\n\n\nHowever, because of the TBDR architecture, Apple silicon GPUs can also read data from tile memory at any given time. This allows fragment shaders to read from and perform calculations on render targets in tile memory, before this data is written to tile memory again. This feature allows the sample to avoid storing geometry buffer data in system memory between the first and second render passes; thus, a deferred lighting renderer can be implemented with a single render pass.\n\nGeometry buffer data is produced and consumed exclusively by the GPU, not the CPU, within the single render pass. Therefore, this data isn’t loaded from system memory before the render pass begins, nor is it stored in system memory after the render pass finishes. Instead of reading geometry buffer data from a texture in system memory, the lighting fragment functions read data from the geometry buffer while it’s still attached to the render pass as a render target. Thus, system memory doesn’t need to be allocated for geometry buffer textures, and each of these textures can be declared with a `MTLStorageMode.memoryless` storage mode.\n\n\n\n\n\n**Deferred Lighting with Raster Order Groups**\n\nBy default, when a fragment shader writes data to a pixel, the GPU waits until the shader has completely finished writing to that pixel before beginning the execution of another fragment shader for that same pixel.\n\n\n\nRaster order groups allow apps to increase the parallelization of the GPU’s fragment shaders. With raster order groups, a fragment function can separate render targets into different execution groups. This separation allows the GPU to read from and perform calculations on render targets in one group, before a previous instance of a fragment shader has finished writing data to pixels in another group.\n\n\n\nIn this sample, some lighting fragment functions use these raster order groups:\n\n- **Raster order group 0.** `AAPLLightingROG` is used for the render target that contains the results of the lighting calculations.\n- **Raster order group 1.** `AAPLGBufferROG` is used for the geometry buffer data in the lighting function.\n\nThese raster order groups allow the GPU to read the geometry buffer in a fragment shader and execute the lighting calculations, before the lighting calculations from a previous instance of a fragment shader have finished writing their output data.\n\n### Render a deferred lighting frame\n\nThe sample renders each full frame by rendering these stages, in this order:\n\n1. Shadow map\n2. Geometry buffer\n3. Directional light\n4. Light mask\n5. Point lights\n6. Skybox\n7. Fairy lights\n\nThe sample’s single pass deferred renderer produces the geometry buffer and performs all subsequent stages in a single render pass. This single-pass implementation is possible due to the TBDR architecture of iOS and tvOS GPUs, which allows a device to read geometry buffer data from render targets in tile memory.\n\n```swift\nencodePass(into: commandBuffer, using: gBufferAndLightingPassDescriptor, label: \"GBuffer & Lighting Pass\") { renderEncoder in\n\n    encodeGBufferStage(using: renderEncoder)\n    encodeDirectionalLightingStage(using: renderEncoder)\n    encodeLightMaskStage(using: renderEncoder)\n    encodePointLightStage(using: renderEncoder)\n    encodeSkyboxStage(using: renderEncoder)\n    encodeFairyBillboardStage(using: renderEncoder)\n}\n```\n\nThe sample’s traditional deferred renderer produces the geometry buffer in one render pass and then performs all subsequent stages in another render pass. This two-pass implementation is necessary with GPUs using an IMR architecture, which don’t support reading render target color data in a fragment function.\n\n```swift\nencodePass(into: commandBuffer,\n           using: gBufferPassDescriptor,\n           label: \"GBuffer Generation Pass\") { renderEncoder in\n\n            encodeGBufferStage(using: renderEncoder)\n}\n```\n\n```swift\nencodePass(into: commandBuffer,\n           using: lightingPassDescriptor,\n           label: \"Lighting Pass\") { (renderEncoder) in\n\n            encodeDirectionalLightingStage(using: renderEncoder)\n            encodeLightMaskStage(using: renderEncoder)\n            encodePointLightStage(using: renderEncoder)\n            encodeSkyboxStage(using: renderEncoder)\n            encodeFairyBillboardStage(using: renderEncoder)\n}\n```\n\n### Render the shadow map\n\nThe sample renders a shadow map for the single directional light in the scene (the sun) by rendering the model from the light’s perspective.\n\n\n\nThe render pipeline for the shadow map has a vertex function but not a fragment function; therefore, the sample can determine the screen-space depth value written to the shadow map without executing further stages of the render pipeline. (Additionally, the render executes quickly because it doesn’t have a fragment function.)\n\n```swift\nlazy var shadowGeneration = makeRenderPipelineState(label: \"Shadow Generation Stage\") { descriptor in\n    descriptor.vertexFunction = library.makeFunction(name: \"shadow_vertex\")\n    descriptor.depthAttachmentPixelFormat = .depth32Float\n}\n```\n\nBefore drawing geometry for the shadow map, the sample sets a depth bias value to reduce shadow artifacts:\n\n```swift\nrenderEncoder.setDepthBias(0.015, slopeScale: 7, clamp: 0.02)\n```\n\nThen, in the fragment function of the geometry buffer stage, the sample tests whether the fragment is occluded and shadowed:\n\n```metal\n\/\/ Compare the depth value in the shadow map to the depth value of the fragment in the sun's.\n\/\/ frame of reference.  If the sample is occluded, it will be zero.\nfloat shadow_sample = shadowMap.sample_compare(shadowSampler, in.shadow_coord.xy, in.shadow_coord.z);\n```\n\nThe sample stores the result of the `sample_compare` function in the `w` component of the `normal_shadow` render target:\n\n```metal\ngBuffer.normal_shadow = half4(eye_normal.xyz, shadow_sample);\n```\n\nIn the directional light and point light composition stages, the sample reads the shadow value from the geometry buffer and applies it to the fragment.\n\n### Render the geometry buffer\n\nThe sample’s geometry buffer contains these textures:\n\n- `albedoSpecular`, which stores albedo and specular data. Albedo data is stored in the `x`, `y`, and `z` components; specular data is stored in the `w` component.\n- `normalShadow`, which stores normal and shadow data. Normal data is stored in the `x`, `y`, and `z` components; shadow data is stored in the `w` component.\n- `depth`, which stores depth values in eye space.\n\n\n\nWhen the sample renders the geometry buffer, both the traditional and single pass deferred renderers attach all the geometry buffer textures as render targets for the render pass. However, because devices using a TBDR architecture can both render the geometry buffer and read from it in a single render pass, the sample creates the geometry buffer textures with a memoryless storage mode, which indicates that system memory isn’t allocated for these textures. Instead, these textures are allocated and populated only in tile memory for the duration of the render pass.\n\nThe sample creates the geometry buffer textures in the implmentation of the common `drawableSizeWillChange` computed property, but the single-pass deferred renderer sets the `storageMode` variable to `MTLStorageMode.memoryless` while the traditional deferred renderer sets it to `MTLStorageMode.private`.\n\n```swift\nvar storageMode = MTLStorageMode.private\n```\n\nFor the traditional deferred renderer, after the sample finishes writing data to the geometry buffer textures, it calls the `endEncoding` method to finalize the geometry buffer render pass. Because the store action for the render command encoder is set to `MTLStoreAction.store`, the GPU writes each of the render target textures to video memory when the encoder completes its execution. This allows the sample to read these textures from video memory in the subsequent deferred lighting and composition render pass.\n\nFor the single pass deferred renderer, after the sample finishes writing data to the geometry buffer textures, the sample doesn’t finalize the render command encoder and instead continues to use it for subsequent stages.\n\n### Apply the directional lighting and shadows\n\nThe sample applies directional lighting and shadows to the drawable that’s destined for the display.\n\nThe traditional deferred renderer reads geometry buffer data from textures set as arguments to a fragment function:\n\n```metal\nfragment half4\ndeferred_directional_lighting_fragment_traditional(\n    QuadInOut                in                      [[ stage_in ]],\n    constant AAPLFrameData & frameData               [[ buffer(AAPLBufferFrameData) ]],\n    texture2d<half>          albedo_specular_GBuffer [[ texture(AAPLRenderTargetAlbedo) ]],\n    texture2d<half>          normal_shadow_GBuffer   [[ texture(AAPLRenderTargetNormal) ]],\n    texture2d<float>         depth_GBuffer           [[ texture(AAPLRenderTargetDepth)  ]])\n```\n\nThe single pass deferred renderer reads geometry buffer data from render targets attached to the render pass:\n\n```metal\nstruct GBufferData\n{\n    half4 lighting        [[color(AAPLRenderTargetLighting), raster_order_group(AAPLLightingROG)]];\n    half4 albedo_specular [[color(AAPLRenderTargetAlbedo),   raster_order_group(AAPLGBufferROG)]];\n    half4 normal_shadow   [[color(AAPLRenderTargetNormal),   raster_order_group(AAPLGBufferROG)]];\n    float depth           [[color(AAPLRenderTargetDepth),    raster_order_group(AAPLGBufferROG)]];\n};\n```\n\n```metal\ndeferred_directional_lighting_fragment_single_pass(\n    QuadInOut                in        [[ stage_in ]],\n    constant AAPLFrameData & frameData [[ buffer(AAPLBufferFrameData) ]],\n    GBufferData              GBuffer)\n```\n\nAlthough these fragment functions have different inputs, they share a common implementation in the `deferred_directional_lighting_fragment_common` fragment function. This function performs these operations:\n\n- Reconstructs the normals from the geometry buffer normal data to calculate the diffuse term.\n- Reconstructs the eye space position from the geometry buffer depth data to apply specular highlights.\n- Uses the geometry buffer shadow data to darken the fragment and apply the shadow to the scene.\n\nBecause this is the first stage that renders to the drawable, the iOS and tvOS renderer obtains a drawable before the earlier geometry buffer stage so that the drawable can be merged with the output of later stages. The traditional deferred renderer, however, delays obtaining a drawable until after the geometry buffer stage is completed and before the directional light stage begins. This delay reduces the amount of time that the app holds onto the drawable and thus improves performance.\n\n\n\n### Cull the light volumes\n\nThe sample creates a stencil mask that’s used to avoid executing expensive lighting calculations for many fragments. It creates this stencil mask by using the depth buffer from the geometry buffer pass, and the stencil buffer, to track whether a light volume intersects any geometry. (If not, then it isn’t casting light on anything.)\n\nIn the `encodeLightMaskStage` implementation, the sample sets the `lightMask` object of the `PipelineStates` class and encodes an instanced draw call to draw only the back faces of icosahedrons, which encompass the volumes of the point lights. If a fragment within this draw call fails the depth test, this result indicates that the back face of the icosahedron is behind some geometry.\n\n```swift\nrenderEncoder.setRenderPipelineState(lightMaskPipelineState)\nrenderEncoder.setDepthStencilState(lightMaskDepthStencilState)\n\nrenderEncoder.setStencilReferenceValue(128)\nrenderEncoder.setCullMode(.front)\n\nrenderEncoder.setVertexBuffer(scene.frameData,\n                              offset: 0,\n                              index: Int(AAPLBufferFrameData.rawValue))\n\nrenderEncoder.setVertexBuffer(scene.pointLights,\n                              offset: 0,\n                              index: Int(AAPLBufferIndexLightsData.rawValue))\n\nrenderEncoder.setVertexBuffer(scene.lightPositions,\n                              offset: 0,\n                              index: Int(AAPLBufferIndexLightsPosition.rawValue))\n\nrenderEncoder.setFragmentBuffer(scene.frameData,\n                                offset: 0,\n                                index: Int(AAPLBufferFrameData.rawValue))\n\nrenderEncoder.draw(meshes: [scene.icosahedron],\n                   instanceCount: scene.numberOfLights,\n                   requiresMaterials: false)\n```\n\nThe `lightMask` pipeline object doesn’t have a fragment function, so no color data is written from this render pipeline. However, due to the set `lightMask` depth and stencil state, any fragment that fails the depth test increments the stencil buffer for that fragment. Fragments that contain geometry have a starting depth value of `128`, which the sample set in the geometry buffer stage. Therefore, any fragment that fails the depth test while `lightMask` depth and stencil state is set increments the depth value to greater than `128`. (Because front face culling is enabled, a fragment that fails the depth test and has a value greater than `128` indicates that at least the back half of the icosahedron is behind all geometry.)\n\nIn the next draw call, in the `encodePointLightStage` implementation, the sample applies the contribution of the point lights to the drawable. The sample tests whether the front half of the icosahedron is in front of all geometry, which determines if the volume intersects some geometry and thus if the fragment should be lit. The depth and stencil state,  `pointLight`, set for this draw call only executes the fragment function if the stencil value for the fragment is greater than the reference value of `128`. (Because the stencil test value is set to `MTLCompareFunction.less`, the sample passes the test only if the reference value of `128` is less than the value in the stencil buffer.)\n\n```swift\nrenderEncoder.setRenderPipelineState(pipelineStates.pointLighting)\nrenderEncoder.setDepthStencilState(depthStencilStates.pointLighting)\n\nif !device.supportsFamily(.apple1) {\n    scene.setGBufferTextures(renderEncoder: renderEncoder)\n}\n\nrenderEncoder.setStencilReferenceValue(128)\nrenderEncoder.setCullMode(.back)\n\nrenderEncoder.setVertexBuffer(scene.frameData,\n                              offset: 0,\n                              index: Int(AAPLBufferFrameData.rawValue))\n\nrenderEncoder.setVertexBuffer(scene.pointLights,\n                              offset: 0,\n                              index: Int(AAPLBufferIndexLightsData.rawValue))\n\nrenderEncoder.setVertexBuffer(scene.lightPositions,\n                              offset: 0,\n                              index: Int(AAPLBufferIndexLightsPosition.rawValue))\n\nrenderEncoder.setFragmentBuffer(scene.frameData,\n                                offset: 0,\n                                index: Int(AAPLBufferFrameData.rawValue))\n\nrenderEncoder.setFragmentBuffer(scene.pointLights,\n                                offset: 0,\n                                index: Int(AAPLBufferIndexLightsData.rawValue))\n\nrenderEncoder.setFragmentBuffer(scene.lightPositions,\n                                offset: 0,\n                                index: Int(AAPLBufferIndexLightsPosition.rawValue))\n\nrenderEncoder.draw(meshes: [scene.icosahedron],\n                   instanceCount: scene.numberOfLights,\n                   requiresMaterials: false)\n```\n\nBecause the draw call in `encodeLightMaskStage` increments the stencil values for fragments that are behind any geometry, the only fragments for which the sample executes the fragment function are those that meet both of these conditions:\n\n- Fragments whose front face passes the depth test and is in front of some geometry.\n- Fragments whose back face fails the depth test and is behind some geometry.\n\nThe following diagrams show the difference in fragment coverage between a rendered frame that uses this stencil mask algorithm and another that doesn’t. When the algorithm is enabled, pixels in green are pixels for which the point light fragment function was executed.\n\n\n\nWhen the algorithm is disabled, pixels in green and red are pixels for which the point light fragment function was executed.\n\n\n\n### Render the skybox and fairy lights\n\nIn the final lighting stages, the sample applies much simpler lighting techniques to the scene.\n\nThe sample applies depth testing to the skybox, against the temple’s geometry, so the renderer only renders to areas of the drawable that have not been filled by some geometry.\n\n```swift\nrenderEncoder.setRenderPipelineState(pipelineStates.skybox)\nrenderEncoder.setDepthStencilState(depthStencilStates.skybox)\n\nrenderEncoder.setCullMode(.front)\n\nrenderEncoder.setVertexBuffer(scene.frameData, offset: 0, index: Int(AAPLBufferFrameData.rawValue))\nrenderEncoder.setFragmentTexture(scene.skyMap, index: Int(AAPLTextureIndexBaseColor.rawValue))\n\nrenderEncoder.draw(meshes: [scene.skyMesh],\n                   requiresMaterials: false)\n```\n\nThe sample renders fairy lights onto the drawable as 2D circles and uses a texture to determine the alpha blending factors for their fragments.\n\n```metal\nhalf4 c = colorMap.sample(linearSampler, float2(in.tex_coord));\n\nhalf3 fragColor = in.color * c.x;\n\nreturn half4(fragColor, c.x);\n```\n\n## Lighting techniques\n\n- **Rendering a scene with forward plus lighting using tile shaders**: Implement a forward plus renderer using the latest features on Apple GPUs.\n- **Rendering a scene with deferred lighting in Objective-C**: Avoid expensive lighting calculations by implementing a deferred lighting renderer optimized for immediate mode and tile-based deferred renderer GPUs.\n- **Rendering a scene with deferred lighting in C++**: Avoid expensive lighting calculations by implementing a deferred lighting renderer optimized for immediate mode and tile-based deferred renderer GPUs.\n- **Rendering reflections with fewer render passes**: Use layer selection to reduce the number of render passes needed to generate an environment map.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Implement a forward plus renderer using the latest features on Apple GPUs.",
          "name" : "Rendering a scene with forward plus lighting using tile shaders",
          "url" : "https:\/\/developer.apple.com\/documentation\/Metal\/rendering-a-scene-with-forward-plus-lighting-using-tile-shaders"
        },
        {
          "description" : "Avoid expensive lighting calculations by implementing a deferred lighting renderer optimized for immediate mode and tile-based deferred renderer GPUs.",
          "name" : "Rendering a scene with deferred lighting in Objective-C",
          "url" : "https:\/\/developer.apple.com\/documentation\/Metal\/rendering-a-scene-with-deferred-lighting-in-objective-c"
        },
        {
          "description" : "Avoid expensive lighting calculations by implementing a deferred lighting renderer optimized for immediate mode and tile-based deferred renderer GPUs.",
          "name" : "Rendering a scene with deferred lighting in C++",
          "url" : "https:\/\/developer.apple.com\/documentation\/Metal\/rendering-a-scene-with-deferred-lighting-in-c++"
        },
        {
          "description" : "Use layer selection to reduce the number of render passes needed to generate an environment map.",
          "name" : "Rendering reflections with fewer render passes",
          "url" : "https:\/\/developer.apple.com\/documentation\/Metal\/rendering-reflections-with-fewer-render-passes"
        }
      ],
      "title" : "Lighting techniques"
    }
  ],
  "source" : "appleJSON",
  "title" : "Rendering a scene with deferred lighting in Swift",
  "url" : "https:\/\/developer.apple.com\/documentation\/Metal\/rendering-a-scene-with-deferred-lighting-in-swift"
}