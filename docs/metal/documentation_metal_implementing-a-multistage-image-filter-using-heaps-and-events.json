{
  "abstract" : "Use events to synchronize access to resources allocated on a heap.",
  "codeExamples" : [
    {
      "code" : "- (void) wait:(_Nonnull id <MTLCommandBuffer>)commandBuffer\n{\n    assert([_event.class conformsToProtocol:@protocol(MTLSharedEvent)] || (commandBuffer.device == _event.device));\n    \n    \/\/ Wait for the event to be signaled\n    [commandBuffer encodeWaitForEvent:_event value:_signalCounter];\n}",
      "language" : "objective-c"
    },
    {
      "code" : "- (void) signal:(_Nonnull id<MTLCommandBuffer>)commandBuffer\n{\n    assert([_event.class conformsToProtocol:@protocol(MTLSharedEvent)] || (commandBuffer.device == _event.device));\n\n    \/\/ Increase the signal counter\n    ++_signalCounter;\n    \/\/ Signal the event\n    [commandBuffer encodeSignalEvent:_event value:_signalCounter];\n}",
      "language" : "objective-c"
    },
    {
      "code" : "[event wait:commandBuffer];",
      "language" : "objective-c"
    },
    {
      "code" : "MTLTextureDescriptor *textureDescriptor = [MTLTextureDescriptor texture2DDescriptorWithPixelFormat:inTexture.pixelFormat\n                                                                                             width:inTexture.width\n                                                                                            height:inTexture.height\n                                                                                         mipmapped:YES];\ntextureDescriptor.storageMode = heap.storageMode;\ntextureDescriptor.usage = MTLTextureUsageShaderWrite | MTLTextureUsageShaderRead;\n\nid <MTLTexture> outTexture = [heap newTextureWithDescriptor:textureDescriptor];",
      "language" : "objective-c"
    },
    {
      "code" : "[blitCommandEncoder copyFromTexture:inTexture\n                        sourceSlice:0\n                        sourceLevel:0\n                       sourceOrigin:(MTLOrigin){ 0, 0, 0 }\n                         sourceSize:(MTLSize){ inTexture.width, inTexture.height, inTexture.depth }\n                          toTexture:outTexture\n                   destinationSlice:0\n                   destinationLevel:0\n                  destinationOrigin:(MTLOrigin){ 0, 0, 0}];\n\n[blitCommandEncoder generateMipmapsForTexture:outTexture];\n\n[blitCommandEncoder endEncoding];",
      "language" : "objective-c"
    },
    {
      "code" : "[event signal:commandBuffer];",
      "language" : "objective-c"
    },
    {
      "code" : "[event wait:commandBuffer];",
      "language" : "objective-c"
    },
    {
      "code" : "id <MTLTexture> intermediaryTexture = [heap newTextureWithDescriptor:textureDescriptor];",
      "language" : "objective-c"
    },
    {
      "code" : "\/\/ Perform horizontal blur using the input texture as an input\n\/\/ and a view of the mipmap level of input texture as the output\n\n[computeEncoder setComputePipelineState:_horizontalKernel];\n\n[computeEncoder setTexture:inTexture\n                   atIndex:AAPLBlurTextureIndexInput];\n\n[computeEncoder setTexture:intermediaryTexture\n                   atIndex:AAPLBlurTextureIndexOutput];\n\n[computeEncoder setBytes:&mipmapLevel\n                  length:sizeof(mipmapLevel)\n                 atIndex:AAPLBlurBufferIndexLOD];\n\n[computeEncoder dispatchThreadgroups:threadgroupCount\n               threadsPerThreadgroup:threadgroupSize];\n\n\/\/ Perform vertical blur using the horizontally blurred texture as an input\n\/\/ and a view of the mipmap level of the input texture as the output\n\n[computeEncoder setComputePipelineState:_verticalKernel];\n\n[computeEncoder setTexture:intermediaryTexture\n                   atIndex:AAPLBlurTextureIndexInput];\n\n[computeEncoder setTexture:outTexture\n                   atIndex:AAPLBlurTextureIndexOutput];\n\nstatic const uint32_t mipmapLevelZero = 0;\n[computeEncoder setBytes:&mipmapLevelZero\n                  length:sizeof(mipmapLevelZero)\n                 atIndex:AAPLBlurBufferIndexLOD];\n\n[computeEncoder dispatchThreadgroups:threadgroupCount\n               threadsPerThreadgroup:threadgroupSize];",
      "language" : "objective-c"
    },
    {
      "code" : "[computeEncoder endEncoding];",
      "language" : "objective-c"
    },
    {
      "code" : "[intermediaryTexture makeAliasable];",
      "language" : "objective-c"
    },
    {
      "code" : "[event signal:commandBuffer];",
      "language" : "objective-c"
    },
    {
      "code" : "\/\/ Wait for the filter graph to complete execution\n[_event wait:commandBuffer];\n\n\/\/ Obtain a render pass descriptor generated from the view's drawable textures\nMTLRenderPassDescriptor* renderPassDescriptor = _view.currentRenderPassDescriptor;",
      "language" : "objective-c"
    },
    {
      "code" : "[commandBuffer presentDrawable:_view.currentDrawable];",
      "language" : "objective-c"
    },
    {
      "code" : "\/\/ Signal event for the frame completion\n[_event signal:commandBuffer];\n\n\/\/ Finalize rendering for the frame\n[commandBuffer commit];",
      "language" : "objective-c"
    }
  ],
  "contentHash" : "77e7fa669231f27d053b72fed6fe20e122d2c4fcae8486c4d70f6c551b24c119",
  "crawledAt" : "2025-12-02T15:31:17Z",
  "id" : "153F01D5-5ADE-489F-8D14-9F955E4BAF69",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "Metal",
  "overview" : "## Overview\n\nThis sample demonstrates:\n\n\n\nThis implementation minimizes memory usage in an orderly fashion for a filter graph with a downsample and Gaussian blur filter. For more information, including implementation details about heaps for static and dynamic textures, see [doc:\/\/com.apple.metal\/documentation\/Metal\/implementing-a-multistage-image-filter-using-heaps-and-fences].\n\n### Getting started\n\nThe Xcode project contains schemes for running the sample on macOS, iOS, or tvOS.  The default scheme is macOS, which runs the sample as is on your Mac.\n\n### Compare events with fences\n\nThe [doc:\/\/com.apple.metal\/documentation\/Metal\/MTLFence] API allows you to specify synchronization points in your app that wait for a workload to complete execution, provided that execution begins *before* a fence is encountered. However, this synchronization mechanism means that your app can’t wait for a workload to complete execution if the execution begins *after* a fence is encountered. A fence can wait for workloads that have already begun, but it can’t wait for future workloads.\n\nFences work well in an image filter graph because each filter in the graph is applied sequentially. You can use a fence to wait for one filter to complete execution before you begin executing another.\n\nIn contrast, although the [doc:\/\/com.apple.metal\/documentation\/Metal\/MTLEvent] API also allows you to specify similar synchronization points in your app, it allows for more flexibility than the `MTLFence` API. Unlike fences, events can wait for workloads that have already begun, as well as future workloads. Additionally, events are specified outside command encoder boundaries, not between the encoded commands of a command encoder. Because the event synchronization mechanism is implemented in the command buffer scheduler, events block workloads at the command buffer level within the GPU. Therefore, command buffers on one queue can execute while a command buffer on another queue is blocked by an event.\n\nEvents also work well in an image filter graph because they provide the equivalent functionality of fences. However, events are easier to specify and track because their synchronization mechanism is managed with a discrete signal value that increases monotonically. Using this signal value, events insert a strict execution order between command encoder boundaries in the GPU.\n\n### Implement an event wrapper for synchronization routines\n\nThe sample wraps the `MTLEvent` API in the `AAPLEventWrapper` protocol accessed through an `AAPLSingleDeviceEventWrapper` object. This convenience wrapper encapsulates the main synchronization mechanism, and primarily manages the discrete signal value through the `_signalCounter` variable.\n\nThe sample calls the `wait:` method to wait for a workload to complete execution.\n\nThe sample calls the `signal:` method to signal that a workload has completed execution. (This method increments the value of `_signalCounter`.)\n\n### Manage dependencies between filters\n\nThe sample uses `_event` to control access to dynamic textures allocated from `_scratchHeap` and prevent GPU race conditions in the filter graph. The event ensures that operations on dynamic textures are completed before the filter graph begins subsequent operations that depend on the result of previous operations.\n\nAt the start of the filter graph, the sample calls the `wait:` method to ensure that the previous frame has completed execution.\n\nThe first filter, implemented by the sample in `AAPLDownsampleFilter`, creates a dynamic texture, `outTexture`, from the heap and allocates enough space for mipmaps.\n\nNext, the downsample filter blits a source texture, `inTexture`, to `outTexture` and generates the mipmaps. The sample then calls the `endEncoding` method to finalize the blit pass.\n\nFinally, the downsample filter calls the `signal:` method to indicate that its operations are complete.\n\nThe second filter, implemented by the sample in `AAPLGaussianBlurFilter`, calls the `wait:` method immediately before creating a compute command encoder. This forces the Gaussian blur filter to wait for the downsample filter to complete its work before beginning its own work. A waiting period is necessary because the Gaussian blur filter depends on dynamic texture data generated by the downsample filter. Without the event, the GPU could execute both filters in parallel, and thus read uninitialized dynamic texture data allocated from the heap.\n\n\n\n### Reuse memory and manage dependencies within a filter\n\nThe Gaussian blur filter performs a horizontal blur and a vertical blur for each mipmap level of the dynamic texture produced by the downsample filter. For each mipmap level, the sample allocates a temporary texture, `intermediaryTexture`, from the dynamic textures heap.\n\nThis texture is temporary because it’s used only as an output destination from the horizontal blur and as an input source to the vertical blur. After the sample executes these blurs, the final texture data is stored in `outTexture` (which is a texture view of `inTexture`). Therefore, the texture data contained in `intermediaryTexture` is unused after each mipmap level iteration.\n\nAfter blurring each mipmap level, the sample calls the `endEncoding` method to indicate that the compute operations for the given mipmap level are complete.\n\nInstead of allocating new memory for each mipmap level, the sample reuses the existing memory allocated for `intermediaryTexture`. After each mipmap level iteration, the sample calls the `makeAliasable` method to indicate that this memory can be reused by subsequent allocations from the same dynamic textures heap.\n\nThis memory reuse creates dynamic texture dependencies between mipmap levels. Therefore, after blurring each mipmap level, the sample also calls the `signal:` method to indicate that the blur operations for the given mipmap level are complete.\n\nBecause the sample already calls the `wait:` method to wait for the downsample filter to complete its work, the sample leverages this same call to wait for any previous mipmap levels to complete their work before beginning a new mipmap level iteration.\n\n\n\n### Manage dependencies between frames\n\nThe sample calls the `wait:` method to wait for the filter graph to complete execution before rendering the filtered image to a drawable.\n\nThe sample then renders the filtered image and schedules a drawable presentation.\n\nFinally, the sample calls the `signal:` method to indicate that the frame has been completed and rendered.\n\n",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/Metal\/implementing-a-multistage-image-filter-using-heaps-and-events\ncrawled: 2025-12-02T15:31:17Z\n---\n\n# Implementing a multistage image filter using heaps and events\n\n**Sample Code**\n\nUse events to synchronize access to resources allocated on a heap.\n\n## Overview\n\nThis sample demonstrates:\n\n- Using events instead of fences to manage resource dependencies and work synchronization\n- Creating heaps for static and dynamic textures\n- Using aliasing to reduce the amount of memory used for temporary resources\n- Using events to manage dependencies between encoders that produce and consume dynamic textures\n\n\n\nThis implementation minimizes memory usage in an orderly fashion for a filter graph with a downsample and Gaussian blur filter. For more information, including implementation details about heaps for static and dynamic textures, see [doc:\/\/com.apple.metal\/documentation\/Metal\/implementing-a-multistage-image-filter-using-heaps-and-fences].\n\n### Getting started\n\nThe Xcode project contains schemes for running the sample on macOS, iOS, or tvOS.  The default scheme is macOS, which runs the sample as is on your Mac.\n\n### Compare events with fences\n\nThe [doc:\/\/com.apple.metal\/documentation\/Metal\/MTLFence] API allows you to specify synchronization points in your app that wait for a workload to complete execution, provided that execution begins *before* a fence is encountered. However, this synchronization mechanism means that your app can’t wait for a workload to complete execution if the execution begins *after* a fence is encountered. A fence can wait for workloads that have already begun, but it can’t wait for future workloads.\n\nFences work well in an image filter graph because each filter in the graph is applied sequentially. You can use a fence to wait for one filter to complete execution before you begin executing another.\n\nIn contrast, although the [doc:\/\/com.apple.metal\/documentation\/Metal\/MTLEvent] API also allows you to specify similar synchronization points in your app, it allows for more flexibility than the `MTLFence` API. Unlike fences, events can wait for workloads that have already begun, as well as future workloads. Additionally, events are specified outside command encoder boundaries, not between the encoded commands of a command encoder. Because the event synchronization mechanism is implemented in the command buffer scheduler, events block workloads at the command buffer level within the GPU. Therefore, command buffers on one queue can execute while a command buffer on another queue is blocked by an event.\n\nEvents also work well in an image filter graph because they provide the equivalent functionality of fences. However, events are easier to specify and track because their synchronization mechanism is managed with a discrete signal value that increases monotonically. Using this signal value, events insert a strict execution order between command encoder boundaries in the GPU.\n\n### Implement an event wrapper for synchronization routines\n\nThe sample wraps the `MTLEvent` API in the `AAPLEventWrapper` protocol accessed through an `AAPLSingleDeviceEventWrapper` object. This convenience wrapper encapsulates the main synchronization mechanism, and primarily manages the discrete signal value through the `_signalCounter` variable.\n\n\n\nThe sample calls the `wait:` method to wait for a workload to complete execution.\n\n```objective-c\n- (void) wait:(_Nonnull id <MTLCommandBuffer>)commandBuffer\n{\n    assert([_event.class conformsToProtocol:@protocol(MTLSharedEvent)] || (commandBuffer.device == _event.device));\n    \n    \/\/ Wait for the event to be signaled\n    [commandBuffer encodeWaitForEvent:_event value:_signalCounter];\n}\n```\n\nThe sample calls the `signal:` method to signal that a workload has completed execution. (This method increments the value of `_signalCounter`.)\n\n```objective-c\n- (void) signal:(_Nonnull id<MTLCommandBuffer>)commandBuffer\n{\n    assert([_event.class conformsToProtocol:@protocol(MTLSharedEvent)] || (commandBuffer.device == _event.device));\n\n    \/\/ Increase the signal counter\n    ++_signalCounter;\n    \/\/ Signal the event\n    [commandBuffer encodeSignalEvent:_event value:_signalCounter];\n}\n```\n\n\n\n### Manage dependencies between filters\n\nThe sample uses `_event` to control access to dynamic textures allocated from `_scratchHeap` and prevent GPU race conditions in the filter graph. The event ensures that operations on dynamic textures are completed before the filter graph begins subsequent operations that depend on the result of previous operations.\n\nAt the start of the filter graph, the sample calls the `wait:` method to ensure that the previous frame has completed execution.\n\n```objective-c\n[event wait:commandBuffer];\n```\n\nThe first filter, implemented by the sample in `AAPLDownsampleFilter`, creates a dynamic texture, `outTexture`, from the heap and allocates enough space for mipmaps.\n\n```objective-c\nMTLTextureDescriptor *textureDescriptor = [MTLTextureDescriptor texture2DDescriptorWithPixelFormat:inTexture.pixelFormat\n                                                                                             width:inTexture.width\n                                                                                            height:inTexture.height\n                                                                                         mipmapped:YES];\ntextureDescriptor.storageMode = heap.storageMode;\ntextureDescriptor.usage = MTLTextureUsageShaderWrite | MTLTextureUsageShaderRead;\n\nid <MTLTexture> outTexture = [heap newTextureWithDescriptor:textureDescriptor];\n```\n\nNext, the downsample filter blits a source texture, `inTexture`, to `outTexture` and generates the mipmaps. The sample then calls the `endEncoding` method to finalize the blit pass.\n\n```objective-c\n[blitCommandEncoder copyFromTexture:inTexture\n                        sourceSlice:0\n                        sourceLevel:0\n                       sourceOrigin:(MTLOrigin){ 0, 0, 0 }\n                         sourceSize:(MTLSize){ inTexture.width, inTexture.height, inTexture.depth }\n                          toTexture:outTexture\n                   destinationSlice:0\n                   destinationLevel:0\n                  destinationOrigin:(MTLOrigin){ 0, 0, 0}];\n\n[blitCommandEncoder generateMipmapsForTexture:outTexture];\n\n[blitCommandEncoder endEncoding];\n```\n\nFinally, the downsample filter calls the `signal:` method to indicate that its operations are complete.\n\n```objective-c\n[event signal:commandBuffer];\n```\n\nThe second filter, implemented by the sample in `AAPLGaussianBlurFilter`, calls the `wait:` method immediately before creating a compute command encoder. This forces the Gaussian blur filter to wait for the downsample filter to complete its work before beginning its own work. A waiting period is necessary because the Gaussian blur filter depends on dynamic texture data generated by the downsample filter. Without the event, the GPU could execute both filters in parallel, and thus read uninitialized dynamic texture data allocated from the heap.\n\n```objective-c\n[event wait:commandBuffer];\n```\n\n\n\n### Reuse memory and manage dependencies within a filter\n\nThe Gaussian blur filter performs a horizontal blur and a vertical blur for each mipmap level of the dynamic texture produced by the downsample filter. For each mipmap level, the sample allocates a temporary texture, `intermediaryTexture`, from the dynamic textures heap.\n\n```objective-c\nid <MTLTexture> intermediaryTexture = [heap newTextureWithDescriptor:textureDescriptor];\n```\n\nThis texture is temporary because it’s used only as an output destination from the horizontal blur and as an input source to the vertical blur. After the sample executes these blurs, the final texture data is stored in `outTexture` (which is a texture view of `inTexture`). Therefore, the texture data contained in `intermediaryTexture` is unused after each mipmap level iteration.\n\n```objective-c\n\/\/ Perform horizontal blur using the input texture as an input\n\/\/ and a view of the mipmap level of input texture as the output\n\n[computeEncoder setComputePipelineState:_horizontalKernel];\n\n[computeEncoder setTexture:inTexture\n                   atIndex:AAPLBlurTextureIndexInput];\n\n[computeEncoder setTexture:intermediaryTexture\n                   atIndex:AAPLBlurTextureIndexOutput];\n\n[computeEncoder setBytes:&mipmapLevel\n                  length:sizeof(mipmapLevel)\n                 atIndex:AAPLBlurBufferIndexLOD];\n\n[computeEncoder dispatchThreadgroups:threadgroupCount\n               threadsPerThreadgroup:threadgroupSize];\n\n\/\/ Perform vertical blur using the horizontally blurred texture as an input\n\/\/ and a view of the mipmap level of the input texture as the output\n\n[computeEncoder setComputePipelineState:_verticalKernel];\n\n[computeEncoder setTexture:intermediaryTexture\n                   atIndex:AAPLBlurTextureIndexInput];\n\n[computeEncoder setTexture:outTexture\n                   atIndex:AAPLBlurTextureIndexOutput];\n\nstatic const uint32_t mipmapLevelZero = 0;\n[computeEncoder setBytes:&mipmapLevelZero\n                  length:sizeof(mipmapLevelZero)\n                 atIndex:AAPLBlurBufferIndexLOD];\n\n[computeEncoder dispatchThreadgroups:threadgroupCount\n               threadsPerThreadgroup:threadgroupSize];\n```\n\nAfter blurring each mipmap level, the sample calls the `endEncoding` method to indicate that the compute operations for the given mipmap level are complete.\n\n```objective-c\n[computeEncoder endEncoding];\n```\n\nInstead of allocating new memory for each mipmap level, the sample reuses the existing memory allocated for `intermediaryTexture`. After each mipmap level iteration, the sample calls the `makeAliasable` method to indicate that this memory can be reused by subsequent allocations from the same dynamic textures heap.\n\n```objective-c\n[intermediaryTexture makeAliasable];\n```\n\nThis memory reuse creates dynamic texture dependencies between mipmap levels. Therefore, after blurring each mipmap level, the sample also calls the `signal:` method to indicate that the blur operations for the given mipmap level are complete.\n\n```objective-c\n[event signal:commandBuffer];\n```\n\nBecause the sample already calls the `wait:` method to wait for the downsample filter to complete its work, the sample leverages this same call to wait for any previous mipmap levels to complete their work before beginning a new mipmap level iteration.\n\n\n\n### Manage dependencies between frames\n\nThe sample calls the `wait:` method to wait for the filter graph to complete execution before rendering the filtered image to a drawable.\n\n```objective-c\n\/\/ Wait for the filter graph to complete execution\n[_event wait:commandBuffer];\n\n\/\/ Obtain a render pass descriptor generated from the view's drawable textures\nMTLRenderPassDescriptor* renderPassDescriptor = _view.currentRenderPassDescriptor;\n```\n\nThe sample then renders the filtered image and schedules a drawable presentation.\n\n```objective-c\n[commandBuffer presentDrawable:_view.currentDrawable];\n```\n\nFinally, the sample calls the `signal:` method to indicate that the frame has been completed and rendered.\n\n```objective-c\n\/\/ Signal event for the frame completion\n[_event signal:commandBuffer];\n\n\/\/ Finalize rendering for the frame\n[commandBuffer commit];\n```\n\n\n\n## Resource memory allocation and management\n\n- **Using argument buffers with resource heaps**: Reduce CPU overhead by using arrays inside argument buffers and combining them with resource heaps.\n- **Implementing a multistage image filter using heaps and fences**: Use fences to synchronize access to resources allocated on a heap.\n- **MTLHeap**: A memory pool from which you can suballocate resources.\n- **MTLHeapDescriptor**: A configuration that customizes the behavior for a Metal memory heap.\n- **MTLHeapType**: The options you use to choose the heap type.\n- **MTLSizeAndAlign**: The size and alignment of a resource, in bytes.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Reduce CPU overhead by using arrays inside argument buffers and combining them with resource heaps.",
          "name" : "Using argument buffers with resource heaps",
          "url" : "https:\/\/developer.apple.com\/documentation\/Metal\/using-argument-buffers-with-resource-heaps"
        },
        {
          "description" : "Use fences to synchronize access to resources allocated on a heap.",
          "name" : "Implementing a multistage image filter using heaps and fences",
          "url" : "https:\/\/developer.apple.com\/documentation\/Metal\/implementing-a-multistage-image-filter-using-heaps-and-fences"
        },
        {
          "description" : "A memory pool from which you can suballocate resources.",
          "name" : "MTLHeap",
          "url" : "https:\/\/developer.apple.com\/documentation\/Metal\/MTLHeap"
        },
        {
          "description" : "A configuration that customizes the behavior for a Metal memory heap.",
          "name" : "MTLHeapDescriptor",
          "url" : "https:\/\/developer.apple.com\/documentation\/Metal\/MTLHeapDescriptor"
        },
        {
          "description" : "The options you use to choose the heap type.",
          "name" : "MTLHeapType",
          "url" : "https:\/\/developer.apple.com\/documentation\/Metal\/MTLHeapType"
        },
        {
          "description" : "The size and alignment of a resource, in bytes.",
          "name" : "MTLSizeAndAlign",
          "url" : "https:\/\/developer.apple.com\/documentation\/Metal\/MTLSizeAndAlign"
        }
      ],
      "title" : "Resource memory allocation and management"
    }
  ],
  "source" : "appleJSON",
  "title" : "Implementing a multistage image filter using heaps and events",
  "url" : "https:\/\/developer.apple.com\/documentation\/Metal\/implementing-a-multistage-image-filter-using-heaps-and-events"
}