{
  "abstract" : "Enable people to find app content that matches their surroundings or objects onscreen with visual intelligence.",
  "codeExamples" : [
    {
      "code" : "struct LandmarkEntity: IndexedEntity {\n    static var typeDisplayRepresentation: TypeDisplayRepresentation {\n        return TypeDisplayRepresentation(\n            name: LocalizedStringResource(\"Landmark\", table: \"AppIntents\", comment: \"The type name for the landmark entity\"),\n            numericFormat: \"\\(placeholder: .int) landmarks\"\n        )\n    }\n\n    var displayRepresentation: DisplayRepresentation {\n        DisplayRepresentation(\n            title: \"\\(name)\",\n            subtitle: \"\\(continent)\",\n            image: .init(data: try! self.thumbnailRepresentationData)\n        )\n    }\n\n    \/\/ ...\n}",
      "language" : "swift"
    },
    {
      "code" : "struct LandmarkIntentValueQuery: IntentValueQuery {\n\n    @Dependency var modelData: ModelData\n\n    func values(for input: SemanticContentDescriptor) async throws -> [VisualSearchResult] {\n\n        guard let pixelBuffer: CVReadOnlyPixelBuffer = input.pixelBuffer else {\n            return []\n        }\n\n        let landmarks = try await modelData.search(matching: pixelBuffer)\n\n        return landmarks\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "private func createImage(_ pixelBuffer: CVReadOnlyPixelBuffer) -> CGImage? {\n    let context = CIContext()\n    let image = CIImage(cvPixelBuffer: pixelBuffer)\n    return context.createCGImage(image, from: image.extent)\n}",
      "language" : "swift"
    },
    {
      "code" : "struct OpenLandmarkIntent: OpenIntent {\n    static let title: LocalizedStringResource = \"Open Landmark\"\n\n    @Parameter(title: \"Landmark\", requestValueDialog: \"Which landmark?\")\n    var target: LandmarkEntity\n}",
      "language" : "swift"
    },
    {
      "code" : "@UnionValue\nenum VisualSearchResult {\n    case landmark(LandmarkEntity)\n    case collection(CollectionEntity)\n}\n\nstruct LandmarkIntentValueQuery: IntentValueQuery {\n\n    @Dependency var modelData: ModelData\n\n    func values(for input: SemanticContentDescriptor) async throws -> [VisualSearchResult] {\n        \/\/ ...\n\n        \/\/ Returned search results are either landmarks or a collection.\n        let landmarks = try await modelData.search(matching: pixelBuffer)\n\n        return landmarks\n    }\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "532d3007c9c90419545339909f8d6e6726bf5a517cc6845aa3603c3fa0cc9002",
  "crawledAt" : "2025-12-02T15:26:01Z",
  "id" : "00714E79-6C9E-4451-94DA-5FE127D085EE",
  "kind" : "article",
  "language" : "swift",
  "module" : "Visual Intelligence",
  "overview" : "## Overview\n\nWith visual intelligence, people can visually search for information and content that matches their surroundings, or an onscreen object. Integrating your app with visual intelligence allows people to view your matching content quickly and launch your app for more detailed information or additional search results, giving it additional visibility.\n\n### Explore the role of the App Intents framework\n\nTo integrate your app with visual intelligence, the Visual Intelligence framework provides information about objects it detects in the visual intelligence camera or a screenshot. To exchange information with your app, the system uses the [doc:\/\/com.apple.documentation\/documentation\/AppIntents] framework and its concepts of app intents and app entities.\n\nWhen a person performs visual search on the visual intelligence camera or a screenshot, the system forwards the information captured to an App Intents query you implement. In your query code, search your app’s content for matching items, and return them to visual intelligence as app entities. Visual intelligence then uses the app entities to display your content in the search results view, right where a person needs it.\n\nTo learn more about a displayed item, someone can tap it to open the item in your app and view information and functionality. For example, an app that allows people to view information about landmarks might show detailed information like hours, a map, or community reviews for the item a person taps in visual search.\n\n### Provide a display representation\n\nVisual Intelligence uses the [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/DisplayRepresentation] of your [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/AppEntity] to organize and present your content in the visual intelligence search experience. Make sure to provide localized, concise, and high-quality display representations that consist of a title, subtitle, and an image. The following code from the [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/adopting-app-intents-to-support-system-experiences] sample code project shows the display representation of an `AppEntity` for a landmark. It uses strings from the model object for simplicity. In your code, make sure to provide a localized display representation.\n\nFor additional information about display representations, refer to [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/Integrating-custom-types-into-your-intents#Provide-a-visual-representation-for-your-entity].\n\n### Provide search results\n\nTo integrate your app with visual search, provide visual intelligence with content that matches a person’s surroundings or onscreen object, as described in the steps below and illustrated in the following image:\n\n\n\nThe following example code from the [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/adopting-app-intents-to-support-system-experiences] sample code project demonstrates how an app that enables people to view information about points of interest and landmarks might access the `pixelBuffer` for its search:\n\nThe `search(matching:)` function asynchronously returns a list of app entities that represent landmarks. Returning results quickly makes for a good search experience, so make sure to limit the list of returned items, if needed. If your app finds a large number of matches — for example, several hundred items — you might return the first hundred results, and give people the opportunity to view the full list in your app as described in [doc:\/\/com.apple.visualintelligence\/documentation\/VisualIntelligence\/integrating-your-app-with-visual-intelligence#Link-to-additional-results-in-your-app].\n\nThe process for matching the provided pixel buffer to app entities depends on your app. A common case is to convert the pixel buffer into an image, then use the image in an image search. The following code snippet shows how you might implement this conversion:\n\n### Open an item in your app\n\nTo allow someone to open your app and view additional information or access additional actions for a visual search, create an [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/OpenIntent]. In the intent’s `perform()` method, open your app to match the app entity that visual intelligence passes to the method, as illustrated in the image below.\n\n\n\nContinuing the example that shows information about points of interest or landmarks, the `OpenIntent` might look like this:\n\nAdopting the `OpenIntent` protocol isn’t specific to integrating your app with visual intelligence. Adopting App Intents, including one or more `OpenIntent` implementations, is a best practice for modern apps that offer additional integration with system experiences. If you’ve already adopted App Intents, you might be able to reuse existing code to open an item in your app with an `OpenIntent`.\n\nFor more information about adopting App Intents in your app, refer to [doc:\/\/com.apple.documentation\/documentation\/AppIntents] and [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/Making-actions-and-content-discoverable-and-widely-available].\n\n### Return different values in one query\n\nYour app can’t contain more than one [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/IntentValueQuery] that takes a [doc:\/\/com.apple.visualintelligence\/documentation\/VisualIntelligence\/SemanticContentDescriptor]. To return more than one [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/AppEntity] type from a single intent value query, use the [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/UnionValue()] Swift macro to return multiple app entity types. The following example uses a union value for its result — indicated by the `@UnionValue` annotation — to return a list of individual landmarks and collections of landmarks:\n\n### Link to additional results in your app\n\nReturning visual search results quickly and limiting the number of items ensures a quick and enjoyable experience for people using your app. However, your app might offer a lot — possibly hundreds — of results, or browsing long lists of items might be part of your app’s core experience. If you need to provide many results, display a limited amount and allow people to open your app from the “More results” button to view more visual search results.\n\nFirst, create a new app intent that conforms to the [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/AssistantSchemas\/VisualIntelligenceIntent\/semanticContentSearch] schema. With App Intents domains and schemas, you can quickly create app intents that follow a predefined form to enable specific functionality, such as opening a content search experience or list of results.\n\nIn the semantic content search intent’s `perform()` method, navigate to your app’s search experience and pass information that the [doc:\/\/com.apple.visualintelligence\/documentation\/VisualIntelligence\/SemanticContentDescriptor] object provides to perform a search and show the full list of results.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/VisualIntelligence\/integrating-your-app-with-visual-intelligence\ncrawled: 2025-12-02T15:26:01Z\n---\n\n# Integrating your app with visual intelligence\n\n**Article**\n\nEnable people to find app content that matches their surroundings or objects onscreen with visual intelligence.\n\n## Overview\n\nWith visual intelligence, people can visually search for information and content that matches their surroundings, or an onscreen object. Integrating your app with visual intelligence allows people to view your matching content quickly and launch your app for more detailed information or additional search results, giving it additional visibility.\n\n### Explore the role of the App Intents framework\n\nTo integrate your app with visual intelligence, the Visual Intelligence framework provides information about objects it detects in the visual intelligence camera or a screenshot. To exchange information with your app, the system uses the [doc:\/\/com.apple.documentation\/documentation\/AppIntents] framework and its concepts of app intents and app entities.\n\nWhen a person performs visual search on the visual intelligence camera or a screenshot, the system forwards the information captured to an App Intents query you implement. In your query code, search your app’s content for matching items, and return them to visual intelligence as app entities. Visual intelligence then uses the app entities to display your content in the search results view, right where a person needs it.\n\nTo learn more about a displayed item, someone can tap it to open the item in your app and view information and functionality. For example, an app that allows people to view information about landmarks might show detailed information like hours, a map, or community reviews for the item a person taps in visual search.\n\n### Provide a display representation\n\nVisual Intelligence uses the [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/DisplayRepresentation] of your [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/AppEntity] to organize and present your content in the visual intelligence search experience. Make sure to provide localized, concise, and high-quality display representations that consist of a title, subtitle, and an image. The following code from the [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/adopting-app-intents-to-support-system-experiences] sample code project shows the display representation of an `AppEntity` for a landmark. It uses strings from the model object for simplicity. In your code, make sure to provide a localized display representation.\n\n```swift\nstruct LandmarkEntity: IndexedEntity {\n    static var typeDisplayRepresentation: TypeDisplayRepresentation {\n        return TypeDisplayRepresentation(\n            name: LocalizedStringResource(\"Landmark\", table: \"AppIntents\", comment: \"The type name for the landmark entity\"),\n            numericFormat: \"\\(placeholder: .int) landmarks\"\n        )\n    }\n\n    var displayRepresentation: DisplayRepresentation {\n        DisplayRepresentation(\n            title: \"\\(name)\",\n            subtitle: \"\\(continent)\",\n            image: .init(data: try! self.thumbnailRepresentationData)\n        )\n    }\n\n    \/\/ ...\n}\n```\n\nFor additional information about display representations, refer to [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/Integrating-custom-types-into-your-intents#Provide-a-visual-representation-for-your-entity].\n\n### Provide search results\n\nTo integrate your app with visual search, provide visual intelligence with content that matches a person’s surroundings or onscreen object, as described in the steps below and illustrated in the following image:\n\n1. In your Xcode project, adopt the [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/IntentValueQuery] protocol and implement its [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/IntentValueQuery\/values(for:)] requirement.\n2. Change the [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/IntentValueQuery\/values(for:)] function to receive a [doc:\/\/com.apple.visualintelligence\/documentation\/VisualIntelligence\/SemanticContentDescriptor] as its `input`. The [doc:\/\/com.apple.visualintelligence\/documentation\/VisualIntelligence\/SemanticContentDescriptor] makes visual intelligence information available to your app.\n3. Use the descriptor’s [doc:\/\/com.apple.visualintelligence\/documentation\/VisualIntelligence\/SemanticContentDescriptor\/labels] to access a list of labels that visual intelligence creates or the [doc:\/\/com.apple.visualintelligence\/documentation\/VisualIntelligence\/SemanticContentDescriptor\/pixelBuffer] of the camera capture.\n4. Search your app’s content using the labels and perform an image search with an image you create from the `pixelBuffer`.\n5. Describe your search results as [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/AppEntity] objects and return them as the result of the query.\n\n\n\n\n\nThe following example code from the [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/adopting-app-intents-to-support-system-experiences] sample code project demonstrates how an app that enables people to view information about points of interest and landmarks might access the `pixelBuffer` for its search:\n\n```swift\nstruct LandmarkIntentValueQuery: IntentValueQuery {\n\n    @Dependency var modelData: ModelData\n\n    func values(for input: SemanticContentDescriptor) async throws -> [VisualSearchResult] {\n\n        guard let pixelBuffer: CVReadOnlyPixelBuffer = input.pixelBuffer else {\n            return []\n        }\n\n        let landmarks = try await modelData.search(matching: pixelBuffer)\n\n        return landmarks\n    }\n}\n```\n\nThe `search(matching:)` function asynchronously returns a list of app entities that represent landmarks. Returning results quickly makes for a good search experience, so make sure to limit the list of returned items, if needed. If your app finds a large number of matches — for example, several hundred items — you might return the first hundred results, and give people the opportunity to view the full list in your app as described in [doc:\/\/com.apple.visualintelligence\/documentation\/VisualIntelligence\/integrating-your-app-with-visual-intelligence#Link-to-additional-results-in-your-app].\n\nThe process for matching the provided pixel buffer to app entities depends on your app. A common case is to convert the pixel buffer into an image, then use the image in an image search. The following code snippet shows how you might implement this conversion:\n\n```swift\nprivate func createImage(_ pixelBuffer: CVReadOnlyPixelBuffer) -> CGImage? {\n    let context = CIContext()\n    let image = CIImage(cvPixelBuffer: pixelBuffer)\n    return context.createCGImage(image, from: image.extent)\n}\n```\n\n### Open an item in your app\n\nTo allow someone to open your app and view additional information or access additional actions for a visual search, create an [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/OpenIntent]. In the intent’s `perform()` method, open your app to match the app entity that visual intelligence passes to the method, as illustrated in the image below.\n\n\n\nContinuing the example that shows information about points of interest or landmarks, the `OpenIntent` might look like this:\n\n```swift\nstruct OpenLandmarkIntent: OpenIntent {\n    static let title: LocalizedStringResource = \"Open Landmark\"\n\n    @Parameter(title: \"Landmark\", requestValueDialog: \"Which landmark?\")\n    var target: LandmarkEntity\n}\n```\n\n\n\nAdopting the `OpenIntent` protocol isn’t specific to integrating your app with visual intelligence. Adopting App Intents, including one or more `OpenIntent` implementations, is a best practice for modern apps that offer additional integration with system experiences. If you’ve already adopted App Intents, you might be able to reuse existing code to open an item in your app with an `OpenIntent`.\n\nFor more information about adopting App Intents in your app, refer to [doc:\/\/com.apple.documentation\/documentation\/AppIntents] and [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/Making-actions-and-content-discoverable-and-widely-available].\n\n### Return different values in one query\n\nYour app can’t contain more than one [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/IntentValueQuery] that takes a [doc:\/\/com.apple.visualintelligence\/documentation\/VisualIntelligence\/SemanticContentDescriptor]. To return more than one [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/AppEntity] type from a single intent value query, use the [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/UnionValue()] Swift macro to return multiple app entity types. The following example uses a union value for its result — indicated by the `@UnionValue` annotation — to return a list of individual landmarks and collections of landmarks:\n\n```swift\n@UnionValue\nenum VisualSearchResult {\n    case landmark(LandmarkEntity)\n    case collection(CollectionEntity)\n}\n\nstruct LandmarkIntentValueQuery: IntentValueQuery {\n\n    @Dependency var modelData: ModelData\n\n    func values(for input: SemanticContentDescriptor) async throws -> [VisualSearchResult] {\n        \/\/ ...\n\n        \/\/ Returned search results are either landmarks or a collection.\n        let landmarks = try await modelData.search(matching: pixelBuffer)\n\n        return landmarks\n    }\n}\n```\n\n### Link to additional results in your app\n\nReturning visual search results quickly and limiting the number of items ensures a quick and enjoyable experience for people using your app. However, your app might offer a lot — possibly hundreds — of results, or browsing long lists of items might be part of your app’s core experience. If you need to provide many results, display a limited amount and allow people to open your app from the “More results” button to view more visual search results.\n\nFirst, create a new app intent that conforms to the [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/AssistantSchemas\/VisualIntelligenceIntent\/semanticContentSearch] schema. With App Intents domains and schemas, you can quickly create app intents that follow a predefined form to enable specific functionality, such as opening a content search experience or list of results.\n\n\n\nIn the semantic content search intent’s `perform()` method, navigate to your app’s search experience and pass information that the [doc:\/\/com.apple.visualintelligence\/documentation\/VisualIntelligence\/SemanticContentDescriptor] object provides to perform a search and show the full list of results.\n\n## Essentials\n\n- **Adopting App Intents to support system experiences**: Create app intents and entities to incorporate system experiences such as Spotlight, visual intelligence, and Shortcuts.\n- **SemanticContentDescriptor**: A type that represents a scene that visual intelligence captures, like a screenshot, photo, or photo and video stream.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Create app intents and entities to incorporate system experiences such as Spotlight, visual intelligence, and Shortcuts.",
          "name" : "Adopting App Intents to support system experiences",
          "url" : "https:\/\/developer.apple.com\/documentation\/AppIntents\/adopting-app-intents-to-support-system-experiences"
        },
        {
          "description" : "A type that represents a scene that visual intelligence captures, like a screenshot, photo, or photo and video stream.",
          "name" : "SemanticContentDescriptor",
          "url" : "https:\/\/developer.apple.com\/documentation\/VisualIntelligence\/SemanticContentDescriptor"
        }
      ],
      "title" : "Essentials"
    }
  ],
  "source" : "appleJSON",
  "title" : "Integrating your app with visual intelligence",
  "url" : "https:\/\/developer.apple.com\/documentation\/VisualIntelligence\/integrating-your-app-with-visual-intelligence"
}