{
  "abstract" : "Arrange your training data for an object detector in one of several different structured ways.",
  "codeExamples" : [
    {
      "code" : "[\n{\n  \"imagefilename\": \"breakfast_0.png\",\n  \"annotation\": [\n    {\n      \"coordinates\": {\n        \"y\": 156.062,\n        \"x\": 195.122,\n        \"height\": 148.872,\n        \"width\": 148.03\n      },\n      \"label\": \"Waffle\"\n    }\n  ]\n},\n{\n  \"imagefilename\": \"breakfast_1.png\",\n  \"annotation\": [\n    {\n      \"coordinates\": {\n        \"y\": 98.9209,\n        \"x\": 128.744,\n        \"height\": 151.798,\n        \"width\": 153.056\n      },\n      \"label\": \"Waffle\"\n    },\n    {\n      \"coordinates\": {\n        \"y\": 182.254,\n        \"x\": 256.172,\n        \"height\": 159.712,\n        \"width\": 208.147\n      },\n      \"label\": \"Croissant\"\n    }\n  ]\n},\n{\n  \"imagefilename\": \"breakfast_2.png\",\n  \"annotation\": [\n    {\n      \"coordinates\": {\n        \"y\": 116.875,\n        \"x\": 354.375,\n        \"height\": 98.7501,\n        \"width\": 85\n      },\n      \"label\": \"Croissant\"\n    }\n  ]\n},\n{\n  \"imagefilename\": \"breakfast_3.png\",\n  \"annotation\": [\n    {\n      \"coordinates\": {\n        \"y\": 149.75,\n        \"x\": 182.114,\n        \"height\": 76.3134,\n        \"width\": 101.063\n      },\n      \"label\": \"Croissant\"\n    }\n  ]\n}\n]"
    },
    {
      "code" : "\"annotation\": [\n{\n  \"coordinates\": {\n    \"y\": 0.13,\n    \"x\": 0.16,\n    \"height\": 0.12,\n    \"width\": 0.12\n  },\n  \"label\": \"Waffle\"\n}\n]"
    },
    {
      "code" : "\/\/\/ A typical annotation JSON file has many more objects in its base\n\/\/\/ array, one for each image file.\ncase boundingBox(units: MLBoundingBoxUnits = .pixel, origin: MLBoundingBoxCoordinatesOrigin = .topLeft, anchor: MLBoundingBoxAnchor = .center)"
    }
  ],
  "contentHash" : "2a38edd772c9120ff0f8282d1fa1833da4cff86fd79a0f0a189081c9062c2e55",
  "crawledAt" : "2025-12-05T13:27:28Z",
  "id" : "F8D2EBFF-5043-45AE-AB5B-940D1D68AFE8",
  "kind" : "article",
  "language" : "swift",
  "module" : "Create ML",
  "overview" : "## Overview\n\nThe general tasks for structuring data for training an object detector are the following:\n\nCreate ML accepts several data-source types for an object detector. Two major kinds of data are inherent in any labeled data set:\n\nThese kinds of data represent the two steps required to build an annotated data set to train an object detector model. First, collect the images you wish to train on. Second, annotate them with labeled bounding boxes that show the location of the objects you want your model to recognize. Both of these steps, as well as how to combine this data into a data source, are detailed below.\n\nFor more information about the object detector data-source types the Create ML API supports, see [doc:\/\/com.apple.createml\/documentation\/CreateML\/MLObjectDetector].\n\n### Collect images\n\nBefore you train, ensure that the training set has sufficient sample size and diversity within each label. Make your data reflect what your model sees in practice.\n\nThe images can be any standard image format. After you collect your training set, put it in a common folder, and proceed with annotations.\n\n### Format a bounding box JSON annotation\n\nAnnotating images in an object detector teaches your model what objects it’s trying to recognize. Define zero, one, or multiple bounding boxes within each image in your training data set that show where an object is and what kind of object it is.\n\nHere is a training data image:\n\n\n\nHere is an illustration of how a bounding box on a training data image is structured:\n\n\n\nOne way to structure your annotations is with a JSON annotation file, structured like this:\n\nEach JSON object in the `annotation` array must have the following JSON object structure.\n\nThe `coordinates` JSON object must have the following structure.\n\nWhen put together, a full object detector annotation file looks like the following:\n\n### Customize annotations\n\nEach element of the annotated JSON references one image with `imagefilename` and defines one array of annotations in `annotation`. Make sure to include the file extension in the filename. You can define any number of labeled objects within a single image. Each element of the array contains `coordinates`, consisting of the `x`-coordinate, `y`-coordinate, `width`, and `height` of the bounding box.\n\nYou can define the origin from which x- and y-coordinates are specified as either `bottom left` or `top left` by using `MLBoundingBoxCoordinatesOrigin`, with `top left` as a default value.\n\nAdditionally, you can define an `MLBoundingBoxAnchor` with possible values of `center`, `top left`, and `bottom left`, with `center` as a default value. The anchor shows the position of the bounding box relative to the origin, so if you select `center`, then the x- and y-values in the annotation point to the center of each bounding box.\n\nFinally, you can define `MLBoundingBoxUnits` as either `pixels` or `normalized`. The default option is `pixels`, which counts integer numbers of pixels from the origin. In contrast, `normalized` is on a scale from 0.0 to 1.0 in both the x- and y-dimensions.\n\nIf `MLBoundingBoxUnits` is set to `normalized`, then the annotation might look like the following:\n\nHere is the full `boundingBox` case defined in code:\n\nFor more detail, see [doc:\/\/com.apple.createml\/documentation\/CreateML\/MLObjectDetector\/AnnotationType].\n\n### Build a data source from images and annotations\n\nThe simplest way to structure your data is using the `directoryWithImagesAndJsonAnnotation` case. Take the collected images and exactly one JSON annotation file, and put them all in a single folder.\n\n\n\nUse this folder to train an object detector. The Create ML App uses this case, and you can drag a folder into a training well to train an object detector.\n\n\n\nAfter you train a model in the Create ML app, you can generate bounding boxes from the trained model in the Preview tab.\n\n### Build a data source from a directory with images\n\nThe `DataSource` case `directoryWithImages` is similar to `directoryWithImagesAndJsonAnnotation`, except you don’t need to nest the JSON annotation file in the folder with the images. Instead, you call it by reference from any specified URL.\n\n\n\n### Build a data source from a data frame\n\nIn the case `frame`, you call an object detector from a single `DataFrame`. Rather than store your data in a directory structure, the format of this table is similar to the annotation file, where the `DataFrame` has the following columns:\n\n",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/createml\/building-an-object-detector-data-source\ncrawled: 2025-12-05T13:27:28Z\n---\n\n# Building an object detector data source\n\n**Article**\n\nArrange your training data for an object detector in one of several different structured ways.\n\n## Overview\n\nThe general tasks for structuring data for training an object detector are the following:\n\n- Collect a set of images with sufficient size and diversity to reflect the data set you expect to predict on.\n- Create annotations for each image, containing labels and bounding boxes for each labeled object.\n- Structure these in a folder with an annotation file or `DataFrame` to best suit your use case.\n\nCreate ML accepts several data-source types for an object detector. Two major kinds of data are inherent in any labeled data set:\n\n\n\nThese kinds of data represent the two steps required to build an annotated data set to train an object detector model. First, collect the images you wish to train on. Second, annotate them with labeled bounding boxes that show the location of the objects you want your model to recognize. Both of these steps, as well as how to combine this data into a data source, are detailed below.\n\nFor more information about the object detector data-source types the Create ML API supports, see [doc:\/\/com.apple.createml\/documentation\/CreateML\/MLObjectDetector].\n\n### Collect images\n\nBefore you train, ensure that the training set has sufficient sample size and diversity within each label. Make your data reflect what your model sees in practice.\n\nThe images can be any standard image format. After you collect your training set, put it in a common folder, and proceed with annotations.\n\n### Format a bounding box JSON annotation\n\nAnnotating images in an object detector teaches your model what objects it’s trying to recognize. Define zero, one, or multiple bounding boxes within each image in your training data set that show where an object is and what kind of object it is.\n\nHere is a training data image:\n\n\n\nHere is an illustration of how a bounding box on a training data image is structured:\n\n\n\nOne way to structure your annotations is with a JSON annotation file, structured like this:\n\n\n\nEach JSON object in the `annotation` array must have the following JSON object structure.\n\n\n\nThe `coordinates` JSON object must have the following structure.\n\n\n\nWhen put together, a full object detector annotation file looks like the following:\n\n```\n[\n{\n  \"imagefilename\": \"breakfast_0.png\",\n  \"annotation\": [\n    {\n      \"coordinates\": {\n        \"y\": 156.062,\n        \"x\": 195.122,\n        \"height\": 148.872,\n        \"width\": 148.03\n      },\n      \"label\": \"Waffle\"\n    }\n  ]\n},\n{\n  \"imagefilename\": \"breakfast_1.png\",\n  \"annotation\": [\n    {\n      \"coordinates\": {\n        \"y\": 98.9209,\n        \"x\": 128.744,\n        \"height\": 151.798,\n        \"width\": 153.056\n      },\n      \"label\": \"Waffle\"\n    },\n    {\n      \"coordinates\": {\n        \"y\": 182.254,\n        \"x\": 256.172,\n        \"height\": 159.712,\n        \"width\": 208.147\n      },\n      \"label\": \"Croissant\"\n    }\n  ]\n},\n{\n  \"imagefilename\": \"breakfast_2.png\",\n  \"annotation\": [\n    {\n      \"coordinates\": {\n        \"y\": 116.875,\n        \"x\": 354.375,\n        \"height\": 98.7501,\n        \"width\": 85\n      },\n      \"label\": \"Croissant\"\n    }\n  ]\n},\n{\n  \"imagefilename\": \"breakfast_3.png\",\n  \"annotation\": [\n    {\n      \"coordinates\": {\n        \"y\": 149.75,\n        \"x\": 182.114,\n        \"height\": 76.3134,\n        \"width\": 101.063\n      },\n      \"label\": \"Croissant\"\n    }\n  ]\n}\n]\n```\n\n### Customize annotations\n\nEach element of the annotated JSON references one image with `imagefilename` and defines one array of annotations in `annotation`. Make sure to include the file extension in the filename. You can define any number of labeled objects within a single image. Each element of the array contains `coordinates`, consisting of the `x`-coordinate, `y`-coordinate, `width`, and `height` of the bounding box.\n\nYou can define the origin from which x- and y-coordinates are specified as either `bottom left` or `top left` by using `MLBoundingBoxCoordinatesOrigin`, with `top left` as a default value.\n\nAdditionally, you can define an `MLBoundingBoxAnchor` with possible values of `center`, `top left`, and `bottom left`, with `center` as a default value. The anchor shows the position of the bounding box relative to the origin, so if you select `center`, then the x- and y-values in the annotation point to the center of each bounding box.\n\nFinally, you can define `MLBoundingBoxUnits` as either `pixels` or `normalized`. The default option is `pixels`, which counts integer numbers of pixels from the origin. In contrast, `normalized` is on a scale from 0.0 to 1.0 in both the x- and y-dimensions.\n\nIf `MLBoundingBoxUnits` is set to `normalized`, then the annotation might look like the following:\n\n```\n\"annotation\": [\n{\n  \"coordinates\": {\n    \"y\": 0.13,\n    \"x\": 0.16,\n    \"height\": 0.12,\n    \"width\": 0.12\n  },\n  \"label\": \"Waffle\"\n}\n]\n```\n\nHere is the full `boundingBox` case defined in code:\n\n```\n\/\/\/ A typical annotation JSON file has many more objects in its base\n\/\/\/ array, one for each image file.\ncase boundingBox(units: MLBoundingBoxUnits = .pixel, origin: MLBoundingBoxCoordinatesOrigin = .topLeft, anchor: MLBoundingBoxAnchor = .center)\n```\n\nFor more detail, see [doc:\/\/com.apple.createml\/documentation\/CreateML\/MLObjectDetector\/AnnotationType].\n\n### Build a data source from images and annotations\n\nThe simplest way to structure your data is using the `directoryWithImagesAndJsonAnnotation` case. Take the collected images and exactly one JSON annotation file, and put them all in a single folder.\n\n\n\nUse this folder to train an object detector. The Create ML App uses this case, and you can drag a folder into a training well to train an object detector.\n\n\n\nAfter you train a model in the Create ML app, you can generate bounding boxes from the trained model in the Preview tab.\n\n### Build a data source from a directory with images\n\nThe `DataSource` case `directoryWithImages` is similar to `directoryWithImagesAndJsonAnnotation`, except you don’t need to nest the JSON annotation file in the folder with the images. Instead, you call it by reference from any specified URL.\n\n\n\n### Build a data source from a data frame\n\nIn the case `frame`, you call an object detector from a single `DataFrame`. Rather than store your data in a directory structure, the format of this table is similar to the annotation file, where the `DataFrame` has the following columns:\n\n- `imageColumn`: contains image file locations\n- `annotationColumn`: contains the annotations for that particular image, using the same JSON structure defined above.\n\n\n\n",
  "sections" : [

  ],
  "source" : "appleJSON",
  "title" : "Building an object detector data source",
  "url" : "https:\/\/developer.apple.com\/documentation\/createml\/building-an-object-detector-data-source"
}