{
  "abstract" : "A machine learning model you train with audio files to recognize and identify sounds on a device.",
  "codeExamples" : [

  ],
  "conformsTo" : [
    "Copyable",
    "CustomDebugStringConvertible",
    "CustomPlaygroundDisplayConvertible",
    "CustomStringConvertible",
    "Sendable",
    "SendableMetatype"
  ],
  "contentHash" : "30843273473e7c7a274dd2cb528482fca658907f3448dadbb8279c4c08df47ce",
  "crawledAt" : "2025-12-07T15:33:46Z",
  "declaration" : {
    "code" : "struct MLSoundClassifier",
    "language" : "swift"
  },
  "id" : "339863A5-0490-460C-909B-7927E5D29F95",
  "kind" : "struct",
  "language" : "swift",
  "module" : "Create ML",
  "overview" : "## Overview\n\nA sound classifier is a machine learning model that identifies and categorizes sounds in an app. Create a sound classifier by gathering a dataset of audio files and use them to train a model with [doc:\/\/com.apple.createml\/documentation\/CreateML\/MLSoundClassifier].\n\nAssemble an audio dataset by recording or gathering audio files that best represent the sounds you want your app to identify. Additionally, create a *negative class* — a group of related noises the sound classifier might hear but aren’t relevant — by collecting or recording example sounds.\n\nFor example, say you’re creating a sound classifier to identify laughter and applause. In addition to gathering audio examples of people laughing and clapping, you can add an additional category for background noise. By adding recordings from various settings, such as theaters and amphitheaters, your sound classifier can distinguish the sounds of interest from environmental noises. In other words, the sound classifier won’t predict “Applause” when there isn’t any. Like any classifier, when you request a prediction, a sound classifier always returns one of the categories it learned from a training dataset.\n\nGather at least 10 audio examples of each sound category you want the sound classifier to learn, plus at least one negative class for background noise. The audio examples can be in any file format that Core Audio supports, including:\n\nReduce a sound classifier’s bias — which can adversely affect its performance — by gathering audio files that use a consistent bit depth and sample rate.\n\nTrain, evaluate, and export your sound classifier by following similar steps to creating any other Create ML model type. For more information about the Create ML training workflow, see:\n\nAdd the sound classifier’s Core ML model to an Xcode project and use it to create an [doc:\/\/com.apple.documentation\/documentation\/SoundAnalysis\/SNClassifySoundRequest] at runtime. Your app uses the sound request to identify sounds in an audio file or audio stream by following the steps in the following articles, respectively:",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst",
    "macOS",
    "visionOS"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\ncrawled: 2025-12-07T15:33:46Z\n---\n\n# MLSoundClassifier\n\n**Structure**\n\nA machine learning model you train with audio files to recognize and identify sounds on a device.\n\n## Declaration\n\n```swift\nstruct MLSoundClassifier\n```\n\n## Overview\n\nA sound classifier is a machine learning model that identifies and categorizes sounds in an app. Create a sound classifier by gathering a dataset of audio files and use them to train a model with [doc:\/\/com.apple.createml\/documentation\/CreateML\/MLSoundClassifier].\n\nAssemble an audio dataset by recording or gathering audio files that best represent the sounds you want your app to identify. Additionally, create a *negative class* — a group of related noises the sound classifier might hear but aren’t relevant — by collecting or recording example sounds.\n\nFor example, say you’re creating a sound classifier to identify laughter and applause. In addition to gathering audio examples of people laughing and clapping, you can add an additional category for background noise. By adding recordings from various settings, such as theaters and amphitheaters, your sound classifier can distinguish the sounds of interest from environmental noises. In other words, the sound classifier won’t predict “Applause” when there isn’t any. Like any classifier, when you request a prediction, a sound classifier always returns one of the categories it learned from a training dataset.\n\nGather at least 10 audio examples of each sound category you want the sound classifier to learn, plus at least one negative class for background noise. The audio examples can be in any file format that Core Audio supports, including:\n\n- M4A\n- MP3\n- AIFF\n- WAV\n\n\n\nReduce a sound classifier’s bias — which can adversely affect its performance — by gathering audio files that use a consistent bit depth and sample rate.\n\nTrain, evaluate, and export your sound classifier by following similar steps to creating any other Create ML model type. For more information about the Create ML training workflow, see:\n\n- [doc:\/\/com.apple.createml\/documentation\/CreateML\/creating-an-image-classifier-model]\n- [doc:\/\/com.apple.createml\/documentation\/CreateML\/creating-an-action-classifier-model]\n\nAdd the sound classifier’s Core ML model to an Xcode project and use it to create an [doc:\/\/com.apple.documentation\/documentation\/SoundAnalysis\/SNClassifySoundRequest] at runtime. Your app uses the sound request to identify sounds in an audio file or audio stream by following the steps in the following articles, respectively:\n\n- [doc:\/\/com.apple.documentation\/documentation\/SoundAnalysis\/classifying-sounds-in-an-audio-file]\n- [doc:\/\/com.apple.documentation\/documentation\/SoundAnalysis\/classifying-sounds-in-an-audio-stream]\n\n## Training a sound classifier asynchronously\n\n- **train(trainingData:parameters:sessionParameters:)**: Begins an asynchronous sound classifier training session with a training dataset represented by a data source.\n- **makeTrainingSession(trainingData:parameters:sessionParameters:)**: Creates an asynchronous training session for a sound classifier.\n- **resume(_:)**: Begins or continues an asynchronous training session for a sound classifier.\n- **restoreTrainingSession(sessionParameters:)**: Creates an asynchronous training session for a sound classifier by restoring an existing training session’s state from its parameters.\n- **extractFeatures(trainingData:parameters:sessionParameters:)**: Begins an asynchronous session that extracts sound features from a data source of sound files.\n- **MLSoundClassifier.FeatureExtractionParameters**: Parameters that affect the process of extracting sound features from audio files.\n\n## Creating a sound classifier from a checkpoint\n\n- **init(checkpoint:)**: Creates a sound classifier from a training session checkpoint.\n\n## Training a sound classifier synchronously\n\n- **init(trainingData:parameters:)**: Creates a sound classifier with a training dataset represented by a data source.\n\n## Evaluating a sound classifier\n\n- **evaluation(on:)**: Generates metrics by evaluating the sound classifier’s performance on a dataset represented by a data source.\n- **trainingMetrics**: Measurements of the classifier’s performance on the training data set.\n- **validationMetrics**: Measurements of the image classifier’s performance on the validation dataset.\n\n## Testing a sound classifier\n\n- **predictions(from:)**: Generates predictions for an array of audio files.\n- **predictions(from:overlapFactor:predictionTimeWindowSize:)**: Generates predictions that use an overlap factor and time window size for an array of audio files.\n\n## Saving a sound classifier\n\n- **write(to:metadata:)**: Exports the sound classifier as a model file to a location in the file system.\n- **write(toFile:metadata:)**: Exports the sound classifier as a model file to a path in the file system.\n\n## Inspecting a sound classifier model\n\n- **model**: The underlying model instance of the sound classifier stored in memory.\n- **modelParameters**: The model configuration parameters the sound classifier used during its training session.\n\n## Describing a sound classifier\n\n- **description**: A text representation of the sound classifier.\n- **debugDescription**: A text representation of the sound classifier that’s suitable for output during debugging.\n- **playgroundDescription**: A description of the sound classifier in a playground.\n\n## Supporting types\n\n- **MLSoundClassifier.DataSource**: A representation of a sound-classifier dataset located in the file system or in a data table.\n- **MLSoundClassifier.ModelParameters**: Parameters that affect the process of training a sound-classifier model.\n\n## Default Implementations\n\n- **CustomDebugStringConvertible Implementations**\n- **CustomPlaygroundDisplayConvertible Implementations**\n- **CustomStringConvertible Implementations**\n\n## Conforms To\n\n- Copyable\n- CustomDebugStringConvertible\n- CustomPlaygroundDisplayConvertible\n- CustomStringConvertible\n- Sendable\n- SendableMetatype\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Begins an asynchronous sound classifier training session with a training dataset represented by a data source.",
          "name" : "train(trainingData:parameters:sessionParameters:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/train(trainingData:parameters:sessionParameters:)"
        },
        {
          "description" : "Creates an asynchronous training session for a sound classifier.",
          "name" : "makeTrainingSession(trainingData:parameters:sessionParameters:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/makeTrainingSession(trainingData:parameters:sessionParameters:)"
        },
        {
          "description" : "Begins or continues an asynchronous training session for a sound classifier.",
          "name" : "resume(_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/resume(_:)"
        },
        {
          "description" : "Creates an asynchronous training session for a sound classifier by restoring an existing training session’s state from its parameters.",
          "name" : "restoreTrainingSession(sessionParameters:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/restoreTrainingSession(sessionParameters:)"
        },
        {
          "description" : "Begins an asynchronous session that extracts sound features from a data source of sound files.",
          "name" : "extractFeatures(trainingData:parameters:sessionParameters:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/extractFeatures(trainingData:parameters:sessionParameters:)"
        },
        {
          "description" : "Parameters that affect the process of extracting sound features from audio files.",
          "name" : "MLSoundClassifier.FeatureExtractionParameters",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/FeatureExtractionParameters"
        }
      ],
      "title" : "Training a sound classifier asynchronously"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "Creates a sound classifier from a training session checkpoint.",
          "name" : "init(checkpoint:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/init(checkpoint:)"
        }
      ],
      "title" : "Creating a sound classifier from a checkpoint"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "Creates a sound classifier with a training dataset represented by a data source.",
          "name" : "init(trainingData:parameters:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/init(trainingData:parameters:)"
        }
      ],
      "title" : "Training a sound classifier synchronously"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "Generates metrics by evaluating the sound classifier’s performance on a dataset represented by a data source.",
          "name" : "evaluation(on:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/evaluation(on:)"
        },
        {
          "description" : "Measurements of the classifier’s performance on the training data set.",
          "name" : "trainingMetrics",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/trainingMetrics"
        },
        {
          "description" : "Measurements of the image classifier’s performance on the validation dataset.",
          "name" : "validationMetrics",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/validationMetrics"
        }
      ],
      "title" : "Evaluating a sound classifier"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "Generates predictions for an array of audio files.",
          "name" : "predictions(from:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/predictions(from:)"
        },
        {
          "description" : "Generates predictions that use an overlap factor and time window size for an array of audio files.",
          "name" : "predictions(from:overlapFactor:predictionTimeWindowSize:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/predictions(from:overlapFactor:predictionTimeWindowSize:)"
        }
      ],
      "title" : "Testing a sound classifier"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "Exports the sound classifier as a model file to a location in the file system.",
          "name" : "write(to:metadata:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/write(to:metadata:)"
        },
        {
          "description" : "Exports the sound classifier as a model file to a path in the file system.",
          "name" : "write(toFile:metadata:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/write(toFile:metadata:)"
        }
      ],
      "title" : "Saving a sound classifier"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "The underlying model instance of the sound classifier stored in memory.",
          "name" : "model",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/model"
        },
        {
          "description" : "The model configuration parameters the sound classifier used during its training session.",
          "name" : "modelParameters",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/modelParameters-swift.property"
        }
      ],
      "title" : "Inspecting a sound classifier model"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "A text representation of the sound classifier.",
          "name" : "description",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/description"
        },
        {
          "description" : "A text representation of the sound classifier that’s suitable for output during debugging.",
          "name" : "debugDescription",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/debugDescription"
        },
        {
          "description" : "A description of the sound classifier in a playground.",
          "name" : "playgroundDescription",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/playgroundDescription"
        }
      ],
      "title" : "Describing a sound classifier"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "A representation of a sound-classifier dataset located in the file system or in a data table.",
          "name" : "MLSoundClassifier.DataSource",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/DataSource"
        },
        {
          "description" : "Parameters that affect the process of training a sound-classifier model.",
          "name" : "MLSoundClassifier.ModelParameters",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/ModelParameters-swift.struct"
        }
      ],
      "title" : "Supporting types"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "",
          "name" : "CustomDebugStringConvertible Implementations",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/CustomDebugStringConvertible-Implementations"
        },
        {
          "description" : "",
          "name" : "CustomPlaygroundDisplayConvertible Implementations",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/CustomPlaygroundDisplayConvertible-Implementations"
        },
        {
          "description" : "",
          "name" : "CustomStringConvertible Implementations",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier\/CustomStringConvertible-Implementations"
        }
      ],
      "title" : "Default Implementations"
    }
  ],
  "source" : "appleJSON",
  "title" : "MLSoundClassifier",
  "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLSoundClassifier"
}