{
  "abstract" : "Arrange your training videos in multiple folders with labels that describe actions, or in a single folder with an annotation file.",
  "codeExamples" : [
    {
      "code" : "video, label, start, end\nlunge23.mov, Lunge, 0, 3.1\nsquats_13.mov, Squat, 0, 5\nJumping Jacks 1.mov, Jumping Jacks, 0, 4.7\nVarious_Exercises.mov, Lunge, 0.0, 2.2\nVarious_Exercises.mov, Lunge, 8:3.2, 8:4.5\nVarious_Exercises.mov, Squat, 1:59:5.8, 1:59:7\nVarious_Exercises.mov, Squat, 1:59:14, 1:59:15.1",
      "language" : "other"
    },
    {
      "code" : " [\n  {\n    \"video\" : \"lunge23.mov\",\n    \"label\" : \"Lunge\",\n    \"start\" : 0,\n    \"end\"   : 3.1\n  },\n  {\n    \"video\" : \"squats_13.mov\",\n    \"label\" : \"Squat\",\n    \"start\" : 0,\n    \"end\"   : 5\n  },\n  {\n    \"video\" : \"Jumping Jacks 1.mov\",\n    \"label\" : \" Jumping Jacks\",\n    \"start\" : 0,\n    \"end\"   : 4.7\n  },\n  {\n    \"video\" : \"Various_Exercises.mov\",\n    \"label\" : \"Lunge\",\n    \"start\" : 0.0,\n    \"end\"   : 2.2\n  },\n  {\n    \"video\" : \"Various_Exercises.mov\",\n    \"label\" : \"Lunge\",\n    \"start\" : \"8:3.2\",\n    \"end\"   : \"8:4.5\"\n  },\n  {\n    \"video\" : \"Various_Exercises.mov\",\n    \"label\" : \"Squat\",\n    \"start\" : \"1:59:5.8\",\n    \"end\"   : \"1:59:7\"\n  },\n  {\n    \"video\" : \"Various_Exercises.mov\",\n    \"label\" : \"Squat\",\n    \"start\" : \"1:59:14\",\n    \"end\"   : \"1:59:15.1\"\n  }\n]",
      "language" : "other"
    }
  ],
  "contentHash" : "8abeaba80060b1a3762a1612e0d0e142de6acbe2fc4ccc3481de1467549bfc87",
  "crawledAt" : "2025-12-09T19:47:37Z",
  "id" : "D34FF47F-C535-48B8-842F-9D243771CDDB",
  "kind" : "article",
  "language" : "swift",
  "module" : "Create ML",
  "overview" : "## Overview\n\nThe Create ML developer tool accepts several data-source types for an action classifier, each with its own file-system arrangement. Typically, you use one of these two types:\n\nIf you’ve trimmed all your example video files and they demonstrate a single action one or more times, you can use either data-source type. Otherwise, if you haven’t trimmed all your videos or if one or more is a *montage* video — a video file that contains two or more distinct actions — you must use annotated videos.\n\nFor more information about the action-classifier data-source types the Create ML API supports, see [doc:\/\/com.apple.createml\/documentation\/CreateML\/MLActionClassifier\/DataSource].\n\nFor general information about how to create an action classifier, see [doc:\/\/com.apple.createml\/documentation\/CreateML\/creating-an-action-classifier-model].\n\n### Build a Data Source with Labeled Folders\n\nCreate a parent folder to contain the labeled folders. Then, create a folder for each action, using the name of the action for the label. Place each example video file into the folder with a label that matches the video’s action. For example, for a data source that has folders named “Jumping Jacks” and “Squats,” place videos of those actions into their corresponding folders.\n\n\n\nEach video file may have more than one instance of an action, but the videos must minimize the time between each instance. If you notice inactivity between action instances, remove the inactivity by breaking the video file into separate files and trimming each video to the action.\n\n### Build a Data Source with Annotated Videos\n\nCreate a folder and place all your example video files in it. Then, create an annotation file using either a CSV — with either `.csv` or `.txt` file extensions — or JSON format.\n\n\n\nEach entry in the annotation file must have these categories:\n\nTo mark the beginning and ending of an example video clip within a larger video file, your annotation file can also include time-index categories:\n\nUse the `start` and `end` time indices for a *montage* video, which is a video that demonstrates two or more different types of action. If your annotation file omits the `start` category, Create ML defaults to the beginning of every video file for every entry. If your annotation file omits the `end` category, Create ML defaults to the end of every video.\n\nCreate ML recognizes several different types of time indices including:\n\nUse the these indices to specify when an example clip begins and ends within its video file. For example, if a single video file contains squats and lunges, you can use that file in multiple annotations with different time indices, as shown in this CSV file example:\n\nTo create an annotation file in JSON format, create an array of annotation objects at the root of the file. The following example shows the JSON equivalent of the preceding CSV file:",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/CreateML\/building-an-action-classifier-data-source\ncrawled: 2025-12-09T19:47:37Z\n---\n\n# Building an Action Classifier Data Source\n\n**Article**\n\nArrange your training videos in multiple folders with labels that describe actions, or in a single folder with an annotation file.\n\n## Overview\n\nThe Create ML developer tool accepts several data-source types for an action classifier, each with its own file-system arrangement. Typically, you use one of these two types:\n\n\n\nIf you’ve trimmed all your example video files and they demonstrate a single action one or more times, you can use either data-source type. Otherwise, if you haven’t trimmed all your videos or if one or more is a *montage* video — a video file that contains two or more distinct actions — you must use annotated videos.\n\nFor more information about the action-classifier data-source types the Create ML API supports, see [doc:\/\/com.apple.createml\/documentation\/CreateML\/MLActionClassifier\/DataSource].\n\nFor general information about how to create an action classifier, see [doc:\/\/com.apple.createml\/documentation\/CreateML\/creating-an-action-classifier-model].\n\n### Build a Data Source with Labeled Folders\n\nCreate a parent folder to contain the labeled folders. Then, create a folder for each action, using the name of the action for the label. Place each example video file into the folder with a label that matches the video’s action. For example, for a data source that has folders named “Jumping Jacks” and “Squats,” place videos of those actions into their corresponding folders.\n\n\n\n\n\nEach video file may have more than one instance of an action, but the videos must minimize the time between each instance. If you notice inactivity between action instances, remove the inactivity by breaking the video file into separate files and trimming each video to the action.\n\n### Build a Data Source with Annotated Videos\n\nCreate a folder and place all your example video files in it. Then, create an annotation file using either a CSV — with either `.csv` or `.txt` file extensions — or JSON format.\n\n\n\nEach entry in the annotation file must have these categories:\n\n\n\nTo mark the beginning and ending of an example video clip within a larger video file, your annotation file can also include time-index categories:\n\n\n\nUse the `start` and `end` time indices for a *montage* video, which is a video that demonstrates two or more different types of action. If your annotation file omits the `start` category, Create ML defaults to the beginning of every video file for every entry. If your annotation file omits the `end` category, Create ML defaults to the end of every video.\n\n\n\nCreate ML recognizes several different types of time indices including:\n\n- An integer of seconds, for example: `0`, `3`, or `5`.\n- A floating point number of seconds, for example: `0.0`, `3.14`, or `60.5`.\n- A string of minutes and seconds, for example: `01:03`.\n- A string of minutes, seconds, and fractional seconds, for example: `01:03.14`.\n- A string of hours, minutes, and seconds, for example: `05:01:03`.\n\nUse the these indices to specify when an example clip begins and ends within its video file. For example, if a single video file contains squats and lunges, you can use that file in multiple annotations with different time indices, as shown in this CSV file example:\n\n```other\nvideo, label, start, end\nlunge23.mov, Lunge, 0, 3.1\nsquats_13.mov, Squat, 0, 5\nJumping Jacks 1.mov, Jumping Jacks, 0, 4.7\nVarious_Exercises.mov, Lunge, 0.0, 2.2\nVarious_Exercises.mov, Lunge, 8:3.2, 8:4.5\nVarious_Exercises.mov, Squat, 1:59:5.8, 1:59:7\nVarious_Exercises.mov, Squat, 1:59:14, 1:59:15.1\n```\n\nTo create an annotation file in JSON format, create an array of annotation objects at the root of the file. The following example shows the JSON equivalent of the preceding CSV file:\n\n```other\n [\n  {\n    \"video\" : \"lunge23.mov\",\n    \"label\" : \"Lunge\",\n    \"start\" : 0,\n    \"end\"   : 3.1\n  },\n  {\n    \"video\" : \"squats_13.mov\",\n    \"label\" : \"Squat\",\n    \"start\" : 0,\n    \"end\"   : 5\n  },\n  {\n    \"video\" : \"Jumping Jacks 1.mov\",\n    \"label\" : \" Jumping Jacks\",\n    \"start\" : 0,\n    \"end\"   : 4.7\n  },\n  {\n    \"video\" : \"Various_Exercises.mov\",\n    \"label\" : \"Lunge\",\n    \"start\" : 0.0,\n    \"end\"   : 2.2\n  },\n  {\n    \"video\" : \"Various_Exercises.mov\",\n    \"label\" : \"Lunge\",\n    \"start\" : \"8:3.2\",\n    \"end\"   : \"8:4.5\"\n  },\n  {\n    \"video\" : \"Various_Exercises.mov\",\n    \"label\" : \"Squat\",\n    \"start\" : \"1:59:5.8\",\n    \"end\"   : \"1:59:7\"\n  },\n  {\n    \"video\" : \"Various_Exercises.mov\",\n    \"label\" : \"Squat\",\n    \"start\" : \"1:59:14\",\n    \"end\"   : \"1:59:15.1\"\n  }\n]\n```\n\n## Action Classifier Data Sources\n\n- **Gathering Training Videos for an Action Classifier**: Collect quality example videos that effectively train action classifiers.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Collect quality example videos that effectively train action classifiers.",
          "name" : "Gathering Training Videos for an Action Classifier",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/gathering-training-videos-for-an-action-classifier"
        }
      ],
      "title" : "Action Classifier Data Sources"
    }
  ],
  "source" : "appleJSON",
  "title" : "Building an Action Classifier Data Source",
  "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/building-an-action-classifier-data-source"
}