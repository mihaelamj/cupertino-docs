{
  "abstract" : "A table comparing the actual and predicted labels for each classification category.",
  "codeExamples" : [
    {
      "code" : "let confusion = model.validationMetrics.confusion\n\n\/\/ Filter for rows which contain mistakes.\nlet errors = confusion[confusion[\"True Label\"] != confusion[\"Predicted\"]]\nlet mostCommonError = errors.rows.max { row1, row2 in\n    row1[\"Count\", Int.self]! < row2[\"Count\", Int.self]!\n}\nprint(mostCommonError ?? \"The confusion table is empty.\")\n\/\/ [\"Predicted\": \"tech\", \"True Label\": \"business\", \"Count\": 9]",
      "language" : "swift"
    },
    {
      "code" : "print(model.validationMetrics)\n\/\/ ...\n\/\/ ******CONFUSION MATRIX******\n\/\/ ----------------------------------\n\/\/ True\\Pred business entertainment politics sport tech\n\/\/ business 113 2 3 0 9\n\/\/ entertainment 1 183 3 2 3\n\/\/ politics 6 8 116 0 3\n\/\/ sport 0 6 1 135 3\n\/\/ tech 2 7 3 0 129\n\/\/ ...",
      "language" : "swift"
    }
  ],
  "contentHash" : "76d9a6ceed2db773f141b7e7453acbe052620e517ff66620a9eeed645f9b874d",
  "crawledAt" : "2025-12-09T19:49:40Z",
  "declaration" : {
    "code" : "var confusion: MLDataTable { get }",
    "language" : "swift"
  },
  "id" : "1C0764DB-1E23-4E60-84EE-AE20B46B0448",
  "kind" : "property",
  "language" : "swift",
  "module" : "Create ML",
  "overview" : "## Discussion\n\nThe confusion data table describes how examples were mislabeled between categories. Each row contains the true label, the predicted label, and the count for each possible combination of categories. For example, the table below lists that “business” was labeled correctly with “business” 113 times, while “business” was confused with “entertainment” 2 times.\n\n\n\nTo gain insight into the performance of your model, you can use this data table to determine what categories your model is most confused about (making the most mistakes on) for a given data set. For example, the code listing below shows how to find the mistake that happens most frequently.\n\nAnother useful view into this data is to compare the actual and predicated labels using a matrix. Printing the [doc:\/\/com.apple.createml\/documentation\/CreateML\/MLClassifierMetrics] directly displays the matrix format.\n\nIn this example, the upper left hand count shows that 113 business examples were correctly labeled as “business”. The second column shows that “entertainment” was predicted for 2 “business” examples. The second row shows that 1 “entertainment” example was mislabeled as “business”.",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst",
    "macOS",
    "tvOS",
    "visionOS"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/CreateML\/MLClassifierMetrics\/confusion\ncrawled: 2025-12-09T19:49:40Z\n---\n\n# confusion\n\n**Instance Property**\n\nA table comparing the actual and predicted labels for each classification category.\n\n## Declaration\n\n```swift\nvar confusion: MLDataTable { get }\n```\n\n## Discussion\n\nThe confusion data table describes how examples were mislabeled between categories. Each row contains the true label, the predicted label, and the count for each possible combination of categories. For example, the table below lists that “business” was labeled correctly with “business” 113 times, while “business” was confused with “entertainment” 2 times.\n\n\n\nTo gain insight into the performance of your model, you can use this data table to determine what categories your model is most confused about (making the most mistakes on) for a given data set. For example, the code listing below shows how to find the mistake that happens most frequently.\n\n```swift\nlet confusion = model.validationMetrics.confusion\n\n\/\/ Filter for rows which contain mistakes.\nlet errors = confusion[confusion[\"True Label\"] != confusion[\"Predicted\"]]\nlet mostCommonError = errors.rows.max { row1, row2 in\n    row1[\"Count\", Int.self]! < row2[\"Count\", Int.self]!\n}\nprint(mostCommonError ?? \"The confusion table is empty.\")\n\/\/ [\"Predicted\": \"tech\", \"True Label\": \"business\", \"Count\": 9]\n```\n\nAnother useful view into this data is to compare the actual and predicated labels using a matrix. Printing the [doc:\/\/com.apple.createml\/documentation\/CreateML\/MLClassifierMetrics] directly displays the matrix format.\n\n```swift\nprint(model.validationMetrics)\n\/\/ ...\n\/\/ ******CONFUSION MATRIX******\n\/\/ ----------------------------------\n\/\/ True\\Pred business entertainment politics sport tech\n\/\/ business 113 2 3 0 9\n\/\/ entertainment 1 183 3 2 3\n\/\/ politics 6 8 116 0 3\n\/\/ sport 0 6 1 135 3\n\/\/ tech 2 7 3 0 129\n\/\/ ...\n```\n\nIn this example, the upper left hand count shows that 113 business examples were correctly labeled as “business”. The second column shows that “entertainment” was predicted for 2 “business” examples. The second row shows that 1 “entertainment” example was mislabeled as “business”.\n\n## Understanding the model\n\n- **classificationError**: The fraction of incorrectly labeled examples.\n- **precisionRecall**: A data table listing the precision and recall percentages for each class.\n- **confusionDataFrame**: A data frame comparing the actual and predicted labels for each class.\n- **precisionRecallDataFrame**: A data frame listing the precision and recall percentages for each class.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "The fraction of incorrectly labeled examples.",
          "name" : "classificationError",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLClassifierMetrics\/classificationError"
        },
        {
          "description" : "A data table listing the precision and recall percentages for each class.",
          "name" : "precisionRecall",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLClassifierMetrics\/precisionRecall"
        },
        {
          "description" : "A data frame comparing the actual and predicted labels for each class.",
          "name" : "confusionDataFrame",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLClassifierMetrics\/confusionDataFrame"
        },
        {
          "description" : "A data frame listing the precision and recall percentages for each class.",
          "name" : "precisionRecallDataFrame",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLClassifierMetrics\/precisionRecallDataFrame"
        }
      ],
      "title" : "Understanding the model"
    }
  ],
  "source" : "appleJSON",
  "title" : "confusion",
  "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/MLClassifierMetrics\/confusion"
}