{
  "abstract" : "Collect quality example videos that effectively train action classifiers.",
  "codeExamples" : [

  ],
  "contentHash" : "56fa97c661247572d200406a4e01958003e93e0f41d4a50ca68224a370fdac64",
  "crawledAt" : "2025-12-09T19:47:36Z",
  "id" : "B0120BA8-BF7E-4EBD-BF62-0340A64267B5",
  "kind" : "article",
  "language" : "swift",
  "module" : "Create ML",
  "overview" : "## Overview\n\nCreate a robust action classifier by collecting high-quality videos that clearly show a person performing each action you want the classifier to recognize. In addition, gather videos of unrelated actions the classifier might witness. Use these to create a *negative class* of irrelevant actions, so the classifier distinguishes between these two types.\n\nFor general information about how to create an action classifier, see [doc:\/\/com.apple.createml\/documentation\/CreateML\/creating-an-action-classifier-model].\n\n### Collect Videos for Each Action\n\nAction classifiers learn from example videos of a human performing an action. Create ML uses the [doc:\/\/com.apple.documentation\/documentation\/Vision] framework to locate key body parts, called *landmarks*, in each video frame. Action classifiers learn how to recognize a human body’s movements by tracking these landmarks frame by frame. Keep the landmarks clearly visible by applying the following guidelines as you record example videos, or when assessing videos from other sources.\n\nYou typically want an action classifier to work with any human that uses your app. Teach your model to account for natural variations by including videos of variations in the action you want the classifier to recognize, such as how the person holds their head or places their arms.\n\nIf you want your app to recognize actions more generally or from various camera angles, add more example videos that capture the action from multiple directions.\n\n### Collect Videos for a Negative Class\n\nA negative class is a category of one or more actions that aren’t important to the action classifier. Negative classes help the classifier distinguish between the actions you want it to recognize and potential nonactions it might encounter in your app.\n\nCreate a negative class by gathering example videos of all unrelated nonactions the classifier might observe. For example, a negative class for an exercise-action classifier might include examples of individuals walking in and out frame. Including this negative class ensures that the action classifier is less likely to confuse walking for an exercise, such as a lunge.\n\nMake additional negative classes for nonactions that take different spans of time, or for static versus dynamic nonactions. For example, the exercise classifier might have another negative class of static actions that include examples of individuals standing still or sitting in a chair.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/CreateML\/gathering-training-videos-for-an-action-classifier\ncrawled: 2025-12-09T19:47:36Z\n---\n\n# Gathering Training Videos for an Action Classifier\n\n**Article**\n\nCollect quality example videos that effectively train action classifiers.\n\n## Overview\n\nCreate a robust action classifier by collecting high-quality videos that clearly show a person performing each action you want the classifier to recognize. In addition, gather videos of unrelated actions the classifier might witness. Use these to create a *negative class* of irrelevant actions, so the classifier distinguishes between these two types.\n\nFor general information about how to create an action classifier, see [doc:\/\/com.apple.createml\/documentation\/CreateML\/creating-an-action-classifier-model].\n\n### Collect Videos for Each Action\n\nAction classifiers learn from example videos of a human performing an action. Create ML uses the [doc:\/\/com.apple.documentation\/documentation\/Vision] framework to locate key body parts, called *landmarks*, in each video frame. Action classifiers learn how to recognize a human body’s movements by tracking these landmarks frame by frame. Keep the landmarks clearly visible by applying the following guidelines as you record example videos, or when assessing videos from other sources.\n\n- Keep the camera still.\n- Record one person at a time.\n- Capture the person’s entire body.\n- Ensure the environment has sufficient lighting.\n- Ensure the person’s clothing isn’t loose or flowing.\n- Ensure the person’s clothing sufficiently contrasts with the environment’s background.\n- Remove foreground objects that might obscure the person as they perform the action.\n- If the person performs multiple instances of a single action in a recording, minimize the time between repetitions.\n\nYou typically want an action classifier to work with any human that uses your app. Teach your model to account for natural variations by including videos of variations in the action you want the classifier to recognize, such as how the person holds their head or places their arms.\n\nIf you want your app to recognize actions more generally or from various camera angles, add more example videos that capture the action from multiple directions.\n\n### Collect Videos for a Negative Class\n\nA negative class is a category of one or more actions that aren’t important to the action classifier. Negative classes help the classifier distinguish between the actions you want it to recognize and potential nonactions it might encounter in your app.\n\nCreate a negative class by gathering example videos of all unrelated nonactions the classifier might observe. For example, a negative class for an exercise-action classifier might include examples of individuals walking in and out frame. Including this negative class ensures that the action classifier is less likely to confuse walking for an exercise, such as a lunge.\n\nMake additional negative classes for nonactions that take different spans of time, or for static versus dynamic nonactions. For example, the exercise classifier might have another negative class of static actions that include examples of individuals standing still or sitting in a chair.\n\n## Action Classifier Data Sources\n\n- **Building an Action Classifier Data Source**: Arrange your training videos in multiple folders with labels that describe actions, or in a single folder with an annotation file.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Arrange your training videos in multiple folders with labels that describe actions, or in a single folder with an annotation file.",
          "name" : "Building an Action Classifier Data Source",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/building-an-action-classifier-data-source"
        }
      ],
      "title" : "Action Classifier Data Sources"
    }
  ],
  "source" : "appleJSON",
  "title" : "Gathering Training Videos for an Action Classifier",
  "url" : "https:\/\/developer.apple.com\/documentation\/CreateML\/gathering-training-videos-for-an-action-classifier"
}