{
  "abstract" : "Create generative experiences that appropriately handle sensitive inputs and respect people.",
  "codeExamples" : [
    {
      "code" : "do {\n    let session = LanguageModelSession()\n    let topic = \/\/ A potentially harmful topic.\n    let prompt = \"Write a respectful and funny story about \\(topic).\"\n    let response = try await session.respond(to: prompt)\n} catch LanguageModelSession.GenerationError.guardrailViolation {\n    \/\/ Handle the safety error.\n}",
      "language" : "swift"
    },
    {
      "code" : "do {\n    let session = LanguageModelSession()\n    let topic = \"\"  \/\/ A sensitive topic.\n    let response = try session.respond(\n        to: \"List five key points about: \\(topic)\",\n        generating: [String].self\n    )\n} catch LanguageModelSession.GenerationError.refusal(let refusal, _) {\n    \/\/ Generate an explanation for the refusal.\n    if let message = try? await refusal.explanation {\n        \/\/ Display the refusal message.\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "enum TopicOptions {\n    case family\n    case nature\n    case work \n}\nlet topicChoice = TopicOptions.nature\nlet prompt = \"\"\"\n    Generate a wholesome and empathetic journal prompt that helps \\\n    this person reflect on \\(topicChoice)\n    \"\"\"",
      "language" : "swift"
    },
    {
      "code" : "@Generable\nenum Breakfast {\n    case waffles\n    case pancakes\n    case bagels\n    case eggs \n}\nlet session = LanguageModelSession()\nlet userInput = \"I want something sweet.\"\nlet prompt = \"Pick the ideal breakfast for request: \\(userInput)\"\nlet response = try await session.respond(to: prompt, generating: Breakfast.self)",
      "language" : "swift"
    },
    {
      "code" : "do {\n    let instructions = \"\"\"\n        ALWAYS respond in a respectful way. \\\n        If someone asks you to generate content that might be sensitive, \\\n        you MUST decline with 'Sorry, I can't do that.'\n        \"\"\"\n    let session = LanguageModelSession(instructions: instructions)\n    let prompt = \/\/ Open input from a person using the app.\n    let response = try await session.respond(to: prompt)\n} catch LanguageModelSession.GenerationError.guardrailViolation {\n    \/\/ Handle the safety error.\n}",
      "language" : "swift"
    },
    {
      "code" : "let userInput = \/\/ The input a person enters in the app.\nlet prompt = \"\"\"\n    Generate a wholesome and empathetic journal prompt that helps \\\n    this person reflect on their day. They said: \\(userInput)\n    \"\"\"",
      "language" : "swift"
    },
    {
      "code" : "let session = LanguageModelSession()\nlet userInput = \/\/ The input a person enters in the app.\nlet prompt = \"Generate a wholesome story about: \\(userInput)\"\n\n\/\/ A function you create that evaluates whether the input \n\/\/ contains anything in your deny list.\nif verifyText(prompt) { \n    let response = try await session.respond(to: prompt)\n    \n    \/\/ Compare the output to evaluate whether it contains anything in your deny list.\n    if verifyText(response.content) { \n        return response \n    } else {\n        \/\/ Handle the unsafe output.\n    }\n} else {\n    \/\/ Handle the unsafe input.\n}",
      "language" : "swift"
    },
    {
      "code" : "let model = SystemLanguageModel(guardrails: .permissiveContentTransformations)",
      "language" : "swift"
    }
  ],
  "contentHash" : "189d35f224bd15453be0770561dd54f0a24f10495951be5e364d57e2ee59a0b9",
  "crawledAt" : "2025-12-02T16:00:07Z",
  "id" : "E7CF7C8B-9A2A-41CB-95A3-49DAB9A65BBE",
  "kind" : "article",
  "language" : "swift",
  "module" : "Foundation Models",
  "overview" : "## Overview\n\nGenerative AI models have powerful creativity, but with this creativity comes the risk of unintended or unexpected results. For any generative AI feature, safety needs to be an essential part of your design.\n\nThe Foundation Models framework has two base layers of safety, where the framework uses:\n\nBecause safety risks are often contextual, some harms might bypass both built-in framework safety layers. It’s vital to design additional safety layers specific to your app. When developing your feature, decide what’s acceptable or might be harmful in your generative AI feature, based on your app’s use case, cultural context, and audience.\n\nFor more information on designing generative AI experiences responsibly, see Human Interface Guidelines > Foundations > [https:\/\/developer.apple.com\/design\/human-interface-guidelines\/generative-ai].\n\n## Handle guardrail errors\n\nWhen you send a prompt to the model, [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/SystemLanguageModel\/Guardrails] check the input prompt and the model’s output. If either fails the guardrail’s safety check, the model session throws a [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/LanguageModelSession\/GenerationError\/guardrailViolation(_:)] error:\n\nIf you encounter a guardrail violation error for any built-in prompt in your app, experiment with re-phrasing the prompt to determine which phrases are activating the guardrails, and avoid those phrases. If the error is thrown in response to a prompt created by someone using your app, give people a clear message that explains the issue. For example, you might say “Sorry, this feature isn’t designed to handle that kind of input” and offer people the opportunity to try a different prompt.\n\n## Handle model refusals\n\nThe on-device language model may not be suitable for handling all requests and may refuse requests for a topic. When you generate a string response, and the model refuses a request, it generates a message that begins with a refusal like “Sorry, I can’t help with”.\n\nDesign your app experience with refusal messages in mind and present the message to the person using your app. You might not be able to programmatically determine whether a string response is a normal response or a refusal, so design the experience to anticipate both. If it’s critical to determine whether the response is a refusal message, initialize a new [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/LanguageModelSession] and prompt the model to classify whether the string is a refusal.\n\nWhen you use guided generation to generate Swift structures or types, there’s no placeholder for a refusal message. Instead, the model throws a [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/LanguageModelSession\/GenerationError\/refusal(_:_:)] error. When you catch the error, you can ask the model to generate a string refusal message:\n\nDisplay the explanation in your app to tell people why a request failed, and offer people the opportunity to try a different prompt. Retrieving an explanation message is asynchronous and takes time for the model to generate.\n\nIf you encounter a refusal message, or refusal error, for any built-in prompts in your app, experiment with re-phrasing your prompt to avoid any sensitive topics that might cause the refusal.\n\nFor more information about guided generation, see [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/generating-swift-data-structures-with-guided-generation].\n\n## Build boundaries on input and output\n\nSafety risks increase when a prompt includes direct input from a person using your app, or from an unverified external source, like a webpage. An untrusted source makes it difficult to anticipate what the input contains. Whether accidentally or on purpose, someone could input sensitive content that causes the model to respond poorly.\n\nWhenever possible, avoid open input in prompts and place boundaries for controlling what the input can be. This approach helps when you want generative content to stay within the bounds of a particular topic or task. For the highest level of safety on input, give people a fixed set of prompts to choose from. This gives you the highest certainty that sensitive content won’t make its way into your app:\n\nIf your app allows people to freely input a prompt, placing boundaries on the output can also offer stronger safety guarantees. Using guided generation, create an enumeration to restrict the model’s output to a set of predefined options designed to be safe no matter what:\n\n## Instruct the model for added safety\n\nConsider adding detailed session [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/Instructions] that tell the model how to handle sensitive content. The language model prioritizes following its instructions over any prompt, so instructions are an effective tool for improving safety and overall generation quality. Use uppercase words to emphasize the importance of certain phrases for the model:\n\nIf you want to include open-input from people, instructions for safety are recommended. For an additional layer of safety, use a format string in normal prompts that wraps people’s input in your own content that specifies how the model should respond:\n\n## Add a deny list of blocked terms\n\nIf you allow prompt input from people or outside sources, consider adding your own deny list of terms. A deny list is anything you don’t want people to be able to input to your app, including unsafe terms, names of people or products, or anything that’s not relevant to the feature you provide. Implement a deny list similarly to guardrails by creating a function that checks the input and the model output:\n\nA deny list can be a simple list of strings in your code that you distribute with your app. Alternatively, you can host a deny list on a server so your app can download the latest deny list when it’s connected to the network. Hosting your deny list allows you to update your list when you need to and avoids requiring a full app update if a safety issue arise.\n\n## Use permissive guardrail mode for sensitive content\n\nThe default [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/SystemLanguageModel] guardrails may throw a [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/LanguageModelSession\/GenerationError\/guardrailViolation(_:)] error for sensitive source material. For example, it may be appropriate for your app to work with certain inputs from people and unverified sources that might contain sensitive content:\n\nTo allow the model to reason about sensitive source material, use [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/SystemLanguageModel\/Guardrails\/permissiveContentTransformations] when you initialize [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/SystemLanguageModel]:\n\nThis mode only works for generating a string value. When you use guided generation, the framework runs the default guardrails against model input and output as usual, and generates [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/LanguageModelSession\/GenerationError\/guardrailViolation(_:)] and [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/LanguageModelSession\/GenerationError\/refusal(_:_:)]errors as usual.\n\nBefore you use permissive content mode, consider what’s appropriate for your audience. The session skips the guardrail checks in this mode, so it never throws a [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/LanguageModelSession\/GenerationError\/guardrailViolation(_:)] error when generating string responses.\n\nHowever, even with the [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/SystemLanguageModel] guardrails off, the on-device system language model still has a layer of safety. For some content, the model may still produce a refusal message that’s similar to, “Sorry, I can’t help with.”\n\n## Create a risk assessment\n\nConduct a risk assessment to proactively address what might go wrong. Risk assessment is an exercise that helps you brainstorm potential safety risks in your app and map each risk to an actionable mitigation. You can write a risk assessment in any format that includes these essential elements:\n\nFor example, an app might include one feature with the fixed-choice input pattern for generation and one feature with the open-input pattern for generation, which is higher safety risk:\n\nBesides obvious harms, like a poor-quality model output, think about how your generative AI feature might affect people, including real-world scenarios where someone might act based on information generated by your app.\n\n## Write and maintain safety tests\n\nAlthough most people will interact with your app in respectful ways, it’s important to anticipate possible failure modes where certain input or contexts could cause the model to generate something harmful. Especially if your app takes input from people, test your experience’s safety on input like:\n\nCreate a list of potentially harmful prompt inputs that you can run as part of your app’s tests. Include every prompt in your app — even safe ones — as part of your app testing. For each prompt test, log the timestamp, full input prompt, the model’s response, and whether it activates any built-in safety or mitigations you’ve included in your app. When starting out, manually read the model’s response on all tests to ensure it meets your design and safety goals. To scale your tests, consider using a frontier LLM to auto-grade the safety of each prompt. Building a test pipeline for prompts and safety is a worthwhile investment for tracking changes in how your app responds over time.\n\nSomeone might purposefully attempt to break your feature or produce bad output — especially someone who won’t be harmed by their actions. But, keep in mind that it’s very important to identify cases where someone might *accidentally* be harmed during normal app use.\n\nDon’t engage in any testing that could cause you or others harm. Apple’s built-in responsible AI and safety measures, like safety guardrails, are built by experts with extensive training and support. These built-in measures aim to block egregious harms, allowing you to focus on the borderline harmful cases that need your judgement. Before conducting any safety testing, ensure that you’re in a safe location and that you have the health and well-being support you need.\n\n## Report safety concerns\n\nSomewhere in your app, it’s important to include a way that people can report potentially harmful content. Continuously monitor the feedback you receive, and be responsive to quickly handling any safety issues that arise. If someone reports a safety concern that you believe isn’t being properly handled by Apple’s built-in guardrails, report it to Apple with [https:\/\/support.apple.com\/guide\/feedback-assistant\/get-started-fbab81460adb\/mac].\n\nThe Foundation Models framework offers utilities for feedback. Use [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/LanguageModelFeedback] to retrieve language model session transcripts from people using your app. After collecting feedback, you can serialize it into a JSON file and include it in the report you send with Feedback Assistant.\n\n## Monitor safety for model or guardrail updates\n\nApple releases updates to the system model as part of regular OS updates. If you participate in the developer beta program you can test your app with new model version ahead of people using your app. When the model updates, it’s important to re-run your full prompt tests in addition to your adversarial safety tests because the model’s response may change. Your risk assessment can help you track any change to safety risks in your app.\n\nApple may update the built-in guardrails at any time outside of the regular OS update cycle. This is done to rapidly respond, for example, to reported safety concerns that require a fast response. Include all of the prompts you use in your app in your test suite, and run tests regularly to identify when prompts start activating the guardrails.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/foundationmodels\/improving-the-safety-of-generative-model-output\ncrawled: 2025-12-02T16:00:07Z\n---\n\n# Improving the safety of generative model output\n\n**Article**\n\nCreate generative experiences that appropriately handle sensitive inputs and respect people.\n\n## Overview\n\nGenerative AI models have powerful creativity, but with this creativity comes the risk of unintended or unexpected results. For any generative AI feature, safety needs to be an essential part of your design.\n\nThe Foundation Models framework has two base layers of safety, where the framework uses:\n\n- An on-device language model that has training to handle sensitive topics with care.\n- *Guardrails* that aim to block harmful or sensitive content, such as self-harm, violence, and adult materials, from both model input and output.\n\nBecause safety risks are often contextual, some harms might bypass both built-in framework safety layers. It’s vital to design additional safety layers specific to your app. When developing your feature, decide what’s acceptable or might be harmful in your generative AI feature, based on your app’s use case, cultural context, and audience.\n\nFor more information on designing generative AI experiences responsibly, see Human Interface Guidelines > Foundations > [https:\/\/developer.apple.com\/design\/human-interface-guidelines\/generative-ai].\n\n## Handle guardrail errors\n\nWhen you send a prompt to the model, [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/SystemLanguageModel\/Guardrails] check the input prompt and the model’s output. If either fails the guardrail’s safety check, the model session throws a [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/LanguageModelSession\/GenerationError\/guardrailViolation(_:)] error:\n\n```swift\ndo {\n    let session = LanguageModelSession()\n    let topic = \/\/ A potentially harmful topic.\n    let prompt = \"Write a respectful and funny story about \\(topic).\"\n    let response = try await session.respond(to: prompt)\n} catch LanguageModelSession.GenerationError.guardrailViolation {\n    \/\/ Handle the safety error.\n}\n```\n\nIf you encounter a guardrail violation error for any built-in prompt in your app, experiment with re-phrasing the prompt to determine which phrases are activating the guardrails, and avoid those phrases. If the error is thrown in response to a prompt created by someone using your app, give people a clear message that explains the issue. For example, you might say “Sorry, this feature isn’t designed to handle that kind of input” and offer people the opportunity to try a different prompt.\n\n## Handle model refusals\n\nThe on-device language model may not be suitable for handling all requests and may refuse requests for a topic. When you generate a string response, and the model refuses a request, it generates a message that begins with a refusal like “Sorry, I can’t help with”.\n\nDesign your app experience with refusal messages in mind and present the message to the person using your app. You might not be able to programmatically determine whether a string response is a normal response or a refusal, so design the experience to anticipate both. If it’s critical to determine whether the response is a refusal message, initialize a new [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/LanguageModelSession] and prompt the model to classify whether the string is a refusal.\n\nWhen you use guided generation to generate Swift structures or types, there’s no placeholder for a refusal message. Instead, the model throws a [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/LanguageModelSession\/GenerationError\/refusal(_:_:)] error. When you catch the error, you can ask the model to generate a string refusal message:\n\n```swift\ndo {\n    let session = LanguageModelSession()\n    let topic = \"\"  \/\/ A sensitive topic.\n    let response = try session.respond(\n        to: \"List five key points about: \\(topic)\",\n        generating: [String].self\n    )\n} catch LanguageModelSession.GenerationError.refusal(let refusal, _) {\n    \/\/ Generate an explanation for the refusal.\n    if let message = try? await refusal.explanation {\n        \/\/ Display the refusal message.\n    }\n}\n```\n\nDisplay the explanation in your app to tell people why a request failed, and offer people the opportunity to try a different prompt. Retrieving an explanation message is asynchronous and takes time for the model to generate.\n\nIf you encounter a refusal message, or refusal error, for any built-in prompts in your app, experiment with re-phrasing your prompt to avoid any sensitive topics that might cause the refusal.\n\nFor more information about guided generation, see [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/generating-swift-data-structures-with-guided-generation].\n\n## Build boundaries on input and output\n\nSafety risks increase when a prompt includes direct input from a person using your app, or from an unverified external source, like a webpage. An untrusted source makes it difficult to anticipate what the input contains. Whether accidentally or on purpose, someone could input sensitive content that causes the model to respond poorly.\n\n\n\nWhenever possible, avoid open input in prompts and place boundaries for controlling what the input can be. This approach helps when you want generative content to stay within the bounds of a particular topic or task. For the highest level of safety on input, give people a fixed set of prompts to choose from. This gives you the highest certainty that sensitive content won’t make its way into your app:\n\n```swift\nenum TopicOptions {\n    case family\n    case nature\n    case work \n}\nlet topicChoice = TopicOptions.nature\nlet prompt = \"\"\"\n    Generate a wholesome and empathetic journal prompt that helps \\\n    this person reflect on \\(topicChoice)\n    \"\"\"\n```\n\nIf your app allows people to freely input a prompt, placing boundaries on the output can also offer stronger safety guarantees. Using guided generation, create an enumeration to restrict the model’s output to a set of predefined options designed to be safe no matter what:\n\n```swift\n@Generable\nenum Breakfast {\n    case waffles\n    case pancakes\n    case bagels\n    case eggs \n}\nlet session = LanguageModelSession()\nlet userInput = \"I want something sweet.\"\nlet prompt = \"Pick the ideal breakfast for request: \\(userInput)\"\nlet response = try await session.respond(to: prompt, generating: Breakfast.self)\n```\n\n## Instruct the model for added safety\n\nConsider adding detailed session [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/Instructions] that tell the model how to handle sensitive content. The language model prioritizes following its instructions over any prompt, so instructions are an effective tool for improving safety and overall generation quality. Use uppercase words to emphasize the importance of certain phrases for the model:\n\n```swift\ndo {\n    let instructions = \"\"\"\n        ALWAYS respond in a respectful way. \\\n        If someone asks you to generate content that might be sensitive, \\\n        you MUST decline with 'Sorry, I can't do that.'\n        \"\"\"\n    let session = LanguageModelSession(instructions: instructions)\n    let prompt = \/\/ Open input from a person using the app.\n    let response = try await session.respond(to: prompt)\n} catch LanguageModelSession.GenerationError.guardrailViolation {\n    \/\/ Handle the safety error.\n}\n```\n\n\n\nIf you want to include open-input from people, instructions for safety are recommended. For an additional layer of safety, use a format string in normal prompts that wraps people’s input in your own content that specifies how the model should respond:\n\n```swift\nlet userInput = \/\/ The input a person enters in the app.\nlet prompt = \"\"\"\n    Generate a wholesome and empathetic journal prompt that helps \\\n    this person reflect on their day. They said: \\(userInput)\n    \"\"\"\n```\n\n## Add a deny list of blocked terms\n\nIf you allow prompt input from people or outside sources, consider adding your own deny list of terms. A deny list is anything you don’t want people to be able to input to your app, including unsafe terms, names of people or products, or anything that’s not relevant to the feature you provide. Implement a deny list similarly to guardrails by creating a function that checks the input and the model output:\n\n```swift\nlet session = LanguageModelSession()\nlet userInput = \/\/ The input a person enters in the app.\nlet prompt = \"Generate a wholesome story about: \\(userInput)\"\n\n\/\/ A function you create that evaluates whether the input \n\/\/ contains anything in your deny list.\nif verifyText(prompt) { \n    let response = try await session.respond(to: prompt)\n    \n    \/\/ Compare the output to evaluate whether it contains anything in your deny list.\n    if verifyText(response.content) { \n        return response \n    } else {\n        \/\/ Handle the unsafe output.\n    }\n} else {\n    \/\/ Handle the unsafe input.\n}\n```\n\nA deny list can be a simple list of strings in your code that you distribute with your app. Alternatively, you can host a deny list on a server so your app can download the latest deny list when it’s connected to the network. Hosting your deny list allows you to update your list when you need to and avoids requiring a full app update if a safety issue arise.\n\n## Use permissive guardrail mode for sensitive content\n\nThe default [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/SystemLanguageModel] guardrails may throw a [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/LanguageModelSession\/GenerationError\/guardrailViolation(_:)] error for sensitive source material. For example, it may be appropriate for your app to work with certain inputs from people and unverified sources that might contain sensitive content:\n\n- When you want the model to tag the topic of conversations in a chat app when some messages contain profanity.\n- When you want to use the model to explain notes in your study app that discuss sensitive topics.\n\nTo allow the model to reason about sensitive source material, use [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/SystemLanguageModel\/Guardrails\/permissiveContentTransformations] when you initialize [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/SystemLanguageModel]:\n\n```swift\nlet model = SystemLanguageModel(guardrails: .permissiveContentTransformations)\n```\n\nThis mode only works for generating a string value. When you use guided generation, the framework runs the default guardrails against model input and output as usual, and generates [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/LanguageModelSession\/GenerationError\/guardrailViolation(_:)] and [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/LanguageModelSession\/GenerationError\/refusal(_:_:)]errors as usual.\n\nBefore you use permissive content mode, consider what’s appropriate for your audience. The session skips the guardrail checks in this mode, so it never throws a [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/LanguageModelSession\/GenerationError\/guardrailViolation(_:)] error when generating string responses.\n\nHowever, even with the [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/SystemLanguageModel] guardrails off, the on-device system language model still has a layer of safety. For some content, the model may still produce a refusal message that’s similar to, “Sorry, I can’t help with.”\n\n## Create a risk assessment\n\nConduct a risk assessment to proactively address what might go wrong. Risk assessment is an exercise that helps you brainstorm potential safety risks in your app and map each risk to an actionable mitigation. You can write a risk assessment in any format that includes these essential elements:\n\n- List each AI feature in your app.\n- For each feature, list possible safety risks that could occur, even if they seem unlikely.\n- For each safety risk, score how serious the harm would be if that thing occurred, from mild to critical.\n- For each safety risk, assign a strategy for how you’ll mitigate the risk in your app.\n\nFor example, an app might include one feature with the fixed-choice input pattern for generation and one feature with the open-input pattern for generation, which is higher safety risk:\n\n\n\nBesides obvious harms, like a poor-quality model output, think about how your generative AI feature might affect people, including real-world scenarios where someone might act based on information generated by your app.\n\n## Write and maintain safety tests\n\nAlthough most people will interact with your app in respectful ways, it’s important to anticipate possible failure modes where certain input or contexts could cause the model to generate something harmful. Especially if your app takes input from people, test your experience’s safety on input like:\n\n- Input that is nonsensical, snippets of code, or random characters.\n- Input that includes sensitive content.\n- Input that includes controversial topics.\n- Vague or unclear input that could be misinterpreted.\n\nCreate a list of potentially harmful prompt inputs that you can run as part of your app’s tests. Include every prompt in your app — even safe ones — as part of your app testing. For each prompt test, log the timestamp, full input prompt, the model’s response, and whether it activates any built-in safety or mitigations you’ve included in your app. When starting out, manually read the model’s response on all tests to ensure it meets your design and safety goals. To scale your tests, consider using a frontier LLM to auto-grade the safety of each prompt. Building a test pipeline for prompts and safety is a worthwhile investment for tracking changes in how your app responds over time.\n\nSomeone might purposefully attempt to break your feature or produce bad output — especially someone who won’t be harmed by their actions. But, keep in mind that it’s very important to identify cases where someone might *accidentally* be harmed during normal app use.\n\n\n\nDon’t engage in any testing that could cause you or others harm. Apple’s built-in responsible AI and safety measures, like safety guardrails, are built by experts with extensive training and support. These built-in measures aim to block egregious harms, allowing you to focus on the borderline harmful cases that need your judgement. Before conducting any safety testing, ensure that you’re in a safe location and that you have the health and well-being support you need.\n\n## Report safety concerns\n\nSomewhere in your app, it’s important to include a way that people can report potentially harmful content. Continuously monitor the feedback you receive, and be responsive to quickly handling any safety issues that arise. If someone reports a safety concern that you believe isn’t being properly handled by Apple’s built-in guardrails, report it to Apple with [https:\/\/support.apple.com\/guide\/feedback-assistant\/get-started-fbab81460adb\/mac].\n\nThe Foundation Models framework offers utilities for feedback. Use [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/LanguageModelFeedback] to retrieve language model session transcripts from people using your app. After collecting feedback, you can serialize it into a JSON file and include it in the report you send with Feedback Assistant.\n\n## Monitor safety for model or guardrail updates\n\nApple releases updates to the system model as part of regular OS updates. If you participate in the developer beta program you can test your app with new model version ahead of people using your app. When the model updates, it’s important to re-run your full prompt tests in addition to your adversarial safety tests because the model’s response may change. Your risk assessment can help you track any change to safety risks in your app.\n\nApple may update the built-in guardrails at any time outside of the regular OS update cycle. This is done to rapidly respond, for example, to reported safety concerns that require a fast response. Include all of the prompts you use in your app in your test suite, and run tests regularly to identify when prompts start activating the guardrails.\n\n## Essentials\n\n- **Generating content and performing tasks with Foundation Models**: Enhance the experience in your app by prompting an on-device large language model.\n- **Supporting languages and locales with Foundation Models**: Generate content in the language people prefer when they interact with your app.\n- **Adding intelligent app features with generative models**: Build robust apps with guided generation and tool calling by adopting the Foundation Models framework.\n- **SystemLanguageModel**: An on-device large language model capable of text generation tasks.\n- **SystemLanguageModel.UseCase**: A type that represents the use case for prompting.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Enhance the experience in your app by prompting an on-device large language model.",
          "name" : "Generating content and performing tasks with Foundation Models",
          "url" : "https:\/\/developer.apple.com\/documentation\/FoundationModels\/generating-content-and-performing-tasks-with-foundation-models"
        },
        {
          "description" : "Generate content in the language people prefer when they interact with your app.",
          "name" : "Supporting languages and locales with Foundation Models",
          "url" : "https:\/\/developer.apple.com\/documentation\/FoundationModels\/supporting-languages-and-locales-with-foundation-models"
        },
        {
          "description" : "Build robust apps with guided generation and tool calling by adopting the Foundation Models framework.",
          "name" : "Adding intelligent app features with generative models",
          "url" : "https:\/\/developer.apple.com\/documentation\/FoundationModels\/adding-intelligent-app-features-with-generative-models"
        },
        {
          "description" : "An on-device large language model capable of text generation tasks.",
          "name" : "SystemLanguageModel",
          "url" : "https:\/\/developer.apple.com\/documentation\/FoundationModels\/SystemLanguageModel"
        },
        {
          "description" : "A type that represents the use case for prompting.",
          "name" : "SystemLanguageModel.UseCase",
          "url" : "https:\/\/developer.apple.com\/documentation\/FoundationModels\/SystemLanguageModel\/UseCase"
        }
      ],
      "title" : "Essentials"
    }
  ],
  "source" : "appleJSON",
  "title" : "Improving the safety of generative model output",
  "url" : "https:\/\/developer.apple.com\/documentation\/foundationmodels\/improving-the-safety-of-generative-model-output"
}