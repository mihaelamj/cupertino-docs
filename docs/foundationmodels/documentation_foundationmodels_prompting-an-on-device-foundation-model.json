{
  "abstract" : "Tailor your prompts to get effective results from an on-device model.",
  "codeExamples" : [
    {
      "code" : "let instructions = \"\"\"\n    You are a friendly innkeeper. Generate a greeting to a new guest that walks in the door.\n    IF the guest is a sorcerer, comment on their magical appearance.\n    IF the guest is a bard, ask if they're willing to play music for the inn tonight.\n    IF the guest is a soldier, ask if there’s been any dangerous activity in the area.\n    There is one single and one double room available.\n    \"\"\"",
      "language" : "swift"
    },
    {
      "code" : "var customGreeting = \"\"\nswitch role {\ncase .bard:\n    customGreeting = \"\"\"\n        This guest is a bard. Ask if they’re willing to play music for the inn tonight.\n        \"\"\"\ncase .soldier:\n    customGreeting = \"\"\"\n        This guest is a soldier. Ask if there’s been any dangerous activity in the area.\n        \"\"\"\ncase .sorcerer:\n    customGreeting = \"\"\"\n        This guest is a sorcerer. Comment on their magical appearance.\n        \"\"\"\ndefault:\n    customGreeting = \"This guest is a weary traveler.\"\n}\n\nlet instructions = \"\"\"\n    You are a friendly inn keeper. Generate a greeting to a new guest that walks in the door.\n    \\(customGreeting)\n    There is one single and one double room available.\n    \"\"\"",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Instructions that contain JSON key-value pairs that represent the structure\n\/\/ of a customer. The structure tells the model that each customer must have\n\/\/ a `name`, `imageDescription`, and `coffeeOrder` fields.\nlet instructions = \"\"\"\n    Create an NPC customer with a fun personality suitable for the dream realm. \\\n    Have the customer order coffee. Here are some examples to inspire you:\n\n    {name: \"Thimblefoot\", imageDescription: \"A horse with a rainbow mane\", \\\n    coffeeOrder: \"I would like a coffee that's refreshing and sweet, like the grass in a summer meadow.\"}\n    {name: \"Spiderkid\", imageDescription: \"A furry spider with a cool baseball cap\", \\\n    coffeeOrder: \"An iced coffee please, that's as spooky as I am!\"}\n    {name: \"Wise Fairy\", imageDescription: \"A blue, glowing fairy that radiates wisdom and sparkles\", \\\n    coffeeOrder: \"Something simple and plant-based, please. A beverage that restores my wise energy.\"}\n    \"\"\"",
      "language" : "swift"
    },
    {
      "code" : "@Generable\nstruct NPC: Equatable {\n    let name: String\n    let coffeeOrder: String\n    let imageDescription: String\n}",
      "language" : "swift"
    },
    {
      "code" : "@Generable\nstruct ReasonableAnswer {\n    \/\/ A property the model uses for reasoning.\n    var reasoningSteps: String\n    \n    @Guide(description: \"The answer only.\")\n    var answer: MyCustomGenerableType \/\/ Replace with your custom generable type.\n}",
      "language" : "swift"
    },
    {
      "code" : "let instructions = \"\"\"\n    Answer the person's question.\n    1. Begin your response with a plan to solve this question.\n    2. Follow your plan's steps and show your work.\n    3. Deliver the final answer in `answer`.\n    \"\"\"\nvar session = LanguageModelSession(instructions: instructions)\n\n\/\/ The answer should be 30 days.\nlet prompt = \"How many days are in the month of September?\"\nlet response = try await session.respond(\n    to: prompt,\n    generating: ReasonableAnswer.self\n)",
      "language" : "swift"
    }
  ],
  "contentHash" : "4745d682333c3e63c7b92e492980593347ba58894b2d203aab0453c7ea11677f",
  "crawledAt" : "2025-12-02T17:13:09Z",
  "id" : "88F30826-B1EA-443B-A71C-F5974DD7F841",
  "kind" : "article",
  "language" : "swift",
  "module" : "Foundation Models",
  "overview" : "## Overview\n\nMany prompting techniques are designed for server-based “frontier” foundation models, because they have a larger context window and thinking capabilities. However, when prompting an on-device model, your prompt engineering technique is even more critical because the model you access is much smaller.\n\nTo generate accurate, hallucination-free responses, your prompt needs to be concise and specific. To get a better output from the model, some techniques you can use include:\n\nYou’ll need to test your prompts throughout development and evaluate the output to provide a great user experience.\n\n## Concepts for creating great prompts\n\nWith prompt engineering, you structure your requests by refining how you phrase questions, provide context, and format instructions. It also requires testing and iteration of your input to get the results your app needs.\n\nYou can also structure prompts to make the model’s response depend on specific conditions or criteria in the input. For example, instead of giving one fixed instruction you can include different conditions, like:\n\n*If it’s a question, answer it directly. If it’s a statement, ask a follow-up question.*\n\n## Keep prompts simple and clear\n\nEffective prompts use simple language that tells the model what output you want it to provide. The model processes text in units, called *tokens*, and each model has a maximum number of tokens it can process — the context window size. An on-device model has fewer parameters and a small context window, so it doesn’t have the resources to handle long or confusing prompts. Input to a frontier model might be the length of a full document, but your input to the on-device model needs to be short and succinct. Ask yourself whether your prompt is understandable to a human if they read it quickly, and consider additional strategies to adjust your tone and writing style:\n\nAn on-device model may get confused with a long and indirect instruction because it contains unnecessary language that doesn’t add value. Instead of indirectly implying what the model needs to do, write a direct command to improve the clarity of the prompt for better results. This clarity also reduces the complexity and context window size for the on-device model.\n\nFor more information on managing the context window size, see [doc:\/\/com.apple.documentation\/documentation\/Technotes\/tn3193-managing-the-on-device-foundation-model-s-context-window].\n\n## Give the model a role, persona, and tone\n\nBy default, the on-device model typically responds to questions in a neutral and respectful tone, with a business-casual persona. Similar to frontier models, you can provide a role or persona to dramatically change how the on-device model responds to your prompt.\n\nA *role* is the functional position or job that you instruct the model to assume, while a *persona* reflects the personality of the model. You often use both in prompts; for example:\n\n*You are a senior software engineer who values mentoring junior developers.*\n\nHere the role is “a senior software engineer,” and the persona is “mentoring junior developers.”\n\nThe model phrases its response differently to match a persona, for example, “mentoring junior developers” or “evaluating developer coding” even when you give it the same input for the same task.\n\nTo give the model a role, use the phrase “you are”:\n\nUse the phrase “expert” to get the model to speak with more authority and detail on a topic.\n\nSimilarly, change the model’s behavior by providing a role or persona for the person using your app. By default, the on-device model thinks it’s talking to a person, so tell the model more about who *that* person is:\n\nThe student persona causes the model to respond as if speaking to a child in the first grade, while the ghost persona causes the model to respond as if speaking to a ghost in an alchemy shop.\n\nChange the model’s tone by writing your prompt in a voice you want the model to match. For example, if you write your prompt in a peppy and cheerful way, or talk like a cowboy, the model responds with a matching tone.\n\n## Iterate and improve instruction following\n\n*Instruction following* refers to a foundation model’s ability to carry out a request exactly as written in your [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/Prompt] and [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/Instructions]. Prompt engineering involves iteration to test and refine input — based on the results you get — to improve accuracy and consistency. If you notice the model isn’t following instructions as well as you need, consider the following strategies:\n\nInstead of trying to enforce accuracy, use a succinct prompt like “Answer this question” and evaluate the results you get.\n\nAfter you try any strategy, take the time to evaluate it to see if the result gets closer to what you need. If the model can’t follow your prompt, it might be unreliable in some use cases. Try cutting back the number of times you repeat a phrase, or the number of words you emphasize, to make your prompt more effective. Unreliable prompts break easily when conditions change slightly.\n\nAnother prompting strategy is to split your request into a series of simpler requests. This is particularly useful after trying different strategies that don’t improve the quality of the results.\n\n## Reduce how much thinking the model needs to do\n\nA model’s reasoning ability is how well it thinks through a problem like a human, handles logical puzzles, or creates a logical plan to handle a request. Because of their smaller size, on-device models have limited reasoning abilities. You may be able to help an on-device model *think through* a challenging task by providing additional support for its reasoning.\n\nFor complex tasks, simple language prompts might not have enough detail about how the model can accomplish a task. Instead, reduce the reasoning burden on the model by giving it a step-by-step plan. This approach tells the model more precisely how to do the task:\n\nIf you find the model isn’t accomplishing the task reliably, break up the steps across multiple [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/LanguageModelSession] instances to focus on one part at a time with a new context window. Typically, it’s a best practice to start with a single request because multiple requests can result in longer inference time. But, if the result doesn’t meet your expectations, try splitting steps into multiple requests.\n\n## Turn conditional prompting into programming logic\n\n*Conditional* prompting is where you embed if-else logic into your prompt. A server-based frontier model has the context window and reasoning abilities to handle a lengthy list of instructions for how to handle different requests. An on-device model can handle some conditionals or light reasoning, like:\n\n*Use the weather tool if the person asks about the weather and the calendar tool if the person asks about events.*\n\nBut, too much conditional complexity can affect the on-device model’s ability to follow instructions.\n\nWhen the on-device model output doesn’t meet your expectations, try customizing your conditional prompt to the current context. For example, the following conditional prompt contains several sentences that break up what the model needs to do:\n\nInstead, use programming logic to customize the prompt based on known information:\n\nWhen you customize instructions programmatically, the model doesn’t get distracted or confused by conditionals that don’t apply in the situation. This approach also reduces the context window size.\n\n## Provide simple input-output examples\n\n*Few-shot* prompting is when you provide the on-device model with a few examples of the output you want. For example, the following shows the model different kinds of coffee shop customers it needs to generate:\n\nFew-shot prompting also works with *guided generation*, which formats the model’s output by using a custom type you define. In the previous prompt, each example might correspond to a [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/Generable] structure you create named `NPC`:\n\nOn-device models need simpler examples for few-shot prompts than what you can use with server-based frontier models. Try giving the model between 2-15 examples, and keep each example as simple as possible. If you provide a long or complex example, the on-device model may start to repeat your example or hallucinate details of your example in its response.\n\nFor more information on guided generation, see [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/generating-swift-data-structures-with-guided-generation].\n\n## Handle on-device reasoning\n\nReasoning prompt techniques, like “think through this problem step by step”, can result in unexpected text being inserted into your [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/Generable] structure if the model doesn’t have a place for its reasoning. To keep reasoning explanations out of your structure, try giving the model a specific field where it can put its reasoning. Make sure the reasoning field is the first property so the model can provide reasoning details before answering the prompt:\n\nUsing your custom [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/Generable] type, prompt the model:\n\nYou may see the model fail to reason its way to a correct answer, or it may answer unreliably — occasionally answering correctly, and sometimes not. If this happens, the tasks in your prompt may be too difficult for the on-device model to process, regardless of how you structure the prompt.\n\n## Provide actionable feedback\n\nWhen you encounter something with the on-device model that you expect to work but it doesn’t, file a report that includes your prompt with Feedback Assistant to help improve the system model. To submit feedback about model behavior through Feedback Assistant, see [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/LanguageModelSession\/logFeedbackAttachment(sentiment:issues:desiredOutput:)].",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/FoundationModels\/prompting-an-on-device-foundation-model\ncrawled: 2025-12-02T17:13:09Z\n---\n\n# Prompting an on-device foundation model\n\n**Article**\n\nTailor your prompts to get effective results from an on-device model.\n\n## Overview\n\nMany prompting techniques are designed for server-based “frontier” foundation models, because they have a larger context window and thinking capabilities. However, when prompting an on-device model, your prompt engineering technique is even more critical because the model you access is much smaller.\n\nTo generate accurate, hallucination-free responses, your prompt needs to be concise and specific. To get a better output from the model, some techniques you can use include:\n\n- Use simple, clear instructions\n- Iterate and improve your prompt based on the output you receive in testing\n- Provide the model with a reasoning field before answering a prompt\n- Reduce the thinking the model needs to do\n- Split complex prompts into a series of simpler requests\n- Add “logic” to conditional prompts with “if-else” statements\n- Leverage shot-based prompting — such as one-shot, few-shot, or zero-shot prompts — to provide the model with specific examples of what you need\n\nYou’ll need to test your prompts throughout development and evaluate the output to provide a great user experience.\n\n## Concepts for creating great prompts\n\nWith prompt engineering, you structure your requests by refining how you phrase questions, provide context, and format instructions. It also requires testing and iteration of your input to get the results your app needs.\n\nYou can also structure prompts to make the model’s response depend on specific conditions or criteria in the input. For example, instead of giving one fixed instruction you can include different conditions, like:\n\n*If it’s a question, answer it directly. If it’s a statement, ask a follow-up question.*\n\n## Keep prompts simple and clear\n\nEffective prompts use simple language that tells the model what output you want it to provide. The model processes text in units, called *tokens*, and each model has a maximum number of tokens it can process — the context window size. An on-device model has fewer parameters and a small context window, so it doesn’t have the resources to handle long or confusing prompts. Input to a frontier model might be the length of a full document, but your input to the on-device model needs to be short and succinct. Ask yourself whether your prompt is understandable to a human if they read it quickly, and consider additional strategies to adjust your tone and writing style:\n\n\n\nAn on-device model may get confused with a long and indirect instruction because it contains unnecessary language that doesn’t add value. Instead of indirectly implying what the model needs to do, write a direct command to improve the clarity of the prompt for better results. This clarity also reduces the complexity and context window size for the on-device model.\n\n\n\nFor more information on managing the context window size, see [doc:\/\/com.apple.documentation\/documentation\/Technotes\/tn3193-managing-the-on-device-foundation-model-s-context-window].\n\n## Give the model a role, persona, and tone\n\nBy default, the on-device model typically responds to questions in a neutral and respectful tone, with a business-casual persona. Similar to frontier models, you can provide a role or persona to dramatically change how the on-device model responds to your prompt.\n\nA *role* is the functional position or job that you instruct the model to assume, while a *persona* reflects the personality of the model. You often use both in prompts; for example:\n\n*You are a senior software engineer who values mentoring junior developers.*\n\nHere the role is “a senior software engineer,” and the persona is “mentoring junior developers.”\n\nThe model phrases its response differently to match a persona, for example, “mentoring junior developers” or “evaluating developer coding” even when you give it the same input for the same task.\n\nTo give the model a role, use the phrase “you are”:\n\n\n\nUse the phrase “expert” to get the model to speak with more authority and detail on a topic.\n\nSimilarly, change the model’s behavior by providing a role or persona for the person using your app. By default, the on-device model thinks it’s talking to a person, so tell the model more about who *that* person is:\n\n\n\nThe student persona causes the model to respond as if speaking to a child in the first grade, while the ghost persona causes the model to respond as if speaking to a ghost in an alchemy shop.\n\nChange the model’s tone by writing your prompt in a voice you want the model to match. For example, if you write your prompt in a peppy and cheerful way, or talk like a cowboy, the model responds with a matching tone.\n\n\n\n## Iterate and improve instruction following\n\n*Instruction following* refers to a foundation model’s ability to carry out a request exactly as written in your [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/Prompt] and [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/Instructions]. Prompt engineering involves iteration to test and refine input — based on the results you get — to improve accuracy and consistency. If you notice the model isn’t following instructions as well as you need, consider the following strategies:\n\n\n\nInstead of trying to enforce accuracy, use a succinct prompt like “Answer this question” and evaluate the results you get.\n\nAfter you try any strategy, take the time to evaluate it to see if the result gets closer to what you need. If the model can’t follow your prompt, it might be unreliable in some use cases. Try cutting back the number of times you repeat a phrase, or the number of words you emphasize, to make your prompt more effective. Unreliable prompts break easily when conditions change slightly.\n\nAnother prompting strategy is to split your request into a series of simpler requests. This is particularly useful after trying different strategies that don’t improve the quality of the results.\n\n## Reduce how much thinking the model needs to do\n\nA model’s reasoning ability is how well it thinks through a problem like a human, handles logical puzzles, or creates a logical plan to handle a request. Because of their smaller size, on-device models have limited reasoning abilities. You may be able to help an on-device model *think through* a challenging task by providing additional support for its reasoning.\n\nFor complex tasks, simple language prompts might not have enough detail about how the model can accomplish a task. Instead, reduce the reasoning burden on the model by giving it a step-by-step plan. This approach tells the model more precisely how to do the task:\n\n\n\nIf you find the model isn’t accomplishing the task reliably, break up the steps across multiple [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/LanguageModelSession] instances to focus on one part at a time with a new context window. Typically, it’s a best practice to start with a single request because multiple requests can result in longer inference time. But, if the result doesn’t meet your expectations, try splitting steps into multiple requests.\n\n## Turn conditional prompting into programming logic\n\n*Conditional* prompting is where you embed if-else logic into your prompt. A server-based frontier model has the context window and reasoning abilities to handle a lengthy list of instructions for how to handle different requests. An on-device model can handle some conditionals or light reasoning, like:\n\n*Use the weather tool if the person asks about the weather and the calendar tool if the person asks about events.*\n\nBut, too much conditional complexity can affect the on-device model’s ability to follow instructions.\n\nWhen the on-device model output doesn’t meet your expectations, try customizing your conditional prompt to the current context. For example, the following conditional prompt contains several sentences that break up what the model needs to do:\n\n```swift\nlet instructions = \"\"\"\n    You are a friendly innkeeper. Generate a greeting to a new guest that walks in the door.\n    IF the guest is a sorcerer, comment on their magical appearance.\n    IF the guest is a bard, ask if they're willing to play music for the inn tonight.\n    IF the guest is a soldier, ask if there’s been any dangerous activity in the area.\n    There is one single and one double room available.\n    \"\"\"\n```\n\nInstead, use programming logic to customize the prompt based on known information:\n\n```swift\nvar customGreeting = \"\"\nswitch role {\ncase .bard:\n    customGreeting = \"\"\"\n        This guest is a bard. Ask if they’re willing to play music for the inn tonight.\n        \"\"\"\ncase .soldier:\n    customGreeting = \"\"\"\n        This guest is a soldier. Ask if there’s been any dangerous activity in the area.\n        \"\"\"\ncase .sorcerer:\n    customGreeting = \"\"\"\n        This guest is a sorcerer. Comment on their magical appearance.\n        \"\"\"\ndefault:\n    customGreeting = \"This guest is a weary traveler.\"\n}\n\nlet instructions = \"\"\"\n    You are a friendly inn keeper. Generate a greeting to a new guest that walks in the door.\n    \\(customGreeting)\n    There is one single and one double room available.\n    \"\"\"\n```\n\nWhen you customize instructions programmatically, the model doesn’t get distracted or confused by conditionals that don’t apply in the situation. This approach also reduces the context window size.\n\n## Provide simple input-output examples\n\n*Few-shot* prompting is when you provide the on-device model with a few examples of the output you want. For example, the following shows the model different kinds of coffee shop customers it needs to generate:\n\n```swift\n\/\/ Instructions that contain JSON key-value pairs that represent the structure\n\/\/ of a customer. The structure tells the model that each customer must have\n\/\/ a `name`, `imageDescription`, and `coffeeOrder` fields.\nlet instructions = \"\"\"\n    Create an NPC customer with a fun personality suitable for the dream realm. \\\n    Have the customer order coffee. Here are some examples to inspire you:\n\n    {name: \"Thimblefoot\", imageDescription: \"A horse with a rainbow mane\", \\\n    coffeeOrder: \"I would like a coffee that's refreshing and sweet, like the grass in a summer meadow.\"}\n    {name: \"Spiderkid\", imageDescription: \"A furry spider with a cool baseball cap\", \\\n    coffeeOrder: \"An iced coffee please, that's as spooky as I am!\"}\n    {name: \"Wise Fairy\", imageDescription: \"A blue, glowing fairy that radiates wisdom and sparkles\", \\\n    coffeeOrder: \"Something simple and plant-based, please. A beverage that restores my wise energy.\"}\n    \"\"\"\n```\n\nFew-shot prompting also works with *guided generation*, which formats the model’s output by using a custom type you define. In the previous prompt, each example might correspond to a [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/Generable] structure you create named `NPC`:\n\n```swift\n@Generable\nstruct NPC: Equatable {\n    let name: String\n    let coffeeOrder: String\n    let imageDescription: String\n}\n```\n\nOn-device models need simpler examples for few-shot prompts than what you can use with server-based frontier models. Try giving the model between 2-15 examples, and keep each example as simple as possible. If you provide a long or complex example, the on-device model may start to repeat your example or hallucinate details of your example in its response.\n\nFor more information on guided generation, see [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/generating-swift-data-structures-with-guided-generation].\n\n## Handle on-device reasoning\n\nReasoning prompt techniques, like “think through this problem step by step”, can result in unexpected text being inserted into your [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/Generable] structure if the model doesn’t have a place for its reasoning. To keep reasoning explanations out of your structure, try giving the model a specific field where it can put its reasoning. Make sure the reasoning field is the first property so the model can provide reasoning details before answering the prompt:\n\n```swift\n@Generable\nstruct ReasonableAnswer {\n    \/\/ A property the model uses for reasoning.\n    var reasoningSteps: String\n    \n    @Guide(description: \"The answer only.\")\n    var answer: MyCustomGenerableType \/\/ Replace with your custom generable type.\n}\n```\n\nUsing your custom [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/Generable] type, prompt the model:\n\n```swift\nlet instructions = \"\"\"\n    Answer the person's question.\n    1. Begin your response with a plan to solve this question.\n    2. Follow your plan's steps and show your work.\n    3. Deliver the final answer in `answer`.\n    \"\"\"\nvar session = LanguageModelSession(instructions: instructions)\n\n\/\/ The answer should be 30 days.\nlet prompt = \"How many days are in the month of September?\"\nlet response = try await session.respond(\n    to: prompt,\n    generating: ReasonableAnswer.self\n)\n```\n\nYou may see the model fail to reason its way to a correct answer, or it may answer unreliably — occasionally answering correctly, and sometimes not. If this happens, the tasks in your prompt may be too difficult for the on-device model to process, regardless of how you structure the prompt.\n\n## Provide actionable feedback\n\nWhen you encounter something with the on-device model that you expect to work but it doesn’t, file a report that includes your prompt with Feedback Assistant to help improve the system model. To submit feedback about model behavior through Feedback Assistant, see [doc:\/\/com.apple.foundationmodels\/documentation\/FoundationModels\/LanguageModelSession\/logFeedbackAttachment(sentiment:issues:desiredOutput:)].\n\n## Prompting\n\n- **LanguageModelSession**: An object that represents a session that interacts with a language model.\n- **Instructions**: Details you provide that define the model’s intended behavior on prompts.\n- **Prompt**: A prompt from a person to the model.\n- **Transcript**: A linear history of entries that reflect an interaction with a session.\n- **GenerationOptions**: Options that control how the model generates its response to a prompt.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "An object that represents a session that interacts with a language model.",
          "name" : "LanguageModelSession",
          "url" : "https:\/\/developer.apple.com\/documentation\/FoundationModels\/LanguageModelSession"
        },
        {
          "description" : "Details you provide that define the model’s intended behavior on prompts.",
          "name" : "Instructions",
          "url" : "https:\/\/developer.apple.com\/documentation\/FoundationModels\/Instructions"
        },
        {
          "description" : "A prompt from a person to the model.",
          "name" : "Prompt",
          "url" : "https:\/\/developer.apple.com\/documentation\/FoundationModels\/Prompt"
        },
        {
          "description" : "A linear history of entries that reflect an interaction with a session.",
          "name" : "Transcript",
          "url" : "https:\/\/developer.apple.com\/documentation\/FoundationModels\/Transcript"
        },
        {
          "description" : "Options that control how the model generates its response to a prompt.",
          "name" : "GenerationOptions",
          "url" : "https:\/\/developer.apple.com\/documentation\/FoundationModels\/GenerationOptions"
        }
      ],
      "title" : "Prompting"
    }
  ],
  "source" : "appleJSON",
  "title" : "Prompting an on-device foundation model",
  "url" : "https:\/\/developer.apple.com\/documentation\/FoundationModels\/prompting-an-on-device-foundation-model"
}