{
  "abstract" : "Stream desktop content like displays, apps, and windows by adopting screen capture in your app.",
  "codeExamples" : [
    {
      "code" : "\/\/ Retrieve the available screen content to capture.\nlet availableContent = try await SCShareableContent.excludingDesktopWindows(false,\n                                                                            onScreenWindowsOnly: true)",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Create a content filter that includes a single window.\nfilter = SCContentFilter(desktopIndependentWindow: window)",
      "language" : "swift"
    },
    {
      "code" : "var excludedApps = [SCRunningApplication]()\n\/\/ If a user chooses to exclude the app from the stream,\n\/\/ exclude it by matching its bundle identifier.\nif isAppExcluded {\n    excludedApps = availableApps.filter { app in\n        Bundle.main.bundleIdentifier == app.bundleIdentifier\n    }\n}\n\/\/ Create a content filter with excluded apps.\nfilter = SCContentFilter(display: display,\n                         excludingApplications: excludedApps,\n                         exceptingWindows: [])",
      "language" : "swift"
    },
    {
      "code" : "var streamConfig = SCStreamConfiguration()\n\nif let dynamicRangePreset = selectedDynamicRangePreset?.scDynamicRangePreset {\n    streamConfig = SCStreamConfiguration(preset: dynamicRangePreset)\n}\n\n\/\/ Configure audio capture.\nstreamConfig.capturesAudio = isAudioCaptureEnabled\nstreamConfig.excludesCurrentProcessAudio = isAppAudioExcluded\nstreamConfig.captureMicrophone = isMicCaptureEnabled\n\n\/\/ Configure the display content width and height.\nif captureType == .display, let display = selectedDisplay {\n    streamConfig.width = display.width * scaleFactor\n    streamConfig.height = display.height * scaleFactor\n}\n\n\/\/ Configure the window content width and height.\nif captureType == .window, let window = selectedWindow {\n    streamConfig.width = Int(window.frame.width) * 2\n    streamConfig.height = Int(window.frame.height) * 2\n}\n\n\/\/ Set the capture interval at 60 fps.\nstreamConfig.minimumFrameInterval = CMTime(value: 1, timescale: 60)\n\n\/\/ Increase the depth of the frame queue to ensure high fps at the expense of increasing\n\/\/ the memory footprint of WindowServer.\nstreamConfig.queueDepth = 5",
      "language" : "swift"
    },
    {
      "code" : "stream = SCStream(filter: filter, configuration: configuration, delegate: streamOutput)\n\n\/\/ Add a stream output to capture screen content.\ntry stream?.addStreamOutput(streamOutput, type: .screen, sampleHandlerQueue: videoSampleBufferQueue)\ntry stream?.addStreamOutput(streamOutput, type: .audio, sampleHandlerQueue: audioSampleBufferQueue)\ntry stream?.addStreamOutput(streamOutput, type: .microphone, sampleHandlerQueue: micSampleBufferQueue)\nstream?.startCapture()",
      "language" : "swift"
    },
    {
      "code" : "try await stream?.updateConfiguration(configuration)\ntry await stream?.updateContentFilter(filter)",
      "language" : "swift"
    },
    {
      "code" : "func stream(_ stream: SCStream, didOutputSampleBuffer sampleBuffer: CMSampleBuffer, of outputType: SCStreamOutputType) {\n    \n    \/\/ Return early if the sample buffer is invalid.\n    guard sampleBuffer.isValid else { return }\n    \n    \/\/ Determine which type of data the sample buffer contains.\n    switch outputType {\n    case .screen:\n\t\t\/\/ Process the screen content.\n    case .audio:\n\t\t\/\/ Process the audio content.\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Retrieve the array of metadata attachments from the sample buffer.\nguard let attachmentsArray = CMSampleBufferGetSampleAttachmentsArray(sampleBuffer,\n                                                                     createIfNecessary: false) as? [[SCStreamFrameInfo: Any]],\n      let attachments = attachmentsArray.first else { return nil }",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Validate the status of the frame. If it isn't `.complete`, return nil.\nguard let statusRawValue = attachments[SCStreamFrameInfo.status] as? Int,\n      let status = SCFrameStatus(rawValue: statusRawValue),\n      status == .complete else { return nil }",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Get the pixel buffer that contains the image data.\nguard let pixelBuffer = sampleBuffer.imageBuffer else { return nil }\n\n\/\/ Get the backing IOSurface.\nguard let surfaceRef = CVPixelBufferGetIOSurface(pixelBuffer)?.takeUnretainedValue() else { return nil }\nlet surface = unsafeBitCast(surfaceRef, to: IOSurface.self)\n\n\/\/ swiftlint:disable force_cast\n\/\/ Retrieve the content rectangle, scale, and scale factor.\nguard let contentRectDict = attachments[.contentRect],\n      let contentRect = CGRect(dictionaryRepresentation: contentRectDict as! CFDictionary),\n      let contentScale = attachments[.contentScale] as? CGFloat,\n      let scaleFactor = attachments[.scaleFactor] as? CGFloat else { return nil }\n\n\/\/ Create a new frame with the relevant data.\nlet frame = CapturedFrame(surface: surface,\n                          contentRect: contentRect,\n                          contentScale: contentScale,\n                          scaleFactor: scaleFactor)",
      "language" : "swift"
    },
    {
      "code" : "private func handleAudio(for buffer: CMSampleBuffer) -> Void? {\n    \/\/ Create an AVAudioPCMBuffer from an audio sample buffer.\n    try? buffer.withAudioBufferList { audioBufferList, blockBuffer in\n        guard let description = buffer.formatDescription?.audioStreamBasicDescription,\n              let format = AVAudioFormat(standardFormatWithSampleRate: description.mSampleRate, channels: description.mChannelsPerFrame),\n              let samples = AVAudioPCMBuffer(pcmFormat: format, bufferListNoCopy: audioBufferList.unsafePointer)\n        else { return }\n        pcmBufferHandler?(samples)\n    }\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "5163981c8caab8eae04978aa0d7280c9be177921589581bc05ae8365585f65f9",
  "crawledAt" : "2025-12-02T15:49:56Z",
  "id" : "6A46A364-76B1-40D1-9847-A1F6D273DA4E",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "ScreenCaptureKit",
  "overview" : "## Overview\n\nThis sample shows how to add high-performance screen capture to your Mac app by using [doc:\/\/com.apple.screencapturekit\/documentation\/ScreenCaptureKit]. The sample explores how to create content filters to capture the displays, apps, and windows you choose. It then shows how to configure your stream output, retrieve video frames and audio samples, and update a running stream.\n\n### Configure the sample code project\n\nTo run this sample app, you’ll need the following:\n\nThe first time you run this sample, the system prompts you to grant the app Screen Recording permission. After you grant permission, you need to restart the app to enable capture.\n\n### Create a content filter\n\nDisplays, running apps, and windows are the shareable content on a device. The sample uses the [doc:\/\/com.apple.screencapturekit\/documentation\/ScreenCaptureKit\/SCShareableContent] class to retrieve the items in the form of [doc:\/\/com.apple.screencapturekit\/documentation\/ScreenCaptureKit\/SCDisplay], [doc:\/\/com.apple.screencapturekit\/documentation\/ScreenCaptureKit\/SCRunningApplication], and [doc:\/\/com.apple.screencapturekit\/documentation\/ScreenCaptureKit\/SCWindow] objects respectively.\n\nBefore the sample begins capture, it creates an [doc:\/\/com.apple.screencapturekit\/documentation\/ScreenCaptureKit\/SCContentFilter] object to specify the content to capture. The sample provides two options that allow for capturing either a single window or an entire display. When the capture type is set to capture a window, the app creates a content filter that only includes that window.\n\nWhen a user specifies to capture the entire display, the sample creates a filter to capture only content from the main display. To illustrate filtering a running app, the sample contains a toggle to specify whether to exclude the sample app from the stream.\n\n### Create a stream configuration\n\nAn [doc:\/\/com.apple.screencapturekit\/documentation\/ScreenCaptureKit\/SCStreamConfiguration] object provides properties to configure the stream’s output size, pixel format, audio capture settings, and more. The app’s configuration throttles frame updates to 60 fps, and configures the number of frames to keep in the queue at 5. Specifying more frames uses more memory, but may allow for processing frame data without stalling the display stream. The default value is 3 and shouldn’t exceed 8 frames.\n\n### Start the capture session\n\nThe sample uses the content filter and stream configuration to initialize a new instance of [doc:\/\/com.apple.screencapturekit\/documentation\/ScreenCaptureKit\/SCStream]. To retrieve audio and video sample data, the app adds stream outputs that capture media of the specified type. When the stream captures new sample buffers, it delivers them to its stream output object on the indicated dispatch queues.\n\nAfter the stream starts, further changes to its configuration and content filter don’t require restarting it. Instead, after you update the capture configuration in the user interface, the sample creates new stream configuration and content filter objects and applies them to the running stream to update its state.\n\n### Process the output\n\nWhen a stream captures a new audio or video sample buffer, it calls the stream output’s [doc:\/\/com.apple.screencapturekit\/documentation\/ScreenCaptureKit\/SCStreamOutput\/stream(_:didOutputSampleBuffer:of:)] method, passing it the captured data and an indicator of its type. The stream output evaluates and processes the sample buffer as shown below.\n\n### Process a video sample buffer\n\nIf the sample buffer contains video data, it retrieves the sample buffer attachments that describe the output video frame.\n\nAn [doc:\/\/com.apple.screencapturekit\/documentation\/ScreenCaptureKit\/SCStreamFrameInfo] structure defines dictionary keys that the sample uses to retrieve metadata attached to a sample buffer. Metadata includes information about the frame’s display time, scale factor, status, and more. To determine whether a frame is available for processing, the sample inspects the status for [doc:\/\/com.apple.screencapturekit\/documentation\/ScreenCaptureKit\/SCFrameStatus\/complete].\n\nThe sample buffer wraps a [https:\/\/developer.apple.com\/documentation\/corevideo\/cvpixelbuffer] that’s backed by an [https:\/\/developer.apple.com\/documentation\/iosurface]. The sample casts the surface reference to an `IOSurface` that it later sets as the layer content of an [https:\/\/developer.apple.com\/documentation\/appkit\/nsview].\n\n### Process an audio sample buffer\n\nIf the sample buffer contains audio, it retrieves the data as an [https:\/\/developer.apple.com\/documentation\/coreaudiotypes\/audiobufferlist] as shown below.\n\nThe app retrieves the audio stream basic description that it uses to create an [https:\/\/developer.apple.com\/documentation\/avfaudio\/avaudioformat]. It then uses the format and the audio buffer list to create a new instance of [https:\/\/developer.apple.com\/documentation\/avfaudio\/avaudiopcmbuffer]. If you enable audio capture in the user interface, the sample uses the buffer to calculate average levels for the captured audio to display in a simple level meter.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/ScreenCaptureKit\/capturing-screen-content-in-macos\ncrawled: 2025-12-02T15:49:56Z\n---\n\n# Capturing screen content in macOS\n\n**Sample Code**\n\nStream desktop content like displays, apps, and windows by adopting screen capture in your app.\n\n## Overview\n\nThis sample shows how to add high-performance screen capture to your Mac app by using [doc:\/\/com.apple.screencapturekit\/documentation\/ScreenCaptureKit]. The sample explores how to create content filters to capture the displays, apps, and windows you choose. It then shows how to configure your stream output, retrieve video frames and audio samples, and update a running stream.\n\n\n\n### Configure the sample code project\n\nTo run this sample app, you’ll need the following:\n\n- A Mac with macOS 15 or later\n- Xcode 16 or later\n\nThe first time you run this sample, the system prompts you to grant the app Screen Recording permission. After you grant permission, you need to restart the app to enable capture.\n\n### Create a content filter\n\nDisplays, running apps, and windows are the shareable content on a device. The sample uses the [doc:\/\/com.apple.screencapturekit\/documentation\/ScreenCaptureKit\/SCShareableContent] class to retrieve the items in the form of [doc:\/\/com.apple.screencapturekit\/documentation\/ScreenCaptureKit\/SCDisplay], [doc:\/\/com.apple.screencapturekit\/documentation\/ScreenCaptureKit\/SCRunningApplication], and [doc:\/\/com.apple.screencapturekit\/documentation\/ScreenCaptureKit\/SCWindow] objects respectively.\n\n```swift\n\/\/ Retrieve the available screen content to capture.\nlet availableContent = try await SCShareableContent.excludingDesktopWindows(false,\n                                                                            onScreenWindowsOnly: true)\n```\n\nBefore the sample begins capture, it creates an [doc:\/\/com.apple.screencapturekit\/documentation\/ScreenCaptureKit\/SCContentFilter] object to specify the content to capture. The sample provides two options that allow for capturing either a single window or an entire display. When the capture type is set to capture a window, the app creates a content filter that only includes that window.\n\n```swift\n\/\/ Create a content filter that includes a single window.\nfilter = SCContentFilter(desktopIndependentWindow: window)\n```\n\nWhen a user specifies to capture the entire display, the sample creates a filter to capture only content from the main display. To illustrate filtering a running app, the sample contains a toggle to specify whether to exclude the sample app from the stream.\n\n```swift\nvar excludedApps = [SCRunningApplication]()\n\/\/ If a user chooses to exclude the app from the stream,\n\/\/ exclude it by matching its bundle identifier.\nif isAppExcluded {\n    excludedApps = availableApps.filter { app in\n        Bundle.main.bundleIdentifier == app.bundleIdentifier\n    }\n}\n\/\/ Create a content filter with excluded apps.\nfilter = SCContentFilter(display: display,\n                         excludingApplications: excludedApps,\n                         exceptingWindows: [])\n```\n\n### Create a stream configuration\n\nAn [doc:\/\/com.apple.screencapturekit\/documentation\/ScreenCaptureKit\/SCStreamConfiguration] object provides properties to configure the stream’s output size, pixel format, audio capture settings, and more. The app’s configuration throttles frame updates to 60 fps, and configures the number of frames to keep in the queue at 5. Specifying more frames uses more memory, but may allow for processing frame data without stalling the display stream. The default value is 3 and shouldn’t exceed 8 frames.\n\n```swift\nvar streamConfig = SCStreamConfiguration()\n\nif let dynamicRangePreset = selectedDynamicRangePreset?.scDynamicRangePreset {\n    streamConfig = SCStreamConfiguration(preset: dynamicRangePreset)\n}\n\n\/\/ Configure audio capture.\nstreamConfig.capturesAudio = isAudioCaptureEnabled\nstreamConfig.excludesCurrentProcessAudio = isAppAudioExcluded\nstreamConfig.captureMicrophone = isMicCaptureEnabled\n\n\/\/ Configure the display content width and height.\nif captureType == .display, let display = selectedDisplay {\n    streamConfig.width = display.width * scaleFactor\n    streamConfig.height = display.height * scaleFactor\n}\n\n\/\/ Configure the window content width and height.\nif captureType == .window, let window = selectedWindow {\n    streamConfig.width = Int(window.frame.width) * 2\n    streamConfig.height = Int(window.frame.height) * 2\n}\n\n\/\/ Set the capture interval at 60 fps.\nstreamConfig.minimumFrameInterval = CMTime(value: 1, timescale: 60)\n\n\/\/ Increase the depth of the frame queue to ensure high fps at the expense of increasing\n\/\/ the memory footprint of WindowServer.\nstreamConfig.queueDepth = 5\n```\n\n### Start the capture session\n\nThe sample uses the content filter and stream configuration to initialize a new instance of [doc:\/\/com.apple.screencapturekit\/documentation\/ScreenCaptureKit\/SCStream]. To retrieve audio and video sample data, the app adds stream outputs that capture media of the specified type. When the stream captures new sample buffers, it delivers them to its stream output object on the indicated dispatch queues.\n\n```swift\nstream = SCStream(filter: filter, configuration: configuration, delegate: streamOutput)\n\n\/\/ Add a stream output to capture screen content.\ntry stream?.addStreamOutput(streamOutput, type: .screen, sampleHandlerQueue: videoSampleBufferQueue)\ntry stream?.addStreamOutput(streamOutput, type: .audio, sampleHandlerQueue: audioSampleBufferQueue)\ntry stream?.addStreamOutput(streamOutput, type: .microphone, sampleHandlerQueue: micSampleBufferQueue)\nstream?.startCapture()\n```\n\nAfter the stream starts, further changes to its configuration and content filter don’t require restarting it. Instead, after you update the capture configuration in the user interface, the sample creates new stream configuration and content filter objects and applies them to the running stream to update its state.\n\n```swift\ntry await stream?.updateConfiguration(configuration)\ntry await stream?.updateContentFilter(filter)\n```\n\n### Process the output\n\nWhen a stream captures a new audio or video sample buffer, it calls the stream output’s [doc:\/\/com.apple.screencapturekit\/documentation\/ScreenCaptureKit\/SCStreamOutput\/stream(_:didOutputSampleBuffer:of:)] method, passing it the captured data and an indicator of its type. The stream output evaluates and processes the sample buffer as shown below.\n\n```swift\nfunc stream(_ stream: SCStream, didOutputSampleBuffer sampleBuffer: CMSampleBuffer, of outputType: SCStreamOutputType) {\n    \n    \/\/ Return early if the sample buffer is invalid.\n    guard sampleBuffer.isValid else { return }\n    \n    \/\/ Determine which type of data the sample buffer contains.\n    switch outputType {\n    case .screen:\n\t\t\/\/ Process the screen content.\n    case .audio:\n\t\t\/\/ Process the audio content.\n    }\n}\n```\n\n### Process a video sample buffer\n\nIf the sample buffer contains video data, it retrieves the sample buffer attachments that describe the output video frame.\n\n```swift\n\/\/ Retrieve the array of metadata attachments from the sample buffer.\nguard let attachmentsArray = CMSampleBufferGetSampleAttachmentsArray(sampleBuffer,\n                                                                     createIfNecessary: false) as? [[SCStreamFrameInfo: Any]],\n      let attachments = attachmentsArray.first else { return nil }\n```\n\nAn [doc:\/\/com.apple.screencapturekit\/documentation\/ScreenCaptureKit\/SCStreamFrameInfo] structure defines dictionary keys that the sample uses to retrieve metadata attached to a sample buffer. Metadata includes information about the frame’s display time, scale factor, status, and more. To determine whether a frame is available for processing, the sample inspects the status for [doc:\/\/com.apple.screencapturekit\/documentation\/ScreenCaptureKit\/SCFrameStatus\/complete].\n\n```swift\n\/\/ Validate the status of the frame. If it isn't `.complete`, return nil.\nguard let statusRawValue = attachments[SCStreamFrameInfo.status] as? Int,\n      let status = SCFrameStatus(rawValue: statusRawValue),\n      status == .complete else { return nil }\n```\n\nThe sample buffer wraps a [https:\/\/developer.apple.com\/documentation\/corevideo\/cvpixelbuffer] that’s backed by an [https:\/\/developer.apple.com\/documentation\/iosurface]. The sample casts the surface reference to an `IOSurface` that it later sets as the layer content of an [https:\/\/developer.apple.com\/documentation\/appkit\/nsview].\n\n```swift\n\/\/ Get the pixel buffer that contains the image data.\nguard let pixelBuffer = sampleBuffer.imageBuffer else { return nil }\n\n\/\/ Get the backing IOSurface.\nguard let surfaceRef = CVPixelBufferGetIOSurface(pixelBuffer)?.takeUnretainedValue() else { return nil }\nlet surface = unsafeBitCast(surfaceRef, to: IOSurface.self)\n\n\/\/ swiftlint:disable force_cast\n\/\/ Retrieve the content rectangle, scale, and scale factor.\nguard let contentRectDict = attachments[.contentRect],\n      let contentRect = CGRect(dictionaryRepresentation: contentRectDict as! CFDictionary),\n      let contentScale = attachments[.contentScale] as? CGFloat,\n      let scaleFactor = attachments[.scaleFactor] as? CGFloat else { return nil }\n\n\/\/ Create a new frame with the relevant data.\nlet frame = CapturedFrame(surface: surface,\n                          contentRect: contentRect,\n                          contentScale: contentScale,\n                          scaleFactor: scaleFactor)\n```\n\n### Process an audio sample buffer\n\nIf the sample buffer contains audio, it retrieves the data as an [https:\/\/developer.apple.com\/documentation\/coreaudiotypes\/audiobufferlist] as shown below.\n\n```swift\nprivate func handleAudio(for buffer: CMSampleBuffer) -> Void? {\n    \/\/ Create an AVAudioPCMBuffer from an audio sample buffer.\n    try? buffer.withAudioBufferList { audioBufferList, blockBuffer in\n        guard let description = buffer.formatDescription?.audioStreamBasicDescription,\n              let format = AVAudioFormat(standardFormatWithSampleRate: description.mSampleRate, channels: description.mChannelsPerFrame),\n              let samples = AVAudioPCMBuffer(pcmFormat: format, bufferListNoCopy: audioBufferList.unsafePointer)\n        else { return }\n        pcmBufferHandler?(samples)\n    }\n}\n```\n\nThe app retrieves the audio stream basic description that it uses to create an [https:\/\/developer.apple.com\/documentation\/avfaudio\/avaudioformat]. It then uses the format and the audio buffer list to create a new instance of [https:\/\/developer.apple.com\/documentation\/avfaudio\/avaudiopcmbuffer]. If you enable audio capture in the user interface, the sample uses the buffer to calculate average levels for the captured audio to display in a simple level meter.\n\n## Essentials\n\n- **ScreenCaptureKit updates**: Learn about important changes to ScreenCaptureKit.\n- **Persistent Content Capture**: A Boolean value that indicates whether a Virtual Network Computing (VNC) app needs persistent access to screen capture.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Learn about important changes to ScreenCaptureKit.",
          "name" : "ScreenCaptureKit updates",
          "url" : "https:\/\/developer.apple.com\/documentation\/Updates\/ScreenCaptureKit"
        },
        {
          "description" : "A Boolean value that indicates whether a Virtual Network Computing (VNC) app needs persistent access to screen capture.",
          "name" : "Persistent Content Capture",
          "url" : "https:\/\/developer.apple.com\/documentation\/BundleResources\/Entitlements\/com.apple.developer.persistent-content-capture"
        }
      ],
      "title" : "Essentials"
    }
  ],
  "source" : "appleJSON",
  "title" : "Capturing screen content in macOS",
  "url" : "https:\/\/developer.apple.com\/documentation\/ScreenCaptureKit\/capturing-screen-content-in-macos"
}