{
  "abstract" : "Create app intents and entities to incorporate system experiences such as Spotlight, visual intelligence, and Shortcuts.",
  "codeExamples" : [
    {
      "code" : "import AppIntents\n\nstruct OpenLandmarkIntent: OpenIntent {\n    static let title: LocalizedStringResource = \"Open Landmark\"\n\n    @Parameter(title: \"Landmark\", requestValueDialog: \"Which landmark?\")\n    var target: LandmarkEntity\n\n    func perform() async throws -> some IntentResult {\n        return .result()\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "struct LandmarkEntity: IndexedEntity {\n    static var typeDisplayRepresentation: TypeDisplayRepresentation {\n        return TypeDisplayRepresentation(\n            name: LocalizedStringResource(\"Landmark\", table: \"AppIntents\", comment: \"The type name for the landmark entity\"),\n            numericFormat: \"\\(placeholder: .int) landmarks\"\n        )\n    }\n\n    var displayRepresentation: DisplayRepresentation {\n        DisplayRepresentation(\n            title: \"\\(name)\",\n            subtitle: \"\\(continent)\",\n            image: .init(data: try! self.thumbnailRepresentationData)\n        )\n    }\n\n    static let defaultQuery = LandmarkEntityQuery()\n\n    var id: Int { landmark.id }\n\n    @ComputedProperty(indexingKey: \\.displayName)\n    var name: String { landmark.name }\n\n    \/\/ Maps the description variable to the Spotlight indexing key `contentDescription`.\n    @ComputedProperty(indexingKey: \\.contentDescription)\n    var description: String { landmark.description }\n\n    @ComputedProperty\n    var continent: String { landmark.continent }\n\n    @DeferredProperty\n    var crowdStatus: Int {\n        get async throws { \/\/ swiftlint:disable:this implicit_getter\n            await modelData.getCrowdStatus(self)\n        }\n    }\n\n    var landmark: Landmark\n    var modelData: ModelData\n\n    init(landmark: Landmark, modelData: ModelData) {\n        self.modelData = modelData\n        self.landmark = landmark\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "import AppIntents\nimport SwiftUI\n\nstruct ClosestLandmarkIntent: AppIntent {\n    static let title: LocalizedStringResource = \"Find Closest Landmark\"\n\n    @Dependency var modelData: ModelData\n\n    func perform() async throws -> some ReturnsValue<LandmarkEntity> & ShowsSnippetIntent & ProvidesDialog {\n        let landmark = await self.findClosestLandmark()\n\n        return .result(\n            value: landmark,\n            dialog: IntentDialog(\n                full: \"The closest landmark is \\(landmark.name).\",\n                supporting: \"\\(landmark.name) is located in \\(landmark.continent).\"\n            ),\n            snippetIntent: LandmarkSnippetIntent(landmark: landmark)\n        )\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "extension LandmarkEntity: Transferable {\n    static var transferRepresentation: some TransferRepresentation {\n        FileRepresentation(exportedContentType: .pdf) { @MainActor landmark in\n            let url = URL.documentsDirectory.appending(path: \"\\(landmark.name).pdf\")\n\n            let renderer = ImageRenderer(content: VStack {\n                Image(landmark.landmark.backgroundImageName)\n                    .resizable()\n                    .aspectRatio(contentMode: .fit)\n                Text(landmark.name)\n                Text(\"Continent: \\(landmark.continent)\")\n                Text(landmark.description)\n            }.frame(width: 600))\n\n            renderer.render { size, renderer in\n                var box = CGRect(x: 0, y: 0, width: size.width, height: size.height)\n\n                guard let pdf = CGContext(url as CFURL, mediaBox: &box, nil) else {\n                    return\n                }\n                pdf.beginPDFPage(nil)\n                renderer(pdf)\n                pdf.endPDFPage()\n                pdf.closePDF()\n            }\n\n            return .init(url)\n        }\n\n        DataRepresentation(exportedContentType: .image) {\n            try $0.imageRepresentationData\n        }\n\n        DataRepresentation(exportedContentType: .plainText) {\n            \"\"\"\n            Landmark: \\($0.name)\n            Description: \\($0.description)\n            \"\"\".data(using: .utf8)!\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "HStack(alignment: .bottom) {\n    Text(landmark.name)\n        .font(.title)\n        .fontWeight(.bold)\n        .userActivity(\n            \"com.landmarks.ViewingLandmark\"\n        ) {\n            $0.title = \"Viewing \\(landmark.name)\"\n            $0.appEntityIdentifier = EntityIdentifier(for: try! modelData.landmarkEntity(id: landmark.id))\n        }\n}",
      "language" : "swift"
    },
    {
      "code" : "struct LandmarkEntity: IndexedEntity {\n    \/\/ ...\n\n    \/\/ Maps the description to the Spotlight indexing key `contentDescription`.\n    @ComputedProperty(indexingKey: \\.contentDescription)\n    var description: String { landmark.description }\n\n    @ComputedProperty\n    var continent: String { landmark.continent }\n\n    \/\/ ...\n}",
      "language" : "swift"
    },
    {
      "code" : "static func donateLandmarks(modelData: ModelData) async throws {\n    let landmarkEntities = await modelData.landmarkEntities\n    try await CSSearchableIndex.default().indexAppEntities(landmarkEntities)\n}",
      "language" : "swift"
    },
    {
      "code" : "@UnionValue\nenum VisualSearchResult {\n    case landmark(LandmarkEntity)\n    case collection(CollectionEntity)\n}\n\nstruct LandmarkIntentValueQuery: IntentValueQuery {\n\n    @Dependency var modelData: ModelData\n\n    func values(for input: SemanticContentDescriptor) async throws -> [VisualSearchResult] {\n\n        guard let pixelBuffer: CVReadOnlyPixelBuffer = input.pixelBuffer else {\n            return []\n        }\n\n        let landmarks = try await modelData.search(matching: pixelBuffer)\n\n        return landmarks\n    }\n}\n\nextension ModelData {\n    \/**\n     This method contains the search functionality that takes the pixel buffer that visual intelligence provides\n     and uses it to find matching app entities. To keep this example app easy to understand, this function always\n     returns the same landmark entity.\n    *\/\n    func search(matching pixels: CVReadOnlyPixelBuffer) throws -> [VisualSearchResult] {\n        let landmarks = landmarkEntities.filter {\n            $0.id != 1005\n        }.map {\n            VisualSearchResult.landmark($0)\n        }.shuffled()\n\n        let collections = userCollections\n            .filter {\n                $0.landmarks.contains(where: { $0.id == 1005 })\n            }\n            .map {\n                CollectionEntity(collection: $0, modelData: self)\n            }\n            .map {\n                VisualSearchResult.collection($0)\n            }\n\n        return [try! .landmark(landmarkEntity(id: 1005))]\n            + collections\n            + landmarks\n    }\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "250f8433d02e005eeb9750199e4a2e8e210b8113e7d428e5d3f5293fbc094a92",
  "crawledAt" : "2025-12-02T15:45:19Z",
  "id" : "C91297C7-0E90-4834-9605-20F3C3F4D780",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "App Intents",
  "overview" : "## Overview\n\nThe app in this sample offers actions in the Shortcuts app that people can use to create custom shortcuts. It includes an App Shortcut to find the closest landmark and find tickets to visit the landmark, all without opening the app. Additionally, the app makes its data available to system experiences like Spotlight, Siri and Apple Intelligence, and visual intelligence.\n\nBy adopting the App Intents framework, the app provides functionality across the system, enabling people to:\n\n### Describe actions as app intents and entities\n\nThe app contains many actions and makes them available to the system as app intents, so people can use them to create custom shortcuts and invoke across system experiences. For example, the app offers key actions like finding the closest landmark or opening a landmark in the app. This app intent opens a landmark in the app:\n\nTo use your data as input and output of app intents and make the data available to the system, you use app entities. App entities often limit the information a model object you persist to storage to what the system needs. They also add required information to understand the data or to use it in system experiences. For example, the `LandmarkEntity` of the sample app provides required `typeDisplayRepresentation` and `displayRepresentation` properties but doesn’t include every property of the `Landmark` model object:\n\nFor more information about describing actions as app intents and app entities, refer to [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/Making-actions-and-content-discoverable-and-widely-available] and [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/Creating-your-first-app-intent].\n\n### Offer interactive snippets\n\nThe app’s “Find Closest” App Shortcut performs an app intent that finds the closest nearby landmark without opening the app, and allows people to find tickets to visit it. Instead of taking them to the app, the app intent displays interactive snippets that appear as overlays at the top of the screen. To display the interactive snippet, the app’s `ClosestLandmarkIntent` returns a [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/SnippetIntent] that presents the interactive snippet in its `perform()` method:\n\nFor more information about displaying interactive snippets, refer to [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/displaying-static-and-interactive-snippets].\n\n### Make your entity available to Siri and Apple Intelligence\n\nTo allow Siri to access the landmark information that’s visible onscreen in the app, its `LandmarkEntity` implements the [doc:\/\/com.apple.documentation\/documentation\/CoreTransferable\/Transferable] protocol and provides plain-text, image, and PDF representations that Siri can understand and forward to other services, including third-party services:\n\nWhen the landmark becomes visible onscreen, the app uses the user activity annotation API to give the system access to the data:\n\nFor more information about making onscreen content available to Siri and Apple Intelligence, refer to [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/Making-onscreen-content-available-to-siri-and-apple-intelligence].\n\n### Add entities to the Spotlight index\n\nThe app describes its data as app entities, so the system can use it when it performs app intents. Additionally, the app donates the entities into the semantic search index, making it possible to find the app entities in Spotlight. The following example shows how the app’s `LandmarkEntity` conforms to [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/IndexedEntity] and uses Swift macros to add the indexing keys that Spotlight needs.\n\nIn a utility function, the app donates the landmark entities to the Spotlight index:\n\nFor more information, refer to [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/making-app-entities-available-in-spotlight].\n\n### Integrate search results with visual intelligence\n\nWith visual intelligence, people circle items onscreen or in visual intelligence camera to search for matching results across apps that support visual intelligence. To support visual intelligence search, the sample app implements an [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/IntentValueQuery] to find matching landmarks:\n\nFor more information about integrating your app with visual intelligence, refer to [doc:\/\/com.apple.documentation\/documentation\/VisualIntelligence].",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/AppIntents\/adopting-app-intents-to-support-system-experiences\ncrawled: 2025-12-02T15:45:19Z\n---\n\n# Adopting App Intents to support system experiences\n\n**Sample Code**\n\nCreate app intents and entities to incorporate system experiences such as Spotlight, visual intelligence, and Shortcuts.\n\n## Overview\n\nThe app in this sample offers actions in the Shortcuts app that people can use to create custom shortcuts. It includes an App Shortcut to find the closest landmark and find tickets to visit the landmark, all without opening the app. Additionally, the app makes its data available to system experiences like Spotlight, Siri and Apple Intelligence, and visual intelligence.\n\nBy adopting the App Intents framework, the app provides functionality across the system, enabling people to:\n\n- In Shortcuts, find and run the app’s app intents.\n- In Shortcuts, create custom shortcuts or view the provided “Find Closest” App Shortcut.\n- In Shortcuts, place custom shortcuts or the App Shortcut on the Home Screen as a bookmark.\n- In Spotlight, search for a landmark or the “Find Closest” App Shortcut.\n- With visual intelligence, circle an object in the visual intelligence camera or onscreen and view matching results from the app.\n- With the Action button, trigger a custom shortcut or the App Shortcut.\n- From Siri suggestions, use custom shortcuts or the App Shortcut.\n- In the app, view information about a landmark, then ask Siri something like “What’s a summary of the history of this place?” or similar to receive a content summary, and more.\n\n### Describe actions as app intents and entities\n\nThe app contains many actions and makes them available to the system as app intents, so people can use them to create custom shortcuts and invoke across system experiences. For example, the app offers key actions like finding the closest landmark or opening a landmark in the app. This app intent opens a landmark in the app:\n\n```swift\nimport AppIntents\n\nstruct OpenLandmarkIntent: OpenIntent {\n    static let title: LocalizedStringResource = \"Open Landmark\"\n\n    @Parameter(title: \"Landmark\", requestValueDialog: \"Which landmark?\")\n    var target: LandmarkEntity\n\n    func perform() async throws -> some IntentResult {\n        return .result()\n    }\n}\n```\n\nTo use your data as input and output of app intents and make the data available to the system, you use app entities. App entities often limit the information a model object you persist to storage to what the system needs. They also add required information to understand the data or to use it in system experiences. For example, the `LandmarkEntity` of the sample app provides required `typeDisplayRepresentation` and `displayRepresentation` properties but doesn’t include every property of the `Landmark` model object:\n\n```swift\nstruct LandmarkEntity: IndexedEntity {\n    static var typeDisplayRepresentation: TypeDisplayRepresentation {\n        return TypeDisplayRepresentation(\n            name: LocalizedStringResource(\"Landmark\", table: \"AppIntents\", comment: \"The type name for the landmark entity\"),\n            numericFormat: \"\\(placeholder: .int) landmarks\"\n        )\n    }\n\n    var displayRepresentation: DisplayRepresentation {\n        DisplayRepresentation(\n            title: \"\\(name)\",\n            subtitle: \"\\(continent)\",\n            image: .init(data: try! self.thumbnailRepresentationData)\n        )\n    }\n\n    static let defaultQuery = LandmarkEntityQuery()\n\n    var id: Int { landmark.id }\n\n    @ComputedProperty(indexingKey: \\.displayName)\n    var name: String { landmark.name }\n\n    \/\/ Maps the description variable to the Spotlight indexing key `contentDescription`.\n    @ComputedProperty(indexingKey: \\.contentDescription)\n    var description: String { landmark.description }\n\n    @ComputedProperty\n    var continent: String { landmark.continent }\n\n    @DeferredProperty\n    var crowdStatus: Int {\n        get async throws { \/\/ swiftlint:disable:this implicit_getter\n            await modelData.getCrowdStatus(self)\n        }\n    }\n\n    var landmark: Landmark\n    var modelData: ModelData\n\n    init(landmark: Landmark, modelData: ModelData) {\n        self.modelData = modelData\n        self.landmark = landmark\n    }\n}\n```\n\nFor more information about describing actions as app intents and app entities, refer to [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/Making-actions-and-content-discoverable-and-widely-available] and [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/Creating-your-first-app-intent].\n\n### Offer interactive snippets\n\nThe app’s “Find Closest” App Shortcut performs an app intent that finds the closest nearby landmark without opening the app, and allows people to find tickets to visit it. Instead of taking them to the app, the app intent displays interactive snippets that appear as overlays at the top of the screen. To display the interactive snippet, the app’s `ClosestLandmarkIntent` returns a [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/SnippetIntent] that presents the interactive snippet in its `perform()` method:\n\n```swift\nimport AppIntents\nimport SwiftUI\n\nstruct ClosestLandmarkIntent: AppIntent {\n    static let title: LocalizedStringResource = \"Find Closest Landmark\"\n\n    @Dependency var modelData: ModelData\n\n    func perform() async throws -> some ReturnsValue<LandmarkEntity> & ShowsSnippetIntent & ProvidesDialog {\n        let landmark = await self.findClosestLandmark()\n\n        return .result(\n            value: landmark,\n            dialog: IntentDialog(\n                full: \"The closest landmark is \\(landmark.name).\",\n                supporting: \"\\(landmark.name) is located in \\(landmark.continent).\"\n            ),\n            snippetIntent: LandmarkSnippetIntent(landmark: landmark)\n        )\n    }\n}\n```\n\nFor more information about displaying interactive snippets, refer to [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/displaying-static-and-interactive-snippets].\n\n### Make your entity available to Siri and Apple Intelligence\n\nTo allow Siri to access the landmark information that’s visible onscreen in the app, its `LandmarkEntity` implements the [doc:\/\/com.apple.documentation\/documentation\/CoreTransferable\/Transferable] protocol and provides plain-text, image, and PDF representations that Siri can understand and forward to other services, including third-party services:\n\n```swift\nextension LandmarkEntity: Transferable {\n    static var transferRepresentation: some TransferRepresentation {\n        FileRepresentation(exportedContentType: .pdf) { @MainActor landmark in\n            let url = URL.documentsDirectory.appending(path: \"\\(landmark.name).pdf\")\n\n            let renderer = ImageRenderer(content: VStack {\n                Image(landmark.landmark.backgroundImageName)\n                    .resizable()\n                    .aspectRatio(contentMode: .fit)\n                Text(landmark.name)\n                Text(\"Continent: \\(landmark.continent)\")\n                Text(landmark.description)\n            }.frame(width: 600))\n\n            renderer.render { size, renderer in\n                var box = CGRect(x: 0, y: 0, width: size.width, height: size.height)\n\n                guard let pdf = CGContext(url as CFURL, mediaBox: &box, nil) else {\n                    return\n                }\n                pdf.beginPDFPage(nil)\n                renderer(pdf)\n                pdf.endPDFPage()\n                pdf.closePDF()\n            }\n\n            return .init(url)\n        }\n\n        DataRepresentation(exportedContentType: .image) {\n            try $0.imageRepresentationData\n        }\n\n        DataRepresentation(exportedContentType: .plainText) {\n            \"\"\"\n            Landmark: \\($0.name)\n            Description: \\($0.description)\n            \"\"\".data(using: .utf8)!\n        }\n    }\n}\n```\n\nWhen the landmark becomes visible onscreen, the app uses the user activity annotation API to give the system access to the data:\n\n```swift\nHStack(alignment: .bottom) {\n    Text(landmark.name)\n        .font(.title)\n        .fontWeight(.bold)\n        .userActivity(\n            \"com.landmarks.ViewingLandmark\"\n        ) {\n            $0.title = \"Viewing \\(landmark.name)\"\n            $0.appEntityIdentifier = EntityIdentifier(for: try! modelData.landmarkEntity(id: landmark.id))\n        }\n}\n```\n\nFor more information about making onscreen content available to Siri and Apple Intelligence, refer to [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/Making-onscreen-content-available-to-siri-and-apple-intelligence].\n\n### Add entities to the Spotlight index\n\nThe app describes its data as app entities, so the system can use it when it performs app intents. Additionally, the app donates the entities into the semantic search index, making it possible to find the app entities in Spotlight. The following example shows how the app’s `LandmarkEntity` conforms to [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/IndexedEntity] and uses Swift macros to add the indexing keys that Spotlight needs.\n\n```swift\nstruct LandmarkEntity: IndexedEntity {\n    \/\/ ...\n\n    \/\/ Maps the description to the Spotlight indexing key `contentDescription`.\n    @ComputedProperty(indexingKey: \\.contentDescription)\n    var description: String { landmark.description }\n\n    @ComputedProperty\n    var continent: String { landmark.continent }\n\n    \/\/ ...\n}\n```\n\nIn a utility function, the app donates the landmark entities to the Spotlight index:\n\n```swift\nstatic func donateLandmarks(modelData: ModelData) async throws {\n    let landmarkEntities = await modelData.landmarkEntities\n    try await CSSearchableIndex.default().indexAppEntities(landmarkEntities)\n}\n```\n\nFor more information, refer to [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/making-app-entities-available-in-spotlight].\n\n### Integrate search results with visual intelligence\n\nWith visual intelligence, people circle items onscreen or in visual intelligence camera to search for matching results across apps that support visual intelligence. To support visual intelligence search, the sample app implements an [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/IntentValueQuery] to find matching landmarks:\n\n```swift\n@UnionValue\nenum VisualSearchResult {\n    case landmark(LandmarkEntity)\n    case collection(CollectionEntity)\n}\n\nstruct LandmarkIntentValueQuery: IntentValueQuery {\n\n    @Dependency var modelData: ModelData\n\n    func values(for input: SemanticContentDescriptor) async throws -> [VisualSearchResult] {\n\n        guard let pixelBuffer: CVReadOnlyPixelBuffer = input.pixelBuffer else {\n            return []\n        }\n\n        let landmarks = try await modelData.search(matching: pixelBuffer)\n\n        return landmarks\n    }\n}\n\nextension ModelData {\n    \/**\n     This method contains the search functionality that takes the pixel buffer that visual intelligence provides\n     and uses it to find matching app entities. To keep this example app easy to understand, this function always\n     returns the same landmark entity.\n    *\/\n    func search(matching pixels: CVReadOnlyPixelBuffer) throws -> [VisualSearchResult] {\n        let landmarks = landmarkEntities.filter {\n            $0.id != 1005\n        }.map {\n            VisualSearchResult.landmark($0)\n        }.shuffled()\n\n        let collections = userCollections\n            .filter {\n                $0.landmarks.contains(where: { $0.id == 1005 })\n            }\n            .map {\n                CollectionEntity(collection: $0, modelData: self)\n            }\n            .map {\n                VisualSearchResult.collection($0)\n            }\n\n        return [try! .landmark(landmarkEntity(id: 1005))]\n            + collections\n            + landmarks\n    }\n}\n```\n\nFor more information about integrating your app with visual intelligence, refer to [doc:\/\/com.apple.documentation\/documentation\/VisualIntelligence].\n\n## Essentials\n\n- **App Intents updates**: Learn about important changes in App Intents.\n- **Making actions and content discoverable and widely available**: Adopt App Intents to make your app discoverable with Spotlight, controls, widgets, and the Action button.\n- **Creating your first app intent**: Create your first app intent that makes your app available in system experiences like Spotlight or the Shortcuts app.\n- **Accelerating app interactions with App Intents**: Enable people to use your app’s features quickly through Siri, Spotlight, and Shortcuts.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Learn about important changes in App Intents.",
          "name" : "App Intents updates",
          "url" : "https:\/\/developer.apple.com\/documentation\/Updates\/AppIntents"
        },
        {
          "description" : "Adopt App Intents to make your app discoverable with Spotlight, controls, widgets, and the Action button.",
          "name" : "Making actions and content discoverable and widely available",
          "url" : "https:\/\/developer.apple.com\/documentation\/AppIntents\/Making-actions-and-content-discoverable-and-widely-available"
        },
        {
          "description" : "Create your first app intent that makes your app available in system experiences like Spotlight or the Shortcuts app.",
          "name" : "Creating your first app intent",
          "url" : "https:\/\/developer.apple.com\/documentation\/AppIntents\/Creating-your-first-app-intent"
        },
        {
          "description" : "Enable people to use your app’s features quickly through Siri, Spotlight, and Shortcuts.",
          "name" : "Accelerating app interactions with App Intents",
          "url" : "https:\/\/developer.apple.com\/documentation\/AppIntents\/AcceleratingAppInteractionsWithAppIntents"
        }
      ],
      "title" : "Essentials"
    }
  ],
  "source" : "appleJSON",
  "title" : "Adopting App Intents to support system experiences",
  "url" : "https:\/\/developer.apple.com\/documentation\/AppIntents\/adopting-app-intents-to-support-system-experiences"
}