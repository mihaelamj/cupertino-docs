{
  "abstract" : "Add app intent schemas to your app so Siri can complete requests, and integrate your app with Apple Intelligence, Spotlight, and other system experiences.",
  "codeExamples" : [
    {
      "code" : "@AppEnum(schema: .photos.assetType)\nenum AssetType: String, AppEnum {\n    case photo\n    case video\n\n    static let caseDisplayRepresentations: [AssetType : DisplayRepresentation]  = [\n        .photo: \"Photo\",\n        .video: \"Video\"\n    ]\n}",
      "language" : "swift"
    },
    {
      "code" : "@AppEntity(schema: .photos.asset)\nstruct AssetEntity: IndexedEntity {\n\n    \/\/ MARK: Static\n\n    static let defaultQuery = AssetQuery()\n\n    \/\/ MARK: Properties\n\n    let id: String\n    let asset: Asset\n\n    @Property(title: \"Title\")\n    var title: String?\n\n    var creationDate: Date?\n    var location: CLPlacemark?\n    var assetType: AssetType?\n    var isFavorite: Bool\n    var isHidden: Bool\n    var hasSuggestedEdits: Bool\n\n    var displayRepresentation: DisplayRepresentation {\n        DisplayRepresentation(\n            title: title.map { \"\\($0)\" } ?? \"Unknown\",\n            subtitle: assetType?.localizedStringResource ?? \"Photo\"\n        )\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "extension AssetEntity: Transferable {\n\n    struct AssetQuery: EntityQuery {\n        @Dependency\n        var library: MediaLibrary\n\n        @MainActor\n        func entities(for identifiers: [AssetEntity.ID]) async throws -> [AssetEntity] {\n            library.assets(for: identifiers).map(\\.entity)\n        }\n\n        @MainActor\n        func suggestedEntities() async throws -> [AssetEntity] {\n            \/\/ Suggest the first three assets in the photo library.\n            library.assets.prefix(3).map(\\.entity)\n        }\n    }\n\n    static var transferRepresentation: some TransferRepresentation {\n        DataRepresentation(exportedContentType: .png) { entity in\n            try await entity.asset.pngData()\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "var body: some View {\n    \/\/ ...\n    MediaView(\n        image: image,\n        duration: asset.duration,\n        isFavorite: asset.isFavorite,\n        proxy: proxy\n    )\n    .userActivity(\n        \"com.example.apple-samplecode.AssistantSchemasExample.ViewingPhoto\",\n        element: asset.entity\n    ) { asset, activity in\n        activity.title = \"Viewing a photo\"\n        activity.appEntityIdentifier = EntityIdentifier(for: asset)\n    }\n    \/\/ ...",
      "language" : "swift"
    }
  ],
  "contentHash" : "191ee298051c593d786d0a3674b1784536b27fe468e22006a717cb21124f92d2",
  "crawledAt" : "2025-12-02T15:46:36Z",
  "id" : "EE657F9D-164E-452C-816D-7FBEA0E06E57",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "App Intents",
  "overview" : "## Overview\n\nUsing this sample app, people can keep track of photos and videos they capture with their device and can use Siri to access app functionality. To make its main functionality available to Siri, the app uses the App Intents framework.\n\n### Make app functionality available to Siri\n\nThis sample uses [doc:\/\/com.apple.AppIntents\/documentation\/AppIntents\/app-intent-domains] to make the [doc:\/\/com.apple.AppIntents\/documentation\/AppIntents\/AppEnum], [doc:\/\/com.apple.AppIntents\/documentation\/AppIntents\/AppEntity], and [doc:\/\/com.apple.AppIntents\/documentation\/AppIntents\/AppIntent] implementations available to Siri as shown in the following example:\n\n### Make data available in Spotlight\n\nPeople can use Spotlight to search for data the sample contains. To enable this functionality, the sample defines an app entity that conforms to [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/IndexedEntity]:\n\n### Make app entities shareable\n\nBy adopting the [doc:\/\/com.apple.documentation\/documentation\/CoreTransferable\/Transferable] protocol, this sample makes the data it describes as app entities more shareable and allows other apps to understand its data formats. For example, the sample’s `AssetEntity` implements `Transferable` to make it easy to share a photo as a PNG image with Siri or the share sheet:\n\n### Make onscreen content available to Siri and Apple Intelligence\n\nWhen the user asks a question about onscreen content or wants to perform an action on it, Siri and Apple Intelligence can retrieve the content to respond to the question and perform the action. If the user explicitly requests it, Siri and Apple Intelligence can send content to supported third-party services. For example, someone could view a photo and use Siri to describe things a person can do with an identified object in the photo by saying or typing a phrase like “Hey Siri, what can I do with the object in this photo?” To integrate onscreen content with current and upcoming personal intelligence features of Siri and Apple Intelligence, the sample’s `AssetEntity` conforms to the [doc:\/\/com.apple.documentation\/documentation\/CoreTransferable\/Transferable] protocol and the `.photos.asset` schema. When a person views a photo, the app associates the asset entity with an [doc:\/\/com.apple.documentation\/documentation\/Foundation\/NSUserActivity] to make the photo available to Siri and Apple Intelligence:\n\nFor more information about making onscreen content available to Siri and Apple Intelligence, refer to [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/Making-onscreen-content-available-to-siri-and-apple-intelligence].",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/AppIntents\/making-your-app-s-functionality-available-to-siri\ncrawled: 2025-12-02T15:46:36Z\n---\n\n# Making your app’s functionality available to Siri\n\n**Sample Code**\n\nAdd app intent schemas to your app so Siri can complete requests, and integrate your app with Apple Intelligence, Spotlight, and other system experiences.\n\n## Overview\n\nUsing this sample app, people can keep track of photos and videos they capture with their device and can use Siri to access app functionality. To make its main functionality available to Siri, the app uses the App Intents framework.\n\n\n\n### Make app functionality available to Siri\n\nThis sample uses [doc:\/\/com.apple.AppIntents\/documentation\/AppIntents\/app-intent-domains] to make the [doc:\/\/com.apple.AppIntents\/documentation\/AppIntents\/AppEnum], [doc:\/\/com.apple.AppIntents\/documentation\/AppIntents\/AppEntity], and [doc:\/\/com.apple.AppIntents\/documentation\/AppIntents\/AppIntent] implementations available to Siri as shown in the following example:\n\n```swift\n@AppEnum(schema: .photos.assetType)\nenum AssetType: String, AppEnum {\n    case photo\n    case video\n\n    static let caseDisplayRepresentations: [AssetType : DisplayRepresentation]  = [\n        .photo: \"Photo\",\n        .video: \"Video\"\n    ]\n}\n```\n\n### Make data available in Spotlight\n\nPeople can use Spotlight to search for data the sample contains. To enable this functionality, the sample defines an app entity that conforms to [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/IndexedEntity]:\n\n```swift\n@AppEntity(schema: .photos.asset)\nstruct AssetEntity: IndexedEntity {\n\n    \/\/ MARK: Static\n\n    static let defaultQuery = AssetQuery()\n\n    \/\/ MARK: Properties\n\n    let id: String\n    let asset: Asset\n\n    @Property(title: \"Title\")\n    var title: String?\n\n    var creationDate: Date?\n    var location: CLPlacemark?\n    var assetType: AssetType?\n    var isFavorite: Bool\n    var isHidden: Bool\n    var hasSuggestedEdits: Bool\n\n    var displayRepresentation: DisplayRepresentation {\n        DisplayRepresentation(\n            title: title.map { \"\\($0)\" } ?? \"Unknown\",\n            subtitle: assetType?.localizedStringResource ?? \"Photo\"\n        )\n    }\n}\n```\n\n### Make app entities shareable\n\nBy adopting the [doc:\/\/com.apple.documentation\/documentation\/CoreTransferable\/Transferable] protocol, this sample makes the data it describes as app entities more shareable and allows other apps to understand its data formats. For example, the sample’s `AssetEntity` implements `Transferable` to make it easy to share a photo as a PNG image with Siri or the share sheet:\n\n```swift\nextension AssetEntity: Transferable {\n\n    struct AssetQuery: EntityQuery {\n        @Dependency\n        var library: MediaLibrary\n\n        @MainActor\n        func entities(for identifiers: [AssetEntity.ID]) async throws -> [AssetEntity] {\n            library.assets(for: identifiers).map(\\.entity)\n        }\n\n        @MainActor\n        func suggestedEntities() async throws -> [AssetEntity] {\n            \/\/ Suggest the first three assets in the photo library.\n            library.assets.prefix(3).map(\\.entity)\n        }\n    }\n\n    static var transferRepresentation: some TransferRepresentation {\n        DataRepresentation(exportedContentType: .png) { entity in\n            try await entity.asset.pngData()\n        }\n    }\n}\n```\n\n### Make onscreen content available to Siri and Apple Intelligence\n\nWhen the user asks a question about onscreen content or wants to perform an action on it, Siri and Apple Intelligence can retrieve the content to respond to the question and perform the action. If the user explicitly requests it, Siri and Apple Intelligence can send content to supported third-party services. For example, someone could view a photo and use Siri to describe things a person can do with an identified object in the photo by saying or typing a phrase like “Hey Siri, what can I do with the object in this photo?” To integrate onscreen content with current and upcoming personal intelligence features of Siri and Apple Intelligence, the sample’s `AssetEntity` conforms to the [doc:\/\/com.apple.documentation\/documentation\/CoreTransferable\/Transferable] protocol and the `.photos.asset` schema. When a person views a photo, the app associates the asset entity with an [doc:\/\/com.apple.documentation\/documentation\/Foundation\/NSUserActivity] to make the photo available to Siri and Apple Intelligence:\n\n```swift\nvar body: some View {\n    \/\/ ...\n    MediaView(\n        image: image,\n        duration: asset.duration,\n        isFavorite: asset.isFavorite,\n        proxy: proxy\n    )\n    .userActivity(\n        \"com.example.apple-samplecode.AssistantSchemasExample.ViewingPhoto\",\n        element: asset.entity\n    ) { asset, activity in\n        activity.title = \"Viewing a photo\"\n        activity.appEntityIdentifier = EntityIdentifier(for: asset)\n    }\n    \/\/ ...\n```\n\nFor more information about making onscreen content available to Siri and Apple Intelligence, refer to [doc:\/\/com.apple.documentation\/documentation\/AppIntents\/Making-onscreen-content-available-to-siri-and-apple-intelligence].\n\n## Siri and Apple Intelligence\n\n- **Integrating actions with Siri and Apple Intelligence**: Create app intents, entities, and enumerations that conform to assistant schemas to tap into the enhanced action capabilities of Siri and Apple Intelligence.\n- **Making onscreen content available to Siri and Apple Intelligence**: Enable Siri and Apple Intelligence to respond to a person’s questions and action requests for your app’s onscreen content.\n- **App intent domains**: Make your app’s actions and content available to Siri and Apple Intelligence with assistant schemas.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Create app intents, entities, and enumerations that conform to assistant schemas to tap into the enhanced action capabilities of Siri and Apple Intelligence.",
          "name" : "Integrating actions with Siri and Apple Intelligence",
          "url" : "https:\/\/developer.apple.com\/documentation\/AppIntents\/Integrating-actions-with-siri-and-apple-intelligence"
        },
        {
          "description" : "Enable Siri and Apple Intelligence to respond to a person’s questions and action requests for your app’s onscreen content.",
          "name" : "Making onscreen content available to Siri and Apple Intelligence",
          "url" : "https:\/\/developer.apple.com\/documentation\/AppIntents\/Making-onscreen-content-available-to-siri-and-apple-intelligence"
        },
        {
          "description" : "Make your app’s actions and content available to Siri and Apple Intelligence with assistant schemas.",
          "name" : "App intent domains",
          "url" : "https:\/\/developer.apple.com\/documentation\/AppIntents\/app-intent-domains"
        }
      ],
      "title" : "Siri and Apple Intelligence"
    }
  ],
  "source" : "appleJSON",
  "title" : "Making your app’s functionality available to Siri",
  "url" : "https:\/\/developer.apple.com\/documentation\/AppIntents\/making-your-app-s-functionality-available-to-siri"
}