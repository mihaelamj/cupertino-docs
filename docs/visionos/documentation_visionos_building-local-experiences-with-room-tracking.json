{
  "abstract" : "Use room tracking in visionOS to provide custom interactions with physical spaces.",
  "codeExamples" : [
    {
      "code" : "func areAllDataProvidersAuthorized() async -> Bool {\n    \/\/ It's sufficient to check that the authorization status isn't `denied`.\n    \/\/ If it's `notdetermined`, ARKit presents a permission pop-up menu that appears as soon\n    \/\/ as the session runs.\n    let authorization = await ARKitSession().queryAuthorization(for: [.worldSensing])\n    return authorization[.worldSensing] != .denied\n}",
      "language" : "swift"
    },
    {
      "code" : "private let session = ARKitSession()\nprivate let worldTracking = WorldTrackingProvider()\nprivate let roomTracking = RoomTrackingProvider()",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ A dictionary that contains `RoomAnchor` structures.\nprivate var roomAnchors = [UUID: RoomAnchor]()\n\/\/\/ A dictionary that contains `WorldAnchor` structures.\nprivate var worldAnchors = [UUID: WorldAnchor]()\n\/\/\/ A dictionary that contains `ModelEntity` structures for spheres.\nprivate var sphereEntities = [UUID: ModelEntity]()\n\/\/\/ A dictionary that contains `ModelEntity` structures for room anchors.\nprivate var roomEntities = [UUID: ModelEntity]()",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Material for spheres in the current room.\nprivate let inRoomSphereMaterial = SimpleMaterial(color: .green, roughness: 0.2, isMetallic: true)\n\/\/ Material for spheres not in the current room.\nprivate let outOfRoomSphereMaterial = SimpleMaterial(color: .red, roughness: 0.2, isMetallic: true)\n\/\/ Material the app applies to room entities to show occlusion effects.\nprivate let occlusionMaterial = OcclusionMaterial()\n\/\/ Material for current room walls.\nprivate var wallMaterial = UnlitMaterial(color: .blue)",
      "language" : "swift"
    },
    {
      "code" : "private func createPreviewSphere() -> ModelEntity {\n    let sphereMesh = MeshResource.generateSphere(radius: 0.1)\n    let sphereMaterial = SimpleMaterial(color: .gray.withAlphaComponent(0.5), roughness: 0.2, isMetallic: false)\n    let sphere = ModelEntity(mesh: sphereMesh, materials: [sphereMaterial])\n    \n    \/\/ Enables gestures on the preview sphere.\n    \/\/ Looking at the preview and using a pinch gesture causes a world anchored sphere to appear.\n    sphere.generateCollisionShapes(recursive: false, static: true)\n    \/\/ Ensures the preview only accepts indirect input (for tap gestures).\n    sphere.components.set(InputTargetComponent(allowedInputTypes: [.indirect]))\n    \n    \/\/ The preview sphere only becomes visible once someone clicks the Add a Sphere button.\n    sphere.isEnabled = false\n    \n    return sphere\n}",
      "language" : "swift"
    },
    {
      "code" : ".gesture(SpatialTapGesture().targetedToAnyEntity().onEnded { event in\n    if event.entity == previewSphere {\n        Task {\n            \/\/ To place a sphere you need to:\n            \/\/ 1. Create a world anchor with the translation of that offset transform and add the anchor to the world tracking provider.\n            \/\/ 2. Create the sphere's geometry in `processWorldTrackingUpdates()` after you have successfully added the world anchor.\n            await appState.addWorldAnchor(at: event.entity.transformMatrix(relativeTo: nil))\n            appState.showPreviewSphere = false\n        }\n    }\n})",
      "language" : "swift"
    },
    {
      "code" : "func processRoomTrackingUpdates() async {\n    for await update in roomTracking.anchorUpdates {\n        let roomAnchor = update.anchor\n        switch update.event {\n        case .removed:\n            if roomAnchor.isCurrentRoom {\n                colliderWallsRoot.children.removeAll()\n                if let currentRenderedWall {\n                    renderWallRoot.removeChild(currentRenderedWall)\n                }\n            }\n            roomAnchors.removeValue(forKey: roomAnchor.id)\n            roomEntities[roomAnchor.id]?.removeFromParent()\n            roomEntities.removeValue(forKey: roomAnchor.id)\n            updateSphereState()\n        case .added, .updated:\n            roomAnchors[roomAnchor.id] = roomAnchor\n            guard let roomMeshResource = roomAnchor.geometry.asMeshResource() else { continue }\n            if update.event == .added {\n                let roomEntity = ModelEntity(mesh: roomMeshResource, materials: [occlusionMaterial])\n                roomEntity.transform = Transform(matrix: roomAnchor.originFromAnchorTransform)\n                roomEntities[roomAnchor.id] = roomEntity\n                roomEntity.isEnabled = roomAnchor.isCurrentRoom\n                roomRoot.addChild(roomEntity)\n                \n            } else if update.event == .updated {\n                guard let roomEntity = roomEntities[roomAnchor.id] else { continue }\n                roomEntity.model?.mesh = roomMeshResource\n                roomEntity.transform = Transform(matrix: roomAnchor.originFromAnchorTransform)\n                roomEntity.isEnabled = roomAnchor.isCurrentRoom\n            }\n            \n            updateSphereState()\n            \n            if roomAnchor.isCurrentRoom {\n                currentRoomID = roomAnchor.id\n                if renderWallRoot.isEnabled {\n                    await updateCurrentRoomWalls(for: roomAnchor)\n                }\n            }\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ Updates the wall in front of the person when a wall isn't in a selected state.\nfunc updateFacingWall() {\n    guard renderWallRoot.isEnabled && !isWallSelectionLocked else {\n        return\n    }\n    \/\/ Update within 10 m.\n    let distance: Float = 10\n    \n    let deviceAnchor = worldTracking.queryDeviceAnchor(atTimestamp: CACurrentMediaTime())\n    guard let deviceAnchor, deviceAnchor.isTracked == true else {\n        return\n    }\n    let deviceInOriginCoordinates = deviceAnchor.originFromAnchorTransform\n    \n    let lookAtPointInDeviceCoordinate = SIMD4<Float>(0, 0, -distance, 1)\n    let lookAtPointInOriginCoordinates = deviceInOriginCoordinates * lookAtPointInDeviceCoordinate\n    \n    guard let scene = colliderWallsRoot.scene else {\n        logger.error(\"Failed to find the scene of `colliderWallsRoot`.\")\n        return\n    }\n    \n    let hitWall = scene.raycast(from: deviceInOriginCoordinates.columns.3.xyz, to: lookAtPointInOriginCoordinates.xyz, query: .nearest)\n    \n    guard !hitWall.isEmpty else {\n        return\n    }\n    \/\/ Render the first hit wall.\n    renderWallRoot.children.removeAll()\n\n    let hitEntity = hitWall[0].entity\n    currentRenderedWall = hitEntity\n    renderWallRoot.addChild(hitEntity)\n}\n\n\/\/\/ Updates walls under the collider walls root.\n\/\/\/\n\/\/\/ If someone has chosen and locked a wall, this method updates and renders that wall.\n\/\/\/ If someone hasn't locked a wall, the method updates and renders the wall in front of\n\/\/\/ them  in `WorldAndRoomView` at a rate of 10 Hz.\nprivate func updateCurrentRoomWalls(for roomAnchor: RoomAnchor) async {\n    let newColliderWalls = Entity()\n    let wallGeometries = roomAnchor.geometries(of: .wall)\n    for wallGeometry in wallGeometries {\n        guard let wallMeshResource = wallGeometry.asMeshResource() else {\n            continue\n        }\n        \n        let wallEntity = ModelEntity(mesh: wallMeshResource, materials: [wallMaterial])\n        wallEntity.transform = Transform(matrix: roomAnchor.originFromAnchorTransform)\n        \n        guard let shape = try? await ShapeResource.generateStaticMesh(from: wallMeshResource) else {\n            logger.error(\"Failed to create ShapeResource from wall geometries.\")\n            continue\n        }\n        \n        wallEntity.collision = CollisionComponent(shapes: [shape], isStatic: true)\n        newColliderWalls.addChild(wallEntity)\n    }\n    \/\/ Clear old walls.\n    colliderWallsRoot.children.removeAll()\n    colliderWallsRoot.addChild(newColliderWalls)\n    \n    if isWallSelectionLocked {\n        let wallCandidateEntities = Array(newColliderWalls.children)\n        updateLockedWall(wallCandidateEntities: wallCandidateEntities)\n    }\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "ba724f88ad7f7e1ba23e5d0419a33b76cc58c04bec569e0a313c0f6881c9afe8",
  "crawledAt" : "2025-12-02T17:33:14Z",
  "id" : "2FF7866E-7777-4D3E-8373-67144E41605C",
  "kind" : "unknown",
  "language" : "swift",
  "overview" : "## Overview\n\nThis sample allows your app to keep track of rooms as discrete, identifiable places, and enables you to provide a customized virtual experience inside a specific room, and to get notified when someone enters or leaves the room. These customizations can be as simple as knowing when to stop room-specific animations, or to support the creation of location-specific virtual content such as in-game treasures, effects, or even portals to virtual worlds that contain other content.\n\nThis sample demonstrates how to use room tracking by enabling a person to place spheres in a space and continuously query the framework as to whether those spheres are in the same room as the person. As someone moves into, through, and out of the room, ARKit delivers [doc:\/\/com.apple.documentation\/documentation\/ARKit\/RoomAnchor] updates that represent the latest knowledge of the current room. This structure provides a [doc:\/\/com.apple.documentation\/documentation\/ARKit\/RoomAnchor\/contains(_:)] query method that you use to determine if the spheres are in the current room, and highlight them accordingly.\n\nThe app has an *occlusion mode*, in which the room geometry the framework renders is a transparent occluder that hides virtual objects outside the room. It also has a *wall selection mode*, in which someone may select a specific wall for the purpose of replacing it with a video or virtual portal.\n\n## Ensure all data providers are in an authorized state\n\nYour app must request permission to use certain visionOS capabilities before being able to access data associated with them. For example, attempting to access the [doc:\/\/com.apple.documentation\/documentation\/ARKit\/RoomTrackingProvider] displays a permission sheet asking the user to authorize your app’s access. If the user has previously denied this request, the app displays an error message in the scene. For information about using a `RoomTrackingProvider`, see [doc:\/\/com.apple.documentation\/documentation\/visionOS\/setting-up-access-to-arkit-data]. For information about best practices for privacy, see [doc:\/\/com.apple.documentation\/documentation\/visionOS\/adopting-best-practices-for-privacy].\n\n## Configure room tracking\n\nSet up room tracking by first configuring an [doc:\/\/com.apple.documentation\/documentation\/ARKit\/ARKitSession] instance, then add a  [doc:\/\/com.apple.documentation\/documentation\/ARKit\/WorldTrackingProvider] and a  [doc:\/\/com.apple.documentation\/documentation\/ARKit\/RoomTrackingProvider] to the session as shown in the following example:\n\nIn addition to instantiating the world and room tracking providers in the `AppState`, you need to create storage for the in-room anchors the app tracks:\n\nYou also need to create the materials the framework uses to render the in-room anchors:\n\n## Allow a person to place room tracking anchors\n\nPlacing a `roomAnchor` object in the room consists of two processes. The first phase allows the person to review the anchor, which the sample renders as a sphere in front of the device from the person’s perspective:\n\nThe second phase allows a person to place the sphere (a [doc:\/\/com.apple.documentation\/documentation\/ARKit\/WorldAnchor]) in their surroundings  with a tap gesture. Gestures such as this are SwiftUI view modifiers you apply to the room’s `View`, as shown below:\n\nAs a person places spheres in the room, they appear in green to indicate they’re anchors in the current room. If a person leaves the room, all of the room anchors in the previous room dim and become red to indicate a person has left the room. If there are anchors in the room a person enters into, they change color to indicate the person is currently in the room.\n\nThis changing state and the property of a room being *current* is what allows an app to make decisions about what actions, animations, or other processes make sense in a specific location.\n\n## Check the current room and respond to updates\n\nAs a person moves from room to room, ARKit’s room tracking process checks to see which room is current and reports back changes to the app through the `RoomTrackingProvider` property  [doc:\/\/com.apple.documentation\/documentation\/ARKit\/RoomTrackingProvider\/anchorUpdates], which is an asynchronous sequence of all anchor updates. As these updates come in, a `Task` view modifier in the app’s `WorldAndRoomView` calls a method that looks for anchors to update, as demonstrated here:\n\n## Find and select walls\n\nRoom tracking also enables someone to find and select walls in the current room. You can use this as an additional interaction surface, such as creating a “portal” to another virtual space. The process of selecting a wall in a room is split into two modes: an *unlocked mode* where actively looking at a specific wall causes ARKit to highlight it in blue, and *locked mode* where a person has selected a wall and it receives continuous updates from the `RoomTrackingProvider`. The *unlocked mode* requires performing a ray cast query in the direction of the a person’s head, which returns the first wall that it hits, as shown here:\n\n## Keep focus on the current room\n\nRoom tracking operates only in the current room a person is in. If someone leaves one room and enters another, the previous room is no longer valid, and the framework only updates mesh-room associations and plane-room associations for the current room. Only use the current room anchor and discard any noncurrent rooms.\n\n## Be aware of limitations\n\nClutter in a room, large furniture elements, and very large spaces may interfere with ARKit’s ability to accurately detect walls and fully detect the dimensions of a room. In the case of very large indoor spaces, or in rooms with low-light conditions, the framework may only provide a floor mesh. Additionally, visionOS doesn’t support using room tracking outdoors or when Apple Vision Pro is in Travel Mode. In these cases, there’s no current room. For more information on implementing immersive experiences, see Human Interface Guidelines > [https:\/\/developer.apple.com\/design\/human-interface-guidelines\/immersive-experiences].",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/visionos\/building-local-experiences-with-room-tracking\ncrawled: 2025-12-02T17:33:14Z\n---\n\n# Building local experiences with room tracking\n\n**Sample Code**\n\nUse room tracking in visionOS to provide custom interactions with physical spaces.\n\n## Overview\n\nThis sample allows your app to keep track of rooms as discrete, identifiable places, and enables you to provide a customized virtual experience inside a specific room, and to get notified when someone enters or leaves the room. These customizations can be as simple as knowing when to stop room-specific animations, or to support the creation of location-specific virtual content such as in-game treasures, effects, or even portals to virtual worlds that contain other content.\n\nThis sample demonstrates how to use room tracking by enabling a person to place spheres in a space and continuously query the framework as to whether those spheres are in the same room as the person. As someone moves into, through, and out of the room, ARKit delivers [doc:\/\/com.apple.documentation\/documentation\/ARKit\/RoomAnchor] updates that represent the latest knowledge of the current room. This structure provides a [doc:\/\/com.apple.documentation\/documentation\/ARKit\/RoomAnchor\/contains(_:)] query method that you use to determine if the spheres are in the current room, and highlight them accordingly.\n\nThe app has an *occlusion mode*, in which the room geometry the framework renders is a transparent occluder that hides virtual objects outside the room. It also has a *wall selection mode*, in which someone may select a specific wall for the purpose of replacing it with a video or virtual portal.\n\n\n\n## Ensure all data providers are in an authorized state\n\nYour app must request permission to use certain visionOS capabilities before being able to access data associated with them. For example, attempting to access the [doc:\/\/com.apple.documentation\/documentation\/ARKit\/RoomTrackingProvider] displays a permission sheet asking the user to authorize your app’s access. If the user has previously denied this request, the app displays an error message in the scene. For information about using a `RoomTrackingProvider`, see [doc:\/\/com.apple.documentation\/documentation\/visionOS\/setting-up-access-to-arkit-data]. For information about best practices for privacy, see [doc:\/\/com.apple.documentation\/documentation\/visionOS\/adopting-best-practices-for-privacy].\n\n\n\n```swift\nfunc areAllDataProvidersAuthorized() async -> Bool {\n    \/\/ It's sufficient to check that the authorization status isn't `denied`.\n    \/\/ If it's `notdetermined`, ARKit presents a permission pop-up menu that appears as soon\n    \/\/ as the session runs.\n    let authorization = await ARKitSession().queryAuthorization(for: [.worldSensing])\n    return authorization[.worldSensing] != .denied\n}\n```\n\n## Configure room tracking\n\nSet up room tracking by first configuring an [doc:\/\/com.apple.documentation\/documentation\/ARKit\/ARKitSession] instance, then add a  [doc:\/\/com.apple.documentation\/documentation\/ARKit\/WorldTrackingProvider] and a  [doc:\/\/com.apple.documentation\/documentation\/ARKit\/RoomTrackingProvider] to the session as shown in the following example:\n\n```swift\nprivate let session = ARKitSession()\nprivate let worldTracking = WorldTrackingProvider()\nprivate let roomTracking = RoomTrackingProvider()\n```\n\nIn addition to instantiating the world and room tracking providers in the `AppState`, you need to create storage for the in-room anchors the app tracks:\n\n```swift\n\/\/\/ A dictionary that contains `RoomAnchor` structures.\nprivate var roomAnchors = [UUID: RoomAnchor]()\n\/\/\/ A dictionary that contains `WorldAnchor` structures.\nprivate var worldAnchors = [UUID: WorldAnchor]()\n\/\/\/ A dictionary that contains `ModelEntity` structures for spheres.\nprivate var sphereEntities = [UUID: ModelEntity]()\n\/\/\/ A dictionary that contains `ModelEntity` structures for room anchors.\nprivate var roomEntities = [UUID: ModelEntity]()\n```\n\nYou also need to create the materials the framework uses to render the in-room anchors:\n\n```swift\n\/\/ Material for spheres in the current room.\nprivate let inRoomSphereMaterial = SimpleMaterial(color: .green, roughness: 0.2, isMetallic: true)\n\/\/ Material for spheres not in the current room.\nprivate let outOfRoomSphereMaterial = SimpleMaterial(color: .red, roughness: 0.2, isMetallic: true)\n\/\/ Material the app applies to room entities to show occlusion effects.\nprivate let occlusionMaterial = OcclusionMaterial()\n\/\/ Material for current room walls.\nprivate var wallMaterial = UnlitMaterial(color: .blue)\n```\n\n## Allow a person to place room tracking anchors\n\nPlacing a `roomAnchor` object in the room consists of two processes. The first phase allows the person to review the anchor, which the sample renders as a sphere in front of the device from the person’s perspective:\n\n```swift\nprivate func createPreviewSphere() -> ModelEntity {\n    let sphereMesh = MeshResource.generateSphere(radius: 0.1)\n    let sphereMaterial = SimpleMaterial(color: .gray.withAlphaComponent(0.5), roughness: 0.2, isMetallic: false)\n    let sphere = ModelEntity(mesh: sphereMesh, materials: [sphereMaterial])\n    \n    \/\/ Enables gestures on the preview sphere.\n    \/\/ Looking at the preview and using a pinch gesture causes a world anchored sphere to appear.\n    sphere.generateCollisionShapes(recursive: false, static: true)\n    \/\/ Ensures the preview only accepts indirect input (for tap gestures).\n    sphere.components.set(InputTargetComponent(allowedInputTypes: [.indirect]))\n    \n    \/\/ The preview sphere only becomes visible once someone clicks the Add a Sphere button.\n    sphere.isEnabled = false\n    \n    return sphere\n}\n```\n\nThe second phase allows a person to place the sphere (a [doc:\/\/com.apple.documentation\/documentation\/ARKit\/WorldAnchor]) in their surroundings  with a tap gesture. Gestures such as this are SwiftUI view modifiers you apply to the room’s `View`, as shown below:\n\n```swift\n.gesture(SpatialTapGesture().targetedToAnyEntity().onEnded { event in\n    if event.entity == previewSphere {\n        Task {\n            \/\/ To place a sphere you need to:\n            \/\/ 1. Create a world anchor with the translation of that offset transform and add the anchor to the world tracking provider.\n            \/\/ 2. Create the sphere's geometry in `processWorldTrackingUpdates()` after you have successfully added the world anchor.\n            await appState.addWorldAnchor(at: event.entity.transformMatrix(relativeTo: nil))\n            appState.showPreviewSphere = false\n        }\n    }\n})\n```\n\nAs a person places spheres in the room, they appear in green to indicate they’re anchors in the current room. If a person leaves the room, all of the room anchors in the previous room dim and become red to indicate a person has left the room. If there are anchors in the room a person enters into, they change color to indicate the person is currently in the room.\n\nThis changing state and the property of a room being *current* is what allows an app to make decisions about what actions, animations, or other processes make sense in a specific location.\n\n## Check the current room and respond to updates\n\nAs a person moves from room to room, ARKit’s room tracking process checks to see which room is current and reports back changes to the app through the `RoomTrackingProvider` property  [doc:\/\/com.apple.documentation\/documentation\/ARKit\/RoomTrackingProvider\/anchorUpdates], which is an asynchronous sequence of all anchor updates. As these updates come in, a `Task` view modifier in the app’s `WorldAndRoomView` calls a method that looks for anchors to update, as demonstrated here:\n\n```swift\nfunc processRoomTrackingUpdates() async {\n    for await update in roomTracking.anchorUpdates {\n        let roomAnchor = update.anchor\n        switch update.event {\n        case .removed:\n            if roomAnchor.isCurrentRoom {\n                colliderWallsRoot.children.removeAll()\n                if let currentRenderedWall {\n                    renderWallRoot.removeChild(currentRenderedWall)\n                }\n            }\n            roomAnchors.removeValue(forKey: roomAnchor.id)\n            roomEntities[roomAnchor.id]?.removeFromParent()\n            roomEntities.removeValue(forKey: roomAnchor.id)\n            updateSphereState()\n        case .added, .updated:\n            roomAnchors[roomAnchor.id] = roomAnchor\n            guard let roomMeshResource = roomAnchor.geometry.asMeshResource() else { continue }\n            if update.event == .added {\n                let roomEntity = ModelEntity(mesh: roomMeshResource, materials: [occlusionMaterial])\n                roomEntity.transform = Transform(matrix: roomAnchor.originFromAnchorTransform)\n                roomEntities[roomAnchor.id] = roomEntity\n                roomEntity.isEnabled = roomAnchor.isCurrentRoom\n                roomRoot.addChild(roomEntity)\n                \n            } else if update.event == .updated {\n                guard let roomEntity = roomEntities[roomAnchor.id] else { continue }\n                roomEntity.model?.mesh = roomMeshResource\n                roomEntity.transform = Transform(matrix: roomAnchor.originFromAnchorTransform)\n                roomEntity.isEnabled = roomAnchor.isCurrentRoom\n            }\n            \n            updateSphereState()\n            \n            if roomAnchor.isCurrentRoom {\n                currentRoomID = roomAnchor.id\n                if renderWallRoot.isEnabled {\n                    await updateCurrentRoomWalls(for: roomAnchor)\n                }\n            }\n        }\n    }\n}\n```\n\n## Find and select walls\n\nRoom tracking also enables someone to find and select walls in the current room. You can use this as an additional interaction surface, such as creating a “portal” to another virtual space. The process of selecting a wall in a room is split into two modes: an *unlocked mode* where actively looking at a specific wall causes ARKit to highlight it in blue, and *locked mode* where a person has selected a wall and it receives continuous updates from the `RoomTrackingProvider`. The *unlocked mode* requires performing a ray cast query in the direction of the a person’s head, which returns the first wall that it hits, as shown here:\n\n```swift\n\/\/\/ Updates the wall in front of the person when a wall isn't in a selected state.\nfunc updateFacingWall() {\n    guard renderWallRoot.isEnabled && !isWallSelectionLocked else {\n        return\n    }\n    \/\/ Update within 10 m.\n    let distance: Float = 10\n    \n    let deviceAnchor = worldTracking.queryDeviceAnchor(atTimestamp: CACurrentMediaTime())\n    guard let deviceAnchor, deviceAnchor.isTracked == true else {\n        return\n    }\n    let deviceInOriginCoordinates = deviceAnchor.originFromAnchorTransform\n    \n    let lookAtPointInDeviceCoordinate = SIMD4<Float>(0, 0, -distance, 1)\n    let lookAtPointInOriginCoordinates = deviceInOriginCoordinates * lookAtPointInDeviceCoordinate\n    \n    guard let scene = colliderWallsRoot.scene else {\n        logger.error(\"Failed to find the scene of `colliderWallsRoot`.\")\n        return\n    }\n    \n    let hitWall = scene.raycast(from: deviceInOriginCoordinates.columns.3.xyz, to: lookAtPointInOriginCoordinates.xyz, query: .nearest)\n    \n    guard !hitWall.isEmpty else {\n        return\n    }\n    \/\/ Render the first hit wall.\n    renderWallRoot.children.removeAll()\n\n    let hitEntity = hitWall[0].entity\n    currentRenderedWall = hitEntity\n    renderWallRoot.addChild(hitEntity)\n}\n\n\/\/\/ Updates walls under the collider walls root.\n\/\/\/\n\/\/\/ If someone has chosen and locked a wall, this method updates and renders that wall.\n\/\/\/ If someone hasn't locked a wall, the method updates and renders the wall in front of\n\/\/\/ them  in `WorldAndRoomView` at a rate of 10 Hz.\nprivate func updateCurrentRoomWalls(for roomAnchor: RoomAnchor) async {\n    let newColliderWalls = Entity()\n    let wallGeometries = roomAnchor.geometries(of: .wall)\n    for wallGeometry in wallGeometries {\n        guard let wallMeshResource = wallGeometry.asMeshResource() else {\n            continue\n        }\n        \n        let wallEntity = ModelEntity(mesh: wallMeshResource, materials: [wallMaterial])\n        wallEntity.transform = Transform(matrix: roomAnchor.originFromAnchorTransform)\n        \n        guard let shape = try? await ShapeResource.generateStaticMesh(from: wallMeshResource) else {\n            logger.error(\"Failed to create ShapeResource from wall geometries.\")\n            continue\n        }\n        \n        wallEntity.collision = CollisionComponent(shapes: [shape], isStatic: true)\n        newColliderWalls.addChild(wallEntity)\n    }\n    \/\/ Clear old walls.\n    colliderWallsRoot.children.removeAll()\n    colliderWallsRoot.addChild(newColliderWalls)\n    \n    if isWallSelectionLocked {\n        let wallCandidateEntities = Array(newColliderWalls.children)\n        updateLockedWall(wallCandidateEntities: wallCandidateEntities)\n    }\n}\n```\n\n## Keep focus on the current room\n\nRoom tracking operates only in the current room a person is in. If someone leaves one room and enters another, the previous room is no longer valid, and the framework only updates mesh-room associations and plane-room associations for the current room. Only use the current room anchor and discard any noncurrent rooms.\n\n## Be aware of limitations\n\nClutter in a room, large furniture elements, and very large spaces may interfere with ARKit’s ability to accurately detect walls and fully detect the dimensions of a room. In the case of very large indoor spaces, or in rooms with low-light conditions, the framework may only provide a floor mesh. Additionally, visionOS doesn’t support using room tracking outdoors or when Apple Vision Pro is in Travel Mode. In these cases, there’s no current room. For more information on implementing immersive experiences, see Human Interface Guidelines > [https:\/\/developer.apple.com\/design\/human-interface-guidelines\/immersive-experiences].\n\n\n\n## ARKit\n\n- **Happy Beam**: Leverage a Full Space to create a fun game using ARKit.\n- **Setting up access to ARKit data**: Check whether your app can use ARKit and respect people’s privacy.\n- **Incorporating real-world surroundings in an immersive experience**: Create an immersive experience by making your app’s content respond to the local shape of the world.\n- **Placing content on detected planes**: Detect horizontal surfaces like tables and floors, as well as vertical planes like walls and doors.\n- **Tracking specific points in world space**: Retrieve the position and orientation of anchors your app stores in ARKit.\n- **Tracking preregistered images in 3D space**: Place content based on the current position of a known image in a person’s surroundings.\n- **Exploring object tracking with ARKit**: Find and track real-world objects in visionOS using reference objects trained with Create ML.\n- **Object tracking with Reality Composer Pro experiences**: Use object tracking in visionOS to attach digital content to real objects to create engaging experiences.\n- **Placing entities using head and device transform**: Query and react to changes in the position and rotation of Apple Vision Pro.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Leverage a Full Space to create a fun game using ARKit.",
          "name" : "Happy Beam",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/happybeam"
        },
        {
          "description" : "Check whether your app can use ARKit and respect people’s privacy.",
          "name" : "Setting up access to ARKit data",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/setting-up-access-to-arkit-data"
        },
        {
          "description" : "Create an immersive experience by making your app’s content respond to the local shape of the world.",
          "name" : "Incorporating real-world surroundings in an immersive experience",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/incorporating-real-world-surroundings-in-an-immersive-experience"
        },
        {
          "description" : "Detect horizontal surfaces like tables and floors, as well as vertical planes like walls and doors.",
          "name" : "Placing content on detected planes",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/placing-content-on-detected-planes"
        },
        {
          "description" : "Retrieve the position and orientation of anchors your app stores in ARKit.",
          "name" : "Tracking specific points in world space",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/tracking-points-in-world-space"
        },
        {
          "description" : "Place content based on the current position of a known image in a person’s surroundings.",
          "name" : "Tracking preregistered images in 3D space",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/tracking-images-in-3d-space"
        },
        {
          "description" : "Find and track real-world objects in visionOS using reference objects trained with Create ML.",
          "name" : "Exploring object tracking with ARKit",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/exploring_object_tracking_with_arkit"
        },
        {
          "description" : "Use object tracking in visionOS to attach digital content to real objects to create engaging experiences.",
          "name" : "Object tracking with Reality Composer Pro experiences",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/object-tracking-with-reality-composer-pro-experiences"
        },
        {
          "description" : "Query and react to changes in the position and rotation of Apple Vision Pro.",
          "name" : "Placing entities using head and device transform",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/placing-entities-using-head-and-device-transform"
        }
      ],
      "title" : "ARKit"
    }
  ],
  "source" : "appleJSON",
  "title" : "Building local experiences with room tracking",
  "url" : "https:\/\/developer.apple.com\/documentation\/visionos\/building-local-experiences-with-room-tracking"
}