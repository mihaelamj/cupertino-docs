{
  "abstract" : "Compare how visionOS handles events and manages its rendering loop differently from other Apple platforms.",
  "codeExamples" : [

  ],
  "contentHash" : "c9da6a172f828899690241f7c67892e8fdfebf3cc37392c0d536631e752f8d18",
  "crawledAt" : "2025-12-02T17:52:13Z",
  "id" : "78827CEB-A753-4381-9BB2-E75AE4BE3FA6",
  "kind" : "article",
  "language" : "swift",
  "overview" : "## Overview\n\nLike other Apple platforms, visionOS renders changes to the UI in response to updates your app makes, input events, callbacks from actions you initiate, timers, and notifications. Unique on Apple Vision Pro, the system renders updates to the images the device displays in order to reposition the UI relative to changes in head position. When the system detects eye and hand movement, deliberate or inadvertent, it requires additional processing to determine what a person is looking at, or interacting with, to calculate a response. The system does a lot more to render an up-to-date UI and account for input from spatial algorithms.\n\nThe following diagram illustrates how input propagates through the system and updates content in visionOS:\n\n\n\nIn the Shared Space, each app updates its own UI and 3D content and syncs the updates to its view hierarchy and content to the shared render server. This server renders the updates from multiple apps running side-by-side. Then, the render server works with the compositor to generate final images to display of the UI and people’s surroundings. Metal apps in a Full Space use the [doc:\/\/com.apple.documentation\/documentation\/CompositorServices] framework to render drawable frames directly to the Compositor, bypassing the render server.\n\n\n\nIf your app running in the Shared Space has content or updates that take too long to render, the render server can miss deadlines. Visual updates you expect in one compositor frame don’t show up until a later frame.\n\n\n\nUse the RealityKit Trace template in Instruments to profile your app and identify workflows with dropped frames and other rendering and responsiveness bottlenecks. For more information, see [doc:\/\/com.apple.visionOS\/documentation\/visionOS\/analyzing-the-performance-of-your-visionOS-app]. To learn about optimizations you can make in your SwiftUI and UIKit interfaces, see [doc:\/\/com.apple.visionOS\/documentation\/visionOS\/reducing-the-rendering-cost-of-your-UI-on-visionOS]. To learn about optimizing your RealityKit content, see [doc:\/\/com.apple.visionOS\/documentation\/visionOS\/reducing-the-rendering-cost-of-RealityKit-content-on-visionOS].\n\nWhen using [doc:\/\/com.apple.documentation\/documentation\/Metal] and the [doc:\/\/com.apple.documentation\/documentation\/CompositorServices] framework to bypass the render server, use the Metal System Trace template to profile your app’s performance. For more info, see [doc:\/\/com.apple.documentation\/documentation\/Xcode\/Analyzing-the-performance-of-your-Metal-app].\n\nTo learn more about implementing fully immersive Metal apps, see [doc:\/\/com.apple.documentation\/documentation\/CompositorServices\/drawing-fully-immersive-content-using-metal] and watch the video [doc:\/\/com.apple.documentation\/videos\/play\/wwdc2023\/10089].",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/visionos\/understanding-the-visionos-render-pipeline\ncrawled: 2025-12-02T17:52:13Z\n---\n\n# Understanding the visionOS render pipeline\n\n**Article**\n\nCompare how visionOS handles events and manages its rendering loop differently from other Apple platforms.\n\n## Overview\n\nLike other Apple platforms, visionOS renders changes to the UI in response to updates your app makes, input events, callbacks from actions you initiate, timers, and notifications. Unique on Apple Vision Pro, the system renders updates to the images the device displays in order to reposition the UI relative to changes in head position. When the system detects eye and hand movement, deliberate or inadvertent, it requires additional processing to determine what a person is looking at, or interacting with, to calculate a response. The system does a lot more to render an up-to-date UI and account for input from spatial algorithms.\n\nThe following diagram illustrates how input propagates through the system and updates content in visionOS:\n\n\n\n\n\nIn the Shared Space, each app updates its own UI and 3D content and syncs the updates to its view hierarchy and content to the shared render server. This server renders the updates from multiple apps running side-by-side. Then, the render server works with the compositor to generate final images to display of the UI and people’s surroundings. Metal apps in a Full Space use the [doc:\/\/com.apple.documentation\/documentation\/CompositorServices] framework to render drawable frames directly to the Compositor, bypassing the render server.\n\n\n\nIf your app running in the Shared Space has content or updates that take too long to render, the render server can miss deadlines. Visual updates you expect in one compositor frame don’t show up until a later frame.\n\n\n\nUse the RealityKit Trace template in Instruments to profile your app and identify workflows with dropped frames and other rendering and responsiveness bottlenecks. For more information, see [doc:\/\/com.apple.visionOS\/documentation\/visionOS\/analyzing-the-performance-of-your-visionOS-app]. To learn about optimizations you can make in your SwiftUI and UIKit interfaces, see [doc:\/\/com.apple.visionOS\/documentation\/visionOS\/reducing-the-rendering-cost-of-your-UI-on-visionOS]. To learn about optimizing your RealityKit content, see [doc:\/\/com.apple.visionOS\/documentation\/visionOS\/reducing-the-rendering-cost-of-RealityKit-content-on-visionOS].\n\nWhen using [doc:\/\/com.apple.documentation\/documentation\/Metal] and the [doc:\/\/com.apple.documentation\/documentation\/CompositorServices] framework to bypass the render server, use the Metal System Trace template to profile your app’s performance. For more info, see [doc:\/\/com.apple.documentation\/documentation\/Xcode\/Analyzing-the-performance-of-your-Metal-app].\n\n- Pace your metal frame submissions so that the compositor receives a new frame from your app on each of its updates.\n- Maintain a consistent metal rendering frame rate that is equal to the Apple Vision Pro display refresh rate. Use the visualizations in the Display instrument timeline to compare your rendered frame times to the display refresh rate — usually 90Hz on the Apple Vision Pro, but it can be higher under certain environmental conditions.\n- Query a new foveation map and pose prediction for each frame immediately before you use it to encode GPU work.\n- Avoid long-running fragment and vertex shader executions from your Metal app, or from custom materials with Reality Composer Pro.\n- Avoid any long frame stalls. If your app takes too long to submit a new frame to the compositor, the system terminates it.\n\nTo learn more about implementing fully immersive Metal apps, see [doc:\/\/com.apple.documentation\/documentation\/CompositorServices\/drawing-fully-immersive-content-using-metal] and watch the video [doc:\/\/com.apple.documentation\/videos\/play\/wwdc2023\/10089].\n\n## Performance\n\n- **Creating a performance plan for your visionOS app**: Identify your app’s performance and power goals and create a plan to measure and assess them.\n- **Analyzing the performance of your visionOS app**: Use the RealityKit Trace template in Instruments to evaluate and improve the performance of your visionOS app.\n- **Reducing the rendering cost of your UI on visionOS**: Optimize your 2D user interface rendering on visionOS.\n- **Reducing the rendering cost of RealityKit content on visionOS**: Optimize your app’s 3D augmented reality content to render efficiently on visionOS.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Identify your app’s performance and power goals and create a plan to measure and assess them.",
          "name" : "Creating a performance plan for your visionOS app",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/creating-a-performance-plan-for-visionos-app"
        },
        {
          "description" : "Use the RealityKit Trace template in Instruments to evaluate and improve the performance of your visionOS app.",
          "name" : "Analyzing the performance of your visionOS app",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/analyzing-the-performance-of-your-visionOS-app"
        },
        {
          "description" : "Optimize your 2D user interface rendering on visionOS.",
          "name" : "Reducing the rendering cost of your UI on visionOS",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/reducing-the-rendering-cost-of-your-UI-on-visionOS"
        },
        {
          "description" : "Optimize your app’s 3D augmented reality content to render efficiently on visionOS.",
          "name" : "Reducing the rendering cost of RealityKit content on visionOS",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/reducing-the-rendering-cost-of-RealityKit-content-on-visionOS"
        }
      ],
      "title" : "Performance"
    }
  ],
  "source" : "appleJSON",
  "title" : "Understanding the visionOS render pipeline",
  "url" : "https:\/\/developer.apple.com\/documentation\/visionos\/understanding-the-visionos-render-pipeline"
}