{
  "abstract" : "Create an immersive experience by making your app’s content respond to the local shape of the world.",
  "codeExamples" : [
    {
      "code" : "RealityView { content in\n    content.add(model.setupContentEntity())\n}\n.task {\n    do {\n        if model.dataProvidersAreSupported && model.isReadyToRun {\n            try await model.session.run([model.sceneReconstruction, model.handTracking])\n        } else {\n            await dismissImmersiveSpace()\n        }\n    } catch {\n        logger.error(\"Failed to start session: \\(error)\")\n        await dismissImmersiveSpace()\n        openWindow(id: \"error\")\n    }\n}\n.task {\n    await model.processHandUpdates()\n}\n.task {\n    await model.monitorSessionEvents()\n}\n.task(priority: .low) {\n    await model.processReconstructionUpdates()\n}\n.gesture(SpatialTapGesture().targetedToAnyEntity().onEnded { value in\n    let location3D = value.convert(value.location3D, from: .global, to: .scene)\n    model.addCube(tapLocation: location3D)\n})",
      "language" : "swift"
    },
    {
      "code" : "func processReconstructionUpdates() async {\n    for await update in sceneReconstruction.anchorUpdates {\n        let meshAnchor = update.anchor\n\n        guard let shape = try? await ShapeResource.generateStaticMesh(from: meshAnchor) else { continue }\n        switch update.event {\n        case .added:\n            let entity = ModelEntity()\n            entity.transform = Transform(matrix: meshAnchor.originFromAnchorTransform)\n            entity.collision = CollisionComponent(shapes: [shape], isStatic: true)\n            entity.components.set(InputTargetComponent())\n            \n            entity.physicsBody = PhysicsBodyComponent(mode: .static)\n            \n            meshEntities[meshAnchor.id] = entity\n            contentEntity.addChild(entity)\n        case .updated:\n            guard let entity = meshEntities[meshAnchor.id] else { continue }\n            entity.transform = Transform(matrix: meshAnchor.originFromAnchorTransform)\n            entity.collision?.shapes = [shape]\n        case .removed:\n            meshEntities[meshAnchor.id]?.removeFromParent()\n            meshEntities.removeValue(forKey: meshAnchor.id)\n        }\n    }\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "3904356e06cb80ae307adf533908a432cc28f4fa515cb45ad997b3071800db72",
  "crawledAt" : "2025-12-02T16:24:06Z",
  "id" : "AA8900EC-7DAC-421B-8784-5A85B04DE342",
  "kind" : "unknown",
  "language" : "swift",
  "overview" : "## Overview\n\nScene reconstruction helps bridge the gap between the rendered 3D content in your app and the person’s surroundings. Use scene reconstruction in ARKit to give your app an idea of the shape of the person’s surroundings and to bring your app experience into their world. Immersive experiences  — those that use the [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/ImmersionStyle\/mixed] space style — are best positioned to incorporate this kind of contextual information: scene reconstruction is only available in spaces and isn’t as relevant for the [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/ImmersionStyle\/full] space style.\n\nIn addition to providing a 3D mesh of the shape of different nearby objects, ARKit gives a classification to each mesh face it detects. For example, it might classify a face of a mesh as being part of an appliance, a piece of furniture, or structural information about the room like the position of walls and floors. The following video shows virtual cubes colliding with the scene reconstruction mesh, which makes the cubes appear to land on a table:\n\n### Configure a scene reconstruction session\n\nScene reconstruction requires the [doc:\/\/com.apple.documentation\/documentation\/ARKit\/ARKitSession\/AuthorizationType\/worldSensing] authorization type and corresponding usage description that you supply in your app’s `Info.plist` file. The following starts a session and processes updates as ARKit refines its reconstruction of the person’s surroundings:\n\n### Add real-world interactivity using collision components\n\nYou can make rendered 3D content more lifelike by having it appear to interact physically with objects in the person’s surroundings, like furniture and floors. Use RealityKit’s collision components and physics support to provide these interactions in your app. The [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/ShapeResource\/generateStaticMesh(from:)] method bridges between scene reconstruction and RealityKit’s physics simulation.\n\nUse low-priority tasks to generate meshes, because generating them is a computationally expensive operation. The following creates a mesh entity with collision shapes using scene reconstruction:\n\nEach object in the scene reconstruction mesh updates its [doc:\/\/com.apple.documentation\/documentation\/ARKit\/MeshAnchor\/originFromAnchorTransform] information independently and requires a separate static mesh because ARKit subdivides its representation of the world into multiple, distinct sections.\n\n### Display scene reconstruction meshes during debugging\n\nPeople using an app that leverages scene reconstruction typically don’t need to see a visual rendering of the scene reconstruction mesh. The system already shows passthrough video in an immersive experience. However, temporarily displaying the scene reconstruction mesh can help while you’re developing and debugging your app. In Xcode’s debugging toolbar, click the Enable Visualizations button and select Collision Shapes. Because each element of the scene reconstruction mesh has a collision component, the details of the mesh appear in the debug visualization. For more information, see [doc:\/\/com.apple.documentation\/documentation\/Xcode\/diagnosing-issues-in-the-appearance-of-your-running-app].",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/visionos\/incorporating-real-world-surroundings-in-an-immersive-experience\ncrawled: 2025-12-02T16:24:06Z\n---\n\n# Incorporating real-world surroundings in an immersive experience\n\n**Sample Code**\n\nCreate an immersive experience by making your app’s content respond to the local shape of the world.\n\n## Overview\n\nScene reconstruction helps bridge the gap between the rendered 3D content in your app and the person’s surroundings. Use scene reconstruction in ARKit to give your app an idea of the shape of the person’s surroundings and to bring your app experience into their world. Immersive experiences  — those that use the [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/ImmersionStyle\/mixed] space style — are best positioned to incorporate this kind of contextual information: scene reconstruction is only available in spaces and isn’t as relevant for the [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/ImmersionStyle\/full] space style.\n\nIn addition to providing a 3D mesh of the shape of different nearby objects, ARKit gives a classification to each mesh face it detects. For example, it might classify a face of a mesh as being part of an appliance, a piece of furniture, or structural information about the room like the position of walls and floors. The following video shows virtual cubes colliding with the scene reconstruction mesh, which makes the cubes appear to land on a table:\n\n\n\n### Configure a scene reconstruction session\n\nScene reconstruction requires the [doc:\/\/com.apple.documentation\/documentation\/ARKit\/ARKitSession\/AuthorizationType\/worldSensing] authorization type and corresponding usage description that you supply in your app’s `Info.plist` file. The following starts a session and processes updates as ARKit refines its reconstruction of the person’s surroundings:\n\n```swift\nRealityView { content in\n    content.add(model.setupContentEntity())\n}\n.task {\n    do {\n        if model.dataProvidersAreSupported && model.isReadyToRun {\n            try await model.session.run([model.sceneReconstruction, model.handTracking])\n        } else {\n            await dismissImmersiveSpace()\n        }\n    } catch {\n        logger.error(\"Failed to start session: \\(error)\")\n        await dismissImmersiveSpace()\n        openWindow(id: \"error\")\n    }\n}\n.task {\n    await model.processHandUpdates()\n}\n.task {\n    await model.monitorSessionEvents()\n}\n.task(priority: .low) {\n    await model.processReconstructionUpdates()\n}\n.gesture(SpatialTapGesture().targetedToAnyEntity().onEnded { value in\n    let location3D = value.convert(value.location3D, from: .global, to: .scene)\n    model.addCube(tapLocation: location3D)\n})\n```\n\n### Add real-world interactivity using collision components\n\nYou can make rendered 3D content more lifelike by having it appear to interact physically with objects in the person’s surroundings, like furniture and floors. Use RealityKit’s collision components and physics support to provide these interactions in your app. The [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/ShapeResource\/generateStaticMesh(from:)] method bridges between scene reconstruction and RealityKit’s physics simulation.\n\n\n\nUse low-priority tasks to generate meshes, because generating them is a computationally expensive operation. The following creates a mesh entity with collision shapes using scene reconstruction:\n\n```swift\nfunc processReconstructionUpdates() async {\n    for await update in sceneReconstruction.anchorUpdates {\n        let meshAnchor = update.anchor\n\n        guard let shape = try? await ShapeResource.generateStaticMesh(from: meshAnchor) else { continue }\n        switch update.event {\n        case .added:\n            let entity = ModelEntity()\n            entity.transform = Transform(matrix: meshAnchor.originFromAnchorTransform)\n            entity.collision = CollisionComponent(shapes: [shape], isStatic: true)\n            entity.components.set(InputTargetComponent())\n            \n            entity.physicsBody = PhysicsBodyComponent(mode: .static)\n            \n            meshEntities[meshAnchor.id] = entity\n            contentEntity.addChild(entity)\n        case .updated:\n            guard let entity = meshEntities[meshAnchor.id] else { continue }\n            entity.transform = Transform(matrix: meshAnchor.originFromAnchorTransform)\n            entity.collision?.shapes = [shape]\n        case .removed:\n            meshEntities[meshAnchor.id]?.removeFromParent()\n            meshEntities.removeValue(forKey: meshAnchor.id)\n        }\n    }\n}\n```\n\n\n\nEach object in the scene reconstruction mesh updates its [doc:\/\/com.apple.documentation\/documentation\/ARKit\/MeshAnchor\/originFromAnchorTransform] information independently and requires a separate static mesh because ARKit subdivides its representation of the world into multiple, distinct sections.\n\n### Display scene reconstruction meshes during debugging\n\nPeople using an app that leverages scene reconstruction typically don’t need to see a visual rendering of the scene reconstruction mesh. The system already shows passthrough video in an immersive experience. However, temporarily displaying the scene reconstruction mesh can help while you’re developing and debugging your app. In Xcode’s debugging toolbar, click the Enable Visualizations button and select Collision Shapes. Because each element of the scene reconstruction mesh has a collision component, the details of the mesh appear in the debug visualization. For more information, see [doc:\/\/com.apple.documentation\/documentation\/Xcode\/diagnosing-issues-in-the-appearance-of-your-running-app].\n\n## ARKit\n\n- **Happy Beam**: Leverage a Full Space to create a fun game using ARKit.\n- **Setting up access to ARKit data**: Check whether your app can use ARKit and respect people’s privacy.\n- **Placing content on detected planes**: Detect horizontal surfaces like tables and floors, as well as vertical planes like walls and doors.\n- **Tracking specific points in world space**: Retrieve the position and orientation of anchors your app stores in ARKit.\n- **Tracking preregistered images in 3D space**: Place content based on the current position of a known image in a person’s surroundings.\n- **Exploring object tracking with ARKit**: Find and track real-world objects in visionOS using reference objects trained with Create ML.\n- **Object tracking with Reality Composer Pro experiences**: Use object tracking in visionOS to attach digital content to real objects to create engaging experiences.\n- **Building local experiences with room tracking**: Use room tracking in visionOS to provide custom interactions with physical spaces.\n- **Placing entities using head and device transform**: Query and react to changes in the position and rotation of Apple Vision Pro.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Leverage a Full Space to create a fun game using ARKit.",
          "name" : "Happy Beam",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/happybeam"
        },
        {
          "description" : "Check whether your app can use ARKit and respect people’s privacy.",
          "name" : "Setting up access to ARKit data",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/setting-up-access-to-arkit-data"
        },
        {
          "description" : "Detect horizontal surfaces like tables and floors, as well as vertical planes like walls and doors.",
          "name" : "Placing content on detected planes",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/placing-content-on-detected-planes"
        },
        {
          "description" : "Retrieve the position and orientation of anchors your app stores in ARKit.",
          "name" : "Tracking specific points in world space",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/tracking-points-in-world-space"
        },
        {
          "description" : "Place content based on the current position of a known image in a person’s surroundings.",
          "name" : "Tracking preregistered images in 3D space",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/tracking-images-in-3d-space"
        },
        {
          "description" : "Find and track real-world objects in visionOS using reference objects trained with Create ML.",
          "name" : "Exploring object tracking with ARKit",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/exploring_object_tracking_with_arkit"
        },
        {
          "description" : "Use object tracking in visionOS to attach digital content to real objects to create engaging experiences.",
          "name" : "Object tracking with Reality Composer Pro experiences",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/object-tracking-with-reality-composer-pro-experiences"
        },
        {
          "description" : "Use room tracking in visionOS to provide custom interactions with physical spaces.",
          "name" : "Building local experiences with room tracking",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/building-local-experiences-with-room-tracking"
        },
        {
          "description" : "Query and react to changes in the position and rotation of Apple Vision Pro.",
          "name" : "Placing entities using head and device transform",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/placing-entities-using-head-and-device-transform"
        }
      ],
      "title" : "ARKit"
    }
  ],
  "source" : "appleJSON",
  "title" : "Incorporating real-world surroundings in an immersive experience",
  "url" : "https:\/\/developer.apple.com\/documentation\/visionos\/incorporating-real-world-surroundings-in-an-immersive-experience"
}