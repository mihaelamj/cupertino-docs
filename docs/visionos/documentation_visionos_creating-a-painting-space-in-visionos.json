{
  "abstract" : "Implement a painting canvas entity, and update its mesh to represent a stroke.",
  "codeExamples" : [

  ],
  "contentHash" : "39b9733b421d8c8e4cf6257905d648498bfb96ece6545efe04186adbec3c58bf",
  "crawledAt" : "2025-12-02T17:30:09Z",
  "declaration" : {
    "code" : "func updateMesh() {\n    \/\/ The starting point where the stroke mesh begins.\n    guard let center = points.first else { return }\n\n\n    \/\/\/ The position, normals, and triangle indices that the points generate.\n    let (positions, normals, triangles) = generateMeshData()\n\n\n    \/\/ ...\n}",
    "language" : "swift"
  },
  "id" : "F1C19FDC-098A-4661-BEC3-ECB2A19308CC",
  "kind" : "class",
  "overview" : "visionOS  Introductory visionOS samples  Creating a 3D painting space  Introductory visionOS samples  Creating a 3D painting space Sample CodeCreating a 3D painting spaceImplement a painting canvas entity, and update its mesh to represent a stroke. Download visionOS 2.0+Xcode 16.0+OverviewThis sample demonstrates how to create a painting space so that people can pinch-to-draw in an augmented reality space. To achieve this, the app uses hand-tracking technology that ARKit provides to monitor a person’s hand movements. To capture and display each stroke, the app creates collision boxes within the environment, stores the person’s drawing points through a pinch gesture, and then updates the mesh at those points to represent the stroke, as the following video shows:\n\nAdd a system and component to enable real-time updatesThe sample uses a custom system and component to handle updates for entities over time:\n\nimport SwiftUI\nimport RealityKit\n\n\nstruct ClosureComponent: Component {\n    \/\/\/ The closure that takes the time interval since the last update.\n    let closure: (TimeInterval) -> Void\n\n\n    init(closure: @escaping (TimeInterval) -> Void) {\n        self.closure = closure\n        ClosureSystem.registerSystem()\n    }\n}\nThe component contains the closure variable to track the time. On initialization, it registers ClosureSystem into the reality view.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/visionos\/creating-a-painting-space-in-visionos\ncrawled: 2025-12-02T17:30:09Z\n---\n\n# Creating a 3D painting space | Apple Developer Documentation\n\n- [ visionOS ](\/documentation\/visionos)\n\n- [ Introductory visionOS samples ](\/documentation\/visionos\/introductory-visionos-samples)\n\n- [ Creating a 3D painting space ](\/documentation\/visionos\/creating-a-painting-space-in-visionos)\n\n- [ Introductory visionOS samples ](\/documentation\/visionos\/introductory-visionos-samples)\n\n-  Creating a 3D painting space \n\nSample Code# Creating a 3D painting space\n\nImplement a painting canvas entity, and update its mesh to represent a stroke.[ Download ](https:\/\/docs-assets.developer.apple.com\/published\/95fb0d2bec32\/CreatingA3DPaintingSpaceInVisionOS.zip)visionOS 2.0+Xcode 16.0+## [Overview](\/documentation\/visionos\/creating-a-painting-space-in-visionos#Overview)\n\nThis sample demonstrates how to create a painting space so that people can pinch-to-draw in an augmented reality space. To achieve this, the app uses hand-tracking technology that [ARKit](\/documentation\/ARKit) provides to monitor a person’s hand movements. To capture and display each stroke, the app creates collision boxes within the environment, stores the person’s drawing points through a pinch gesture, and then updates the mesh at those points to represent the stroke, as the following video shows:\n\n Video with custom controls.  Content description: A video of a visionOS app with a translucent window labeled Painting Example. The app reacts to pinch-and-drag gestures to draw the word Hello. [ Play ](#)### [Add a system and component to enable real-time updates](\/documentation\/visionos\/creating-a-painting-space-in-visionos#Add-a-system-and-component-to-enable-real-time-updates)\n\nThe sample uses a custom system and component to handle updates for entities over time:\n\n\n\n```\nimport SwiftUI\nimport RealityKit\n\n\nstruct ClosureComponent: Component {\n    \/\/\/ The closure that takes the time interval since the last update.\n    let closure: (TimeInterval) -> Void\n\n\n    init(closure: @escaping (TimeInterval) -> Void) {\n        self.closure = closure\n        ClosureSystem.registerSystem()\n    }\n}\n\n```\n\nThe component contains the `closure` variable to track the time. On initialization, it registers `ClosureSystem` into the reality view.\n\nThe `ClosureSystem` constructs a query using the [`EntityQuery`](\/documentation\/RealityKit\/EntityQuery) to retrieve all entities with the `ClosureComponent` from the scene. Then it passes the delta time, which is the elapsed time since the last update, to the `closure` variable for each entity:\n\n\n\n```\nimport SwiftUI\nimport RealityKit\n\n\nstruct ClosureSystem: System {\n    \/\/\/ The query to find entities that contain `ClosureComponent`.\n    static let query = EntityQuery(where: .has(ClosureComponent.self))\n\n\n    init(scene: RealityKit.Scene) {}\n\n\n    \/\/\/ Update entities with `ClosureComponent` at each render frame.\n    func update(context: SceneUpdateContext) {\n        for entity in context.entities(matching: Self.query, updatingSystemWhen: .rendering) {\n            guard let comp = entity.components[ClosureComponent.self] else { continue }\n            comp.closure(context.deltaTime)\n        }\n    }\n}\n\n```\n\n### [Set up hand tracking for painting action](\/documentation\/visionos\/creating-a-painting-space-in-visionos#Set-up-hand-tracking-for-painting-action)\n\nThe sample creates `PaintingHandTracking` to track the person’s hand with ARKit and store the latest detected hand anchors in the `latestLeftHand` and `latestRightHand` properties:\n\n\n\n```\nimport RealityKit\nimport ARKit\n\n\n@MainActor class PaintingHandTracking: ObservableObject {\n    \/\/\/ The ARKit session for hand tracking.\n    let arSession = ARKitSession()\n\n\n    \/\/\/ The `HandTrackingProvider` for hand tracking.\n    let handTracking = HandTrackingProvider()\n\n\n    \/\/\/ The current left hand anchor that the app detects.\n    @Published var latestLeftHand: HandAnchor?\n\n\n    \/\/\/ The current right hand anchor that the app detects.\n    @Published var latestRightHand: HandAnchor?\n\n\n    \/\/ ...\n}\n\n```\n\nThe `startTracking()` method checks device compatibility for hand tracking and starts the [`ARKitSession`](\/documentation\/ARKit\/ARKitSession) with the [`HandTrackingProvider`](\/documentation\/ARKit\/HandTrackingProvider), while handling potential errors. It continuously waits for anchor updates and assigns the latest left or right hand based on the chirality value:\n\n\n\n```\nimport RealityKit\nimport ARKit\n\n\n@MainActor class PaintingHandTracking: ObservableObject {\n    \/\/ ...\n\n\n    \/\/\/ Check whether the device supports hand tracking, and start the ARKit session.\n    func startTracking() async {\n        guard HandTrackingProvider.isSupported else {\n            print(\"HandTrackingProvider is not supported on this device.\")\n            return\n        }\n\n\n        do {\n            try await arSession.run([handTracking])\n        } catch let error as ARKitSession.Error {\n            print(\"Encountered an error while running providers: (error.localizedDescription)\")\n        } catch let error {\n            print(\"Encountered an unexpected error: (error.localizedDescription)\")\n        }\n\n\n        \/\/ Assign the left and right hand based on the anchor updates.\n        for await anchorUpdate in handTracking.anchorUpdates {\n            switch anchorUpdate.anchor.chirality {\n            case .left:\n                self.latestLeftHand = anchorUpdate.anchor\n            case .right:\n                self.latestRightHand = anchorUpdate.anchor\n            }\n        }\n    }\n}\n\n```\n\n### [Implement the structural representation of a stroke](\/documentation\/visionos\/creating-a-painting-space-in-visionos#Implement-the-structural-representation-of-a-stroke)\n\nThe sample creates the `Stroke` structure to represent the current stroke the person creates with the drag gesture input:\n\n\n\n```\nimport SwiftUI\nimport RealityKit\n\n\nstruct Stroke {\n    \/\/\/ The stroke that represents the stroke.\n    var entity = Entity()\n\n\n    \/\/\/ The collection of points in 3D space that represent the stroke.\n    var points: [SIMD3<Float>] = []\n\n\n    \/\/\/ The maximum radius of the stroke.\n    let maxRadius: Float = 1E-2\n\n\n    \/\/\/ The number of points in each ring of the mesh.\n    let pointsPerRing = 8\n\n\n    \/\/ ...\n}\n\n```\n\nTo update the mesh of the stroke, use `updateMesh()` in the `Stroke` structure. This method uses the initial point as the center and initializes the `positions`, `normal`, and `triangle` indices from `generateMeshData()`, which iterates through the `points` array to construct a path for the mesh:\n\n\n\n```\nfunc updateMesh() {\n    \/\/ The starting point where the stroke mesh begins.\n    guard let center = points.first else { return }\n\n\n    \/\/\/ The position, normals, and triangle indices that the points generate.\n    let (positions, normals, triangles) = generateMeshData()\n\n\n    \/\/ ...\n}\n\n```\n\nThe method creates the [`MeshResource.Contents`](\/documentation\/RealityKit\/MeshResource\/Contents-swift.struct) to represent the content of the mesh that the stroke updates:\n\n\n\n```\nfunc updateMesh() {\n    \/\/ ...\n\n\n    \/\/\/ The `MeshResource.Contents` instance.\n    var contents = MeshResource.Contents()\n    \n    \/\/ Create and assign an instance to `contents`.\n    contents.instances = [MeshResource.Instance(id: \"main\", model: \"model\")]\n\n\n    \/\/ Create the part for the model, and set the vertex positions, triangle indices, and normals.\n    var part = MeshResource.Part(id: \"part\", materialIndex: 0)\n    part.positions = MeshBuffer(positions)\n    part.triangleIndices = MeshBuffer(triangles)\n    part.normals = MeshBuffer(normals)\n\n\n    \/\/ Create and assign a model that consists of the `part`.\n    contents.models = [MeshResource.Model(id: \"model\", parts: [part])]\n\n\n    \/\/ ...\n}\n\n```\n\nThe method creates a [`MeshResource.Instance`](\/documentation\/RealityKit\/MeshResource\/Instance), then define a [`MeshResource.Part`](\/documentation\/RealityKit\/MeshResource\/Part) for the model, where [`MeshBuffer`](\/documentation\/RealityKit\/MeshBuffer) assigns vertex positions, triangle indices, and normals. Finally, it creates a [`MeshResource.Model`](\/documentation\/RealityKit\/MeshResource\/Model) with `part` and assigns to `contents`.\n\nThe method either updates an existing mesh component on the entity, or creates a new mesh. If a mesh component already exists, it updates it with the `contents`; otherwise, it generates a new mesh with `contents` and assigns it to the entity:\n\n\n\n```\nfunc updateMesh() {\n    \/\/ ...\n\n\n    \/\/ Replace the mesh with `contents` if there is a mesh component on the entity.\n    if let mesh = entity.model?.mesh {\n        do {\n            try mesh.replace(with: contents)\n        } catch {\n            print(\"Error replacing mesh: (error.localizedDescription)\")\n        }\n    } else {\n        \/\/\/ The new mesh that generates with `content`.\n        guard let mesh = try? MeshResource.generate(from: contents) else {\n        print(\"Error generating mesh\")\n            return\n        }\n\n\n        \/\/ Set the model component to the new mesh and assign a simple material.\n        entity.components.set(ModelComponent(\n            mesh: mesh,\n            materials: [SimpleMaterial(color: .white, roughness: 1.0, isMetallic: false)]\n        ))\n\n\n        \/\/ Set the entity's transform and position.\n        entity.setTransformMatrix(.identity, relativeTo: nil)\n        entity.setPosition(center, relativeTo: nil)\n    }\n}\n\n```\n\n### [Create the painting canvas to store strokes](\/documentation\/visionos\/creating-a-painting-space-in-visionos#Create-the-painting-canvas-to-store-strokes)\n\nTo create the 3D painting environment, the sample uses `PaintingCanvas` to set up `root`, which represents the painting canvas, and `currentStroke`, which represents the stroke the person creates:\n\n\n\n```\nimport SwiftUI\nimport RealityKit\n\n\nclass PaintingCanvas {\n    \/\/\/ The main root entity for the painting canvas.\n    let root = Entity()\n\n\n    \/\/\/ The stroke the person creates.\n    var currentStroke: Stroke?\n\n\n    \/\/\/ The distance for the box that extends in the positive direction.\n    let big: Float = 1E2\n\n\n    \/\/\/ The distance for the box that extends in the negative direction.\n    let small: Float = 1E-2\n\n\n    init() {\n        root.addChild(addBox(size: [big, big, small], position: [0, 0, -0.5 * big]))\n        root.addChild(addBox(size: [big, big, small], position: [0, 0, +0.5 * big]))\n        \n        \/\/ ...\n    }\n\n\n    \/\/\/ Create a collision box that takes in user input with the drag gesture.\n    private func addBox(size: SIMD3<Float>, position: SIMD3<Float>) -> Entity {\n        let box = Entity()\n        box.components.set(InputTargetComponent())\n        box.components.set(CollisionComponent(shapes: [.generateBox(size: size)], isStatic: true))\n        box.position = position\n        return box\n    }\n\n\n    \/\/ ...\n}\n\n```\n\nAfter the initialization of the class, the app sets up the root by adding six boxes to represent the canvas. User input can target these boxes, and the boxes can interact with other collision entities, stacking along the x, y, and z-axes.\n\nUse `addPoint(_:)` to add the points that the person targets into the `currentStroke`, and update the stroke’s mesh to display the stroke on the painting canvas:\n\n\n\n```\nclass PaintingCanvas {\n    \/\/ ...\n\n\n    func addPoint(_ position: SIMD3<Float>) {\n        \/\/\/ The maximum distance between two points before requiring a new point.\n        let threshold: Float = 1E-9\n\n\n        \/\/ Start a new stroke if no stroke exists.\n        if currentStroke == nil {\n            currentStroke = Stroke()\n\n\n            \/\/ Add the stroke to the root.\n            root.addChild(currentStroke!.entity)\n        }\n\n\n        \/\/ Check if the length between the current hand position and the previous point meets the threshold.\n        if let previousPoint = currentStroke?.points.last, length(position - previousPoint) < threshold {\n            return\n        }\n\n\n        \/\/ Add the current position to the stroke.\n        currentStroke?.points.append(position)\n\n\n        \/\/ Update the current stroke mesh.\n        currentStroke?.updateMesh()\n    }\n\n\n    func finishStroke() {\n        if let stroke = currentStroke {\n            \/\/ Trigger the update mesh operation.\n            stroke.updateMesh()\n\n\n            \/\/ Clear the current stroke.\n            currentStroke = nil\n        }\n    }\n}\n\n```\n\nWhen the person ends a painting action, `finishStroke()` updates the stroke’s mesh and sets the `currentStroke` to nil, to mark the end of the stroke.\n\n### [Set up the painting space](\/documentation\/visionos\/creating-a-painting-space-in-visionos#Set-up-the-painting-space)\n\nThe sample creates a `PaintingView` to combine the hand tracking, painting canvas, and drag gesture detection. It creates the instance of the `PaintingHandTracking` class, the instance of the `PaintingCanvas` class, and the `lastIndexPose` to store the last position of the index finger:\n\n\n\n```\nimport SwiftUI\nimport RealityKit\nimport ARKit\n\n\nstruct PaintingView: View {\n    \/\/\/ The `PaintingHandTracking` class instance.\n    var paintingHandTracking = PaintingHandTracking()\n\n\n    \/\/\/ The instance of the `PaintingCanvas` class to handle painting operations.\n    @State var canvas = PaintingCanvas()\n\n\n    \/\/\/ The last position of the index finger.\n    @State var lastIndexPose: SIMD3<Float>?\n\n\n    \/\/ ...\n}\n\n```\n\nWithin the main body of the view, the app attaches a `ClosureComponent` to the root entity, allowing the hand anchors to update over time:\n\n\n\n```\nvar body: some View {\n    RealityView { content in\n        \/\/\/ The root entity from the painting canvas.\n        let root = canvas.root\n        content.add(root)\n\n\n        root.components.set(ClosureComponent(closure: { deltaTime in\n            \/\/\/ The collection of `HandAnchor` instances.\n            var anchors = [HandAnchor]()\n\n\n            if let latestLeftHand = paintingHandTracking.latestLeftHand {\n                anchors.append(latestLeftHand)\n            }\n            if let latestRightHand = paintingHandTracking.latestRightHand {\n                anchors.append(latestRightHand)\n            }\n\n\n            \/\/ Loop through each anchor that the app detects.\n            for anchor in anchors {\n                guard let handSkeleton = anchor.handSkeleton else {\n                    continue\n                }\n\n\n                let thumbPos = (anchor.originFromAnchorTransform * handSkeleton.joint(.thumbTip).anchorFromJointTransform).translation()\n                let indexPos = (anchor.originFromAnchorTransform * handSkeleton.joint(.indexFingerTip).anchorFromJointTransform).translation()\n\n\n                \/\/\/ The threshold to check whether the index and thumb are close.\n                let pinchThreshold: Float = 0.05\n                if length(thumbPos - indexPos) < pinchThreshold {\n                    lastIndexPose = indexPos\n                }\n            }\n        }))\n    }\n\n\n    \/\/ ...\n}\n\n```\n\nThe `ClosureComponent` iterates through the collection of hand anchors and attempts to retrieve the hand skeleton with each anchor. Then it calculates the distance between the tip of the thumb and the tip of the index finger, using the data from the hand skeleton. If this distance is less than the defined threshold for a pinch gesture, it updates the last known index finger position.\n\nThe app attaches the main body view to the `.gesture()` modifier, to recognize user inputs. When the [`DragGesture`](\/documentation\/SwiftUI\/DragGesture) recognizes an `.onChanged` action, it interprets this as the person initiating a painting action, adding a point to the canvas. When the [`DragGesture`](\/documentation\/SwiftUI\/DragGesture) recognizes an `.onEnded` action, it interprets this as the person finishing a painting action, ending the stroke on the canvas:\n\n\n\n```\n.gesture(\n    DragGesture(minimumDistance: 0)\n    \/\/ Enable the gesture to target an entity.\n    .targetedToAnyEntity()\n    .onChanged({ _ in\n        \/\/ Get the current position.\n        if let pos = lastIndexPose {\n            \/\/ Add a point at the current position.\n            canvas.addPoint(pos)\n        }\n    })\n    .onEnded({ _ in\n        \/\/ End the current stroke when the drag gesture ends.\n        canvas.finishStroke()\n    })\n)\n.task {\n    \/\/ Enable hand tracking when the view starts.\n    await paintingHandTracking.startTracking()\n}\n\n```\n\nThe app starts the hand-tracking session within the `.task()` modifier, which allows the app to enable hand tracking asynchronously before the view appears.\n\n#### [Related samples](\/documentation\/visionos\/creating-a-painting-space-in-visionos#Related-samples)\n\n[Creating a spatial drawing app with RealityKit](\/documentation\/RealityKit\/creating-a-spatial-drawing-app-with-realitykit)Use low-level mesh and texture APIs to achieve fast updates to a person’s brush strokes by integrating RealityKit with ARKit and SwiftUI.[Tracking and visualizing hand movement](\/documentation\/visionos\/tracking-and-visualizing-hand-movement)Use hand-tracking anchors to display a visual representation of hand transforms in visionOS.[Displaying an entity that follows a person’s view](\/documentation\/visionos\/displaying-a-3d-object-that-moves-to-stay-in-a-person)Create an entity that tracks and follows head movement in an immersive scene.[Applying mesh to real-world surroundings](\/documentation\/visionos\/applying-mesh-to-real-world-surroundings)Add a layer of mesh to objects in the real world, using scene reconstruction in ARKit.",
  "sections" : [
    {
      "content" : "",
      "title" : "Overview"
    }
  ],
  "source" : "appleWebKit",
  "title" : "Creating a 3D painting space | Apple Developer Documentation",
  "url" : "https:\/\/developer.apple.com\/documentation\/visionos\/creating-a-painting-space-in-visionos"
}