{
  "abstract" : "Create a more immersive experience by adding video reflections in a custom environment.",
  "codeExamples" : [

  ],
  "contentHash" : "fab17fedeb1ade412e36fd5306f96f5f7af172c417a51dcb31c0698fe73f3e7d",
  "crawledAt" : "2025-12-02T16:03:16Z",
  "id" : "4EE58504-C2B4-49AB-A1BF-96A2A970A073",
  "kind" : "unknown",
  "overview" : "visionOS  Enabling video reflections in an immersive environment  Enabling video reflections in an immersive environment Sample CodeEnabling video reflections in an immersive environmentCreate a more immersive experience by adding video reflections in a custom environment. Download (1.2 GB) visionOS 2.0+Xcode 16.0+OverviewRealityKit and Reality Composer Pro provide the tools to build immersive media viewing environments in visionOS. The Destination Video sample uses these features to build a realistic custom environment called Studio. The environment adds to its realism and makes the video player feel grounded in the space by applying reflections of the player’s content onto the surfaces of the scene.\n\nRealityKit and Reality Composer Pro support two types of video reflections:\n\nSpecular reflections provide a direct reflection of the video content, and are typically useful to apply to glossy surfaces like metals and water.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/visionos\/enabling-video-reflections-in-an-immersive-environment\ncrawled: 2025-12-02T16:03:16Z\n---\n\n# Enabling video reflections in an immersive environment | Apple Developer Documentation\n\n- [ visionOS ](\/documentation\/visionos)\n\n- [ Enabling video reflections in an immersive environment ](\/documentation\/visionos\/enabling-video-reflections-in-an-immersive-environment)\n\n-  Enabling video reflections in an immersive environment \n\nSample Code# Enabling video reflections in an immersive environment\n\nCreate a more immersive experience by adding video reflections in a custom environment.[ Download (1.2 GB) ](https:\/\/docs-assets.developer.apple.com\/published\/08bd3c5ea0b3\/DestinationVideo.zip)visionOS 2.0+Xcode 16.0+## [Overview](\/documentation\/visionos\/enabling-video-reflections-in-an-immersive-environment#Overview)\n\nRealityKit and Reality Composer Pro provide the tools to build immersive media viewing environments in visionOS. The [Destination Video](\/documentation\/visionos\/destination-video) sample uses these features to build a realistic custom environment called Studio. The environment adds to its realism and makes the video player feel grounded in the space by applying reflections of the player’s content onto the surfaces of the scene.\n\n Video with custom controls.  Content description: A video showing a virtual theater with a movie screen that shows a robot rolling toward the camera. The floor of the virtual environment reflects the movie screen on its surface. [ Play ](#)RealityKit and Reality Composer Pro support two types of video reflections:\n\n- Specular reflections provide a direct reflection of the video content, and are typically useful to apply to glossy surfaces like metals and water.\n\n- Diffuse reflections provide a softer falloff of video content, and are useful to apply to rougher, more organic surfaces.\n\nThis article describes how to adopt reflections in your own environment, and shows how Destination Video’s Studio environment supports these effects to create a compelling media viewing experience.\n\n## [Define a video docking location](\/documentation\/visionos\/enabling-video-reflections-in-an-immersive-environment#Define-a-video-docking-location)\n\nApps that use [`AVPlayerViewController`](\/documentation\/AVKit\/AVPlayerViewController) to present video participate in system docking behavior. When you play a full-window video inside an immersive space, the system docks the video screen into a fixed location and presents streamlined playback controls. By default, the system determines the docking location for the scene, but starting in visionOS 2, you can customize this location by specifying a custom docking region.\n\nThe Studio environment defines a custom docking region that anchors the player to the walkway at the top of the staircase like shown below.\n\nTo create the docking region, the project defines a `Player` entity that contains a [`DockingRegionComponent`](\/documentation\/RealityKit\/DockingRegionComponent). This component defines the bounding region for the player, which has a depth of 0 and uses a fixed 2.4:1 aspect ratio. You configure the docking region’s size through its `width` property, and you can optionally specify a preview video to display in the docking region’s space within Reality Composer Pro.\n\nTo provide an optimal viewing experience, the Studio environment minimizes objects between the viewer and the video. Additionally, it provides a comfortable viewing angle to avoid causing strain or discomfort during longer viewing sessions. Using Reality Composer Pro to define the docking region is a great way to visualize how it looks in context, but always review your environment on Apple Vision Pro to get a true sense of layout and scale.\n\nNote\n\nReality Composer Pro provides a template to set up a docking region and default video reflection configuration. You can access this template from the Insert menu by selecting Insert > Environment > Video Dock.\n\n## [Display specular video reflections](\/documentation\/visionos\/enabling-video-reflections-in-an-immersive-environment#Display-specular-video-reflections)\n\nSpecular reflections, like shown below, provide a direct reflection of the video’s content onto surrounding surfaces. You typically apply this type of reflection to glossy surfaces such as metals, mirrors, and water.\n\nTo enable this type of reflection, you define a material with the [`Reflection Specular (RealityKit)`](\/documentation\/ShaderGraph\/realitykit\/Reflection-Specular-(RealityKit)) node connected and apply it to a surface in your scene. The system automatically calculates the appropriate reflection based on your viewing angle relative to the docking region.\n\nThe output of this node contains the reflected color in the RGB channels, and a blend factor in the alpha channel, which you can use to composite the reflection with your existing material.\n\nDestination Video uses subtle specular reflections in its custom environment like shown below. Applying specular reflections helps to add depth and space to the experience. To learn more about how the environment uses specular reflections, open the Studio project in Reality Composer Pro to view its configuration.\n\n## [Provide diffuse video reflections](\/documentation\/visionos\/enabling-video-reflections-in-an-immersive-environment#Provide-diffuse-video-reflections)\n\nDiffuse reflections provide a softer falloff of media content, which can be useful to apply to rougher, more organic surfaces like a concrete or wood floor. The image below shows a diffuse reflection from a video screen.\n\nYou enable diffuse reflections by adding a material on a surface with the [`Reflection Diffuse (RealityKit)`](\/documentation\/ShaderGraph\/realitykit\/Reflection-Diffuse-(RealityKit)) node connected.\n\nThis node requires the following inputs:\n\nEmitter UVThis UV samples the system-generated emitter texture that contains low-frequency light and color information from the docked video. The diffuse reflection node uses the UV to calculate where to show the soft light reflection.\n\nAttenuation UVThis UV samples the provided attenuation mask texture. An attenuation texture contains a soft falloff mask that’s used to shape the light from the emitter. Use a higher bit-depth texture format, such as `.exr,` to reduce any possible banding artifacts.\n\nDestination Video’s custom environment applies diffuse reflections to the surfaces immediately surrounding the docked video screen as shown below:\n\nEnabling diffuse reflections enhances the level of immersion by making the video player feel grounded in the experience.\n\nTo calculate Emitter UVs, iterate over each vertex of the surface mesh, and sample a set number of random points on the docking region. The u-value and v-value of each of the random sample points on the docking region are weighted by measuring both the distance and the angle to the mesh vertex. The resulting emitter UV set is the average of the weighted docking region UV values.\n\nA visualization of the emitter UVs generated from the docking region.\n\nAn example that uses a debug texture to show how different colors from the docking region map on to the surface mesh.\n\nImportant\n\nThe number of random points sampled from the docking region can have a large impact on the overall computation time when generating emitter UVs. You can configure how many samples to use when calculating emitter UVs with the [ComputeDiffuseReflectionUVs](https:\/\/developer.apple.com\/sample-code\/ar\/WWDC_2024_Diffuse_Reflection_UV_Computation_Tool.zip) python script.\n\nAttenuation UVs are a top-down projection of the attenuation texture onto the input geometry (UV-coordinate system). An attenuation texture contains a soft falloff mask that shapes the light from the emitter.\n\nA visualization of the attenuation UVs generated from the docking region.\n\nReality Composer Pro’s default attenuation texture on the visualization.\n\nThe attenuation texture contains a falloff pattern that shapes the the diffuse reflection on to the surface mesh. The image below shows the default Reality Composer Pro attenuation texture.\n\nThe default falloff pattern doesn’t extend all the way to the edges of the texture. In order to generate the attenuation UV set, calculate the edges of the falloff pattern from the texture. The image below shows the default falloff pattern in a standard UV-coordinate system, with the top-left point equal to `(0,0)` and the bottom-right point equal to `(1,1)`.\n\nThe following four values define the attenuation UV set:\n\n`uStart`The UV-space value where the sharp line of the falloff pattern starts horizontally.\n\n`uEnd`The UV-space value where the sharp line of the falloff pattern ends horizontally.\n\n`vStart`The UV-space value where the sharp line starts vertically.\n\n`vEnd`The UV-space value where the falloff pattern ends in black.\n\nAfter calculating the attenuation texture, map it to the geometry. To visualize the attenuation texture mapping, the image below shows a square red mesh as the custom surface mesh that extends towards the user with sides that are equal to the width of the docking region.\n\nThe attenuation UVs are calculated from mapping the surface mesh, in world space, to the area defined by the `uStart`, `uEnd`, `vStart`, and `vEnd` values, in the UV-coordinate space. The image below shows the surface mesh with the attenuation texture applied.\n\nNote\n\nWhen using the [ComputeDiffuseReflectionUVs](https:\/\/developer.apple.com\/sample-code\/ar\/WWDC_2024_Diffuse_Reflection_UV_Computation_Tool.zip) python script for mapping using a custom attenuation texture, you only need to measure the the `uStart`, `uEnd`, `vStart`, and `vEnd` values of your attenuation texture. If you’re using the default attenuation texture in Reality Composer Pro, the script uses the default values.\n\nTo learn more about how the environment sets up and applies diffuse reflections, open the Studio project in Reality Composer Pro to view its configuration.\n\n## [See Also](\/documentation\/visionos\/enabling-video-reflections-in-an-immersive-environment#See-Also)\n\n#### [Related samples](\/documentation\/visionos\/enabling-video-reflections-in-an-immersive-environment#Related-samples)\n\n[Destination Video](\/documentation\/visionos\/destination-video)Leverage SwiftUI to build an immersive media experience in a multiplatform app.#### [Related articles](\/documentation\/visionos\/enabling-video-reflections-in-an-immersive-environment#Related-articles)\n\n[Building an immersive media viewing experience](\/documentation\/visionos\/building-an-immersive-media-viewing-experience)Add a deeper level of immersion to media playback in your app with RealityKit and Reality Composer Pro.#### [Related videos](\/documentation\/visionos\/enabling-video-reflections-in-an-immersive-environment#Related-videos)\n\n[ Enhance the immersion of media viewing in custom environments ](https:\/\/developer.apple.com\/videos\/play\/wwdc2024\/10115)[ Create custom environments for your immersive apps in visionOS ](https:\/\/developer.apple.com\/videos\/play\/wwdc2024\/10087)",
  "sections" : [
    {
      "content" : "",
      "title" : "Overview"
    },
    {
      "content" : "",
      "title" : "Define a video docking location"
    },
    {
      "content" : "",
      "title" : "Display specular video reflections"
    },
    {
      "content" : "",
      "title" : "Provide diffuse video reflections"
    },
    {
      "content" : "",
      "title" : "See Also"
    }
  ],
  "source" : "appleWebKit",
  "title" : "Enabling video reflections in an immersive environment | Apple Developer Documentation",
  "url" : "https:\/\/developer.apple.com\/documentation\/visionos\/enabling-video-reflections-in-an-immersive-environment"
}