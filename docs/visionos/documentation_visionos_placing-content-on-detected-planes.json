{
  "abstract" : "Detect horizontal surfaces like tables and floors, as well as vertical planes like walls and doors.",
  "codeExamples" : [
    {
      "code" : "AnchorEntity(.plane(.horizontal, classification: .table, minimumBounds: [0.5, 0.5]))",
      "language" : "swift"
    },
    {
      "code" : "let session = ARKitSession()\nlet planeData = PlaneDetectionProvider(alignments: [.horizontal, .vertical])\n\nTask {\n    try await session.run([planeData])\n    \n    for await update in planeData.anchorUpdates {\n        if update.anchor.classification == .window {\n            \/\/ Skip planes that are windows.\n            continue\n        }\n        switch update.event {\n        case .added, .updated:\n            await updatePlane(update.anchor)\n        case .removed:\n            await removePlane(update.anchor)\n        }\n        \n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "@MainActor var planeAnchors: [UUID: PlaneAnchor] = [:]\n@MainActor var entityMap: [UUID: Entity] = [:]\n\n@MainActor\nfunc updatePlane(_ anchor: PlaneAnchor) {\n    if planeAnchors[anchor.id] == nil {\n        \/\/ Add a new entity to represent this plane.\n        let entity = ModelEntity(mesh: .generateText(anchor.classification.description))\n        entityMap[anchor.id] = entity\n        rootEntity.addChild(entity)\n    }\n    \n    entityMap[anchor.id]?.transform = Transform(matrix: anchor.originFromAnchorTransform)\n}\n\n@MainActor\nfunc removePlane(_ anchor: PlaneAnchor) {\n    entityMap[anchor.id]?.removeFromParent()\n    entityMap.removeValue(forKey: anchor.id)\n    planeAnchors.removeValue(forKey: anchor.id)\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "5ce462ca43b42084d795b05d9720517b8400da174ce398f0d027a6d749928f4a",
  "crawledAt" : "2025-12-02T16:05:20Z",
  "id" : "B3176505-8B5D-4E95-B49D-DF48F8F240BA",
  "kind" : "unknown",
  "language" : "swift",
  "overview" : "## Overview\n\nFlat surfaces are an ideal place to position content in an app that uses a Full Space in visionOS. They provide a place for virtual 3D content to live alongside a person’s surroundings. Use plane detection in ARKit to detect these kinds of surfaces and filter the available planes based on criteria your app might need, such as the size of the plane, its proximity to someone, or a required plane orientation.\n\n### Use RealityKit anchor entities for basic plane anchoring\n\nIf you don’t need a specific plane in your app and you’re rendering your app’s 3D content in RealityKit, you can use an [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/AnchorEntity] instead. This approach lets you attach 3D content to a plane without prompting the person for world-sensing permission and without any particular knowledge of where that plane is relative to the person.\n\nThe following shows an anchor that you can use to attach entities to a table:\n\nAnchor entities don’t let you choose a specific plane in a person’s surroundings, but rather let you ask for a plane with certain characteristics. When you need more specific plane selection or real-time information about the plane’s position and orientation in the world, use `ARKitSession` and `PlaneDetectionProvider`.\n\n### Configure an ARKit session for plane detection\n\nPlane-detection information comes from an [doc:\/\/com.apple.documentation\/documentation\/ARKit\/ARKitSession] that’s configured to use a [doc:\/\/com.apple.documentation\/documentation\/ARKit\/PlaneDetectionProvider]. You can choose to detect horizontal planes, vertical planes, or both. Each plane that ARKit detects comes with a classification, like [doc:\/\/com.apple.documentation\/documentation\/ARKit\/PlaneAnchor\/Classification-swift.enum\/table] or [doc:\/\/com.apple.documentation\/documentation\/ARKit\/PlaneAnchor\/Classification-swift.enum\/floor]. You can use these classifications to further refine which kinds of planes your app uses to present content. Plane detection requires [doc:\/\/com.apple.documentation\/documentation\/ARKit\/ARKitSession\/AuthorizationType\/worldSensing] authorization.\n\nThe following starts a session that detects both horizontal and vertical planes, but filters out planes classified as windows:\n\n### Create and update entities associated with each plane\n\nIf you’re displaying content that needs to appear attached to a particular plane, update your content whenever you receive new information from ARKit. When a plane is no longer available in the person’s surroundings, ARKit sends a removal event. Respond to these events by removing content associated with the plane.\n\nThe following shows plane updates that place a text entity on each plane in a person’s surroundings; the text entity displays the kind of plane ARKit detected:",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/visionos\/placing-content-on-detected-planes\ncrawled: 2025-12-02T16:05:20Z\n---\n\n# Placing content on detected planes\n\n**Sample Code**\n\nDetect horizontal surfaces like tables and floors, as well as vertical planes like walls and doors.\n\n## Overview\n\nFlat surfaces are an ideal place to position content in an app that uses a Full Space in visionOS. They provide a place for virtual 3D content to live alongside a person’s surroundings. Use plane detection in ARKit to detect these kinds of surfaces and filter the available planes based on criteria your app might need, such as the size of the plane, its proximity to someone, or a required plane orientation.\n\n\n\n### Use RealityKit anchor entities for basic plane anchoring\n\nIf you don’t need a specific plane in your app and you’re rendering your app’s 3D content in RealityKit, you can use an [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/AnchorEntity] instead. This approach lets you attach 3D content to a plane without prompting the person for world-sensing permission and without any particular knowledge of where that plane is relative to the person.\n\nThe following shows an anchor that you can use to attach entities to a table:\n\n```swift\nAnchorEntity(.plane(.horizontal, classification: .table, minimumBounds: [0.5, 0.5]))\n```\n\nAnchor entities don’t let you choose a specific plane in a person’s surroundings, but rather let you ask for a plane with certain characteristics. When you need more specific plane selection or real-time information about the plane’s position and orientation in the world, use `ARKitSession` and `PlaneDetectionProvider`.\n\n### Configure an ARKit session for plane detection\n\nPlane-detection information comes from an [doc:\/\/com.apple.documentation\/documentation\/ARKit\/ARKitSession] that’s configured to use a [doc:\/\/com.apple.documentation\/documentation\/ARKit\/PlaneDetectionProvider]. You can choose to detect horizontal planes, vertical planes, or both. Each plane that ARKit detects comes with a classification, like [doc:\/\/com.apple.documentation\/documentation\/ARKit\/PlaneAnchor\/Classification-swift.enum\/table] or [doc:\/\/com.apple.documentation\/documentation\/ARKit\/PlaneAnchor\/Classification-swift.enum\/floor]. You can use these classifications to further refine which kinds of planes your app uses to present content. Plane detection requires [doc:\/\/com.apple.documentation\/documentation\/ARKit\/ARKitSession\/AuthorizationType\/worldSensing] authorization.\n\nThe following starts a session that detects both horizontal and vertical planes, but filters out planes classified as windows:\n\n```swift\nlet session = ARKitSession()\nlet planeData = PlaneDetectionProvider(alignments: [.horizontal, .vertical])\n\nTask {\n    try await session.run([planeData])\n    \n    for await update in planeData.anchorUpdates {\n        if update.anchor.classification == .window {\n            \/\/ Skip planes that are windows.\n            continue\n        }\n        switch update.event {\n        case .added, .updated:\n            await updatePlane(update.anchor)\n        case .removed:\n            await removePlane(update.anchor)\n        }\n        \n    }\n}\n```\n\n### Create and update entities associated with each plane\n\nIf you’re displaying content that needs to appear attached to a particular plane, update your content whenever you receive new information from ARKit. When a plane is no longer available in the person’s surroundings, ARKit sends a removal event. Respond to these events by removing content associated with the plane.\n\nThe following shows plane updates that place a text entity on each plane in a person’s surroundings; the text entity displays the kind of plane ARKit detected:\n\n```swift\n@MainActor var planeAnchors: [UUID: PlaneAnchor] = [:]\n@MainActor var entityMap: [UUID: Entity] = [:]\n\n@MainActor\nfunc updatePlane(_ anchor: PlaneAnchor) {\n    if planeAnchors[anchor.id] == nil {\n        \/\/ Add a new entity to represent this plane.\n        let entity = ModelEntity(mesh: .generateText(anchor.classification.description))\n        entityMap[anchor.id] = entity\n        rootEntity.addChild(entity)\n    }\n    \n    entityMap[anchor.id]?.transform = Transform(matrix: anchor.originFromAnchorTransform)\n}\n\n@MainActor\nfunc removePlane(_ anchor: PlaneAnchor) {\n    entityMap[anchor.id]?.removeFromParent()\n    entityMap.removeValue(forKey: anchor.id)\n    planeAnchors.removeValue(forKey: anchor.id)\n}\n```\n\n## ARKit\n\n- **Happy Beam**: Leverage a Full Space to create a fun game using ARKit.\n- **Setting up access to ARKit data**: Check whether your app can use ARKit and respect people’s privacy.\n- **Incorporating real-world surroundings in an immersive experience**: Create an immersive experience by making your app’s content respond to the local shape of the world.\n- **Tracking specific points in world space**: Retrieve the position and orientation of anchors your app stores in ARKit.\n- **Tracking preregistered images in 3D space**: Place content based on the current position of a known image in a person’s surroundings.\n- **Exploring object tracking with ARKit**: Find and track real-world objects in visionOS using reference objects trained with Create ML.\n- **Object tracking with Reality Composer Pro experiences**: Use object tracking in visionOS to attach digital content to real objects to create engaging experiences.\n- **Building local experiences with room tracking**: Use room tracking in visionOS to provide custom interactions with physical spaces.\n- **Placing entities using head and device transform**: Query and react to changes in the position and rotation of Apple Vision Pro.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Leverage a Full Space to create a fun game using ARKit.",
          "name" : "Happy Beam",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/happybeam"
        },
        {
          "description" : "Check whether your app can use ARKit and respect people’s privacy.",
          "name" : "Setting up access to ARKit data",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/setting-up-access-to-arkit-data"
        },
        {
          "description" : "Create an immersive experience by making your app’s content respond to the local shape of the world.",
          "name" : "Incorporating real-world surroundings in an immersive experience",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/incorporating-real-world-surroundings-in-an-immersive-experience"
        },
        {
          "description" : "Retrieve the position and orientation of anchors your app stores in ARKit.",
          "name" : "Tracking specific points in world space",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/tracking-points-in-world-space"
        },
        {
          "description" : "Place content based on the current position of a known image in a person’s surroundings.",
          "name" : "Tracking preregistered images in 3D space",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/tracking-images-in-3d-space"
        },
        {
          "description" : "Find and track real-world objects in visionOS using reference objects trained with Create ML.",
          "name" : "Exploring object tracking with ARKit",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/exploring_object_tracking_with_arkit"
        },
        {
          "description" : "Use object tracking in visionOS to attach digital content to real objects to create engaging experiences.",
          "name" : "Object tracking with Reality Composer Pro experiences",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/object-tracking-with-reality-composer-pro-experiences"
        },
        {
          "description" : "Use room tracking in visionOS to provide custom interactions with physical spaces.",
          "name" : "Building local experiences with room tracking",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/building-local-experiences-with-room-tracking"
        },
        {
          "description" : "Query and react to changes in the position and rotation of Apple Vision Pro.",
          "name" : "Placing entities using head and device transform",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/placing-entities-using-head-and-device-transform"
        }
      ],
      "title" : "ARKit"
    }
  ],
  "source" : "appleJSON",
  "title" : "Placing content on detected planes",
  "url" : "https:\/\/developer.apple.com\/documentation\/visionos\/placing-content-on-detected-planes"
}