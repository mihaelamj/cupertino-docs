{
  "abstract" : "Find and track real-world objects in visionOS using reference objects trained with Create ML.",
  "codeExamples" : [
    {
      "code" : "func loadBuiltInReferenceObjects() async {\n    \/\/ Only allow one loading operation at any given time.\n    guard !didStartLoading else { return }\n    didStartLoading.toggle()\n    \n    print(\"Looking for reference objects in the main bundle ...\")\n\n    \/\/ Get a list of all reference object files in the app's main bundle and attempt to load each.\n    var referenceObjectFiles: [String] = []\n    if let resourcesPath = Bundle.main.resourcePath {\n        try? referenceObjectFiles = FileManager.default.contentsOfDirectory(atPath: resourcesPath).filter { $0.hasSuffix(\".referenceobject\") }\n    }\n    \n    fileCount = referenceObjectFiles.count\n    updateProgress()\n    \n    await withTaskGroup(of: Void.self) { group in\n        for file in referenceObjectFiles {\n            let objectURL = Bundle.main.bundleURL.appending(path: file)\n            group.addTask {\n                await self.loadReferenceObject(objectURL)\n                await self.finishedOneFile()\n            }\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : ".fileImporter(isPresented: $fileImporterIsOpen, allowedContentTypes: [referenceObjectUTType], allowsMultipleSelection: true) { results in\n    switch results {\n    case .success(let fileURLs):\n        Task {\n            \/\/ Try to load each selected file as a reference object.\n            for fileURL in fileURLs {\n                guard fileURL.startAccessingSecurityScopedResource() else {\n                    print(\"Failed to get sandboxed access to the file \\(fileURL)\")\n                    return\n                }\n                await appState.referenceObjectLoader.addReferenceObject(fileURL)\n                fileURL.stopAccessingSecurityScopedResource()\n            }\n        }\n    case .failure(let error):\n        print(\"Failed to open file with error: \\(error)\")\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "func startTracking() async -> ObjectTrackingProvider? {\n    let referenceObjects = referenceObjectLoader.enabledReferenceObjects\n    \n    guard !referenceObjects.isEmpty else {\n        fatalError(\"No reference objects to start tracking\")\n    }\n    \n    \/\/ Run a new provider every time when entering the immersive space.\n    let objectTracking = ObjectTrackingProvider(referenceObjects: referenceObjects)\n    do {\n        try await arkitSession.run([objectTracking])\n    } catch {\n        print(\"Error: \\(error)\" )\n        return nil\n    }\n    self.objectTracking = objectTracking\n    return objectTracking\n}",
      "language" : "swift"
    },
    {
      "code" : "Task {\n    let objectTracking = await appState.startTracking()\n    guard let objectTracking else {\n        return\n    }\n    \n    \/\/ Wait for object anchor updates and maintain a dictionary of visualizations\n    \/\/ that are attached to those anchors.\n    for await anchorUpdate in objectTracking.anchorUpdates {\n        let anchor = anchorUpdate.anchor\n        let id = anchor.id\n        \n        switch anchorUpdate.event {\n        case .added:\n            \/\/ Create a new visualization for the reference object that ARKit just detected.\n            \/\/ The app displays the USDZ file that the reference object was trained on as\n            \/\/ a wireframe on top of the real-world object, if the .referenceobject file contains\n            \/\/ that USDZ file. If the original USDZ isn't available, the app displays a bounding box instead.\n            let model = appState.referenceObjectLoader.usdzsPerReferenceObjectID[anchor.referenceObject.id]\n            let visualization = ObjectAnchorVisualization(for: anchor, withModel: model)\n            self.objectVisualizations[id] = visualization\n            root.addChild(visualization.entity)\n        case .updated:\n            objectVisualizations[id]?.update(with: anchor)\n        case .removed:\n            objectVisualizations[id]?.entity.removeFromParent()\n            objectVisualizations.removeValue(forKey: id)\n        }\n    }\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "b59d63f4ff106ac753393c6a2e85cc273ca65270b58f2a889616996b330dad0a",
  "crawledAt" : "2025-12-02T17:33:12Z",
  "id" : "BEF19624-4687-4A2C-A40F-A5B189D29B98",
  "kind" : "unknown",
  "language" : "swift",
  "overview" : "## Overview\n\nThe sample app demonstrates how to use a reference object to discover and track a specific object in a person’s surroundings in visionOS. This capability allows you to create engaging experiences based on objects in a person’s surroundings and lets you  attach digital content to these objects. For example, you can build an app that uses reference objects to describe the specific assembly of a machine a person is testing or repairing. Using this reference model, when ARKit recognizes that object, you can attach digital content to it, such as a diagram of the device, more information about its function, and so on.\n\nThis sample includes a reference object that ARKit uses to recognize a Magic Keyboard in someone’s surroundings.\n\n## Configure the sample code project\n\n## Import the reference object to track a specific object\n\nObject tracking demonstrates two methods for importing a reference object into the app. The first loads reference objects directly from the app’s bundle, as shown here:\n\nIn the second method, a person can provide a URL for a reference object file through a file importer dialog:\n\n## Run object tracking on a session\n\nTo start receiving events, create an [doc:\/\/com.apple.documentation\/documentation\/ARKit\/ObjectTrackingProvider] that you initialize with a reference object, and then start an [doc:\/\/com.apple.documentation\/documentation\/ARKit\/ARKitSession] with the `ObjectTrackingProvider` you created, as shown below:\n\n## Handle adding, updating, removing, and visualizing objects\n\nARKit delivers an asynchronous stream of updates as it detects changes in the scene. Your app needs to process these as they arrive and update the scene in response. The example below demonstrates handling these events inside the app’s [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/RealityView]:\n\nWhen the app adds objects to the scene, it attaches virtual content to the reference object using the `ObjectAnchorVisualization` entity to render a wireframe that shows the reference object’s outline.\n\n## Create your own reference objects\n\nCreating your own reference objects requires an iPhone, iPad, or other device that you can use to create high-fidelity scans of the physical object you want to model, and a Mac with an M2 chip or later to process the images and create a reference object using Create ML.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/visionos\/exploring_object_tracking_with_arkit\ncrawled: 2025-12-02T17:33:12Z\n---\n\n# Exploring object tracking with ARKit\n\n**Sample Code**\n\nFind and track real-world objects in visionOS using reference objects trained with Create ML.\n\n## Overview\n\nThe sample app demonstrates how to use a reference object to discover and track a specific object in a person’s surroundings in visionOS. This capability allows you to create engaging experiences based on objects in a person’s surroundings and lets you  attach digital content to these objects. For example, you can build an app that uses reference objects to describe the specific assembly of a machine a person is testing or repairing. Using this reference model, when ARKit recognizes that object, you can attach digital content to it, such as a diagram of the device, more information about its function, and so on.\n\nThis sample includes a reference object that ARKit uses to recognize a Magic Keyboard in someone’s surroundings.\n\n\n\n## Configure the sample code project\n\n\n\n1. In the project’s settings, select Signing and Capabilities.\n2. Select your team name from the drop-down menu.\n3. Pair Xcode with your device wirelessly or using the developer strap.\n4. Click Run or press Command-R to launch the app.\n\n## Import the reference object to track a specific object\n\nObject tracking demonstrates two methods for importing a reference object into the app. The first loads reference objects directly from the app’s bundle, as shown here:\n\n```swift\nfunc loadBuiltInReferenceObjects() async {\n    \/\/ Only allow one loading operation at any given time.\n    guard !didStartLoading else { return }\n    didStartLoading.toggle()\n    \n    print(\"Looking for reference objects in the main bundle ...\")\n\n    \/\/ Get a list of all reference object files in the app's main bundle and attempt to load each.\n    var referenceObjectFiles: [String] = []\n    if let resourcesPath = Bundle.main.resourcePath {\n        try? referenceObjectFiles = FileManager.default.contentsOfDirectory(atPath: resourcesPath).filter { $0.hasSuffix(\".referenceobject\") }\n    }\n    \n    fileCount = referenceObjectFiles.count\n    updateProgress()\n    \n    await withTaskGroup(of: Void.self) { group in\n        for file in referenceObjectFiles {\n            let objectURL = Bundle.main.bundleURL.appending(path: file)\n            group.addTask {\n                await self.loadReferenceObject(objectURL)\n                await self.finishedOneFile()\n            }\n        }\n    }\n}\n```\n\nIn the second method, a person can provide a URL for a reference object file through a file importer dialog:\n\n```swift\n.fileImporter(isPresented: $fileImporterIsOpen, allowedContentTypes: [referenceObjectUTType], allowsMultipleSelection: true) { results in\n    switch results {\n    case .success(let fileURLs):\n        Task {\n            \/\/ Try to load each selected file as a reference object.\n            for fileURL in fileURLs {\n                guard fileURL.startAccessingSecurityScopedResource() else {\n                    print(\"Failed to get sandboxed access to the file \\(fileURL)\")\n                    return\n                }\n                await appState.referenceObjectLoader.addReferenceObject(fileURL)\n                fileURL.stopAccessingSecurityScopedResource()\n            }\n        }\n    case .failure(let error):\n        print(\"Failed to open file with error: \\(error)\")\n    }\n}\n```\n\n## Run object tracking on a session\n\nTo start receiving events, create an [doc:\/\/com.apple.documentation\/documentation\/ARKit\/ObjectTrackingProvider] that you initialize with a reference object, and then start an [doc:\/\/com.apple.documentation\/documentation\/ARKit\/ARKitSession] with the `ObjectTrackingProvider` you created, as shown below:\n\n```swift\nfunc startTracking() async -> ObjectTrackingProvider? {\n    let referenceObjects = referenceObjectLoader.enabledReferenceObjects\n    \n    guard !referenceObjects.isEmpty else {\n        fatalError(\"No reference objects to start tracking\")\n    }\n    \n    \/\/ Run a new provider every time when entering the immersive space.\n    let objectTracking = ObjectTrackingProvider(referenceObjects: referenceObjects)\n    do {\n        try await arkitSession.run([objectTracking])\n    } catch {\n        print(\"Error: \\(error)\" )\n        return nil\n    }\n    self.objectTracking = objectTracking\n    return objectTracking\n}\n```\n\n## Handle adding, updating, removing, and visualizing objects\n\nARKit delivers an asynchronous stream of updates as it detects changes in the scene. Your app needs to process these as they arrive and update the scene in response. The example below demonstrates handling these events inside the app’s [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/RealityView]:\n\n```swift\nTask {\n    let objectTracking = await appState.startTracking()\n    guard let objectTracking else {\n        return\n    }\n    \n    \/\/ Wait for object anchor updates and maintain a dictionary of visualizations\n    \/\/ that are attached to those anchors.\n    for await anchorUpdate in objectTracking.anchorUpdates {\n        let anchor = anchorUpdate.anchor\n        let id = anchor.id\n        \n        switch anchorUpdate.event {\n        case .added:\n            \/\/ Create a new visualization for the reference object that ARKit just detected.\n            \/\/ The app displays the USDZ file that the reference object was trained on as\n            \/\/ a wireframe on top of the real-world object, if the .referenceobject file contains\n            \/\/ that USDZ file. If the original USDZ isn't available, the app displays a bounding box instead.\n            let model = appState.referenceObjectLoader.usdzsPerReferenceObjectID[anchor.referenceObject.id]\n            let visualization = ObjectAnchorVisualization(for: anchor, withModel: model)\n            self.objectVisualizations[id] = visualization\n            root.addChild(visualization.entity)\n        case .updated:\n            objectVisualizations[id]?.update(with: anchor)\n        case .removed:\n            objectVisualizations[id]?.entity.removeFromParent()\n            objectVisualizations.removeValue(forKey: id)\n        }\n    }\n}\n```\n\nWhen the app adds objects to the scene, it attaches virtual content to the reference object using the `ObjectAnchorVisualization` entity to render a wireframe that shows the reference object’s outline.\n\n## Create your own reference objects\n\nCreating your own reference objects requires an iPhone, iPad, or other device that you can use to create high-fidelity scans of the physical object you want to model, and a Mac with an M2 chip or later to process the images and create a reference object using Create ML.\n\n## ARKit\n\n- **Happy Beam**: Leverage a Full Space to create a fun game using ARKit.\n- **Setting up access to ARKit data**: Check whether your app can use ARKit and respect people’s privacy.\n- **Incorporating real-world surroundings in an immersive experience**: Create an immersive experience by making your app’s content respond to the local shape of the world.\n- **Placing content on detected planes**: Detect horizontal surfaces like tables and floors, as well as vertical planes like walls and doors.\n- **Tracking specific points in world space**: Retrieve the position and orientation of anchors your app stores in ARKit.\n- **Tracking preregistered images in 3D space**: Place content based on the current position of a known image in a person’s surroundings.\n- **Object tracking with Reality Composer Pro experiences**: Use object tracking in visionOS to attach digital content to real objects to create engaging experiences.\n- **Building local experiences with room tracking**: Use room tracking in visionOS to provide custom interactions with physical spaces.\n- **Placing entities using head and device transform**: Query and react to changes in the position and rotation of Apple Vision Pro.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Leverage a Full Space to create a fun game using ARKit.",
          "name" : "Happy Beam",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/happybeam"
        },
        {
          "description" : "Check whether your app can use ARKit and respect people’s privacy.",
          "name" : "Setting up access to ARKit data",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/setting-up-access-to-arkit-data"
        },
        {
          "description" : "Create an immersive experience by making your app’s content respond to the local shape of the world.",
          "name" : "Incorporating real-world surroundings in an immersive experience",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/incorporating-real-world-surroundings-in-an-immersive-experience"
        },
        {
          "description" : "Detect horizontal surfaces like tables and floors, as well as vertical planes like walls and doors.",
          "name" : "Placing content on detected planes",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/placing-content-on-detected-planes"
        },
        {
          "description" : "Retrieve the position and orientation of anchors your app stores in ARKit.",
          "name" : "Tracking specific points in world space",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/tracking-points-in-world-space"
        },
        {
          "description" : "Place content based on the current position of a known image in a person’s surroundings.",
          "name" : "Tracking preregistered images in 3D space",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/tracking-images-in-3d-space"
        },
        {
          "description" : "Use object tracking in visionOS to attach digital content to real objects to create engaging experiences.",
          "name" : "Object tracking with Reality Composer Pro experiences",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/object-tracking-with-reality-composer-pro-experiences"
        },
        {
          "description" : "Use room tracking in visionOS to provide custom interactions with physical spaces.",
          "name" : "Building local experiences with room tracking",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/building-local-experiences-with-room-tracking"
        },
        {
          "description" : "Query and react to changes in the position and rotation of Apple Vision Pro.",
          "name" : "Placing entities using head and device transform",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/placing-entities-using-head-and-device-transform"
        }
      ],
      "title" : "ARKit"
    }
  ],
  "source" : "appleJSON",
  "title" : "Exploring object tracking with ARKit",
  "url" : "https:\/\/developer.apple.com\/documentation\/visionos\/exploring_object_tracking_with_arkit"
}