{
  "abstract" : "Integrate the features of an array of frameworks seamlessly to enhance your spatial app.",
  "codeExamples" : [
    {
      "code" : "func setupBouncingBall(_ content: (inout RealityViewContent) {\n    \/\/ Create a bouncing ball with without physics simulation by default.\n    bouncingBall.position = SIMD3<Float>(x: 0, y: 1.3, z: -0.9)\n    bouncingBall.physicsBody = PhysicsBodyComponent(mode: .kinematic)\n    bouncingBall.collision = CollisionComponent(shapes: [ShapeResource.generateSphere(radius: ImmersiveView.ballRadius)])\n    bouncingBall.name = \"Bouncing Ball\"\n    bouncingBall.components.set(InputTargetComponent())\n    \n    \/\/ Add the bouncing ball to the reality view content.\n    content.add(bouncingBall)\n}",
      "language" : "swift"
    },
    {
      "code" : "@State var spatialTrackingSession = SpatialTrackingSession()\n\n\/\/\/ Run a spatial tracking session within the reality view content.\nfunc runSpatialTrackingSession(_ content: (inout RealityViewContent)) async {\n    \/\/ Request spatial tracking of hands and planes with scene understanding \n    \/\/ for collisions and physics.\n    let configuration = SpatialTrackingSession.Configuration(\n        tracking: [.hand, .plane], \n        sceneUnderstanding: [.collision, .physics]\n    )\n    \n    if let unavailableCapabilities = await spatialTrackingSession.run(configuration) {\n        \/\/ Handle any unavailable capabilities.\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Create an anchor entity to the player's right hand index finger tip.\nlet rightIndexFingerTip = AnchorEntity(.hand(.right, location: .indexFingerTip))\n\n\/\/ ...\n\nif let container = bouncingBall.parent {\n    bouncingBall.components.set(GestureComponent(\n        \/\/ Add a drag gesture to the bouncing ball's gesture component so people \n        \/\/ can throw the ball.\n        DragGesture(coordinateSpace3D: container)\n            .onChanged { value in\n                \/\/ Reset the bouncing ball's `physicsBody` to `.kinematic` mode \n                \/\/ during the drag gesture so it doesn't react to physics.\n                bouncingBall.physicsBody = PhysicsBodyComponent(mode: .kinematic)\n                \n                \/\/ Track the bouncing ball with the player's finger during the drag.\n                if let fingerTransform = rightIndexFingerTip.transformMatrix(relativeTo: .immersiveSpace) {\n                    bouncingBall.move(to: fingerTransform,\n                                   relativeTo: container,\n                                   duration: 0,\n                                   timingFunction: .easeInOut)\n                }\n                \n                \/\/ Record the timestamp and position during the drag.\n            }\n            .onEnded { value in\n                \/\/ Release the ball by setting it's physics body mode to `.dynamic` \n                \/\/ with a high bounce value.\n                let material = PhysicsMaterialResource.generate(friction: 0.2, restitution: 1.0)\n                bouncingBall.physicsBody = PhysicsBodyComponent(material: material, mode: .dynamic)\n            \n                \/\/ Determine the release velocity vector by comparing the \n                \/\/ previous timestamp and position to the current timestamp \n                \/\/ and position.\n                \n                \/\/ Apply the velocity to the bouncing ball.\n            }\n    ))\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Create anchor entitites for various objects in the real world.\nlet wallEntity = AnchorEntity(.plane(.vertical, classification: .wall, minimumBounds: [1, 1]))\ncontent.add(wallEntity)\n\nlet floorEntity = AnchorEntity(.plane(.horizontal, classification: .floor, minimumBounds: [1, 1]))\ncontent.add(floorEntity)\n\nlet ceilingEntity = AnchorEntity(.plane(.horizontal, classification: .ceiling, minimumBounds: [1, 1]))\ncontent.add(ceilingEntity)\n\nlet tableEntity = AnchorEntity(.plane(.horizontal, classification: .table, minimumBounds: [0.1, 0.1]))\ncontent.add(tableEntity)",
      "language" : "swift"
    },
    {
      "code" : "@State var didAnchor: EventSubscription? = nil\n\n\/\/ ...\n\n\/\/ Subscribe to anchor events.\ndidAnchor = content.subscribe(to: AnchorStateEvents.DidAnchor.self, { event in\n    if event.reason == .newAnchor,\n       let anchorComponent = event.entity.components[ARKitAnchorComponent.self],\n       let planeAnchor = anchorComponent.anchor as? PlaneAnchor {\n        \n        \/\/ Create visual plane meshes for the plane anchors for recognition \n        \/\/ by Apple Vision Pro.\n        let planeModel = ModelEntity(\n            mesh: MeshResource.generatePlane(\n                width: planeAnchor.geometry.extent.width,\n                depth: planeAnchor.geometry.extent.height\n            ),\n            materials: [SimpleMaterial(\n                color: UIColor(hue: 0.66, saturation: 0.5, brightness: 1.0, alpha: 0.2),\n                isMetallic: false\n            )]\n        )\n        \n        planeModel.transform = Transform(matrix: planeAnchor.originFromAnchorTransform)\n        \n        if let container = bouncingBall.parent {\n            container.addChild(planeModel)\n        }\n    }\n})",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Attach a `PlaneView` showing the number of points in each registered plane anchor.\nlet wallEntity = AnchorEntity(.plane(.vertical, classification: .wall, minimumBounds: [0.1, 0.1]))\n\nwallEntity.components[ViewAttachmentComponent.self] = ViewAttachmentComponent(\n    rootView: PlaneView(text: \"30 points\")\n)\n\ncontent.add(wallEntity)",
      "language" : "swift"
    },
    {
      "code" : "@main\nstruct BouncingBallApp: App {\n    @State var appModel = AppModel()\n    \n    \/\/ Make the bouncing ball entity avialable to the immersive view and \n    \/\/ the observing tracking view.\n    @State var bouncingBall = ModelEntity(\n        mesh: MeshResource.generateSphere(radius: ImmersiveView.ballRadius),\n        materials: [SimpleMaterial(color: .red, isMetallic: false)]\n    )\n    \n    var body: some SwiftUI.Scene {\n        WindowGroup {\n            ContentView()\n                .environment(appModel)\n            TrackingView(bouncingBall: bouncingBall)\n                .environment(appModel)\n        }\n        \n        ImmersiveSpace(id: appModel.immersiveSpaceID) {\n            ImmersiveView(bouncingBall: bouncingBall)\n                .environment(appModel)\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "struct TrackingView: View {\n    let bouncingBall: ModelEntity\n\n    var body: some View {\n        VStack {\n            Button {\n                \/\/ Reset the bouncing ball to its initial state when pressing the button.\n                bouncingBall.position = SIMD3<Float>(x: 0, y: 1.3, z: -0.9)\n                bouncingBall.physicsBody = PhysicsBodyComponent(mode: .kinematic)\n            } label: {\n                Text(\"Reset bouncing ball\")\n            }\n            .disabled(distance() < 5) \/\/ Enable the reset button when the bouncing ball is too far away.\n        }\n        .padding()\n    }\n\n    \/\/\/ Calculate the bouncing ball's distance from the origin.\n    func distance() -> Float {\n        let position = bouncingBall.observable.position\n    \n        return sqrt(position.x * position.x + position.y * position.y + position.z * position.z)\n    }\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "2c539de3cb42ce2579ad294c8008fa421558eacc6021e2a0c87d414902f08e58",
  "crawledAt" : "2025-12-02T15:45:15Z",
  "id" : "A1955975-B56D-4B0E-A674-5944E2B435E2",
  "kind" : "article",
  "language" : "swift",
  "overview" : "## Overview\n\nWhen building spatial computing experiences on visionOS, developers leverage a powerful combination of [doc:\/\/com.apple.documentation\/documentation\/ARKit], [doc:\/\/com.apple.documentation\/documentation\/RealityKit], and [doc:\/\/com.apple.documentation\/documentation\/SwiftUI] that work seamlessly together. This article demonstrates how these frameworks interoperate by walking through the creation of a bouncing ball game that interacts with the physical world around you.\n\n\n\n## Create a realistic bouncing ball\n\nBegin by creating a bouncing ball entity that has physics simulation, collision detection, and input handling. The `setupBouncingBall()` method configures a RealityKit entity with multiple components:\n\nThe [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/PhysicsBodyComponent] enables physics simulation, initially set to [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/PhysicsBodyMode\/kinematic] mode for manual control. [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/CollisionComponent] defines the collision shape using a sphere matching the visual mesh. Setting the [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/InputTargetComponent] allows the entity to receive gestures, enabling a player to interact with the model. The method receives a [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/RealityViewContent] object the [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/RealityView] `make:` closure provides and places the bouncing ball entity within it.\n\n## Recognize your environment\n\nTo make the ball interactive, your app needs hand tracking, gesture, and spatial recognition support. Configure the [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/SpatialTrackingSession] to track hands and planes in addition to scene understanding:\n\nThese capabilities form the basis of the bouncing ball entity, allowing it to interact with the player and the environment.\n\n## Add UI gestures to spatial entities\n\nAn example of cross-framework interoperability is how the SwiftUI gesture system integrates with RealityKit entities through the [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/GestureComponent]. This component allows you to attach SwiftUI gestures directly to RealityKit entities:\n\nThe [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/DragGesture] structure’s initializer,  [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/DragGesture\/init(minimumDistance:coordinateSpace3D:)], is a spatial SwiftUI gesture that supports motion in three dimensions. During the gesture, switch the physics body to [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/PhysicsBodyMode\/kinematic] mode, to allow manual positioning. While dragging, the ball follows your index finger using an [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/AnchorEntity] which ARKit attaches to your right finger tip making it appear as if you are moving the bouncing ball with your hand. On release, switch the physics body to [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/PhysicsBodyMode\/dynamic] mode to re-engage physics simulation on the ball and provide an impulse the app bases on the calculating the velocity from position and time deltas. With this interaction, you can throw the ball in a natural gesture.\n\n## Define how the ball interacts with the scene\n\nThe key to making the ball interact with the real world is to:\n\nRealityKit uses the information to detect and create collision geometry for surfaces in the physical environment without explicitly invoking ARKit APIs. Allowing RealityKit to perform this work on your behalf can be convenience if you don’t require the detailed information `ARKit` API provides. The scene understanding happens entirely within the RealityKit physics simulation system. After enabling scene understanding, the ball bounces off real-world surfaces like floors, walls, tables, and ceilings.\n\n## Visualize the physical world with plane anchors\n\nWhile scene understanding provides invisible collision geometry, you might want to visualize the surfaces or create targets to hit. You provide this support by adding [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/AnchorEntity] instances for different plane classifications:\n\nThese anchor entities automatically position themselves in scene space when RealityKit detects planes matching their classification, including walls, floors, ceilings, and tables in the player’s environment. You can access the ARKit plane anchor directly from the entity’s [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/ARKitAnchorComponent] in the event. When necessary, you have access to all the detail ARKit has to provide in the convenient context of RealityKit event handling. To visualize these planes, subscribe to anchor events and add plane entities to your scene:\n\nThis subscription creates a semitransparent blue plane mesh for each surface RealityKit detects, providing visual feedback about where the ball can bounce.\n\n## Attach views to spatial entities\n\nYou can attach SwiftUI views directly to RealityKit entities using the `ViewAttachmentComponent` initializer, [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/ViewAttachmentComponent\/init(rootView:)]. In the context of the game example, it allows you to display game point values or other UI elements that track with the planes:\n\nThe SwiftUI renders views as part of the 3D scene and automatically positions them with their container entities.\n\n## Observe spatial entities in views\n\nSwiftUI views can observe RealityKit entities in nearly the same manner as SwiftUI `@State` and `@Binding` properties. By creating the bouncing ball entity up one level, at the app level in this case, and passing it to both the immersive space and a tracking view, you can observe changes in the bouncing ball in SwiftUI:\n\n`TrackingView` observes the ball’s position in real time using the entity’s [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/Entity\/observable-swift.property] property:\n\nAs the ball moves in the immersive space, the tracking view automatically updates in the separate window. Explicit use of the `observable` property is the only difference from standard `@State` property observation. When the distance is greater than `5` meters, it can be difficult for the player to recall the ball with the pinch gesture. In this condition, the `TrackingView` enables the bouncing ball’s Reset button, allowing the player to return the ball to its original state. This demonstrates how you can fully integrate RealityKit entities with the SwiftUI observation system, allowing seamless data flow between 3D spatial content and traditional and a 2D UI.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/visionos\/combining-spatial-support-from-multiple-frameworks\ncrawled: 2025-12-02T15:45:15Z\n---\n\n# Combining spatial support from multiple frameworks\n\n**Article**\n\nIntegrate the features of an array of frameworks seamlessly to enhance your spatial app.\n\n## Overview\n\nWhen building spatial computing experiences on visionOS, developers leverage a powerful combination of [doc:\/\/com.apple.documentation\/documentation\/ARKit], [doc:\/\/com.apple.documentation\/documentation\/RealityKit], and [doc:\/\/com.apple.documentation\/documentation\/SwiftUI] that work seamlessly together. This article demonstrates how these frameworks interoperate by walking through the creation of a bouncing ball game that interacts with the physical world around you.\n\n\n\n## Create a realistic bouncing ball\n\nBegin by creating a bouncing ball entity that has physics simulation, collision detection, and input handling. The `setupBouncingBall()` method configures a RealityKit entity with multiple components:\n\n```swift\nfunc setupBouncingBall(_ content: (inout RealityViewContent) {\n    \/\/ Create a bouncing ball with without physics simulation by default.\n    bouncingBall.position = SIMD3<Float>(x: 0, y: 1.3, z: -0.9)\n    bouncingBall.physicsBody = PhysicsBodyComponent(mode: .kinematic)\n    bouncingBall.collision = CollisionComponent(shapes: [ShapeResource.generateSphere(radius: ImmersiveView.ballRadius)])\n    bouncingBall.name = \"Bouncing Ball\"\n    bouncingBall.components.set(InputTargetComponent())\n    \n    \/\/ Add the bouncing ball to the reality view content.\n    content.add(bouncingBall)\n}\n```\n\nThe [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/PhysicsBodyComponent] enables physics simulation, initially set to [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/PhysicsBodyMode\/kinematic] mode for manual control. [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/CollisionComponent] defines the collision shape using a sphere matching the visual mesh. Setting the [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/InputTargetComponent] allows the entity to receive gestures, enabling a player to interact with the model. The method receives a [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/RealityViewContent] object the [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/RealityView] `make:` closure provides and places the bouncing ball entity within it.\n\n## Recognize your environment\n\nTo make the ball interactive, your app needs hand tracking, gesture, and spatial recognition support. Configure the [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/SpatialTrackingSession] to track hands and planes in addition to scene understanding:\n\n```swift\n@State var spatialTrackingSession = SpatialTrackingSession()\n\n\/\/\/ Run a spatial tracking session within the reality view content.\nfunc runSpatialTrackingSession(_ content: (inout RealityViewContent)) async {\n    \/\/ Request spatial tracking of hands and planes with scene understanding \n    \/\/ for collisions and physics.\n    let configuration = SpatialTrackingSession.Configuration(\n        tracking: [.hand, .plane], \n        sceneUnderstanding: [.collision, .physics]\n    )\n    \n    if let unavailableCapabilities = await spatialTrackingSession.run(configuration) {\n        \/\/ Handle any unavailable capabilities.\n    }\n}\n```\n\nThese capabilities form the basis of the bouncing ball entity, allowing it to interact with the player and the environment.\n\n## Add UI gestures to spatial entities\n\nAn example of cross-framework interoperability is how the SwiftUI gesture system integrates with RealityKit entities through the [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/GestureComponent]. This component allows you to attach SwiftUI gestures directly to RealityKit entities:\n\n```swift\n\/\/ Create an anchor entity to the player's right hand index finger tip.\nlet rightIndexFingerTip = AnchorEntity(.hand(.right, location: .indexFingerTip))\n\n\/\/ ...\n\nif let container = bouncingBall.parent {\n    bouncingBall.components.set(GestureComponent(\n        \/\/ Add a drag gesture to the bouncing ball's gesture component so people \n        \/\/ can throw the ball.\n        DragGesture(coordinateSpace3D: container)\n            .onChanged { value in\n                \/\/ Reset the bouncing ball's `physicsBody` to `.kinematic` mode \n                \/\/ during the drag gesture so it doesn't react to physics.\n                bouncingBall.physicsBody = PhysicsBodyComponent(mode: .kinematic)\n                \n                \/\/ Track the bouncing ball with the player's finger during the drag.\n                if let fingerTransform = rightIndexFingerTip.transformMatrix(relativeTo: .immersiveSpace) {\n                    bouncingBall.move(to: fingerTransform,\n                                   relativeTo: container,\n                                   duration: 0,\n                                   timingFunction: .easeInOut)\n                }\n                \n                \/\/ Record the timestamp and position during the drag.\n            }\n            .onEnded { value in\n                \/\/ Release the ball by setting it's physics body mode to `.dynamic` \n                \/\/ with a high bounce value.\n                let material = PhysicsMaterialResource.generate(friction: 0.2, restitution: 1.0)\n                bouncingBall.physicsBody = PhysicsBodyComponent(material: material, mode: .dynamic)\n            \n                \/\/ Determine the release velocity vector by comparing the \n                \/\/ previous timestamp and position to the current timestamp \n                \/\/ and position.\n                \n                \/\/ Apply the velocity to the bouncing ball.\n            }\n    ))\n}\n```\n\nThe [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/DragGesture] structure’s initializer,  [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/DragGesture\/init(minimumDistance:coordinateSpace3D:)], is a spatial SwiftUI gesture that supports motion in three dimensions. During the gesture, switch the physics body to [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/PhysicsBodyMode\/kinematic] mode, to allow manual positioning. While dragging, the ball follows your index finger using an [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/AnchorEntity] which ARKit attaches to your right finger tip making it appear as if you are moving the bouncing ball with your hand. On release, switch the physics body to [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/PhysicsBodyMode\/dynamic] mode to re-engage physics simulation on the ball and provide an impulse the app bases on the calculating the velocity from position and time deltas. With this interaction, you can throw the ball in a natural gesture.\n\n\n\n## Define how the ball interacts with the scene\n\nThe key to making the ball interact with the real world is to:\n\n- Enable [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/SpatialTrackingSession\/Configuration\/SceneUnderstandingCapability\/physics] and [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/SpatialTrackingSession\/Configuration\/SceneUnderstandingCapability\/collision] for the `sceneUnderstanding` parameter in the spatial tracking session. RealityKit requires data on the surfaces surrounding the player to determine where the ball will bounce.\n- Provide [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/HasPhysicsBody\/physicsBody] and [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/HasCollision\/collision] components to the bouncing ball entity. The ball derives mass, restitution, and shape from these attributes defining its interaction with the player’s environment.\n\nRealityKit uses the information to detect and create collision geometry for surfaces in the physical environment without explicitly invoking ARKit APIs. Allowing RealityKit to perform this work on your behalf can be convenience if you don’t require the detailed information `ARKit` API provides. The scene understanding happens entirely within the RealityKit physics simulation system. After enabling scene understanding, the ball bounces off real-world surfaces like floors, walls, tables, and ceilings.\n\n## Visualize the physical world with plane anchors\n\nWhile scene understanding provides invisible collision geometry, you might want to visualize the surfaces or create targets to hit. You provide this support by adding [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/AnchorEntity] instances for different plane classifications:\n\n```swift\n\/\/ Create anchor entitites for various objects in the real world.\nlet wallEntity = AnchorEntity(.plane(.vertical, classification: .wall, minimumBounds: [1, 1]))\ncontent.add(wallEntity)\n\nlet floorEntity = AnchorEntity(.plane(.horizontal, classification: .floor, minimumBounds: [1, 1]))\ncontent.add(floorEntity)\n\nlet ceilingEntity = AnchorEntity(.plane(.horizontal, classification: .ceiling, minimumBounds: [1, 1]))\ncontent.add(ceilingEntity)\n\nlet tableEntity = AnchorEntity(.plane(.horizontal, classification: .table, minimumBounds: [0.1, 0.1]))\ncontent.add(tableEntity)\n```\n\nThese anchor entities automatically position themselves in scene space when RealityKit detects planes matching their classification, including walls, floors, ceilings, and tables in the player’s environment. You can access the ARKit plane anchor directly from the entity’s [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/ARKitAnchorComponent] in the event. When necessary, you have access to all the detail ARKit has to provide in the convenient context of RealityKit event handling. To visualize these planes, subscribe to anchor events and add plane entities to your scene:\n\n```swift\n@State var didAnchor: EventSubscription? = nil\n\n\/\/ ...\n\n\/\/ Subscribe to anchor events.\ndidAnchor = content.subscribe(to: AnchorStateEvents.DidAnchor.self, { event in\n    if event.reason == .newAnchor,\n       let anchorComponent = event.entity.components[ARKitAnchorComponent.self],\n       let planeAnchor = anchorComponent.anchor as? PlaneAnchor {\n        \n        \/\/ Create visual plane meshes for the plane anchors for recognition \n        \/\/ by Apple Vision Pro.\n        let planeModel = ModelEntity(\n            mesh: MeshResource.generatePlane(\n                width: planeAnchor.geometry.extent.width,\n                depth: planeAnchor.geometry.extent.height\n            ),\n            materials: [SimpleMaterial(\n                color: UIColor(hue: 0.66, saturation: 0.5, brightness: 1.0, alpha: 0.2),\n                isMetallic: false\n            )]\n        )\n        \n        planeModel.transform = Transform(matrix: planeAnchor.originFromAnchorTransform)\n        \n        if let container = bouncingBall.parent {\n            container.addChild(planeModel)\n        }\n    }\n})\n```\n\nThis subscription creates a semitransparent blue plane mesh for each surface RealityKit detects, providing visual feedback about where the ball can bounce.\n\n## Attach views to spatial entities\n\nYou can attach SwiftUI views directly to RealityKit entities using the `ViewAttachmentComponent` initializer, [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/ViewAttachmentComponent\/init(rootView:)]. In the context of the game example, it allows you to display game point values or other UI elements that track with the planes:\n\n```swift\n\/\/ Attach a `PlaneView` showing the number of points in each registered plane anchor.\nlet wallEntity = AnchorEntity(.plane(.vertical, classification: .wall, minimumBounds: [0.1, 0.1]))\n\nwallEntity.components[ViewAttachmentComponent.self] = ViewAttachmentComponent(\n    rootView: PlaneView(text: \"30 points\")\n)\n\ncontent.add(wallEntity)\n```\n\nThe SwiftUI renders views as part of the 3D scene and automatically positions them with their container entities.\n\n## Observe spatial entities in views\n\nSwiftUI views can observe RealityKit entities in nearly the same manner as SwiftUI `@State` and `@Binding` properties. By creating the bouncing ball entity up one level, at the app level in this case, and passing it to both the immersive space and a tracking view, you can observe changes in the bouncing ball in SwiftUI:\n\n```swift\n@main\nstruct BouncingBallApp: App {\n    @State var appModel = AppModel()\n    \n    \/\/ Make the bouncing ball entity avialable to the immersive view and \n    \/\/ the observing tracking view.\n    @State var bouncingBall = ModelEntity(\n        mesh: MeshResource.generateSphere(radius: ImmersiveView.ballRadius),\n        materials: [SimpleMaterial(color: .red, isMetallic: false)]\n    )\n    \n    var body: some SwiftUI.Scene {\n        WindowGroup {\n            ContentView()\n                .environment(appModel)\n            TrackingView(bouncingBall: bouncingBall)\n                .environment(appModel)\n        }\n        \n        ImmersiveSpace(id: appModel.immersiveSpaceID) {\n            ImmersiveView(bouncingBall: bouncingBall)\n                .environment(appModel)\n        }\n    }\n}\n```\n\n`TrackingView` observes the ball’s position in real time using the entity’s [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/Entity\/observable-swift.property] property:\n\n```swift\nstruct TrackingView: View {\n    let bouncingBall: ModelEntity\n\n    var body: some View {\n        VStack {\n            Button {\n                \/\/ Reset the bouncing ball to its initial state when pressing the button.\n                bouncingBall.position = SIMD3<Float>(x: 0, y: 1.3, z: -0.9)\n                bouncingBall.physicsBody = PhysicsBodyComponent(mode: .kinematic)\n            } label: {\n                Text(\"Reset bouncing ball\")\n            }\n            .disabled(distance() < 5) \/\/ Enable the reset button when the bouncing ball is too far away.\n        }\n        .padding()\n    }\n\n    \/\/\/ Calculate the bouncing ball's distance from the origin.\n    func distance() -> Float {\n        let position = bouncingBall.observable.position\n    \n        return sqrt(position.x * position.x + position.y * position.y + position.z * position.z)\n    }\n}\n```\n\nAs the ball moves in the immersive space, the tracking view automatically updates in the separate window. Explicit use of the `observable` property is the only difference from standard `@State` property observation. When the distance is greater than `5` meters, it can be difficult for the player to recall the ball with the pinch gesture. In this condition, the `TrackingView` enables the bouncing ball’s Reset button, allowing the player to return the ball to its original state. This demonstrates how you can fully integrate RealityKit entities with the SwiftUI observation system, allowing seamless data flow between 3D spatial content and traditional and a 2D UI.\n\n## App construction\n\n- **Creating your first visionOS app**: Build a new visionOS app using SwiftUI and add platform-specific features.\n- **Adding 3D content to your app**: Add depth and dimension to your visionOS app and discover how to incorporate your app’s content into a person’s surroundings.\n- **Creating fully immersive experiences in your app**: Build fully immersive experiences by combining spaces with content you create using RealityKit or Metal.\n- **Drawing sharp layer-based content in visionOS**: Deliver text and vector images at multiple resolutions from custom Core Animation layers in visionOS.\n- **Introductory visionOS samples**: Learn the fundamentals of building apps for visionOS with beginner-friendly sample code projects.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Build a new visionOS app using SwiftUI and add platform-specific features.",
          "name" : "Creating your first visionOS app",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/creating-your-first-visionos-app"
        },
        {
          "description" : "Add depth and dimension to your visionOS app and discover how to incorporate your app’s content into a person’s surroundings.",
          "name" : "Adding 3D content to your app",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/adding-3d-content-to-your-app"
        },
        {
          "description" : "Build fully immersive experiences by combining spaces with content you create using RealityKit or Metal.",
          "name" : "Creating fully immersive experiences in your app",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/creating-fully-immersive-experiences"
        },
        {
          "description" : "Deliver text and vector images at multiple resolutions from custom Core Animation layers in visionOS.",
          "name" : "Drawing sharp layer-based content in visionOS",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/drawing-sharp-layer-based-content"
        },
        {
          "description" : "Learn the fundamentals of building apps for visionOS with beginner-friendly sample code projects.",
          "name" : "Introductory visionOS samples",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/introductory-visionOS-samples"
        }
      ],
      "title" : "App construction"
    }
  ],
  "source" : "appleJSON",
  "title" : "Combining spatial support from multiple frameworks",
  "url" : "https:\/\/developer.apple.com\/documentation\/visionos\/combining-spatial-support-from-multiple-frameworks"
}