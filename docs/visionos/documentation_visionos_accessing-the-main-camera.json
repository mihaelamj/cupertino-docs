{
  "abstract" : "Add camera-based features to enterprise apps.",
  "codeExamples" : [
    {
      "code" : "struct MainCameraView: View {\n    @State private var sessionManager = CameraSessionManager()\n    \n    var body: some View {\n        Group {\n            if CameraSessionManager.isSupported == false {\n                \/\/ ...\n            } else {\n                HStack {\n                    CameraFrameView(preview: sessionManager.leftPreview)\n                    CameraFrameView(preview: sessionManager.rightPreview)\n                }\n            }\n        }\n        .task {\n            await sessionManager.run()\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "final class CameraSessionManager {\n    \n    \/\/ ...\n\n    \/\/\/ Begin reading and rendering the main camera's frames.\n    func run() async {\n        await withTaskGroup(of: Void.self) { group in\n            group.addTask {\n                await self.runCameraFrameProvider()\n            }\n            \n            \/\/ ...\n        }\n    }\n\n    private func runCameraFrameProvider() async {\n        let arkitSession = ARKitSession()\n        \n        \/\/ ...\n        \n        let cameraFrameProvider = CameraFrameProvider()\n        try? await arkitSession.run([cameraFrameProvider])\n       \n        \/\/ ...\n        \n        \/\/ See the next section for the implementation of `observeCameraFrameUpdates`.\n        await observeCameraFrameUpdates()\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "final class CameraSessionManager {\n    \/\/...\n\n    private var leftCameraFeed = CameraFeed()\n    private var rightCameraFeed = CameraFeed()\n\n    \/\/ ...\n    \n    \/\/\/ The layer displaying the left camera preview.\n    var leftPreview: AVSampleBufferDisplayLayer {\n        leftCameraFeed.preview\n    }\n    \n    \/\/\/ The layer displaying the right camera preview.\n    var rightPreview: AVSampleBufferDisplayLayer {\n        rightCameraFeed.preview\n    }\n\n    \/\/...\n\n    private func observeCameraFrameUpdates() async {\n        guard let cameraFrameProvider else { return }\n        \n        \/\/ Find the `CameraVideoFormat` that corresponds to the `CameraConfiguration`.\n        let formats = CameraVideoFormat\n            .supportedVideoFormats(for: .main, cameraPositions: configuration.cameraPositions)\n            .filter({ $0.cameraRectification == configuration.cameraRectification })\n        \n        \/\/ Find the resolution format.\n        let desiredFormat = isHighResolution ?\n        formats.max { $0.frameSize.height < $1.frameSize.height }\n        : formats.min { $0.frameSize.height < $1.frameSize.height }\n\n        \/\/ Request an asynchronous sequence of camera frames.\n        guard let desiredFormat,\n              let cameraFrameUpdates = cameraFrameProvider.cameraFrameUpdates(for: desiredFormat) else {\n            return\n        }\n        \n        for await cameraFrame in cameraFrameUpdates {            \n            if let leftSample = cameraFrame.sample(for: .left) {\n                try? await leftCameraFeed.update(using: leftSample)\n            }\n            \n            if let rightSample = cameraFrame.sample(for: .right) {\n                try? await rightCameraFeed.update(using: rightSample)\n            }\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "final class CameraFeed {\n    \/\/\/ A preview layer that presents the captured video frames.\n    let preview = AVSampleBufferDisplayLayer()\n    \n    \/\/\/ Renders the `pixelBuffer` in the `Sample` to the preview layer.\n    \/\/\/ - Parameters:\n    \/\/\/     - using: The `sample` to render to the preview layer.\n    func update(using sample: CameraFrame.Sample?) async throws {\n        guard let sample else {\n            await preview.sampleBufferRenderer.flush(removingDisplayedImage: true)\n            return\n        }\n\n        if preview.sampleBufferRenderer.requiresFlushToResumeDecoding {\n            preview.sampleBufferRenderer.flush()\n        }\n        \n        let presentationTimeStamp = CMTime(seconds: sample.parameters.captureTimestamp, preferredTimescale: CMTimeScale(NSEC_PER_SEC))\n        let timingInfo = CMSampleTimingInfo(duration: .invalid,\n                                            presentationTimeStamp: presentationTimeStamp,\n                                            decodeTimeStamp: .invalid)\n\n        try? sample.buffer.withUnsafeBuffer { pixelBuffer in\n            let sampleBuffer = try CMSampleBuffer(imageBuffer: pixelBuffer,\n                                                  formatDescription: CMVideoFormatDescription(imageBuffer: pixelBuffer),\n                                                  sampleTiming: timingInfo)\n            if preview.sampleBufferRenderer.isReadyForMoreMediaData {\n                preview.sampleBufferRenderer.enqueue(sampleBuffer)\n            }\n        }\n    }\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "f2a6fa2663049937a021e1864f601295412848f5f1a5175b8dfc225342793630",
  "crawledAt" : "2025-12-02T17:33:32Z",
  "id" : "FAD8C5E7-C611-4408-A01E-F1FFF3CA95D7",
  "kind" : "unknown",
  "language" : "swift",
  "overview" : "## Overview\n\nThis sample code project demonstrates how to use ARKit to access and display the main camera feed in your visionOS app. You can use this functionality to implement computer vision-powered experiences or livestreaming in your enterprise app. For instance, support technicians can livestream their surroundings to remote experts for improved guidance.\n\n### Configure the sample code project\n\nReplace `Enterprise.license` with your license file. The sample app requires a valid license file to display the main camera.\n\n### Request the entitlement\n\nMain camera access is a part of enterprise APIs for visionOS, a collection of APIs that unlock capabilities for enterprise customers. To use main camera access, you need to apply for the [doc:\/\/com.apple.documentation\/documentation\/BundleResources\/Entitlements\/com.apple.developer.arkit.main-camera-access.allow] entitlement. For more information, including how to apply for this entitlement, see [doc:\/\/com.apple.documentation\/documentation\/visionOS\/building-spatial-experiences-for-business-apps-with-enterprise-apis].\n\n### Add usage descriptions for ARKit data access\n\nTo help protect people’s privacy, visionOS limits app access to cameras and other sensors in Apple Vision Pro. You need to add an [doc:\/\/com.apple.documentation\/documentation\/BundleResources\/Information-Property-List\/NSMainCameraUsageDescription] to your app’s information property list file to provide a usage description that explains how your app uses the data those sensors provide. People see this description when your app prompts for access to camera data.\n\n### Access and display main camera frames\n\nThe code example below accesses and displays the main camera feed, which is a stereo camera that consists of left and right cameras. A person can configure the app to display the left, right, or combined stereo frames, with or without rectification, at one of two resolutions.\n\nThe `MainCameraView` renders two instances of `CameraFrameView`, one to display a preview of the left camera feed and another to display a preview of the right camera feed. The left and right previews are instances of [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVSampleBufferDisplayLayer], and `CameraSessionManager` manages their updates. `CameraSessionManager` accesses camera frames using ARKit and renders them to their respective display layers. The `MainCameraView` uses a [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/View\/task(priority:_:)] modifier to run `CameraSessionManager`.\n\nTo access the main camera, `CameraSessionManager` starts an [doc:\/\/com.apple.documentation\/documentation\/ARKit\/ARKitSession] with a [doc:\/\/com.apple.documentation\/documentation\/ARKit\/CameraFrameProvider], and then requests [doc:\/\/com.apple.documentation\/documentation\/ARKit\/CameraFrameProvider\/CameraFrameUpdates] in the format that the person using the app specifies.\n\nARKit delivers a stream of [doc:\/\/com.apple.documentation\/documentation\/ARKit\/CameraFrame] instances, and each frame includes a [doc:\/\/com.apple.documentation\/documentation\/ARKit\/CameraFrame\/Sample]. As each `CameraFrame` arrives, `CameraSessionManager` updates its left and right `CameraFeed` instances using the respective `CameraFrame.Sample`.\n\nThe `CameraFeed` class updates its `preview`, an instance of [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVSampleBufferDisplayLayer], by rendering the [doc:\/\/com.apple.documentation\/documentation\/ARKit\/CameraFrame\/Sample] to the display layer. This is the same instance of [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVSampleBufferDisplayLayer] that `MainCameraView` displays.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/visionos\/accessing-the-main-camera\ncrawled: 2025-12-02T17:33:32Z\n---\n\n# Accessing the main camera\n\n**Sample Code**\n\nAdd camera-based features to enterprise apps.\n\n## Overview\n\nThis sample code project demonstrates how to use ARKit to access and display the main camera feed in your visionOS app. You can use this functionality to implement computer vision-powered experiences or livestreaming in your enterprise app. For instance, support technicians can livestream their surroundings to remote experts for improved guidance.\n\n### Configure the sample code project\n\nReplace `Enterprise.license` with your license file. The sample app requires a valid license file to display the main camera.\n\n### Request the entitlement\n\nMain camera access is a part of enterprise APIs for visionOS, a collection of APIs that unlock capabilities for enterprise customers. To use main camera access, you need to apply for the [doc:\/\/com.apple.documentation\/documentation\/BundleResources\/Entitlements\/com.apple.developer.arkit.main-camera-access.allow] entitlement. For more information, including how to apply for this entitlement, see [doc:\/\/com.apple.documentation\/documentation\/visionOS\/building-spatial-experiences-for-business-apps-with-enterprise-apis].\n\n### Add usage descriptions for ARKit data access\n\nTo help protect people’s privacy, visionOS limits app access to cameras and other sensors in Apple Vision Pro. You need to add an [doc:\/\/com.apple.documentation\/documentation\/BundleResources\/Information-Property-List\/NSMainCameraUsageDescription] to your app’s information property list file to provide a usage description that explains how your app uses the data those sensors provide. People see this description when your app prompts for access to camera data.\n\n\n\n### Access and display main camera frames\n\nThe code example below accesses and displays the main camera feed, which is a stereo camera that consists of left and right cameras. A person can configure the app to display the left, right, or combined stereo frames, with or without rectification, at one of two resolutions.\n\nThe `MainCameraView` renders two instances of `CameraFrameView`, one to display a preview of the left camera feed and another to display a preview of the right camera feed. The left and right previews are instances of [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVSampleBufferDisplayLayer], and `CameraSessionManager` manages their updates. `CameraSessionManager` accesses camera frames using ARKit and renders them to their respective display layers. The `MainCameraView` uses a [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/View\/task(priority:_:)] modifier to run `CameraSessionManager`.\n\n```swift\nstruct MainCameraView: View {\n    @State private var sessionManager = CameraSessionManager()\n    \n    var body: some View {\n        Group {\n            if CameraSessionManager.isSupported == false {\n                \/\/ ...\n            } else {\n                HStack {\n                    CameraFrameView(preview: sessionManager.leftPreview)\n                    CameraFrameView(preview: sessionManager.rightPreview)\n                }\n            }\n        }\n        .task {\n            await sessionManager.run()\n        }\n    }\n}\n```\n\nTo access the main camera, `CameraSessionManager` starts an [doc:\/\/com.apple.documentation\/documentation\/ARKit\/ARKitSession] with a [doc:\/\/com.apple.documentation\/documentation\/ARKit\/CameraFrameProvider], and then requests [doc:\/\/com.apple.documentation\/documentation\/ARKit\/CameraFrameProvider\/CameraFrameUpdates] in the format that the person using the app specifies.\n\n```swift\nfinal class CameraSessionManager {\n    \n    \/\/ ...\n\n    \/\/\/ Begin reading and rendering the main camera's frames.\n    func run() async {\n        await withTaskGroup(of: Void.self) { group in\n            group.addTask {\n                await self.runCameraFrameProvider()\n            }\n            \n            \/\/ ...\n        }\n    }\n\n    private func runCameraFrameProvider() async {\n        let arkitSession = ARKitSession()\n        \n        \/\/ ...\n        \n        let cameraFrameProvider = CameraFrameProvider()\n        try? await arkitSession.run([cameraFrameProvider])\n       \n        \/\/ ...\n        \n        \/\/ See the next section for the implementation of `observeCameraFrameUpdates`.\n        await observeCameraFrameUpdates()\n    }\n}\n```\n\nARKit delivers a stream of [doc:\/\/com.apple.documentation\/documentation\/ARKit\/CameraFrame] instances, and each frame includes a [doc:\/\/com.apple.documentation\/documentation\/ARKit\/CameraFrame\/Sample]. As each `CameraFrame` arrives, `CameraSessionManager` updates its left and right `CameraFeed` instances using the respective `CameraFrame.Sample`.\n\n```swift\nfinal class CameraSessionManager {\n    \/\/...\n\n    private var leftCameraFeed = CameraFeed()\n    private var rightCameraFeed = CameraFeed()\n\n    \/\/ ...\n    \n    \/\/\/ The layer displaying the left camera preview.\n    var leftPreview: AVSampleBufferDisplayLayer {\n        leftCameraFeed.preview\n    }\n    \n    \/\/\/ The layer displaying the right camera preview.\n    var rightPreview: AVSampleBufferDisplayLayer {\n        rightCameraFeed.preview\n    }\n\n    \/\/...\n\n    private func observeCameraFrameUpdates() async {\n        guard let cameraFrameProvider else { return }\n        \n        \/\/ Find the `CameraVideoFormat` that corresponds to the `CameraConfiguration`.\n        let formats = CameraVideoFormat\n            .supportedVideoFormats(for: .main, cameraPositions: configuration.cameraPositions)\n            .filter({ $0.cameraRectification == configuration.cameraRectification })\n        \n        \/\/ Find the resolution format.\n        let desiredFormat = isHighResolution ?\n        formats.max { $0.frameSize.height < $1.frameSize.height }\n        : formats.min { $0.frameSize.height < $1.frameSize.height }\n\n        \/\/ Request an asynchronous sequence of camera frames.\n        guard let desiredFormat,\n              let cameraFrameUpdates = cameraFrameProvider.cameraFrameUpdates(for: desiredFormat) else {\n            return\n        }\n        \n        for await cameraFrame in cameraFrameUpdates {            \n            if let leftSample = cameraFrame.sample(for: .left) {\n                try? await leftCameraFeed.update(using: leftSample)\n            }\n            \n            if let rightSample = cameraFrame.sample(for: .right) {\n                try? await rightCameraFeed.update(using: rightSample)\n            }\n        }\n    }\n}\n```\n\nThe `CameraFeed` class updates its `preview`, an instance of [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVSampleBufferDisplayLayer], by rendering the [doc:\/\/com.apple.documentation\/documentation\/ARKit\/CameraFrame\/Sample] to the display layer. This is the same instance of [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVSampleBufferDisplayLayer] that `MainCameraView` displays.\n\n```swift\nfinal class CameraFeed {\n    \/\/\/ A preview layer that presents the captured video frames.\n    let preview = AVSampleBufferDisplayLayer()\n    \n    \/\/\/ Renders the `pixelBuffer` in the `Sample` to the preview layer.\n    \/\/\/ - Parameters:\n    \/\/\/     - using: The `sample` to render to the preview layer.\n    func update(using sample: CameraFrame.Sample?) async throws {\n        guard let sample else {\n            await preview.sampleBufferRenderer.flush(removingDisplayedImage: true)\n            return\n        }\n\n        if preview.sampleBufferRenderer.requiresFlushToResumeDecoding {\n            preview.sampleBufferRenderer.flush()\n        }\n        \n        let presentationTimeStamp = CMTime(seconds: sample.parameters.captureTimestamp, preferredTimescale: CMTimeScale(NSEC_PER_SEC))\n        let timingInfo = CMSampleTimingInfo(duration: .invalid,\n                                            presentationTimeStamp: presentationTimeStamp,\n                                            decodeTimeStamp: .invalid)\n\n        try? sample.buffer.withUnsafeBuffer { pixelBuffer in\n            let sampleBuffer = try CMSampleBuffer(imageBuffer: pixelBuffer,\n                                                  formatDescription: CMVideoFormatDescription(imageBuffer: pixelBuffer),\n                                                  sampleTiming: timingInfo)\n            if preview.sampleBufferRenderer.isReadyForMoreMediaData {\n                preview.sampleBufferRenderer.enqueue(sampleBuffer)\n            }\n        }\n    }\n}\n```\n\n## Enterprise APIs for visionOS\n\n- **Building spatial experiences for business apps with enterprise APIs for visionOS**: Grant enhanced sensor access and increased platform control to your visionOS app by using entitlements.\n- **Locating and decoding barcodes in 3D space**: Create engaging, hands-free experiences based on barcodes in a person’s surroundings.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Grant enhanced sensor access and increased platform control to your visionOS app by using entitlements.",
          "name" : "Building spatial experiences for business apps with enterprise APIs for visionOS",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/building-spatial-experiences-for-business-apps-with-enterprise-apis"
        },
        {
          "description" : "Create engaging, hands-free experiences based on barcodes in a person’s surroundings.",
          "name" : "Locating and decoding barcodes in 3D space",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/locating-and-decoding-barcodes-in-3d-space"
        }
      ],
      "title" : "Enterprise APIs for visionOS"
    }
  ],
  "source" : "appleJSON",
  "title" : "Accessing the main camera",
  "url" : "https:\/\/developer.apple.com\/documentation\/visionos\/accessing-the-main-camera"
}