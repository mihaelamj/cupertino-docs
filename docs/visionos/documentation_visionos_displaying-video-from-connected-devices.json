{
  "abstract" : "Show video from devices connected with the Developer Strap in your visionOS app.",
  "codeExamples" : [
    {
      "code" : "\/\/ ConnectionManager\n\nprivate let discoverySession = AVCaptureDevice.DiscoverySession(deviceTypes: [.external],\n                                                                mediaType: .video,\n                                                                position: .unspecified)\n\nprivate func updateDeviceList() {\n        \/\/ Transform the `AVCaptureDevice` instances.\n        let devices = discoverySession\n            .devices\n            .map { Device(id: $0.uniqueID, name: $0.localizedName) }\n        \n        ...\n    }",
      "language" : "swift"
    },
    {
      "code" : "\/\/ ConnectionManager\n\nprivate func observeDeviceConnectionStates() {\n    Task {\n        \/\/ Await notification of the system connecting a new device.\n        for await _ in NotificationCenter.default.notifications(named: AVCaptureDevice.wasConnectedNotification) {\n            updateDeviceList()\n        }\n    }\n    \n    Task {\n        \/\/ Await notification of the system disconnecting a device.\n        for await _ in NotificationCenter.default.notifications(named: AVCaptureDevice.wasDisconnectedNotification) {\n            updateDeviceList()\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ ContentView\n\nPicker(\"Device Picker\", selection: $previewManager.selectedDevice) {\n    Text(\"Select Device\").tag(nil as Device?)\n    ForEach(devices) {\n        Text($0.name).tag($0)\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ CaptureManager\n\nprivate let captureSession = AVCaptureSession()\nprivate let videoDataOutput = AVCaptureVideoDataOutput()\n\n...\n\nprivate func setUpSession() {\n    \/\/ Bracket the following configuration in a begin\/commit configuration pair.\n    captureSession.beginConfiguration()\n    defer { captureSession.commitConfiguration() }\n    \n    \/\/ Drop frames that don't render in a timely manner.\n    videoDataOutput.alwaysDiscardsLateVideoFrames = true\n    videoDataOutput.setSampleBufferDelegate(self, queue: sessionQueue)\n    \n    if captureSession.canAddOutput(videoDataOutput) {\n        captureSession.addOutput(videoDataOutput)\n    } else {\n        assertionFailure(\"Unable to add video data output to the capture session.\")\n    }\n}\n\n\/\/\/ Stops capture from the previously selected device and, if provided, begins capture from the provided device.\n\/\/\/ - Parameter device: The device to capture video from, or nil to stop capture altogether.\nfunc select(device: Device?) {\n    \/\/ Bracket the following configuration in a begin\/commit configuration pair.\n    captureSession.beginConfiguration()\n    defer { captureSession.commitConfiguration() }\n    \n    \/\/ Remove previous input, if it exists.\n    for input in captureSession.inputs {\n        captureSession.removeInput(input)\n    }\n    \n    \/\/ Prepare the renderer to receive content from a new device.\n    videoRenderer.flush(removingDisplayedImage: true)\n\n    \/\/ Return early if the passed device is nil.\n    guard let captureDevice = device?.captureDevice else { return }\n    \n    do {\n        let authorizationStatus = AVCaptureDevice.authorizationStatus(for: .video)\n        \n        \/\/\/ In the context of this sample, this check generally passes because `ContentView`\n        \/\/\/ displays a message and terminates when the system denies access to the camera.\n        precondition(authorizationStatus == .authorized,\n                        \"Camera authorization is required to set up a device capture session.\")\n        \n        let input = try AVCaptureDeviceInput(device: captureDevice)\n        \n        \/\/ Add the new input, if possible.\n        if captureSession.canAddInput(input) {\n            captureSession.addInput(input)\n        } else {\n            assertionFailure(\"Unable to add the input to the capture session.\")\n        }\n    } catch {\n        fatalError(\"Unable to create input for the device. \\(error)\")\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ CaptureManager\n\n\/\/\/ Begin the flow of data from the capture session's inputs to its outputs.\nfunc start() {\n    captureSession.startRunning()\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ CaptureManager\n\n\/\/\/ The video renderer from the `AVSampleBufferDisplayLayer`\n\/\/\/ this app uses to display video.\nnonisolated private let videoRenderer: AVSampleBufferVideoRenderer\n\n...\n\nextension CaptureManager: AVCaptureVideoDataOutputSampleBufferDelegate {\n    nonisolated func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {\n        \n        \/\/ If the renderer is ready for more data, queue the sample buffer for presentation.\n        if videoRenderer.isReadyForMoreMediaData {\n            videoRenderer.enqueue(sampleBuffer)\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "struct DevicePreview: UIViewRepresentable {\n    \/*\n     In this sample, `preview` is an instance of `AVSampleBufferDisplayLayer`.\n     `AVCaptureVideoDataOutputSampleBufferDelegate.captureOutput`\n     uses the layer's `sampleBufferRenderer` to enqueue the provided\n     `CMSampleBuffer` for rendering.\n     *\/\n    private let preview: CALayer\n\n    init(preview: CALayer) {\n        self.preview = preview\n    }\n    \n    func makeUIView(context: Context) -> SampleBufferPreview {\n        SampleBufferPreview(preview: preview)\n    }\n    \n    func updateUIView(_ previewView: SampleBufferPreview, context: Context) {\n        \/\/ Updates the state of the specified view with new information from SwiftUI.\n    }\n\n    class SampleBufferPreview: UIView {\n\n        let preview: CALayer\n\n        init(preview: CALayer) {\n            self.preview = preview\n            super.init(frame: .zero)\n            layer.addSublayer(preview)\n        }\n        \n        required init?(coder: NSCoder) {\n            fatalError(\"init(coder:) hasn't been implemented\")\n        }\n\n        override func layoutSubviews() {\n            preview.frame = bounds\n        }\n    }\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "f76de6b5dd1f48cff47cefd140438c64039cd2e6f4c8db5631e8b705658b6e47",
  "crawledAt" : "2025-12-02T17:33:15Z",
  "id" : "E6DAFCBB-C152-4CB9-8A07-CC99A8CD0FBA",
  "kind" : "unknown",
  "language" : "swift",
  "overview" : "## Overview\n\nApple’s audiovisual frameworks allow your visionOS app to access video from USB video class (UVC) devices connected with the [https:\/\/developer.apple.com\/visionos\/developer-strap\/purchase] for Apple Vision Pro. You can use this functionality to display real-time video in your app. For example, a medical professional can view the output from an endoscopic camera during a procedure. This article outlines the requirements to access UVC devices in visionOS. The sample code project shows a picker for each device connected to Apple Vision Pro, and displays the selected device’s video feed.\n\n### Add usage descriptions for camera access\n\nTo help protect people’s privacy, visionOS limits app access to cameras and other sensors in Apple Vision Pro. You need to add an [doc:\/\/com.apple.documentation\/documentation\/BundleResources\/Information-Property-List\/NSCameraUsageDescription] to your app’s information property list file to provide a usage description that explains how your app uses the data those sensors provide. People see this description when your app prompts for access to camera data.\n\n### Create the device picker\n\nUse an [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureDevice\/DiscoverySession] to obtain an array of connected devices.\n\nNext, observe [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureDevice\/wasConnectedNotification] and [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureDevice\/wasDisconnectedNotification] to update the array when a device connects or disconnects.\n\nRender a picker with an option for each device.\n\n### Display the selected device’s video feed\n\nConfigure an [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureSession] to capture [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureDeviceInput] from the selected device and output it to an [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureVideoDataOutput].\n\nCall [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureSession\/startRunning()] on the capture session to start the flow of data from the capture session’s inputs to its outputs.\n\n`AVCaptureSession` delivers a steady stream of updates to the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureVideoDataOutputSampleBufferDelegate] assigned to the `AVCaptureVideoDataOutput`. Each update includes a [doc:\/\/com.apple.documentation\/documentation\/CoreMedia\/CMSampleBuffer] that contains the latest video frame from the device. Render the `CMSampleBuffer` to an [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVSampleBufferDisplayLayer] using the layer’s [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVSampleBufferVideoRenderer].\n\nAdd the `AVSampleBufferDisplayLayer` to a [doc:\/\/com.apple.documentation\/documentation\/UIKit\/UIView] and use a [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/UIViewRepresentable] to display the `UIView` in a SwiftUI view.\n\n### Display a prompt when denying access to the camera\n\nIf the person denies camera access, the sample app prompts them to grant access in the Settings app. For more information about providing camera access in your app, see [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/requesting-authorization-to-capture-and-save-media].",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/visionos\/displaying-video-from-connected-devices\ncrawled: 2025-12-02T17:33:15Z\n---\n\n# Displaying video from connected devices\n\n**Sample Code**\n\nShow video from devices connected with the Developer Strap in your visionOS app.\n\n## Overview\n\nApple’s audiovisual frameworks allow your visionOS app to access video from USB video class (UVC) devices connected with the [https:\/\/developer.apple.com\/visionos\/developer-strap\/purchase] for Apple Vision Pro. You can use this functionality to display real-time video in your app. For example, a medical professional can view the output from an endoscopic camera during a procedure. This article outlines the requirements to access UVC devices in visionOS. The sample code project shows a picker for each device connected to Apple Vision Pro, and displays the selected device’s video feed.\n\n### Add usage descriptions for camera access\n\nTo help protect people’s privacy, visionOS limits app access to cameras and other sensors in Apple Vision Pro. You need to add an [doc:\/\/com.apple.documentation\/documentation\/BundleResources\/Information-Property-List\/NSCameraUsageDescription] to your app’s information property list file to provide a usage description that explains how your app uses the data those sensors provide. People see this description when your app prompts for access to camera data.\n\n### Create the device picker\n\nUse an [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureDevice\/DiscoverySession] to obtain an array of connected devices.\n\n```swift\n\/\/ ConnectionManager\n\nprivate let discoverySession = AVCaptureDevice.DiscoverySession(deviceTypes: [.external],\n                                                                mediaType: .video,\n                                                                position: .unspecified)\n\nprivate func updateDeviceList() {\n        \/\/ Transform the `AVCaptureDevice` instances.\n        let devices = discoverySession\n            .devices\n            .map { Device(id: $0.uniqueID, name: $0.localizedName) }\n        \n        ...\n    }\n```\n\nNext, observe [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureDevice\/wasConnectedNotification] and [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureDevice\/wasDisconnectedNotification] to update the array when a device connects or disconnects.\n\n```swift\n\/\/ ConnectionManager\n\nprivate func observeDeviceConnectionStates() {\n    Task {\n        \/\/ Await notification of the system connecting a new device.\n        for await _ in NotificationCenter.default.notifications(named: AVCaptureDevice.wasConnectedNotification) {\n            updateDeviceList()\n        }\n    }\n    \n    Task {\n        \/\/ Await notification of the system disconnecting a device.\n        for await _ in NotificationCenter.default.notifications(named: AVCaptureDevice.wasDisconnectedNotification) {\n            updateDeviceList()\n        }\n    }\n}\n```\n\nRender a picker with an option for each device.\n\n```swift\n\/\/ ContentView\n\nPicker(\"Device Picker\", selection: $previewManager.selectedDevice) {\n    Text(\"Select Device\").tag(nil as Device?)\n    ForEach(devices) {\n        Text($0.name).tag($0)\n    }\n}\n```\n\n### Display the selected device’s video feed\n\nConfigure an [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureSession] to capture [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureDeviceInput] from the selected device and output it to an [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureVideoDataOutput].\n\n```swift\n\/\/ CaptureManager\n\nprivate let captureSession = AVCaptureSession()\nprivate let videoDataOutput = AVCaptureVideoDataOutput()\n\n...\n\nprivate func setUpSession() {\n    \/\/ Bracket the following configuration in a begin\/commit configuration pair.\n    captureSession.beginConfiguration()\n    defer { captureSession.commitConfiguration() }\n    \n    \/\/ Drop frames that don't render in a timely manner.\n    videoDataOutput.alwaysDiscardsLateVideoFrames = true\n    videoDataOutput.setSampleBufferDelegate(self, queue: sessionQueue)\n    \n    if captureSession.canAddOutput(videoDataOutput) {\n        captureSession.addOutput(videoDataOutput)\n    } else {\n        assertionFailure(\"Unable to add video data output to the capture session.\")\n    }\n}\n\n\/\/\/ Stops capture from the previously selected device and, if provided, begins capture from the provided device.\n\/\/\/ - Parameter device: The device to capture video from, or nil to stop capture altogether.\nfunc select(device: Device?) {\n    \/\/ Bracket the following configuration in a begin\/commit configuration pair.\n    captureSession.beginConfiguration()\n    defer { captureSession.commitConfiguration() }\n    \n    \/\/ Remove previous input, if it exists.\n    for input in captureSession.inputs {\n        captureSession.removeInput(input)\n    }\n    \n    \/\/ Prepare the renderer to receive content from a new device.\n    videoRenderer.flush(removingDisplayedImage: true)\n\n    \/\/ Return early if the passed device is nil.\n    guard let captureDevice = device?.captureDevice else { return }\n    \n    do {\n        let authorizationStatus = AVCaptureDevice.authorizationStatus(for: .video)\n        \n        \/\/\/ In the context of this sample, this check generally passes because `ContentView`\n        \/\/\/ displays a message and terminates when the system denies access to the camera.\n        precondition(authorizationStatus == .authorized,\n                        \"Camera authorization is required to set up a device capture session.\")\n        \n        let input = try AVCaptureDeviceInput(device: captureDevice)\n        \n        \/\/ Add the new input, if possible.\n        if captureSession.canAddInput(input) {\n            captureSession.addInput(input)\n        } else {\n            assertionFailure(\"Unable to add the input to the capture session.\")\n        }\n    } catch {\n        fatalError(\"Unable to create input for the device. \\(error)\")\n    }\n}\n```\n\nCall [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureSession\/startRunning()] on the capture session to start the flow of data from the capture session’s inputs to its outputs.\n\n```swift\n\/\/ CaptureManager\n\n\/\/\/ Begin the flow of data from the capture session's inputs to its outputs.\nfunc start() {\n    captureSession.startRunning()\n}\n```\n\n`AVCaptureSession` delivers a steady stream of updates to the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureVideoDataOutputSampleBufferDelegate] assigned to the `AVCaptureVideoDataOutput`. Each update includes a [doc:\/\/com.apple.documentation\/documentation\/CoreMedia\/CMSampleBuffer] that contains the latest video frame from the device. Render the `CMSampleBuffer` to an [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVSampleBufferDisplayLayer] using the layer’s [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVSampleBufferVideoRenderer].\n\n```swift\n\/\/ CaptureManager\n\n\/\/\/ The video renderer from the `AVSampleBufferDisplayLayer`\n\/\/\/ this app uses to display video.\nnonisolated private let videoRenderer: AVSampleBufferVideoRenderer\n\n...\n\nextension CaptureManager: AVCaptureVideoDataOutputSampleBufferDelegate {\n    nonisolated func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {\n        \n        \/\/ If the renderer is ready for more data, queue the sample buffer for presentation.\n        if videoRenderer.isReadyForMoreMediaData {\n            videoRenderer.enqueue(sampleBuffer)\n        }\n    }\n}\n```\n\nAdd the `AVSampleBufferDisplayLayer` to a [doc:\/\/com.apple.documentation\/documentation\/UIKit\/UIView] and use a [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/UIViewRepresentable] to display the `UIView` in a SwiftUI view.\n\n```swift\nstruct DevicePreview: UIViewRepresentable {\n    \/*\n     In this sample, `preview` is an instance of `AVSampleBufferDisplayLayer`.\n     `AVCaptureVideoDataOutputSampleBufferDelegate.captureOutput`\n     uses the layer's `sampleBufferRenderer` to enqueue the provided\n     `CMSampleBuffer` for rendering.\n     *\/\n    private let preview: CALayer\n\n    init(preview: CALayer) {\n        self.preview = preview\n    }\n    \n    func makeUIView(context: Context) -> SampleBufferPreview {\n        SampleBufferPreview(preview: preview)\n    }\n    \n    func updateUIView(_ previewView: SampleBufferPreview, context: Context) {\n        \/\/ Updates the state of the specified view with new information from SwiftUI.\n    }\n\n    class SampleBufferPreview: UIView {\n\n        let preview: CALayer\n\n        init(preview: CALayer) {\n            self.preview = preview\n            super.init(frame: .zero)\n            layer.addSublayer(preview)\n        }\n        \n        required init?(coder: NSCoder) {\n            fatalError(\"init(coder:) hasn't been implemented\")\n        }\n\n        override func layoutSubviews() {\n            preview.frame = bounds\n        }\n    }\n}\n```\n\n### Display a prompt when denying access to the camera\n\nIf the person denies camera access, the sample app prompts them to grant access in the Settings app. For more information about providing camera access in your app, see [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/requesting-authorization-to-capture-and-save-media].\n\n## Video playback\n\n- **Destination Video**: Leverage SwiftUI to build an immersive media experience in a multiplatform app.\n- **Playing immersive media with RealityKit**: Create an immersive video playback experience with RealityKit.\n- **Rendering stereoscopic video with RealityKit**: Render stereoscopic video in visionOS with RealityKit.\n- **Creating a multiview video playback experience in visionOS**: Build an interface that plays multiple videos simultaneously and handles transitions to different experience types gracefully.\n- **Configuring your app for media playback**: Configure apps to enable standard media playback behavior.\n- **Adopting the system player interface in visionOS**: Provide an optimized viewing experience for watching 3D video content.\n- **Controlling the transport behavior of a player**: Play, pause, and seek through a media presentation.\n- **Monitoring playback progress in your app**: Observe the playback of a media asset to update your app’s user-interface state.\n- **Trimming and exporting media in visionOS**: Display standard controls in your app to edit the timeline of the currently playing media.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Leverage SwiftUI to build an immersive media experience in a multiplatform app.",
          "name" : "Destination Video",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/destination-video"
        },
        {
          "description" : "Create an immersive video playback experience with RealityKit.",
          "name" : "Playing immersive media with RealityKit",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/playing-immersive-media-with-realitykit"
        },
        {
          "description" : "Render stereoscopic video in visionOS with RealityKit.",
          "name" : "Rendering stereoscopic video with RealityKit",
          "url" : "https:\/\/developer.apple.com\/documentation\/RealityKit\/rendering-stereoscopic-video-with-realitykit"
        },
        {
          "description" : "Build an interface that plays multiple videos simultaneously and handles transitions to different experience types gracefully.",
          "name" : "Creating a multiview video playback experience in visionOS",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVKit\/creating-a-multiview-video-playback-experience-in-visionos"
        },
        {
          "description" : "Configure apps to enable standard media playback behavior.",
          "name" : "Configuring your app for media playback",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/configuring-your-app-for-media-playback"
        },
        {
          "description" : "Provide an optimized viewing experience for watching 3D video content.",
          "name" : "Adopting the system player interface in visionOS",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVKit\/adopting-the-system-player-interface-in-visionos"
        },
        {
          "description" : "Play, pause, and seek through a media presentation.",
          "name" : "Controlling the transport behavior of a player",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/controlling-the-transport-behavior-of-a-player"
        },
        {
          "description" : "Observe the playback of a media asset to update your app’s user-interface state.",
          "name" : "Monitoring playback progress in your app",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVFoundation\/monitoring-playback-progress-in-your-app"
        },
        {
          "description" : "Display standard controls in your app to edit the timeline of the currently playing media.",
          "name" : "Trimming and exporting media in visionOS",
          "url" : "https:\/\/developer.apple.com\/documentation\/AVKit\/trimming-and-exporting-media-in-visionos"
        }
      ],
      "title" : "Video playback"
    }
  ],
  "source" : "appleJSON",
  "title" : "Displaying video from connected devices",
  "url" : "https:\/\/developer.apple.com\/documentation\/visionos\/displaying-video-from-connected-devices"
}