{
  "abstract" : "Import a reference object file to track a real-world object in your visionOS app.",
  "codeExamples" : [
    {
      "code" : "\/\/ ImmersiveView.swift\nstruct ImmersiveView: View {\n    var body: some View {\n        RealityView { content in\n            \/\/ Find the URL of the reference object file.\n            let objectURL = Bundle.main.url(forResource: \"VintageGramophone\", withExtension: \".referenceobject\")!\n            \n            \/\/ Create an object-anchoring entity.\n            let anchorSource = AnchoringComponent.ObjectAnchoringSource(objectURL)\n            let objectAnchor = AnchorEntity(.referenceObject(from: anchorSource))\n            content.add(objectAnchor)\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "import ARKit\n\n\/\/ Use the same reference object file URL to load the object. \nguard let referenceObject = try? await ReferenceObject(from: objectURL) else {\n    print(\"Failed to load reference object\")\n    return\n}\n\n\/\/ Load the USDZ file from your reference object.\nguard let modelURL = referenceObject.usdzFile else {\n    print(\"The USDZ file isn't bundled in the reference object\")\n    return\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Load the `modelEntity` with the `modelURL` containing the USDZ file.\nlet modelEntity = try! await ModelEntity(contentsOf: modelURL)\n\n\/\/ Swap out the material of the `modelEntity` with `OcclusionMaterial`.\nmodelEntity.model?.materials = [OcclusionMaterial()]\nobjectAnchor.addChild(modelEntity)",
      "language" : "swift"
    },
    {
      "code" : "RealityView { content, attachments in\n        \/\/ Set up the object-tracking `AnchorEntity`.\n\n        \/\/ Add an attachment to the object anchor.\n        if let attachment = attachments.entity(for: \"TextPrompt\") {\n            objectAnchor.addChild(attachment)\n        }\n    \n} attachments: {\n        Attachment(id: \"TextPrompt\") {\n            Text(\"Hello World\")\n        }\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ AppModel.swift\nimport RealityKit\n\n@MainActor\n@Observable\n\nclass AppModel {\n    \/\/ Other existing properties in `AppModel`.\n\n    let spatialTrackingSession = SpatialTrackingSession()\n} ",
      "language" : "swift"
    },
    {
      "code" : "struct ImmersiveView: View {\n    @Environment(AppModel.self) private var appModel\n\n    var body: some View {\n        RealityView { content in\n            \/\/ Initialize a spatial-tracking session.\n            let configuration = SpatialTrackingSession.Configuration(tracking: [.object])\n            await appModel.spatialTrackingSession.run(configuration)\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "let objectAnchor = AnchorEntity(.referenceObject(from: anchorSource), trackingMode: .continuous, physicsSimulation: .none)",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Create a `MeshResource` to define a high-level representation of your model. \nlet referenceModel: MeshResource = modelEntity.model!.mesh\n\n\/\/  Use `ShapeResource` to create the shape of your model to apply collision to. \nlet referenceShape= try! await ShapeResource.generateConvex(from: referenceModel)\nmodelEntity.components.set(CollisionComponent(shapes: [referenceShape], mode: .colliding))",
      "language" : "swift"
    },
    {
      "code" : "modelEntity.components.set(InputTargetComponent(allowedInputTypes: .all))",
      "language" : "swift"
    },
    {
      "code" : "modelEntity.components.set(HoverEffectComponent())",
      "language" : "swift"
    }
  ],
  "contentHash" : "d986f68880e0c4f149f0c67c30aaa9c878465e911c5114c33f2b45c7d55d4246",
  "crawledAt" : "2025-12-02T17:35:17Z",
  "id" : "C3E2F9D1-D789-4EF2-8F41-469898244925",
  "kind" : "article",
  "language" : "swift",
  "overview" : "## Overview\n\nYou can use reference objects in your visionOS app to track real-world objects in a person’s surroundings. With this capability, you can attach virtual content to those real-world objects to create engaging experiences. For example, with a reference object, you can track a household item and add virtual content to it, like highlighting a part and showing how to fix it.\n\nTo create a reference object, first, obtain a 3D model asset, train a machine learning model with that 3D model asset in Create ML, and then import the resulting reference object file into your app. You can use RealityKit to import the reference object file, track objects, and add virtual content for interactiveness.\n\nFor more information about creating a reference object file, see [doc:\/\/com.apple.visionOS\/documentation\/visionOS\/implementing-object-tracking-in-your-visionOS-app].\n\n### Add your reference object and track it with an anchoring component\n\nAfter generating the `.referenceobject` file with Create ML, add the reference object file to your Xcode project’s main bundle. Then, create an [https:\/\/developer.apple.com\/documentation\/realitykit\/anchoringcomponent] to track the position of the reference object file. This allows the system to move the anchoring component to follow your real-world object, and to anchor virtual content to it. Using this `AnchoringComponent` is equivalent to setting the Anchoring option on your component in Reality Composer Pro.\n\nIn the `ImmersiveView` file, initialize either an `AnchoringComponent` or an [https:\/\/developer.apple.com\/documentation\/realitykit\/anchorentity] with the [https:\/\/developer.apple.com\/documentation\/realitykit\/anchoringcomponent\/target-swift.enum\/referenceobject(from:)] target, and create an [https:\/\/developer.apple.com\/documentation\/realitykit\/anchoringcomponent\/objectanchoringsource] to configure the anchoring.\n\n### Import the USDZ file to visualize the model\n\nReference object files can optionally bundle with the USDZ file that Create ML uses for training. Adding the USDZ file directly, even if your reference object doesn’t have the model file packaged within it, lets you directly load the `objectURL` into your project. Retrieve this USDZ file through ARKit and use it as a guide for RealityKit content creation.\n\n### Add occlusion to your object\n\nThe RealityKit [https:\/\/developer.apple.com\/documentation\/realitykit\/occlusionmaterial\/] provides a way to render an entity that blocks and occludes other entities to reveal camera passthrough. This is useful in creating the correct depth perception between the real-world object the system tracks and the virtual content you add to your app. You can use the imported USDZ file from your reference object file to create a [https:\/\/developer.apple.com\/documentation\/realitykit\/modelentity] that generates a virtual representation of your physical object, so you can add occlusion behavior to it.\n\n\n\n### Add attachments to the model\n\nBefore adding interaction, incorporate guidance in your app so that anyone can navigate through using it. Use [https:\/\/developer.apple.com\/documentation\/realitykit\/realityview\/init(make:update:attachments:)] to combine SwiftUI content with RealityKit content to create these guiding features. Attach SwiftUI labels and interfaces to the object-tracking entities.\n\n\n\n### Add interaction and virtual content to your app\n\nRealityKit provides a seamless way to incorporate virtual content within your app. Use virtual effects to increase interaction with features like occlusion, physics simulation, collisions, and SwiftUI gestures. To add effects, use ARKit to load the model and add content on top of the real-world object.\n\n### Access the transform with a tracking session\n\nBefore adding any interaction or virtual content to your app, you have the option to configure a [https:\/\/developer.apple.com\/documentation\/realitykit\/spatialtrackingsession\/].  A `SpatialTrackingSession` requests world-tracking permission from the user, and provides the accurate position and rotation of the object.\n\nCreating an `AnchoringComponent` allows the system to properly track and anchor virtual content to the object. However, due to privacy reasons, transform information for the component isn’t available without user authorization. The transform information allows you to track the accurate position and orientation in the real world.\n\nTo ask for world-tracking authorization from the user, set up the `SpatialTrackingSession` with [https:\/\/developer.apple.com\/documentation\/realitykit\/spatialtrackingsession\/configuration\/anchorcapability\/object] anchor capability. If you’re using the ARKit framework in your app, this authorization is always required.\n\nTo set up a `SpatialTrackingSession` that your app shares, configure the session within `AppModel`.\n\nIn the `ImmersiveView` file, configure the `SpatialTrackingSession` to begin tracking the object.\n\n### Set up collision and physics simulation on your objects\n\nIncorporate virtual actions into your app by adding collisions and physics simulations.\n\nTo enable collision and physics on your object-tracking entity, configure the [https:\/\/developer.apple.com\/documentation\/realitykit\/anchoringcomponent\/physicssimulation-swift.enum] property of your `AnchorEntity`. This property allows you to select entities with an `AnchoringComponent`  to interact with each other physically.\n\nWith your `PhysicsSimulation` in place, add a [https:\/\/developer.apple.com\/documentation\/realitykit\/collisioncomponent] with the object’s shape. This creates a collision effect with other entities that are also collision components.\n\nWith the collision feature in place, use [https:\/\/developer.apple.com\/documentation\/realitykit\/collisionevents] to detect collisions when they begin, update, and end for your entities.\n\n### Configure interaction with gestures\n\nTo respond to SwiftUI gestures like [https:\/\/developer.apple.com\/documentation\/swiftui\/spatialtapgesture\/], [https:\/\/developer.apple.com\/documentation\/swiftui\/draggesture\/], and [https:\/\/developer.apple.com\/documentation\/swiftui\/rotategesture3d\/], the object-anchored entity needs to have a `CollisionComponent` for input detection and an [https:\/\/developer.apple.com\/documentation\/realitykit\/inputtargetcomponent\/] to handle all forms of input in your app.\n\nAdditionally, to make the object respond to a person’s gaze with a highlight, add a [https:\/\/developer.apple.com\/documentation\/realitykit\/hovereffectcomponent\/].",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/visionOS\/using-a-reference-object-with-realitykit\ncrawled: 2025-12-02T17:35:17Z\n---\n\n# Using a reference object with RealityKit\n\n**Article**\n\nImport a reference object file to track a real-world object in your visionOS app.\n\n## Overview\n\nYou can use reference objects in your visionOS app to track real-world objects in a person’s surroundings. With this capability, you can attach virtual content to those real-world objects to create engaging experiences. For example, with a reference object, you can track a household item and add virtual content to it, like highlighting a part and showing how to fix it.\n\nTo create a reference object, first, obtain a 3D model asset, train a machine learning model with that 3D model asset in Create ML, and then import the resulting reference object file into your app. You can use RealityKit to import the reference object file, track objects, and add virtual content for interactiveness.\n\nFor more information about creating a reference object file, see [doc:\/\/com.apple.visionOS\/documentation\/visionOS\/implementing-object-tracking-in-your-visionOS-app].\n\n### Add your reference object and track it with an anchoring component\n\nAfter generating the `.referenceobject` file with Create ML, add the reference object file to your Xcode project’s main bundle. Then, create an [https:\/\/developer.apple.com\/documentation\/realitykit\/anchoringcomponent] to track the position of the reference object file. This allows the system to move the anchoring component to follow your real-world object, and to anchor virtual content to it. Using this `AnchoringComponent` is equivalent to setting the Anchoring option on your component in Reality Composer Pro.\n\nIn the `ImmersiveView` file, initialize either an `AnchoringComponent` or an [https:\/\/developer.apple.com\/documentation\/realitykit\/anchorentity] with the [https:\/\/developer.apple.com\/documentation\/realitykit\/anchoringcomponent\/target-swift.enum\/referenceobject(from:)] target, and create an [https:\/\/developer.apple.com\/documentation\/realitykit\/anchoringcomponent\/objectanchoringsource] to configure the anchoring.\n\n```swift\n\/\/ ImmersiveView.swift\nstruct ImmersiveView: View {\n    var body: some View {\n        RealityView { content in\n            \/\/ Find the URL of the reference object file.\n            let objectURL = Bundle.main.url(forResource: \"VintageGramophone\", withExtension: \".referenceobject\")!\n            \n            \/\/ Create an object-anchoring entity.\n            let anchorSource = AnchoringComponent.ObjectAnchoringSource(objectURL)\n            let objectAnchor = AnchorEntity(.referenceObject(from: anchorSource))\n            content.add(objectAnchor)\n        }\n    }\n}\n```\n\n### Import the USDZ file to visualize the model\n\nReference object files can optionally bundle with the USDZ file that Create ML uses for training. Adding the USDZ file directly, even if your reference object doesn’t have the model file packaged within it, lets you directly load the `objectURL` into your project. Retrieve this USDZ file through ARKit and use it as a guide for RealityKit content creation.\n\n```swift\nimport ARKit\n\n\/\/ Use the same reference object file URL to load the object. \nguard let referenceObject = try? await ReferenceObject(from: objectURL) else {\n    print(\"Failed to load reference object\")\n    return\n}\n\n\/\/ Load the USDZ file from your reference object.\nguard let modelURL = referenceObject.usdzFile else {\n    print(\"The USDZ file isn't bundled in the reference object\")\n    return\n}\n```\n\n### Add occlusion to your object\n\nThe RealityKit [https:\/\/developer.apple.com\/documentation\/realitykit\/occlusionmaterial\/] provides a way to render an entity that blocks and occludes other entities to reveal camera passthrough. This is useful in creating the correct depth perception between the real-world object the system tracks and the virtual content you add to your app. You can use the imported USDZ file from your reference object file to create a [https:\/\/developer.apple.com\/documentation\/realitykit\/modelentity] that generates a virtual representation of your physical object, so you can add occlusion behavior to it.\n\n\n\n```swift\n\/\/ Load the `modelEntity` with the `modelURL` containing the USDZ file.\nlet modelEntity = try! await ModelEntity(contentsOf: modelURL)\n\n\/\/ Swap out the material of the `modelEntity` with `OcclusionMaterial`.\nmodelEntity.model?.materials = [OcclusionMaterial()]\nobjectAnchor.addChild(modelEntity)\n```\n\n### Add attachments to the model\n\nBefore adding interaction, incorporate guidance in your app so that anyone can navigate through using it. Use [https:\/\/developer.apple.com\/documentation\/realitykit\/realityview\/init(make:update:attachments:)] to combine SwiftUI content with RealityKit content to create these guiding features. Attach SwiftUI labels and interfaces to the object-tracking entities.\n\n\n\n```swift\nRealityView { content, attachments in\n        \/\/ Set up the object-tracking `AnchorEntity`.\n\n        \/\/ Add an attachment to the object anchor.\n        if let attachment = attachments.entity(for: \"TextPrompt\") {\n            objectAnchor.addChild(attachment)\n        }\n    \n} attachments: {\n        Attachment(id: \"TextPrompt\") {\n            Text(\"Hello World\")\n        }\n}\n```\n\n### Add interaction and virtual content to your app\n\nRealityKit provides a seamless way to incorporate virtual content within your app. Use virtual effects to increase interaction with features like occlusion, physics simulation, collisions, and SwiftUI gestures. To add effects, use ARKit to load the model and add content on top of the real-world object.\n\n### Access the transform with a tracking session\n\nBefore adding any interaction or virtual content to your app, you have the option to configure a [https:\/\/developer.apple.com\/documentation\/realitykit\/spatialtrackingsession\/].  A `SpatialTrackingSession` requests world-tracking permission from the user, and provides the accurate position and rotation of the object.\n\nCreating an `AnchoringComponent` allows the system to properly track and anchor virtual content to the object. However, due to privacy reasons, transform information for the component isn’t available without user authorization. The transform information allows you to track the accurate position and orientation in the real world.\n\nTo ask for world-tracking authorization from the user, set up the `SpatialTrackingSession` with [https:\/\/developer.apple.com\/documentation\/realitykit\/spatialtrackingsession\/configuration\/anchorcapability\/object] anchor capability. If you’re using the ARKit framework in your app, this authorization is always required.\n\nTo set up a `SpatialTrackingSession` that your app shares, configure the session within `AppModel`.\n\n```swift\n\/\/ AppModel.swift\nimport RealityKit\n\n@MainActor\n@Observable\n\nclass AppModel {\n    \/\/ Other existing properties in `AppModel`.\n\n    let spatialTrackingSession = SpatialTrackingSession()\n} \n```\n\nIn the `ImmersiveView` file, configure the `SpatialTrackingSession` to begin tracking the object.\n\n```swift\nstruct ImmersiveView: View {\n    @Environment(AppModel.self) private var appModel\n\n    var body: some View {\n        RealityView { content in\n            \/\/ Initialize a spatial-tracking session.\n            let configuration = SpatialTrackingSession.Configuration(tracking: [.object])\n            await appModel.spatialTrackingSession.run(configuration)\n        }\n    }\n}\n```\n\n### Set up collision and physics simulation on your objects\n\nIncorporate virtual actions into your app by adding collisions and physics simulations.\n\n\n\nTo enable collision and physics on your object-tracking entity, configure the [https:\/\/developer.apple.com\/documentation\/realitykit\/anchoringcomponent\/physicssimulation-swift.enum] property of your `AnchorEntity`. This property allows you to select entities with an `AnchoringComponent`  to interact with each other physically.\n\n```swift\nlet objectAnchor = AnchorEntity(.referenceObject(from: anchorSource), trackingMode: .continuous, physicsSimulation: .none)\n```\n\nWith your `PhysicsSimulation` in place, add a [https:\/\/developer.apple.com\/documentation\/realitykit\/collisioncomponent] with the object’s shape. This creates a collision effect with other entities that are also collision components.\n\n```swift\n\/\/ Create a `MeshResource` to define a high-level representation of your model. \nlet referenceModel: MeshResource = modelEntity.model!.mesh\n\n\/\/  Use `ShapeResource` to create the shape of your model to apply collision to. \nlet referenceShape= try! await ShapeResource.generateConvex(from: referenceModel)\nmodelEntity.components.set(CollisionComponent(shapes: [referenceShape], mode: .colliding))\n```\n\nWith the collision feature in place, use [https:\/\/developer.apple.com\/documentation\/realitykit\/collisionevents] to detect collisions when they begin, update, and end for your entities.\n\n### Configure interaction with gestures\n\nTo respond to SwiftUI gestures like [https:\/\/developer.apple.com\/documentation\/swiftui\/spatialtapgesture\/], [https:\/\/developer.apple.com\/documentation\/swiftui\/draggesture\/], and [https:\/\/developer.apple.com\/documentation\/swiftui\/rotategesture3d\/], the object-anchored entity needs to have a `CollisionComponent` for input detection and an [https:\/\/developer.apple.com\/documentation\/realitykit\/inputtargetcomponent\/] to handle all forms of input in your app.\n\n```swift\nmodelEntity.components.set(InputTargetComponent(allowedInputTypes: .all))\n```\n\nAdditionally, to make the object respond to a person’s gaze with a highlight, add a [https:\/\/developer.apple.com\/documentation\/realitykit\/hovereffectcomponent\/].\n\n```swift\nmodelEntity.components.set(HoverEffectComponent())\n```\n\n## Object tracking within an app\n\n- **Using a reference object with Reality Composer Pro**: Import a reference object file to track a real-world object in your visionOS app.\n- **Using a reference object with ARKit**: Import a reference object file and track a real-world object in your visionOS app.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Import a reference object file to track a real-world object in your visionOS app.",
          "name" : "Using a reference object with Reality Composer Pro",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/using-a-reference-object-with-reality-composer-pro"
        },
        {
          "description" : "Import a reference object file and track a real-world object in your visionOS app.",
          "name" : "Using a reference object with ARKit",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/using-a-reference-object-with-arkit"
        }
      ],
      "title" : "Object tracking within an app"
    }
  ],
  "source" : "appleJSON",
  "title" : "Using a reference object with RealityKit",
  "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/using-a-reference-object-with-realitykit"
}