{
  "abstract" : "Create and adjust spatial audio in visionOS with RealityKit.",
  "codeExamples" : [

  ],
  "contentHash" : "c4eca80e9a0b72c33110f2fe246d9895ad48551114f48e60c116a18bbcb89805",
  "crawledAt" : "2025-12-02T17:49:01Z",
  "declaration" : {
    "code" : "var audioSource: some View {\n    RealityView { content in\n        \/\/ Add the entity to the `RealityView`.\n        content.add(entity)\n\n\n        \/\/\/ The name of the audio source.\n        let audioName: String = \"FunkySynth.m4a\"\n\n\n        \/\/\/ The configuration to loop the audio file continuously.\n        let configuration = AudioFileResource.Configuration(shouldLoop: true)\n\n\n        \/\/ Load the audio source and set its configuration.\n        guard let audio = try? AudioFileResource.load(\n            named: audioName,\n            configuration: configuration\n        ) else {\n            print(\"Failed to load audio file.\")\n            return\n        }\n\n\n        \/\/\/ The focus for the directivity of the spatial audio.\n        let focus: Double = 0.5\n\n\n        \/\/ Add a spatial component to the entity that emits in the forward direction.\n        entity.spatialAudio = SpatialAudioComponent(directivity: .beam(focus: focus))\n\n\n        \/\/ Set the entity to play audio.\n        entity.playAudio(audio)\n    }\n\n\n    \/\/ ...\n}",
    "language" : "swift"
  },
  "id" : "C5A99E3F-ABF3-4A82-B019-387C385B3D99",
  "kind" : "unknown",
  "overview" : "visionOS  Introductory visionOS samples  Playing spatial audio  Introductory visionOS samples  Playing spatial audio Sample CodePlaying spatial audioCreate and adjust spatial audio in visionOS with RealityKit. Download visionOS 2.0+Xcode 16.0+OverviewThis sample demonstrates how to load and play a spatial audio file in a visionOS app with the SpatialAudioComponent. As the following image shows, you can use this component to configure how an entity emits sounds into a person’s environment:\n\nUsing existing RealityKit shapes, the sample creates an axis visualizer in the app’s main view to represent the x, y, and z axes of the audio source:",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/visionos\/playing-spatial-audio-in-visionos\ncrawled: 2025-12-02T17:49:01Z\n---\n\n# Playing spatial audio | Apple Developer Documentation\n\n- [ visionOS ](\/documentation\/visionos)\n\n- [ Introductory visionOS samples ](\/documentation\/visionos\/introductory-visionos-samples)\n\n- [ Playing spatial audio ](\/documentation\/visionos\/playing-spatial-audio-in-visionos)\n\n- [ Introductory visionOS samples ](\/documentation\/visionos\/introductory-visionos-samples)\n\n-  Playing spatial audio \n\nSample Code# Playing spatial audio\n\nCreate and adjust spatial audio in visionOS with RealityKit.[ Download ](https:\/\/docs-assets.developer.apple.com\/published\/d80280abbf97\/PlayingSpatialAudioInVisionOS.zip)visionOS 2.0+Xcode 16.0+## [Overview](\/documentation\/visionos\/playing-spatial-audio-in-visionos#Overview)\n\nThis sample demonstrates how to load and play a spatial audio file in a visionOS app with the [`SpatialAudioComponent`](\/documentation\/RealityKit\/SpatialAudioComponent). As the following image shows, you can use this component to configure how an entity emits sounds into a person’s environment:\n\n### [Create the axis visualizer](\/documentation\/visionos\/playing-spatial-audio-in-visionos#Create-the-axis-visualizer)\n\nUsing existing RealityKit shapes, the sample creates an axis visualizer in the app’s main view to represent the x, y, and z axes of the audio source:\n\n\n\n```\nimport RealityKit\n\n\nstruct AxisVisualizer {\n    static func make() -> Entity {\n        \/\/\/ The entity that contains four different meshes.\n        let entity = Entity()\n\n\n        \/\/\/ The width, length, and radius values that each mesh uses.\n        let width: Float = 0.0025\n        let length: Float = 0.1\n        let radius: Float = 0.005\n        \n        \/\/ ...\n    }\n}\n\n```\n\nThe structure uses a `make()` method to create the entity that contains the axis mesh.\n\nTo create the representation of the x-axis, the app creates a box mesh with [`generateBox(size:cornerRadius:)`](\/documentation\/RealityKit\/MeshResource\/generateBox(size:cornerRadius:)-2ovma) and a red [`UnlitMaterial`](\/documentation\/RealityKit\/UnlitMaterial), then combines the two with [`ModelEntity`](\/documentation\/RealityKit\/ModelEntity):\n\n\n\n```\nstatic func make() -> Entity {\n    \/\/ ...\n\n\n    \/\/\/ The box for the x-axis.\n    let xAxisMesh = MeshResource.generateBox(size: [length, width, width])\n\n\n    \/\/\/ The unlit red material.\n    let xAxisMaterial = UnlitMaterial(color: .systemRed)\n\n\n    \/\/\/ The entity with the box and material that represents the x-axis.\n    let xAxisEntity = ModelEntity(mesh: xAxisMesh, materials: [xAxisMaterial])\n\n\n    \/\/ Set the position of the x-axis entity in 3D space.\n    xAxisEntity.position = [0.5 * length, 0, 0]\n\n\n    \/\/ Add the x-axis to the parent entity.\n    entity.addChild(xAxisEntity)\n\n\n    \/\/ ...\n}\n\n```\n\nThe app follows similar steps to create the representation of the y and z axes, by adjusting the color of the material, the position of the entity, and the corresponding vector of three scalar values representing the width, height, and depth of the box.\n\nTo create an origin point and complete the visualizer, the app creates a white sphere at the default position, using [`generateSphere(radius:)`](\/documentation\/RealityKit\/MeshResource\/generateSphere(radius:)):\n\n\n\n```\nstatic func make() -> Entity {\n    \/\/ ...\n\n\n    \/\/\/ The sphere for the origin point.\n    let originMesh = MeshResource.generateSphere(radius: radius)\n\n\n    \/\/\/ The unlit white material.\n    let originMaterial = UnlitMaterial(color: .white)\n\n\n    \/\/\/ The entity with the sphere and white material that represents the origin point.\n    let originEntity = ModelEntity(mesh: originMesh, materials: [originMaterial])\n\n\n    \/\/ Add the origin entity to the main entity.\n    entity.addChild(originEntity)\n\n\n    return entity\n}\n\n```\n\n### [Create a decibel slider](\/documentation\/visionos\/playing-spatial-audio-in-visionos#Create-a-decibel-slider)\n\nTo adjust the decibels for the gain, the direct level, and the reverb level of the audio source, the app creates the `DecibelSlider` view that the app adds to the main body view. The view contains a `name` property, which represents the name of the property that it controls, and a `value` variable, which stores the decibel value:\n\n\n\n```\nimport SwiftUI\n\n\n\/\/\/ A view that formats as a slider to adjust decibel values.\nstruct DecibelSlider: View {\n    \/\/\/ The name of the value that changes.\n    let name: String\n\n\n    \/\/\/ The binding to a numerical double that stores the decibel value.\n    let value: Binding<Double>\n\n\n    var body: some View {\n        VStack {\n            HStack {\n                Text(name)\n                Spacer()\n                Text(value.wrappedValue.formatted(.number.precision(.fractionLength(.zero))) + \"dB\")\n                    .monospacedDigit()\n            }\n            \/\/\/ The slider with a range of -60 to 0.\n            Slider(value: value, in: -60 ... 0)\n        }\n    }\n}\n\n```\n\nUsing a [`Slider`](\/documentation\/SwiftUI\/Slider) within the view, a person can control the `value` property from the bounded linear range of values between -60 and 0.\n\n### [Set up the main window](\/documentation\/visionos\/playing-spatial-audio-in-visionos#Set-up-the-main-window)\n\nTo create the main view, the app combines the following subviews:\n\n- `audioSource`\n\n- `description`\n\n- `config`\n\n\n\n```\nimport SwiftUI\nimport RealityKit\n\n\nstruct SpatialAudioView: View {\n    \/\/\/ The new entity to contain the audio sample.\n    let entity = Entity()\n\n\n    \/\/\/ The gain value of the audio source.\n    @State private var gain: Audio.Decibel = .zero\n\n\n    \/\/\/ The direct signal that emits from the audio source.\n    @State private var directLevel: Audio.Decibel = .zero\n\n\n    \/\/\/ The reverb of the audio source.\n    @State private var reverbLevel: Audio.Decibel = .zero\n\n\n    var body: some View {\n        HStack {\n            audioSource\n\n\n            VStack {\n                description\n                Spacer()\n                configuration\n            }\n            .padding(30)\n            .frame(width: 350)\n        }\n    }\n\n\n    \/\/ ...\n}\n\n```\n\nThe view creates `entity` to hold the audio sample. The `gain`, `directLevel`, and `reverbLevel` properties represent the default values for the audio source.\n\n### [Play the spatial audio](\/documentation\/visionos\/playing-spatial-audio-in-visionos#Play-the-spatial-audio)\n\nWhen the necessary properties are in place, the `audioSource` view loads the audio file and configures it for continuous playback. Then the app sets up `entity` with a `SpatialAudioComponent` to play the audio in the reality view:\n\n\n\n```\nvar audioSource: some View {\n    RealityView { content in\n        \/\/ Add the entity to the `RealityView`.\n        content.add(entity)\n\n\n        \/\/\/ The name of the audio source.\n        let audioName: String = \"FunkySynth.m4a\"\n\n\n        \/\/\/ The configuration to loop the audio file continuously.\n        let configuration = AudioFileResource.Configuration(shouldLoop: true)\n\n\n        \/\/ Load the audio source and set its configuration.\n        guard let audio = try? AudioFileResource.load(\n            named: audioName,\n            configuration: configuration\n        ) else {\n            print(\"Failed to load audio file.\")\n            return\n        }\n\n\n        \/\/\/ The focus for the directivity of the spatial audio.\n        let focus: Double = 0.5\n\n\n        \/\/ Add a spatial component to the entity that emits in the forward direction.\n        entity.spatialAudio = SpatialAudioComponent(directivity: .beam(focus: focus))\n\n\n        \/\/ Set the entity to play audio.\n        entity.playAudio(audio)\n    }\n\n\n    \/\/ ...\n}\n\n```\n\nAfter adding the entity to the reality view, the app attaches the `.onAppear` and `.onChange` modifiers to spawn the axis visualizer. Then the app enables the view to modify the entity’s `gain`, `directLevel`, and `reverbLevel` by adjusting the corresponding representation values:\n\n\n\n```\nvar audioSource: some View {\n    RealityView { content in\n\n\n        \/\/ ...\n\n\n    }\n    \/\/ Create a 3D axis representation and add it as a child.\n    .onAppear { entity.addChild(AxisVisualizer.make()) }\n\n\n    \/\/ Enable the view to change the gain parameter.\n    .onChange(of: gain) { entity.spatialAudio?.gain = gain }\n\n\n    \/\/ Enable the view to change the direct level parameter.\n    .onChange(of: directLevel) { entity.spatialAudio?.directLevel = directLevel }\n\n\n    \/\/ Enable the view to change the reverb parameter.\n    .onChange(of: reverbLevel) { entity.spatialAudio?.reverbLevel = reverbLevel }\n}\n\n```\n\n### [Showcase text descriptions](\/documentation\/visionos\/playing-spatial-audio-in-visionos#Showcase-text-descriptions)\n\nThe sample creates the `description` view to display the collection of texts and guide people through the audio experience in the app:\n\n\n\n```\nvar description: some View {\n    VStack(alignment: .leading, spacing: 12) {\n        Text(\"Spatial Audio\")\n            .font(.title)\n\n\n        Text(\"Push the app away from you, then bring it closer to you\")\n        Text(\"Notice how the sound gets quieter and louder as it moves\")\n            .foregroundStyle(.secondary)\n\n\n        Text(\"Move the app around you\")\n        Text(\"Notice how the sound emanates around you as it moves\")\n            .foregroundStyle(.secondary)\n\n\n        Text(\"Rotate your head\")\n        Text(\"Notice how the sound radiates from the app's location\")\n            .foregroundStyle(.secondary)\n\n\n        Text(\"Move your head towards the red axis\")\n        Text(\"Notice how the sound gets louder as you move towards the emitter\")\n            .foregroundStyle(.secondary)\n    }\n}\n\n```\n\n### [Control the spatial audio](\/documentation\/visionos\/playing-spatial-audio-in-visionos#Control-the-spatial-audio)\n\nTo adjust the `gain`, `directLevel`, and `reverbLevel` properties, the sample implements the `configuration` view, adding the `decibelSlider` that allows people to drag to adjust the values:\n\n\n\n```\nvar configuration: some View {\n    VStack {\n        \/\/\/ The slider to control the gain value.\n        DecibelSlider(name: \"Gain\", value: $gain)\n\n\n        \/\/\/ The slider to control the direct level value.\n        DecibelSlider(name: \"Direct Level\", value: $directLevel)\n\n\n        \/\/\/ The slider to control the reverb level.\n        DecibelSlider(name: \"Reverb Level\", value: $reverbLevel)\n    }\n}\n\n\n\n```\n\n#### [Related to](\/documentation\/visionos\/playing-spatial-audio-in-visionos#Related-to)\n\n[`struct SpatialAudioComponent`](\/documentation\/RealityKit\/SpatialAudioComponent)A component that configures how sounds emit from an entity into a person’s environment.",
  "sections" : [
    {
      "content" : "",
      "title" : "Overview"
    }
  ],
  "source" : "appleWebKit",
  "title" : "Playing spatial audio | Apple Developer Documentation",
  "url" : "https:\/\/developer.apple.com\/documentation\/visionos\/playing-spatial-audio-in-visionos"
}