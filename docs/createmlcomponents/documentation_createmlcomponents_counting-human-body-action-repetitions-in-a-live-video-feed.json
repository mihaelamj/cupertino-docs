{
  "abstract" : "Use Create ML Components to analyze a series of video frames and count a person’s repetitive or periodic body movements.",
  "codeExamples" : [
    {
      "code" : "\/\/\/ The camera configuration to define the basic camera position, pixel format, and resolution to use.\nprivate var configuration = VideoReader.CameraConfiguration()",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ Start the video-processing pipeline by displaying the poses in the camera frames and\n\/\/\/ starting the action repetition count prediction stream.\nfunc startVideoProcessingPipeline() {\n\n    if let displayCameraTask = displayCameraTask {\n        displayCameraTask.cancel()\n    }\n\n    displayCameraTask = Task {\n        \/\/ Display poses on top of each camera frame.\n        try await self.displayPoseInCamera()\n    }\n\n    if predictionTask == nil {\n        predictionTask = Task {\n            \/\/ Predict the action repetition count.\n            try await self.predictCount()\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ A Create ML Components transformer to extract human body poses from a single image or a video frame.\nprivate let poseExtractor = HumanBodyPoseExtractor()",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Extract poses in every frame.\nlet poses = try await poseExtractor.applied(to: frame.feature)",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ An asynchronous channel to divert the pose stream for another consumer.\nprivate let poseStream = AsyncChannel<TemporalFeature<[Pose]>>()",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ The counter to count action repetitions from a pose stream.\nprivate let actionCounter = ActionCounter()",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Use an optional Downsampler transformer to downsample the\n\/\/ incoming frames (that is, effectively speed up the observed actions).\nlet pipeline = Downsampler(factor: 1)",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Use a PoseSelector transformer to select one pose to count if\n\/\/ the system detects multiple poses.\n    .appending(PoseSelector(strategy: .maximumBoundingBoxArea))",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Use an optional JointsSelector transformer to specifically ignore\n\/\/ or select a set of joints in a pose to include in counting.\n    .appending(JointsSelector(ignoredJoints: [.nose, .leftEye, .leftEar, .rightEye, .rightEar]))",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Use a SlidingWindowTransformer to group frames into windows, and\n\/\/ prepare them for prediction.\n    .appending(SlidingWindowTransformer<Pose>(stride: 5, length: 90))",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Use a HumanBodyActionCounter transformer to count actions from\n\/\/ each window and produce cumulative counts for the input stream.\n    .appending(HumanBodyActionCounter())",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Draw all the poses Vision finds in the frame.\nfor pose in poses {\n    \/\/ Draw each pose as a wireframe at the scale of the image.\n    pose.drawWireframe(to: context, applying: pointTransform)\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "5b504779ad3e71a726259af9e952ced15888ae41529eed463ffdbb8963d6d3b3",
  "crawledAt" : "2025-12-02T15:30:13Z",
  "id" : "98AD240C-6554-4B03-8048-E6934C82485F",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "Create ML Components",
  "overview" : "## Overview\n\nThis sample app counts a person’s repetitive or periodic body movements (*actions*) by analyzing a series of video frames and making a prediction with a human body action repetition counter. The counter in this sample can count arbitrary body moves that occur at moderate speed, such as jumping jacks, dance spins, and waving arms.\n\n\n\nThe app continually presents the current action repetition count on top of a live, full-screen video feed from the camera in portrait orientation. When the app detects one or more people in the frame, it overlays a wireframe body pose on each person. At the same time, the app predicts the action repetition count about the most prominent person across multiple frames, typically whoever is closest to the camera.\n\nThe app begins by configuring a camera to generate video frames, then directs the frames through a series of transformers it chains together with [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents]. These methods work together to:\n\n## Configure the sample code project\n\nThis sample code project requires a device with iOS 16 or later, or iPadOS 16 or later. To build this project:\n\n## Start a live video feed\n\nThe app uses [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/VideoReader] to configure the device’s camera and generate an asynchronous video frame sequence. The [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/VideoReader\/CameraConfiguration] specifies the front- or rear-facing camera, and configures its pixel format and resolution. This app supports portrait orientation only. Low lighting and other factors can vary the frame rate, which may affect the counting performance, so ensure the person’s full body is visible in bright environments.\n\nWhen the app first launches — or when the user toggles the camera — the video reader configures a camera device, starts the video-processing pipeline, and produces a frame sequence output with [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/VideoReader\/readCamera(configuration:)].\n\n## Analyze each frame for body poses\n\nThe [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/HumanBodyPoseExtractor] is a transformer that can locate any human body poses from an image or a video frame.\n\nWhen the transformation completes, the method creates and returns a [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/Pose] array that contains one pose for every detected person in the same frame.\n\nThe `Pose` structure serves the following purposes:\n\nFor more information about the underlying human body pose model, see [doc:\/\/com.apple.documentation\/documentation\/Vision\/detecting-human-body-poses-in-images].\n\n## Create a pose stream\n\n[https:\/\/github.com\/apple\/swift-async-algorithms\/blob\/main\/Sources\/AsyncAlgorithms\/AsyncAlgorithms.docc\/Guides\/Channel.md] sends the extracted poses to a separate asynchronous stream. This allows additional consumers to obtain poses from the upstream asynchronous sequence. `AsyncChannel` requires the inclusion of the [https:\/\/github.com\/apple\/swift-async-algorithms] Swift package.\n\n## Create an action repetition counting pipeline\n\nThe `ActionCounter` structure consists of a pipeline of [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents] transformers to achieve continuous action repetition counting. It takes a pose stream as input and returns an asynchronous sequence of cumulative counts.\n\n## Downsample a pose stream\n\nThe first optional transformer in the pipeline, [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/Downsampler], downsamples the incoming pose sequence by an integer factor. This allows the pipeline to process and count much slower actions. For example, without downsampling, the original counter model can handle moderate speed actions, about one repetition per second, such as jumping jacks. A downsampling factor of three can effectively speed up slower actions, such as pushups or a complex dance sequence with about one repetition per 3 seconds, and still allow the model to count the actions.\n\n## Isolate a body pose\n\nThe next transformer in the pipeline, [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/PoseSelector], selects a single pose from the array of poses by using the default strategy, namely, selecting the most prominent person by their maximum bounding box area.\n\nThe goal of this strategy is to consistently select the same person’s pose from a crowd over time.\n\n\n\n## Select a subset of body joints\n\nThe next optional transformer in the pipeline, [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/JointsSelector], selects or ignores a specified subset of body joints from the pose.\n\n\n\nFor example, to count only upper-body movements, the transformer can ignore lower-body joints in the pose, such as knees and ankles, which can eliminate noise by ignoring any leg movements.\n\n## Gather a window of poses\n\nThe next transformer in the pipeline is a [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/SlidingWindowTransformer] that receives a pose sequence from its upstream and gathers the frames into an array by providing the following parameters:\n\n\n\nThe action repetition counter assumes a fixed length of 90, where the sliding window transformer groups 90 frames together to generate a single prediction count. The stride is adjustable. An example is a stride of 10 frames, indicating the count updates every 10 frames, which is about 0.3 seconds if the frame rate is 30 frames per second. When the stride is smaller than the length, the windows overlap.\n\n## Predict the person’s action repetition count\n\nThe next transformer in the pipeline, [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/HumanBodyActionCounter], takes a stream of grouped pose windows as input and produces a [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/HumanBodyActionCounter\/CumulativeSumSequence] where each result is a cumulative count of the actions in the sequence. Live counting occurs by iterating each item in the resulted sequence.\n\n## Present the count to the user\n\nThe final count appears as a [doc:\/\/com.apple.documentation\/documentation\/SwiftUI] label on the screen using the `OverlayView` structure on the main thread.\n\n## Present the poses to the user\n\nThe app visualizes the result of each detected human body pose by drawing the poses on top of the frame that [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/HumanBodyPoseExtractor] finds them in. Each time the `poseExtractor` creates an array of [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/Pose] instances, the `PosesView` iterates each detected pose and draws it by calling its `drawWireframe(to:applying:)` method, which draws the pose as a wireframe of connection lines and joint circles.\n\nThe `ViewModel` presents the image and poses onscreen by calling `display(image:, poses:)` method.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/counting-human-body-action-repetitions-in-a-live-video-feed\ncrawled: 2025-12-02T15:30:13Z\n---\n\n# Counting human body action repetitions in a live video feed\n\n**Sample Code**\n\nUse Create ML Components to analyze a series of video frames and count a person’s repetitive or periodic body movements.\n\n## Overview\n\nThis sample app counts a person’s repetitive or periodic body movements (*actions*) by analyzing a series of video frames and making a prediction with a human body action repetition counter. The counter in this sample can count arbitrary body moves that occur at moderate speed, such as jumping jacks, dance spins, and waving arms.\n\n\n\nThe app continually presents the current action repetition count on top of a live, full-screen video feed from the camera in portrait orientation. When the app detects one or more people in the frame, it overlays a wireframe body pose on each person. At the same time, the app predicts the action repetition count about the most prominent person across multiple frames, typically whoever is closest to the camera.\n\nThe app begins by configuring a camera to generate video frames, then directs the frames through a series of transformers it chains together with [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents]. These methods work together to:\n\n1. Read camera frames in real time using [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/VideoReader].\n2. Analyze each frame to locate any human body poses using [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/HumanBodyPoseExtractor], and redirect the pose stream with an [https:\/\/github.com\/apple\/swift-async-algorithms\/blob\/main\/Sources\/AsyncAlgorithms\/AsyncAlgorithms.docc\/Guides\/Channel.md] to allow multiple consumers.\n3. Optionally, downsample the stream using a [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/Downsampler] to process the observed actions in different speeds. To improve performance, you can move the downsampler to an earlier stage in the pipeline if you don’t need to render poses on every frame.\n4. Isolate the prominent pose using [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/PoseSelector].\n5. Optionally, use [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/JointsSelector] to select only joints of interest for counting.\n6. Aggregate the prominent pose’s position data over time using [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/SlidingWindowTransformer].\n7. Predict action repetitions by sending aggregate data to the [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/HumanBodyActionCounter].\n\n\n\n## Configure the sample code project\n\nThis sample code project requires a device with iOS 16 or later, or iPadOS 16 or later. To build this project:\n\n1. Double-click the `CountMyActions.xcodeproj` project to open it in Xcode.\n2. In Xcode, from the Project navigator, select the `CountMyActions` project and click the Signing & Capabilities tab.\n3. Select your development team from the Add Account pop-up menu.\n4. Select your target device from the scheme menu, and choose Product > Run.\n\n## Start a live video feed\n\nThe app uses [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/VideoReader] to configure the device’s camera and generate an asynchronous video frame sequence. The [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/VideoReader\/CameraConfiguration] specifies the front- or rear-facing camera, and configures its pixel format and resolution. This app supports portrait orientation only. Low lighting and other factors can vary the frame rate, which may affect the counting performance, so ensure the person’s full body is visible in bright environments.\n\n```swift\n\/\/\/ The camera configuration to define the basic camera position, pixel format, and resolution to use.\nprivate var configuration = VideoReader.CameraConfiguration()\n```\n\nWhen the app first launches — or when the user toggles the camera — the video reader configures a camera device, starts the video-processing pipeline, and produces a frame sequence output with [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/VideoReader\/readCamera(configuration:)].\n\n```swift\n\/\/\/ Start the video-processing pipeline by displaying the poses in the camera frames and\n\/\/\/ starting the action repetition count prediction stream.\nfunc startVideoProcessingPipeline() {\n\n    if let displayCameraTask = displayCameraTask {\n        displayCameraTask.cancel()\n    }\n\n    displayCameraTask = Task {\n        \/\/ Display poses on top of each camera frame.\n        try await self.displayPoseInCamera()\n    }\n\n    if predictionTask == nil {\n        predictionTask = Task {\n            \/\/ Predict the action repetition count.\n            try await self.predictCount()\n        }\n    }\n}\n```\n\n## Analyze each frame for body poses\n\nThe [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/HumanBodyPoseExtractor] is a transformer that can locate any human body poses from an image or a video frame.\n\n```swift\n\/\/\/ A Create ML Components transformer to extract human body poses from a single image or a video frame.\nprivate let poseExtractor = HumanBodyPoseExtractor()\n```\n\nWhen the transformation completes, the method creates and returns a [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/Pose] array that contains one pose for every detected person in the same frame.\n\n```swift\n\/\/ Extract poses in every frame.\nlet poses = try await poseExtractor.applied(to: frame.feature)\n```\n\nThe `Pose` structure serves the following purposes:\n\n- Calculates the pose’s area within a frame (See the “Isolate a body pose” section below.).\n- Draws each detected pose as a wireframe of points and lines (See the “Present the poses to the user” section below.).\n\nFor more information about the underlying human body pose model, see [doc:\/\/com.apple.documentation\/documentation\/Vision\/detecting-human-body-poses-in-images].\n\n## Create a pose stream\n\n[https:\/\/github.com\/apple\/swift-async-algorithms\/blob\/main\/Sources\/AsyncAlgorithms\/AsyncAlgorithms.docc\/Guides\/Channel.md] sends the extracted poses to a separate asynchronous stream. This allows additional consumers to obtain poses from the upstream asynchronous sequence. `AsyncChannel` requires the inclusion of the [https:\/\/github.com\/apple\/swift-async-algorithms] Swift package.\n\n```swift\n\/\/\/ An asynchronous channel to divert the pose stream for another consumer.\nprivate let poseStream = AsyncChannel<TemporalFeature<[Pose]>>()\n```\n\n## Create an action repetition counting pipeline\n\nThe `ActionCounter` structure consists of a pipeline of [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents] transformers to achieve continuous action repetition counting. It takes a pose stream as input and returns an asynchronous sequence of cumulative counts.\n\n```swift\n\/\/\/ The counter to count action repetitions from a pose stream.\nprivate let actionCounter = ActionCounter()\n```\n\n## Downsample a pose stream\n\nThe first optional transformer in the pipeline, [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/Downsampler], downsamples the incoming pose sequence by an integer factor. This allows the pipeline to process and count much slower actions. For example, without downsampling, the original counter model can handle moderate speed actions, about one repetition per second, such as jumping jacks. A downsampling factor of three can effectively speed up slower actions, such as pushups or a complex dance sequence with about one repetition per 3 seconds, and still allow the model to count the actions.\n\n```swift\n\/\/ Use an optional Downsampler transformer to downsample the\n\/\/ incoming frames (that is, effectively speed up the observed actions).\nlet pipeline = Downsampler(factor: 1)\n```\n\n## Isolate a body pose\n\nThe next transformer in the pipeline, [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/PoseSelector], selects a single pose from the array of poses by using the default strategy, namely, selecting the most prominent person by their maximum bounding box area.\n\n```swift\n\/\/ Use a PoseSelector transformer to select one pose to count if\n\/\/ the system detects multiple poses.\n    .appending(PoseSelector(strategy: .maximumBoundingBoxArea))\n```\n\nThe goal of this strategy is to consistently select the same person’s pose from a crowd over time.\n\n\n\n\n\n## Select a subset of body joints\n\nThe next optional transformer in the pipeline, [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/JointsSelector], selects or ignores a specified subset of body joints from the pose.\n\n\n\nFor example, to count only upper-body movements, the transformer can ignore lower-body joints in the pose, such as knees and ankles, which can eliminate noise by ignoring any leg movements.\n\n```swift\n\/\/ Use an optional JointsSelector transformer to specifically ignore\n\/\/ or select a set of joints in a pose to include in counting.\n    .appending(JointsSelector(ignoredJoints: [.nose, .leftEye, .leftEar, .rightEye, .rightEar]))\n```\n\n## Gather a window of poses\n\nThe next transformer in the pipeline is a [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/SlidingWindowTransformer] that receives a pose sequence from its upstream and gathers the frames into an array by providing the following parameters:\n\n- A `stride` that determines the number of frames to count before updating the pose window\n- A `length` that determines the window size, namely, how many frames to group together\n\n```swift\n\/\/ Use a SlidingWindowTransformer to group frames into windows, and\n\/\/ prepare them for prediction.\n    .appending(SlidingWindowTransformer<Pose>(stride: 5, length: 90))\n```\n\n\n\nThe action repetition counter assumes a fixed length of 90, where the sliding window transformer groups 90 frames together to generate a single prediction count. The stride is adjustable. An example is a stride of 10 frames, indicating the count updates every 10 frames, which is about 0.3 seconds if the frame rate is 30 frames per second. When the stride is smaller than the length, the windows overlap.\n\n## Predict the person’s action repetition count\n\nThe next transformer in the pipeline, [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/HumanBodyActionCounter], takes a stream of grouped pose windows as input and produces a [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/HumanBodyActionCounter\/CumulativeSumSequence] where each result is a cumulative count of the actions in the sequence. Live counting occurs by iterating each item in the resulted sequence.\n\n```swift\n\/\/ Use a HumanBodyActionCounter transformer to count actions from\n\/\/ each window and produce cumulative counts for the input stream.\n    .appending(HumanBodyActionCounter())\n```\n\n## Present the count to the user\n\nThe final count appears as a [doc:\/\/com.apple.documentation\/documentation\/SwiftUI] label on the screen using the `OverlayView` structure on the main thread.\n\n## Present the poses to the user\n\nThe app visualizes the result of each detected human body pose by drawing the poses on top of the frame that [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/HumanBodyPoseExtractor] finds them in. Each time the `poseExtractor` creates an array of [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/Pose] instances, the `PosesView` iterates each detected pose and draws it by calling its `drawWireframe(to:applying:)` method, which draws the pose as a wireframe of connection lines and joint circles.\n\n```swift\n\/\/ Draw all the poses Vision finds in the frame.\nfor pose in poses {\n    \/\/ Draw each pose as a wireframe at the scale of the image.\n    pose.drawWireframe(to: context, applying: pointTransform)\n}\n```\n\nThe `ViewModel` presents the image and poses onscreen by calling `display(image:, poses:)` method.\n\n## Pose components\n\n- **Pose**: A pose that contains joint keypoints from a person, a hand, or a combination.\n- **JointKey**: A key that uniquely identifies a joint.\n- **JointPoint**: A joint in a pose that contains a location and scoring information.\n- **PoseSelector**: A transformer that selects one pose from an array of poses.\n- **PoseSelectionStrategy**: Pose selection strategy.\n- **JointsSelector**: Joints selector from a pose.\n- **HumanBodyPoseExtractor**: The human body pose image feature extractor.\n- **HumanHandPoseExtractor**: The human hand pose image feature extractor.\n- **HumanBodyActionCounter**: A human body action repetition counting transformer that takes window of human body poses and produces cumulative human body action repetition counts.\n- **HumanBodyActionPeriodPredictor**: A human body action period predictor transformer that takes window of poses and produces a window of predictions.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "A pose that contains joint keypoints from a person, a hand, or a combination.",
          "name" : "Pose",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/Pose"
        },
        {
          "description" : "A key that uniquely identifies a joint.",
          "name" : "JointKey",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/JointKey"
        },
        {
          "description" : "A joint in a pose that contains a location and scoring information.",
          "name" : "JointPoint",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/JointPoint"
        },
        {
          "description" : "A transformer that selects one pose from an array of poses.",
          "name" : "PoseSelector",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/PoseSelector"
        },
        {
          "description" : "Pose selection strategy.",
          "name" : "PoseSelectionStrategy",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/PoseSelectionStrategy"
        },
        {
          "description" : "Joints selector from a pose.",
          "name" : "JointsSelector",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/JointsSelector"
        },
        {
          "description" : "The human body pose image feature extractor.",
          "name" : "HumanBodyPoseExtractor",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/HumanBodyPoseExtractor"
        },
        {
          "description" : "The human hand pose image feature extractor.",
          "name" : "HumanHandPoseExtractor",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/HumanHandPoseExtractor"
        },
        {
          "description" : "A human body action repetition counting transformer that takes window of human body poses and produces cumulative human body action repetition counts.",
          "name" : "HumanBodyActionCounter",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/HumanBodyActionCounter"
        },
        {
          "description" : "A human body action period predictor transformer that takes window of poses and produces a window of predictions.",
          "name" : "HumanBodyActionPeriodPredictor",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/HumanBodyActionPeriodPredictor"
        }
      ],
      "title" : "Pose components"
    }
  ],
  "source" : "appleJSON",
  "title" : "Counting human body action repetitions in a live video feed",
  "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/counting-human-body-action-repetitions-in-a-live-video-feed"
}