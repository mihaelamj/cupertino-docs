{
  "abstract" : "Train a machine learning model to assign multiple labels to an image.",
  "codeExamples" : [
    {
      "code" : "[\n    {\n        \"filename\": \"image1.jpg\",\n        \"annotations\": [\"window_sill\", \"aloe\", \"pot\"]\n    },\n    {\n        \"filename\": \"image2.jpg\",\n        \"annotations\": [\"cactus\", \"pot\", \"person\"]\n    }\n]",
      "language" : "json"
    },
    {
      "code" : "\/\/ Define an annotation.\nstruct AnnotatedFile: Decodable {\n    var filename: String\n    var annotations: Set<String>\n}\n\n\/\/ Specify the input directory.\nlet directoryURL = URL(filePath: \"\/path\/to\/files\", directoryHint: .isDirectory)\n\n\/\/ Decode the annotations.\nlet decoder = JSONDecoder()\nlet data = try Data(contentsOf: directory.appending(component: \"annotations.json\"))\nlet annotatedFiles = try decoder.decode([AnnotatedFile].self, from: data)\n\n\/\/ Convert the annotations to an array of `AnnotatedFeature`.\nlet annotatedFeatures = annotatedFiles.map {\n    AnnotatedFeature(\n        feature: directory.appending(component: $0.filename),\n        annotation: $0.annotations\n    )\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ List all the labels.\nlet labels = [\"aloe\", \"cactus\", \"person\", \"pot\", \"window_sill\"]\n\n\/\/ Compose the estimator.\nlet estimator = ImageReader()\n    .appending(ImageFeaturePrint())\n    .appending(FullyConnectedNetworkMultiLabelClassifier<Float, String>(labels: labels))",
      "language" : "swift"
    },
    {
      "code" : "let (training, validation) = annotatedFeatures.annotatedFiles.randomSplit(by: 0.8)\nlet model = try await estimator.fitted(\n    to: training,\n    validateOn: validation\n)",
      "language" : "swift"
    },
    {
      "code" : "let predicted = try await model.prediction(from: testAnnotatedFiles)\nlet metrics = try MultiLabelClassificationMetrics(\n    classifications: predicted.map(\\.prediction),\n    groundTruth: predicted.map(\\.annotation),\n    strategy: .balancedPrecisionAndRecall,\n    labels: labels\n)\nprint(metrics.meanAveragePrecision)",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Export to Core ML\nlet modelURL = URL(filePath: \"\/path\/to\/model\")\ntry model.export(to: modelURL)",
      "language" : "swift"
    },
    {
      "code" : "import Vision\nimport CoreML\n\nlet handler = VNImageRequestHandler(url: URL(filePath: \"image.jpg\"))\nlet visionModel = try VNCoreMLModel(for: compiledModel)\nlet request = VNCoreMLRequest(model: visionModel)\ntry handler.perform([request])\nif let observations = request.results as? [VNClassificationObservation] {\n    \/\/ Filter observations using a target precision and recall.\n    let filteredObservations = observations.filter {\n        $0.hasMinimumPrecision(0.3, forRecall: 0.7)\n    }\n    \/\/ Use observations.\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "de86c14f0bf258f59ed17bc2bf5fcd01723b7658a3c97e3fc8a05ed7eb4a15e3",
  "crawledAt" : "2025-12-01T17:06:44Z",
  "id" : "3C619F1C-5E49-4F41-B366-FBF2958F3993",
  "kind" : "article",
  "module" : "Create ML Components",
  "overview" : "## Overview\n\nA single-label image classifier takes an input image and assigns one label, which helps identify the most relevant subject in the image. However, there’s often additional information and context in an image that identifying the most relevant subject doesn’t consider. A multi-label image classifier takes an input image and assigns multiple labels. A multi-label classifier is better at describing an image where there are multiple subjects, or when the environment is relevant.\n\nTraining a multi-label image classifier is similar to training a single-label image classifier. You collect and label images, build an estimator pipeline, train and evaluate the model, and export the model to use with [doc:\/\/com.apple.documentation\/documentation\/Vision]. For more information about single-label image classifiers, see [doc:\/\/com.apple.documentation\/documentation\/CreateML\/creating-an-image-classifier-model].\n\n### Prepare your training data\n\nFirst, collect images and assign labels. Put all images in a folder and create a JSON file in the same folder. For example if you have two images, then your folder contains three files: `image1.jpg`, `image2.jpg`, and `annotations.json`. The JSON file contains the labels for each image. The following example includes possible labels for two images: `image1.jpg` is an image of a potted aloe plant on a window sill and `image2.jpg` is an image of a potted cactus with a person standing next to it.\n\nCreate a [doc:\/\/com.apple.documentation\/documentation\/Swift\/Decodable] structure and populate it with the file names and labels from your JSON file. Then, convert them to an [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/AnnotatedFeature] structure.\n\n### Build a multi-label estimator pipeline\n\nAfter preparing your training data, you can create your estimator pipeline. When using Create ML Components, you compose estimators and transformers into pipelines that you can train to produce models. As with a single-label image classifier, use an image reader and a feature extractor. But the last component is a [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/FullyConnectedNetworkMultiLabelClassifier] instead of a [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/FullyConnectedNetworkClassifier].\n\n### Train and evaluate the model\n\nWhen you validate as you train, you can stop training when the validation metrics stop improving. So set aside some of the images for validation. Then, call the [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/SupervisedEstimator\/fitted(to:validateOn:eventHandler:)] method to train.\n\nAfter training the model, evaluate it using test images. The mean-average precision (MAP) is a good measure for a multi-label classifier.\n\n### Export the model to use with Vision\n\nAfter you train the model, you can export it as a [doc:\/\/com.apple.documentation\/documentation\/CoreML] model.\n\nThen, use [doc:\/\/com.apple.documentation\/documentation\/Vision] to classify images in your app.\n\nThe observations include all labels and their probabilities. This includes labels for which the model predicted a low probability. Including all observations results in high recall but low precision, in other words, your model prioritizes predicting additional labels. To balance the precision and recall, include only the labels that have a high probability. To do this you can choose a probability threshold for each label, or use one of the methods from [doc:\/\/com.apple.documentation\/documentation\/Vision]. The [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNClassificationObservation\/hasMinimumPrecision(_:forRecall:)] and [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNClassificationObservation\/hasMinimumRecall(_:forPrecision:)] methods allow you to choose only observations that strike a specific balance between precision and recall.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/createmlcomponents\/creating-a-multi-label-image-classifier\ncrawled: 2025-12-01T17:06:44Z\n---\n\n# Creating a multi-label image classifier\n\n**Article**\n\nTrain a machine learning model to assign multiple labels to an image.\n\n## Overview\n\nA single-label image classifier takes an input image and assigns one label, which helps identify the most relevant subject in the image. However, there’s often additional information and context in an image that identifying the most relevant subject doesn’t consider. A multi-label image classifier takes an input image and assigns multiple labels. A multi-label classifier is better at describing an image where there are multiple subjects, or when the environment is relevant.\n\nTraining a multi-label image classifier is similar to training a single-label image classifier. You collect and label images, build an estimator pipeline, train and evaluate the model, and export the model to use with [doc:\/\/com.apple.documentation\/documentation\/Vision]. For more information about single-label image classifiers, see [doc:\/\/com.apple.documentation\/documentation\/CreateML\/creating-an-image-classifier-model].\n\n### Prepare your training data\n\nFirst, collect images and assign labels. Put all images in a folder and create a JSON file in the same folder. For example if you have two images, then your folder contains three files: `image1.jpg`, `image2.jpg`, and `annotations.json`. The JSON file contains the labels for each image. The following example includes possible labels for two images: `image1.jpg` is an image of a potted aloe plant on a window sill and `image2.jpg` is an image of a potted cactus with a person standing next to it.\n\n```json\n[\n    {\n        \"filename\": \"image1.jpg\",\n        \"annotations\": [\"window_sill\", \"aloe\", \"pot\"]\n    },\n    {\n        \"filename\": \"image2.jpg\",\n        \"annotations\": [\"cactus\", \"pot\", \"person\"]\n    }\n]\n```\n\nCreate a [doc:\/\/com.apple.documentation\/documentation\/Swift\/Decodable] structure and populate it with the file names and labels from your JSON file. Then, convert them to an [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/AnnotatedFeature] structure.\n\n```swift\n\/\/ Define an annotation.\nstruct AnnotatedFile: Decodable {\n    var filename: String\n    var annotations: Set<String>\n}\n\n\/\/ Specify the input directory.\nlet directoryURL = URL(filePath: \"\/path\/to\/files\", directoryHint: .isDirectory)\n\n\/\/ Decode the annotations.\nlet decoder = JSONDecoder()\nlet data = try Data(contentsOf: directory.appending(component: \"annotations.json\"))\nlet annotatedFiles = try decoder.decode([AnnotatedFile].self, from: data)\n\n\/\/ Convert the annotations to an array of `AnnotatedFeature`.\nlet annotatedFeatures = annotatedFiles.map {\n    AnnotatedFeature(\n        feature: directory.appending(component: $0.filename),\n        annotation: $0.annotations\n    )\n}\n```\n\n### Build a multi-label estimator pipeline\n\nAfter preparing your training data, you can create your estimator pipeline. When using Create ML Components, you compose estimators and transformers into pipelines that you can train to produce models. As with a single-label image classifier, use an image reader and a feature extractor. But the last component is a [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/FullyConnectedNetworkMultiLabelClassifier] instead of a [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/FullyConnectedNetworkClassifier].\n\n```swift\n\/\/ List all the labels.\nlet labels = [\"aloe\", \"cactus\", \"person\", \"pot\", \"window_sill\"]\n\n\/\/ Compose the estimator.\nlet estimator = ImageReader()\n    .appending(ImageFeaturePrint())\n    .appending(FullyConnectedNetworkMultiLabelClassifier<Float, String>(labels: labels))\n```\n\n### Train and evaluate the model\n\nWhen you validate as you train, you can stop training when the validation metrics stop improving. So set aside some of the images for validation. Then, call the [doc:\/\/com.apple.createmlcomponents\/documentation\/CreateMLComponents\/SupervisedEstimator\/fitted(to:validateOn:eventHandler:)] method to train.\n\n```swift\nlet (training, validation) = annotatedFeatures.annotatedFiles.randomSplit(by: 0.8)\nlet model = try await estimator.fitted(\n    to: training,\n    validateOn: validation\n)\n```\n\nAfter training the model, evaluate it using test images. The mean-average precision (MAP) is a good measure for a multi-label classifier.\n\n```swift\nlet predicted = try await model.prediction(from: testAnnotatedFiles)\nlet metrics = try MultiLabelClassificationMetrics(\n    classifications: predicted.map(\\.prediction),\n    groundTruth: predicted.map(\\.annotation),\n    strategy: .balancedPrecisionAndRecall,\n    labels: labels\n)\nprint(metrics.meanAveragePrecision)\n```\n\n### Export the model to use with Vision\n\nAfter you train the model, you can export it as a [doc:\/\/com.apple.documentation\/documentation\/CoreML] model.\n\n```swift\n\/\/ Export to Core ML\nlet modelURL = URL(filePath: \"\/path\/to\/model\")\ntry model.export(to: modelURL)\n```\n\nThen, use [doc:\/\/com.apple.documentation\/documentation\/Vision] to classify images in your app.\n\n```swift\nimport Vision\nimport CoreML\n\nlet handler = VNImageRequestHandler(url: URL(filePath: \"image.jpg\"))\nlet visionModel = try VNCoreMLModel(for: compiledModel)\nlet request = VNCoreMLRequest(model: visionModel)\ntry handler.perform([request])\nif let observations = request.results as? [VNClassificationObservation] {\n    \/\/ Filter observations using a target precision and recall.\n    let filteredObservations = observations.filter {\n        $0.hasMinimumPrecision(0.3, forRecall: 0.7)\n    }\n    \/\/ Use observations.\n}\n```\n\nThe observations include all labels and their probabilities. This includes labels for which the model predicted a low probability. Including all observations results in high recall but low precision, in other words, your model prioritizes predicting additional labels. To balance the precision and recall, include only the labels that have a high probability. To do this you can choose a probability threshold for each label, or use one of the methods from [doc:\/\/com.apple.documentation\/documentation\/Vision]. The [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNClassificationObservation\/hasMinimumPrecision(_:forRecall:)] and [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNClassificationObservation\/hasMinimumRecall(_:forPrecision:)] methods allow you to choose only observations that strike a specific balance between precision and recall.\n\n## Image components\n\n- **Augmenting images to expand your training data**: Improve your model by using transformed versions of your training images.\n- **ImageReader**: An image file reader.\n- **ImageFeatureExtractor**: A transformer that takes an image and outputs image features.\n- **ImageCropper**: An image crop transformer.\n- **ImageScaler**: An image scaling transformer.\n- **ImageFeaturePrint**: ImageFeaturePrint image feature extractor.\n- **ImageBlur**: An image blurring transformer.\n- **ImageColorTransformer**: An image color transformer.\n- **ImageExposureAdjuster**: An image exposure adjusting transformer.\n- **ImageFlipper**: An image flipper transformer.\n- **ImageRotator**: An image rotating transformer.\n- **RandomImageNoiseGenerator**: A transformer that adds random noise to an image.\n- **MLModelImageFeatureExtractor**: An image feature extractor provided by an MLModel.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Improve your model by using transformed versions of your training images.",
          "name" : "Augmenting images to expand your training data",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/augmenting-images-to-expand-your-training-data"
        },
        {
          "description" : "An image file reader.",
          "name" : "ImageReader",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/ImageReader"
        },
        {
          "description" : "A transformer that takes an image and outputs image features.",
          "name" : "ImageFeatureExtractor",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/ImageFeatureExtractor"
        },
        {
          "description" : "An image crop transformer.",
          "name" : "ImageCropper",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/ImageCropper"
        },
        {
          "description" : "An image scaling transformer.",
          "name" : "ImageScaler",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/ImageScaler"
        },
        {
          "description" : "ImageFeaturePrint image feature extractor.",
          "name" : "ImageFeaturePrint",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/ImageFeaturePrint"
        },
        {
          "description" : "An image blurring transformer.",
          "name" : "ImageBlur",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/ImageBlur"
        },
        {
          "description" : "An image color transformer.",
          "name" : "ImageColorTransformer",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/ImageColorTransformer"
        },
        {
          "description" : "An image exposure adjusting transformer.",
          "name" : "ImageExposureAdjuster",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/ImageExposureAdjuster"
        },
        {
          "description" : "An image flipper transformer.",
          "name" : "ImageFlipper",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/ImageFlipper"
        },
        {
          "description" : "An image rotating transformer.",
          "name" : "ImageRotator",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/ImageRotator"
        },
        {
          "description" : "A transformer that adds random noise to an image.",
          "name" : "RandomImageNoiseGenerator",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/RandomImageNoiseGenerator"
        },
        {
          "description" : "An image feature extractor provided by an MLModel.",
          "name" : "MLModelImageFeatureExtractor",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MLModelImageFeatureExtractor"
        }
      ],
      "title" : "Image components"
    }
  ],
  "source" : "appleJSON",
  "title" : "Creating a multi-label image classifier",
  "url" : "https:\/\/developer.apple.com\/documentation\/createmlcomponents\/creating-a-multi-label-image-classifier"
}