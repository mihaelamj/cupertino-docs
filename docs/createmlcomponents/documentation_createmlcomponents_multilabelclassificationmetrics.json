{
  "abstract" : "Multi-label classification metrics.",
  "codeExamples" : [

  ],
  "conformsTo" : [
    "Sendable",
    "SendableMetatype"
  ],
  "contentHash" : "8950463e0dec7da59b8492f930290f632a765ce18367e21aad8538843c205a6e",
  "crawledAt" : "2025-12-03T04:27:51Z",
  "declaration" : {
    "code" : "struct MultiLabelClassificationMetrics<Label> where Label : Hashable",
    "language" : "swift"
  },
  "id" : "D092529A-5C01-4938-B2F1-4E6CA06C0067",
  "kind" : "struct",
  "language" : "swift",
  "module" : "Create ML Components",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst",
    "macOS",
    "tvOS",
    "visionOS",
    "watchOS"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\ncrawled: 2025-12-03T04:27:51Z\n---\n\n# MultiLabelClassificationMetrics\n\n**Structure**\n\nMulti-label classification metrics.\n\n## Declaration\n\n```swift\nstruct MultiLabelClassificationMetrics<Label> where Label : Hashable\n```\n\n## Creating the distribution\n\n- **init(_:strategy:)**: Creates multi-label classification metrics for classifications and ground truth labels.\n- **init(_:strategy:labels:)**: Creates multi-label classification metrics for classifications and ground truth labels.\n- **init(classifications:groundTruth:strategy:)**: Creates multi-label classification metrics for classifications and ground truth labels.\n- **init(classifications:groundTruth:strategy:labels:)**: Creates multi-label classification metrics for classifications and ground truth labels.\n- **init(confidenceThresholds:)**: Creates empty multi-label classification metrics.\n- **MultiLabelClassificationMetrics.ThresholdSelectionStrategy**: A strategy for selecting a confidence threshold.\n\n## Getting the properties\n\n- **confidenceThresholds**: A dictionary of label and confidence thresholds.\n- **exampleCount**: The number of examples used to compute the metrics.\n- **labels**: The classifier labels.\n- **meanAveragePrecision**: The mean average precision.\n\n## Computing and scoring\n\n- **count(of:)**: Returns the number of times a label appeared in the ground truth collection.\n- **f1Score(for:)**: Computes the F1 score from predicted and ground truth values.\n- **falseNegativeCount(of:)**: Returns the number of times a true label was not predicted.\n- **falsePositiveCount(of:)**: Returns the number of times the predicted label did not match the true label.\n- **precisionScore(for:)**: Computes the precision score for a class label.\n- **recallScore(for:)**: Computes the recall score for a class label.\n- **trueNegativeCount(of:)**: Returns the number of times a label was not in the predicted or ground truth collections.\n- **truePositiveCount(of:)**: Returns the number of times the predicted label matched the true label.\n\n## Updating the metrics\n\n- **add(_:)**: Updates the metrics with more pairs of classifications and ground truth labels.\n- **add(classifications:groundTruth:)**: Updates the metrics with more classifications and ground truth labels.\n\n## Computing the precision\n\n- **meanAveragePrecisionScore(_:)**: Computes the mean average precision.\n- **meanAveragePrecisionScore(_:labels:)**: Computes the mean average precision.\n- **meanAveragePrecisionScore(classifications:groundTruth:)**: Computes the mean average precision.\n- **meanAveragePrecisionScore(classifications:groundTruth:labels:)**: Computes the mean average precision.\n\n## Metrics\n\n- **Classification**: An item in a classification result.\n- **ClassificationDistribution**: A classification distribution that contains a probability for each classification label.\n- **ClassificationMetrics**: Classification metrics.\n- **rootMeanSquaredError(_:)**: Computes the root mean squared error between predicted and ground truth values.\n- **rootMeanSquaredError(_:_:)**: Computes the root mean squared error between predicted and ground truth values.\n- **maximumAbsoluteError(_:)**: Computes the maximum absolute error between predicted and ground truth values.\n- **maximumAbsoluteError(_:_:)**: Computes the maximum absolute error between predicted and ground truth values.\n- **meanAbsoluteError(_:)**: Computes the mean absolute error between predicted and ground truth values.\n- **meanAbsoluteError(_:_:)**: Computes the mean absolute error between predicted and ground truth values.\n- **meanAbsolutePercentageError(_:)**: Computes the mean absolute percentage error between predicted and ground truth values.\n- **meanSquaredError(_:)**: Computes the root mean squared error between predicted and ground truth values.\n- **meanSquaredError(_:_:)**: Computes the mean squared error between predicted and ground truth values.\n\n## Conforms To\n\n- Sendable\n- SendableMetatype\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Creates multi-label classification metrics for classifications and ground truth labels.",
          "name" : "init(_:strategy:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\/init(_:strategy:)"
        },
        {
          "description" : "Creates multi-label classification metrics for classifications and ground truth labels.",
          "name" : "init(_:strategy:labels:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\/init(_:strategy:labels:)"
        },
        {
          "description" : "Creates multi-label classification metrics for classifications and ground truth labels.",
          "name" : "init(classifications:groundTruth:strategy:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\/init(classifications:groundTruth:strategy:)"
        },
        {
          "description" : "Creates multi-label classification metrics for classifications and ground truth labels.",
          "name" : "init(classifications:groundTruth:strategy:labels:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\/init(classifications:groundTruth:strategy:labels:)"
        },
        {
          "description" : "Creates empty multi-label classification metrics.",
          "name" : "init(confidenceThresholds:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\/init(confidenceThresholds:)"
        },
        {
          "description" : "A strategy for selecting a confidence threshold.",
          "name" : "MultiLabelClassificationMetrics.ThresholdSelectionStrategy",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\/ThresholdSelectionStrategy"
        }
      ],
      "title" : "Creating the distribution"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "A dictionary of label and confidence thresholds.",
          "name" : "confidenceThresholds",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\/confidenceThresholds"
        },
        {
          "description" : "The number of examples used to compute the metrics.",
          "name" : "exampleCount",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\/exampleCount"
        },
        {
          "description" : "The classifier labels.",
          "name" : "labels",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\/labels"
        },
        {
          "description" : "The mean average precision.",
          "name" : "meanAveragePrecision",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\/meanAveragePrecision"
        }
      ],
      "title" : "Getting the properties"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "Returns the number of times a label appeared in the ground truth collection.",
          "name" : "count(of:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\/count(of:)"
        },
        {
          "description" : "Computes the F1 score from predicted and ground truth values.",
          "name" : "f1Score(for:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\/f1Score(for:)"
        },
        {
          "description" : "Returns the number of times a true label was not predicted.",
          "name" : "falseNegativeCount(of:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\/falseNegativeCount(of:)"
        },
        {
          "description" : "Returns the number of times the predicted label did not match the true label.",
          "name" : "falsePositiveCount(of:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\/falsePositiveCount(of:)"
        },
        {
          "description" : "Computes the precision score for a class label.",
          "name" : "precisionScore(for:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\/precisionScore(for:)"
        },
        {
          "description" : "Computes the recall score for a class label.",
          "name" : "recallScore(for:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\/recallScore(for:)"
        },
        {
          "description" : "Returns the number of times a label was not in the predicted or ground truth collections.",
          "name" : "trueNegativeCount(of:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\/trueNegativeCount(of:)"
        },
        {
          "description" : "Returns the number of times the predicted label matched the true label.",
          "name" : "truePositiveCount(of:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\/truePositiveCount(of:)"
        }
      ],
      "title" : "Computing and scoring"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "Updates the metrics with more pairs of classifications and ground truth labels.",
          "name" : "add(_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\/add(_:)"
        },
        {
          "description" : "Updates the metrics with more classifications and ground truth labels.",
          "name" : "add(classifications:groundTruth:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\/add(classifications:groundTruth:)"
        }
      ],
      "title" : "Updating the metrics"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "Computes the mean average precision.",
          "name" : "meanAveragePrecisionScore(_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\/meanAveragePrecisionScore(_:)"
        },
        {
          "description" : "Computes the mean average precision.",
          "name" : "meanAveragePrecisionScore(_:labels:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\/meanAveragePrecisionScore(_:labels:)"
        },
        {
          "description" : "Computes the mean average precision.",
          "name" : "meanAveragePrecisionScore(classifications:groundTruth:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\/meanAveragePrecisionScore(classifications:groundTruth:)"
        },
        {
          "description" : "Computes the mean average precision.",
          "name" : "meanAveragePrecisionScore(classifications:groundTruth:labels:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics\/meanAveragePrecisionScore(classifications:groundTruth:labels:)"
        }
      ],
      "title" : "Computing the precision"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "An item in a classification result.",
          "name" : "Classification",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/Classification"
        },
        {
          "description" : "A classification distribution that contains a probability for each classification label.",
          "name" : "ClassificationDistribution",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/ClassificationDistribution"
        },
        {
          "description" : "Classification metrics.",
          "name" : "ClassificationMetrics",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/ClassificationMetrics"
        },
        {
          "description" : "Computes the root mean squared error between predicted and ground truth values.",
          "name" : "rootMeanSquaredError(_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/rootMeanSquaredError(_:)"
        },
        {
          "description" : "Computes the root mean squared error between predicted and ground truth values.",
          "name" : "rootMeanSquaredError(_:_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/rootMeanSquaredError(_:_:)"
        },
        {
          "description" : "Computes the maximum absolute error between predicted and ground truth values.",
          "name" : "maximumAbsoluteError(_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/maximumAbsoluteError(_:)"
        },
        {
          "description" : "Computes the maximum absolute error between predicted and ground truth values.",
          "name" : "maximumAbsoluteError(_:_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/maximumAbsoluteError(_:_:)"
        },
        {
          "description" : "Computes the mean absolute error between predicted and ground truth values.",
          "name" : "meanAbsoluteError(_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/meanAbsoluteError(_:)"
        },
        {
          "description" : "Computes the mean absolute error between predicted and ground truth values.",
          "name" : "meanAbsoluteError(_:_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/meanAbsoluteError(_:_:)"
        },
        {
          "description" : "Computes the mean absolute percentage error between predicted and ground truth values.",
          "name" : "meanAbsolutePercentageError(_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/meanAbsolutePercentageError(_:)"
        },
        {
          "description" : "Computes the root mean squared error between predicted and ground truth values.",
          "name" : "meanSquaredError(_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/meanSquaredError(_:)"
        },
        {
          "description" : "Computes the mean squared error between predicted and ground truth values.",
          "name" : "meanSquaredError(_:_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/meanSquaredError(_:_:)"
        }
      ],
      "title" : "Metrics"
    }
  ],
  "source" : "appleJSON",
  "title" : "MultiLabelClassificationMetrics",
  "url" : "https:\/\/developer.apple.com\/documentation\/CreateMLComponents\/MultiLabelClassificationMetrics"
}