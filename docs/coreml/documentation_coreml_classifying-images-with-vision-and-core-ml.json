{
  "abstract" : "Crop and scale photos using the Vision framework and classify them with a Core ML model.",
  "codeExamples" : [

  ],
  "contentHash" : "ab41071dfd23371a8a4ed00f96dab3913399ef7d711a13981b85f9dfe7db4555",
  "crawledAt" : "2025-12-02T15:48:22Z",
  "id" : "2E27162E-85FC-4685-B110-FA9D6613D9A7",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "Core ML",
  "overview" : "## Overview\n\nThe app in this sample identifies the most prominent object in an image by using MobileNet, an open source image classifier model that recognizes around 1,000 different categories.\n\n\n\nEach time a user selects a photo from the library or takes a photo with a camera, the app passes it to a [doc:\/\/com.apple.documentation\/documentation\/Vision] image classification request. Vision resizes and crops the photo to meet the MobileNet model’s constraints for its image input, and then passes the photo to the model using the [doc:\/\/com.apple.documentation\/documentation\/CoreML] framework behind the scenes. Once the model generates a prediction, Vision relays it back to the app, which presents the results to the user.\n\nThe sample uses MobileNet as an example of how to use a third-party Core ML model. You can download open source models — including a newer version of MobileNet — on the [https:\/\/developer.apple.com\/machine-learning\/models].\n\nBefore you integrate a third-party model to solve a problem — which may increase the size of your app — consider using an API in the SDK. For example, the [doc:\/\/com.apple.documentation\/documentation\/Vision] framework’s [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNClassifyImageRequest] class offers the same functionality as MobileNet, but with potentially better performance and without increasing the size of your app (see [doc:\/\/com.apple.documentation\/documentation\/Vision\/classifying-images-for-categorization-and-search]).\n\n### Configure the sample code project\n\nThe sample targets iOS 14 or later, but the MobileNet model in the project works with:\n\nTo take photos within the app, run the sample on a device with a camera. Otherwise, you can select photos from the library in Simulator.\n\n### Create an image classifier instance\n\nAt launch, the `ImagePredictor` class creates an image classifier singleton by calling its `createImageClassifier()` type method.\n\nThe method creates a Core ML model instance for Vision by:\n\nThe Image Predictor class minimizes runtime by only creating a single instance it shares across the app.\n\n### Create an image classification request\n\nThe Image Predictor class creates an image classification request — a [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNCoreMLRequest] instance — by passing the shared image classifier model instance and a request handler to its initializer.\n\nThe method tells Vision how to adjust images that don’t meet the model’s image input constraints by setting the request’s [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNCoreMLRequest\/imageCropAndScaleOption] property to [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNImageCropAndScaleOption\/centerCrop].\n\n### Create a request handler\n\nThe Image Predictor’s `makePredictions(for photo, ...)` method creates a [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNImageRequestHandler] for each image by passing the image and its orientation to the initializer.\n\nVision rotates the image based on `orientation` — a [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/CGImagePropertyOrientation] instance — before sending the image to the model.\n\nIf the image you want to classify has a URL, create a Vision image request handler with one of these initializers:\n\n### Start the Request\n\nThe [`makePredictions(for photo, ...)`][makePredictions] method starts the request by adding it into a [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNRequest] array and passes it to the handler’s [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNImageRequestHandler\/perform(_:)] method.\n\n### Retrieve the request’s results\n\nWhen the image classification request is finished, Vision notifies the Image Predictor by calling the request’s completion handler, `visionRequestHandler(_:error:)`. The method retrieves the request’s [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNRequest\/results] by:\n\nThe Image Predictor converts each result to `Prediction` instances, a simple structure with two string properties.\n\nThe method sends the `predictions` array to the Image Predictor’s client — the main view controller — by calling the client’s completion handler.\n\n### Format and present the predictions\n\nThe main view controller’s `imagePredictionHandler(_:)` method formats the individual predictions into a single string and updates a label in the app’s UI using helper methods.\n\nThe `updatePredictionLabel(_:)` helper method safely updates the UI by updating the label’s text on the main dispatch queue.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/CoreML\/classifying-images-with-vision-and-core-ml\ncrawled: 2025-12-02T15:48:22Z\n---\n\n# Classifying Images with Vision and Core ML\n\n**Sample Code**\n\nCrop and scale photos using the Vision framework and classify them with a Core ML model.\n\n## Overview\n\nThe app in this sample identifies the most prominent object in an image by using MobileNet, an open source image classifier model that recognizes around 1,000 different categories.\n\n\n\nEach time a user selects a photo from the library or takes a photo with a camera, the app passes it to a [doc:\/\/com.apple.documentation\/documentation\/Vision] image classification request. Vision resizes and crops the photo to meet the MobileNet model’s constraints for its image input, and then passes the photo to the model using the [doc:\/\/com.apple.documentation\/documentation\/CoreML] framework behind the scenes. Once the model generates a prediction, Vision relays it back to the app, which presents the results to the user.\n\nThe sample uses MobileNet as an example of how to use a third-party Core ML model. You can download open source models — including a newer version of MobileNet — on the [https:\/\/developer.apple.com\/machine-learning\/models].\n\nBefore you integrate a third-party model to solve a problem — which may increase the size of your app — consider using an API in the SDK. For example, the [doc:\/\/com.apple.documentation\/documentation\/Vision] framework’s [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNClassifyImageRequest] class offers the same functionality as MobileNet, but with potentially better performance and without increasing the size of your app (see [doc:\/\/com.apple.documentation\/documentation\/Vision\/classifying-images-for-categorization-and-search]).\n\n\n\n### Configure the sample code project\n\nThe sample targets iOS 14 or later, but the MobileNet model in the project works with:\n\n- iOS 11 or later\n- macOS 10.13 or later\n\nTo take photos within the app, run the sample on a device with a camera. Otherwise, you can select photos from the library in Simulator.\n\n\n\n### Create an image classifier instance\n\nAt launch, the `ImagePredictor` class creates an image classifier singleton by calling its `createImageClassifier()` type method.\n\nThe method creates a Core ML model instance for Vision by:\n\n1. Creating an instance of the model’s wrapper class that Xcode auto-generates at compile time\n2. Retrieving the wrapper class instance’s underlying [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLModel] property\n3. Passing the model instance to a [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNCoreMLModel] initializer\n\nThe Image Predictor class minimizes runtime by only creating a single instance it shares across the app.\n\n\n\n### Create an image classification request\n\nThe Image Predictor class creates an image classification request — a [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNCoreMLRequest] instance — by passing the shared image classifier model instance and a request handler to its initializer.\n\nThe method tells Vision how to adjust images that don’t meet the model’s image input constraints by setting the request’s [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNCoreMLRequest\/imageCropAndScaleOption] property to [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNImageCropAndScaleOption\/centerCrop].\n\n### Create a request handler\n\nThe Image Predictor’s `makePredictions(for photo, ...)` method creates a [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNImageRequestHandler] for each image by passing the image and its orientation to the initializer.\n\nVision rotates the image based on `orientation` — a [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/CGImagePropertyOrientation] instance — before sending the image to the model.\n\nIf the image you want to classify has a URL, create a Vision image request handler with one of these initializers:\n\n- [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNImageRequestHandler\/init(url:options:)]\n- [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNImageRequestHandler\/init(url:orientation:options:)]\n\n### Start the Request\n\nThe [`makePredictions(for photo, ...)`][makePredictions] method starts the request by adding it into a [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNRequest] array and passes it to the handler’s [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNImageRequestHandler\/perform(_:)] method.\n\n\n\n### Retrieve the request’s results\n\nWhen the image classification request is finished, Vision notifies the Image Predictor by calling the request’s completion handler, `visionRequestHandler(_:error:)`. The method retrieves the request’s [doc:\/\/com.apple.documentation\/documentation\/Vision\/VNRequest\/results] by:\n\n1. Checking the `error` parameter\n2. Casting `results` to a `VNClassificationObservation` array\n\nThe Image Predictor converts each result to `Prediction` instances, a simple structure with two string properties.\n\nThe method sends the `predictions` array to the Image Predictor’s client — the main view controller — by calling the client’s completion handler.\n\n### Format and present the predictions\n\nThe main view controller’s `imagePredictionHandler(_:)` method formats the individual predictions into a single string and updates a label in the app’s UI using helper methods.\n\nThe `updatePredictionLabel(_:)` helper method safely updates the UI by updating the label’s text on the main dispatch queue.\n\n\n\n## Image classification models\n\n- **Using Core ML for semantic image segmentation**: Identify multiple objects in an image by using the DEtection TRansformer image-segmentation model.\n- **Detecting human body poses in an image**: Locate people and the stance of their bodies by analyzing an image with a PoseNet model.\n- **Understanding a Dice Roll with Vision and Object Detection**: Detect dice position and values shown in a camera frame, and determine the end of a roll by leveraging a dice detection model.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Identify multiple objects in an image by using the DEtection TRansformer image-segmentation model.",
          "name" : "Using Core ML for semantic image segmentation",
          "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/using-core-ml-for-semantic-image-segmentation"
        },
        {
          "description" : "Locate people and the stance of their bodies by analyzing an image with a PoseNet model.",
          "name" : "Detecting human body poses in an image",
          "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/detecting-human-body-poses-in-an-image"
        },
        {
          "description" : "Detect dice position and values shown in a camera frame, and determine the end of a roll by leveraging a dice detection model.",
          "name" : "Understanding a Dice Roll with Vision and Object Detection",
          "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/understanding-a-dice-roll-with-vision-and-object-detection"
        }
      ],
      "title" : "Image classification models"
    }
  ],
  "source" : "appleJSON",
  "title" : "Classifying Images with Vision and Core ML",
  "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/classifying-images-with-vision-and-core-ml"
}