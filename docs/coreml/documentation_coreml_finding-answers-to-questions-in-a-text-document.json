{
  "abstract" : "Locate relevant passages in a document by asking the Bidirectional Encoder Representations from Transformers (BERT) model a question.",
  "codeExamples" : [
    {
      "code" : "\/\/ Store the tokenized substrings into an array.\nvar wordTokens = [Substring]()\n\n\/\/ Use Natural Language's NLTagger to tokenize the input by word.\nlet tagger = NLTagger(tagSchemes: [.tokenType])\ntagger.string = rawString\n\n\/\/ Find all tokens in the string and append to the array.\ntagger.enumerateTags(in: rawString.startIndex..<rawString.endIndex,\n                     unit: .word,\n                     scheme: .tokenType,\n                     options: [.omitWhitespace]) { (_, range) -> Bool in\n    wordTokens.append(rawString[range])\n    return true\n}\n\nreturn wordTokens",
      "language" : "swift"
    },
    {
      "code" : "let subTokenID = BERTVocabulary.tokenID(of: searchTerm)",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Start the wordID array with the `classification start` token.\nvar wordIDs = [BERTVocabulary.classifyStartTokenID]\n\n\/\/ Add the question tokens and a separator.\nwordIDs += question.tokenIDs\nwordIDs += [BERTVocabulary.separatorTokenID]\n\n\/\/ Add the document tokens and a separator.\nwordIDs += document.tokenIDs\nwordIDs += [BERTVocabulary.separatorTokenID]\n\n\/\/ Fill the remaining token slots with padding tokens.\nlet tokenIDPadding = BERTInput.maxTokens - wordIDs.count\nwordIDs += Array(repeating: BERTVocabulary.paddingTokenID, count: tokenIDPadding)",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Set all of the token types before the document to 0.\nvar wordTypes = Array(repeating: 0, count: documentOffset)\n\n\/\/ Set all of the document token types to 1.\nwordTypes += Array(repeating: 1, count: document.tokens.count)\n\n\/\/ Set the remaining token types to 0.\nlet tokenTypePadding = BERTInput.maxTokens - wordTypes.count\nwordTypes += Array(repeating: 0, count: tokenTypePadding)",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Create the MLMultiArray instances.\nlet tokenIDMultiArray = try? MLMultiArray(wordIDs)\nlet wordTypesMultiArray = try? MLMultiArray(wordTypes)\n\n\/\/ Unwrap the MLMultiArray optionals.\nguard let tokenIDInput = tokenIDMultiArray else {\n    fatalError(\"Couldn't create wordID MLMultiArray input\")\n}\n\nguard let tokenTypeInput = wordTypesMultiArray else {\n    fatalError(\"Couldn't create wordType MLMultiArray input\")\n}\n\n\/\/ Create the BERT input MLFeatureProvider.\nlet modelInput = BERTQAFP16Input(wordIDs: tokenIDInput,\n                                 wordTypes: tokenTypeInput)",
      "language" : "swift"
    },
    {
      "code" : "guard let prediction = try? bertModel.prediction(input: modelInput) else {\n    return \"The BERT model is unable to make a prediction.\"\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Convert the logits MLMultiArrays to [Double].\nlet startLogits = prediction.startLogits.doubleArray()\nlet endLogits = prediction.endLogits.doubleArray()\n\n\/\/ Isolate the logits for the document.\nlet startLogitsOfDoc = [Double](startLogits[range])\nlet endLogitsOfDoc = [Double](endLogits[range])\n\n\/\/ Only keep the top 20 (out of the possible ~380) indices for faster searching.\nlet topStartIndices = startLogitsOfDoc.indicesOfLargest(20)\nlet topEndIndices = endLogitsOfDoc.indicesOfLargest(20)\n\n\/\/ Search for the highest valued logit pairing.\nlet bestPair = findBestLogitPair(startLogits: startLogitsOfDoc,\n                                 bestStartIndices: topStartIndices,\n                                 endLogits: endLogitsOfDoc,\n                                 bestEndIndices: topEndIndices)",
      "language" : "swift"
    }
  ],
  "contentHash" : "f5f0df704d08cd53ed818fb294fb00a87e134645524b6fe100815edaf9652992",
  "crawledAt" : "2025-12-02T15:30:16Z",
  "id" : "317CD8FE-016E-4F5B-9AD6-08C58413BBE5",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "Core ML",
  "overview" : "## Overview\n\nThis sample app leverages the BERT model to find the answer to a user’s question in a body of text. The model accepts text from a document and a question, in natural English, about the document. The model responds with the location of a passage within the document text that answers the question. For example, given the text, “The quick brown fox jumps over the lethargic dog.”, with the question “Who jumped over the dog?”, the BERT model’s predicted answer is, “the quick brown fox”.\n\nThe BERT model does not generate new sentences to answer a given question. It finds the passage in a document that’s most likely to answer the question.\n\n\n\nThe sample leverages the BERT model by:\n\n### Configure the sample code project\n\nBefore you run the sample code project in Xcode, use a device with either:\n\n### Build the vocabulary\n\nThe first step to using the BERT model is to import its vocabulary. The sample creates a vocabulary dictionary by splitting the vocabulary file into lines, each of which has one token. The function assigns each token’s (zero-based) line number as its value. For example, the first token,  `\"[PAD]\"`, has an ID of `0`, and the 5,001st token, `\"knight\"`, has an ID of `5000`.\n\n### Split the text into word tokens\n\nThe BERT model requires you to convert each word into one or more token IDs. Before you can use the vocabulary dictionary to find those IDs, you must divide the document’s text and the question’s text into word tokens.\n\n\n\nThe sample does this by using an [doc:\/\/com.apple.documentation\/documentation\/NaturalLanguage\/NLTagger], which breaks up a string into word tokens, each of which is a substring of the original.\n\nThe sample app leverages the tagger to split each string into tokens by using its [doc:\/\/com.apple.documentation\/documentation\/NaturalLanguage\/NLTagger\/enumerateTags(in:unit:scheme:options:using:)] method with the [doc:\/\/com.apple.documentation\/documentation\/NaturalLanguage\/NLTagScheme\/tokenType] tagging scheme and the [doc:\/\/com.apple.documentation\/documentation\/NaturalLanguage\/NLTokenUnit\/word] token unit.\n\n### Convert word or wordpiece tokens into their IDs\n\nFor speed and efficiency, the BERT model operates on token IDs, which are numbers that represent tokens, rather than operating on the text tokens themselves.\n\nIf a word token doesn’t exist in the vocabulary, the method looks for subtokens, or *wordpieces*. A wordpiece is a component of a larger word token. For example, the word *lethargic* isn’t in the vocabulary but its wordpieces, *let*, *har*, and *gic* are. Dividing the vocabulary’s large words into wordpieces reduces the vocabulary size and makes the BERT model more flexible. The model can understand words that aren’t explicitly in the vocabulary by combining their wordpieces.\n\nSecondary wordpieces, such as *har* and *gic*, each appear in the vocabulary with two leading pound signs, as `##har` and `##gic`.\n\nContinuing the example, the method converts document text into the word and wordpiece token IDs shown in the following figure.\n\n\n\n### Prepare the model input\n\nThe BERT model has two inputs:\n\nThe sample creates the `wordIDs` array by arranging the token IDs in the following order:\n\nNext, the sample prepares the `wordTypes` input by creating an array of the same length, where all the elements that correspond to the document text are `1` and all others are `0`.\n\nContinuing the example, the sample arranges the two input arrays with the values shown in the figure below.\n\n\n\nNext, the sample creates an [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLMultiArray] for each input and copies the contents from the arrays, which it uses to create a `BERTQAFP16Input` feature provider.\n\n### Make a prediction\n\nYou use the BERT model to predict where to find an answer to the question in the document text, by giving the model your input feature provider with the input [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLMultiArray] instances.\n\n### Find the answer\n\nYou locate the answer to the question by analyzing the output from the BERT model. The model produces two outputs, `startLogits` and `endLogits`. Each *logit* is a raw confidence score of where the BERT model predicts the beginning and the end of an answer is.\n\n\n\nIn this example, the best start and end logits are `6.08` and `7.53` for the tokens `\"the\"` and `\"fox\"`, respectively. The sample finds the indices of the highest-value starting and ending logits by:\n\nIn this example, the indices of the best start and end logits are `8` and `11`, respectively. The answer substring, located between indices `8` and `11` of the original text, is `“the quick brown fox”`.\n\n### Scale for larger documents\n\nThe BERT model included in this sample can process up to 384 tokens, including the three overhead tokens—one “classification start” token and two separator tokens—leaving 381 tokens for your text and question, combined. For larger texts that exceed this limitation, consider using one of these techniques:",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/CoreML\/finding-answers-to-questions-in-a-text-document\ncrawled: 2025-12-02T15:30:16Z\n---\n\n# Finding answers to questions in a text document\n\n**Sample Code**\n\nLocate relevant passages in a document by asking the Bidirectional Encoder Representations from Transformers (BERT) model a question.\n\n## Overview\n\nThis sample app leverages the BERT model to find the answer to a user’s question in a body of text. The model accepts text from a document and a question, in natural English, about the document. The model responds with the location of a passage within the document text that answers the question. For example, given the text, “The quick brown fox jumps over the lethargic dog.”, with the question “Who jumped over the dog?”, the BERT model’s predicted answer is, “the quick brown fox”.\n\nThe BERT model does not generate new sentences to answer a given question. It finds the passage in a document that’s most likely to answer the question.\n\n\n\nThe sample leverages the BERT model by:\n\n1. Importing the BERT model’s vocabulary into a dictionary\n2. Breaking up the document and question texts into tokens\n3. Converting the tokens to ID numbers using the vocabulary dictionary\n4. Packing the converted token IDs into the model’s input format\n5. Calling the BERT model’s [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLModel\/prediction(from:)-9y2aa] method\n6. Locating the answer by analyzing the BERT model’s output\n7. Extracting that answer from the original document text\n\n### Configure the sample code project\n\nBefore you run the sample code project in Xcode, use a device with either:\n\n- iOS 13 or later\n- macOS 10.15 or later\n\n### Build the vocabulary\n\nThe first step to using the BERT model is to import its vocabulary. The sample creates a vocabulary dictionary by splitting the vocabulary file into lines, each of which has one token. The function assigns each token’s (zero-based) line number as its value. For example, the first token,  `\"[PAD]\"`, has an ID of `0`, and the 5,001st token, `\"knight\"`, has an ID of `5000`.\n\n### Split the text into word tokens\n\nThe BERT model requires you to convert each word into one or more token IDs. Before you can use the vocabulary dictionary to find those IDs, you must divide the document’s text and the question’s text into word tokens.\n\n\n\nThe sample does this by using an [doc:\/\/com.apple.documentation\/documentation\/NaturalLanguage\/NLTagger], which breaks up a string into word tokens, each of which is a substring of the original.\n\n```swift\n\/\/ Store the tokenized substrings into an array.\nvar wordTokens = [Substring]()\n\n\/\/ Use Natural Language's NLTagger to tokenize the input by word.\nlet tagger = NLTagger(tagSchemes: [.tokenType])\ntagger.string = rawString\n\n\/\/ Find all tokens in the string and append to the array.\ntagger.enumerateTags(in: rawString.startIndex..<rawString.endIndex,\n                     unit: .word,\n                     scheme: .tokenType,\n                     options: [.omitWhitespace]) { (_, range) -> Bool in\n    wordTokens.append(rawString[range])\n    return true\n}\n\nreturn wordTokens\n```\n\nThe sample app leverages the tagger to split each string into tokens by using its [doc:\/\/com.apple.documentation\/documentation\/NaturalLanguage\/NLTagger\/enumerateTags(in:unit:scheme:options:using:)] method with the [doc:\/\/com.apple.documentation\/documentation\/NaturalLanguage\/NLTagScheme\/tokenType] tagging scheme and the [doc:\/\/com.apple.documentation\/documentation\/NaturalLanguage\/NLTokenUnit\/word] token unit.\n\n### Convert word or wordpiece tokens into their IDs\n\nFor speed and efficiency, the BERT model operates on token IDs, which are numbers that represent tokens, rather than operating on the text tokens themselves.\n\n```swift\nlet subTokenID = BERTVocabulary.tokenID(of: searchTerm)\n```\n\nIf a word token doesn’t exist in the vocabulary, the method looks for subtokens, or *wordpieces*. A wordpiece is a component of a larger word token. For example, the word *lethargic* isn’t in the vocabulary but its wordpieces, *let*, *har*, and *gic* are. Dividing the vocabulary’s large words into wordpieces reduces the vocabulary size and makes the BERT model more flexible. The model can understand words that aren’t explicitly in the vocabulary by combining their wordpieces.\n\nSecondary wordpieces, such as *har* and *gic*, each appear in the vocabulary with two leading pound signs, as `##har` and `##gic`.\n\nContinuing the example, the method converts document text into the word and wordpiece token IDs shown in the following figure.\n\n\n\n### Prepare the model input\n\nThe BERT model has two inputs:\n\n- `wordIDs` — Accepts the document and question texts\n- `wordTypes` — Tells the BERT model which elements of `wordIDs` are from the document\n\nThe sample creates the `wordIDs` array by arranging the token IDs in the following order:\n\n1. A *classification start* token ID, which has a value of `101` and appears as `\"[CLS]\"` in the vocabulary file\n2. The token IDs from the question string\n3. A *separator* token ID, which has a value of `102` and appears as `\"[SEP]\"` in the vocabulary file\n4. The token IDs from the text string\n5. Another separator token ID\n6. One or more *padding* token IDs for the remaining, unused elements, which have a value of `0` and appear as `\"[PAD]\"` in the vocabulary file\n\n```swift\n\/\/ Start the wordID array with the `classification start` token.\nvar wordIDs = [BERTVocabulary.classifyStartTokenID]\n\n\/\/ Add the question tokens and a separator.\nwordIDs += question.tokenIDs\nwordIDs += [BERTVocabulary.separatorTokenID]\n\n\/\/ Add the document tokens and a separator.\nwordIDs += document.tokenIDs\nwordIDs += [BERTVocabulary.separatorTokenID]\n\n\/\/ Fill the remaining token slots with padding tokens.\nlet tokenIDPadding = BERTInput.maxTokens - wordIDs.count\nwordIDs += Array(repeating: BERTVocabulary.paddingTokenID, count: tokenIDPadding)\n```\n\nNext, the sample prepares the `wordTypes` input by creating an array of the same length, where all the elements that correspond to the document text are `1` and all others are `0`.\n\n```swift\n\/\/ Set all of the token types before the document to 0.\nvar wordTypes = Array(repeating: 0, count: documentOffset)\n\n\/\/ Set all of the document token types to 1.\nwordTypes += Array(repeating: 1, count: document.tokens.count)\n\n\/\/ Set the remaining token types to 0.\nlet tokenTypePadding = BERTInput.maxTokens - wordTypes.count\nwordTypes += Array(repeating: 0, count: tokenTypePadding)\n```\n\nContinuing the example, the sample arranges the two input arrays with the values shown in the figure below.\n\n\n\nNext, the sample creates an [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLMultiArray] for each input and copies the contents from the arrays, which it uses to create a `BERTQAFP16Input` feature provider.\n\n\n\n```swift\n\/\/ Create the MLMultiArray instances.\nlet tokenIDMultiArray = try? MLMultiArray(wordIDs)\nlet wordTypesMultiArray = try? MLMultiArray(wordTypes)\n\n\/\/ Unwrap the MLMultiArray optionals.\nguard let tokenIDInput = tokenIDMultiArray else {\n    fatalError(\"Couldn't create wordID MLMultiArray input\")\n}\n\nguard let tokenTypeInput = wordTypesMultiArray else {\n    fatalError(\"Couldn't create wordType MLMultiArray input\")\n}\n\n\/\/ Create the BERT input MLFeatureProvider.\nlet modelInput = BERTQAFP16Input(wordIDs: tokenIDInput,\n                                 wordTypes: tokenTypeInput)\n```\n\n### Make a prediction\n\nYou use the BERT model to predict where to find an answer to the question in the document text, by giving the model your input feature provider with the input [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLMultiArray] instances.\n\n```swift\nguard let prediction = try? bertModel.prediction(input: modelInput) else {\n    return \"The BERT model is unable to make a prediction.\"\n}\n```\n\n### Find the answer\n\nYou locate the answer to the question by analyzing the output from the BERT model. The model produces two outputs, `startLogits` and `endLogits`. Each *logit* is a raw confidence score of where the BERT model predicts the beginning and the end of an answer is.\n\n\n\nIn this example, the best start and end logits are `6.08` and `7.53` for the tokens `\"the\"` and `\"fox\"`, respectively. The sample finds the indices of the highest-value starting and ending logits by:\n\n1. Converting each output logit [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLMultiArray] into a `Double` array.\n2. Isolating the logits relevant to the document.\n3. Finding the indices, in each array, to the 20 logits with the highest values.\n4. Searching through the 20 x 20 or fewer combinations of logits for the best combination.\n\n```swift\n\/\/ Convert the logits MLMultiArrays to [Double].\nlet startLogits = prediction.startLogits.doubleArray()\nlet endLogits = prediction.endLogits.doubleArray()\n\n\/\/ Isolate the logits for the document.\nlet startLogitsOfDoc = [Double](startLogits[range])\nlet endLogitsOfDoc = [Double](endLogits[range])\n\n\/\/ Only keep the top 20 (out of the possible ~380) indices for faster searching.\nlet topStartIndices = startLogitsOfDoc.indicesOfLargest(20)\nlet topEndIndices = endLogitsOfDoc.indicesOfLargest(20)\n\n\/\/ Search for the highest valued logit pairing.\nlet bestPair = findBestLogitPair(startLogits: startLogitsOfDoc,\n                                 bestStartIndices: topStartIndices,\n                                 endLogits: endLogitsOfDoc,\n                                 bestEndIndices: topEndIndices)\n```\n\nIn this example, the indices of the best start and end logits are `8` and `11`, respectively. The answer substring, located between indices `8` and `11` of the original text, is `“the quick brown fox”`.\n\n### Scale for larger documents\n\nThe BERT model included in this sample can process up to 384 tokens, including the three overhead tokens—one “classification start” token and two separator tokens—leaving 381 tokens for your text and question, combined. For larger texts that exceed this limitation, consider using one of these techniques:\n\n- Use a search mechanism to narrow down the relevant document text.\n- Break up the document text into sections, such as by paragraph, and make a prediction for each section.\n\n",
  "sections" : [

  ],
  "source" : "appleJSON",
  "title" : "Finding answers to questions in a text document",
  "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/finding-answers-to-questions-in-a-text-document"
}