{
  "abstract" : "Integrate a recurrent neural network model to process sequences of inputs.",
  "codeExamples" : [
    {
      "code" : "\/\/ Create the prompt to use as an example\nlet prompt = [\"O\", \"Romeo\"]\n\/\/ Use the generated input API to create the network's input, with no state\nlet modelInput = ShakespeareLanguageModelInput(previousWord: prompt[0], stateIn: nil)\n\/\/ Predict the 2nd word and generate a model state for \"O\"\nvar modelOutput = try model.prediction(input: modelInput)",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Set up the input for the second word (ignoring the predicted words)\nmodelInput.previousWord = prompt[1]\n\/\/ Use the output model state as the input model state for the next word\nmodelInput.stateIn = modelOutput.stateOut\n\/\/ Predict the third word\nmodelOutput = try model.prediction(input: modelInput)\n\/\/ The third word is now in modelOutput.nextWord",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Feed the next word and output state back into the network,\n\/\/ while the predicted word isn't the end of the sentence.\nwhile modelOutput.nextWord != \"<\/s>\" {\n    \/\/ Update the inputs from the network's output\n    modelInput.previousWord = modelOutput.nextWord\n    modelInput.stateIn = modelOutput.stateOut\n    \/\/ Predict the next word\n    modelOutput = try model.prediction(input: modelInput)\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "79f4ba445ed3aa57661e81e36a03fa44dac0a597dc22e3aac178b8ab7b5447bd",
  "crawledAt" : "2025-12-03T04:14:38Z",
  "id" : "96097A4C-012A-4815-8338-86BE1F01ED8E",
  "kind" : "article",
  "language" : "swift",
  "module" : "Core ML",
  "overview" : "## Overview\n\nSome machine learning problems require more than a single set of inputs and need to process a sequence of inputs over time. Neural network models can process a sequence of inputs, but require some state of the neural network to be maintained between inputs. Core ML provides a straightforward way to maintain the state of the network and process a sequence of inputs.\n\n### Understand the neural network workflow\n\nProcessing natural language is a difficult task for machine learning models because the number of possible sentences is infinite, making it impossible to encode all the inputs to the model. A common approach to reducing the number of possible inputs is to use letters or words as the input to the model, instead of processing the entire sentence as a single input. However, the model then needs a way to maintain state to “remember” what letters or words it has been presented previously in the sequence.\n\nConsider a neural network model trained to generate the Shakespeare play *Romeo and Juliet*. The neural network encodes the relationship between words and their neighboring words, without using explicit rules. In the popular line, “O, Romeo, Romeo, wherefore art thou Romeo?” the word Romeo appears three times, but each occurrence has a different word following it. The model needs a way to differentiate between the uses. Recurrent neural networks are a class of neural networks that address this problem by using the state of the model after processing each word as additional input when processing a word.\n\n\n\n[\/documentation\/coreml\/making_predictions_with_a_sequence_of_inputs#2940382] shows an example workflow of a network that has learned *Romeo and Juliet*. To start the phrase, “O” and a `nil` state are provided as input. The next word is predicted, and the network also generates a representation of its state for the input “O”, referred to as *f*(“O”). The next input word “Romeo” is combined with the previous state, *f*(“O”), to create the next input. Given that input, the model again outputs “Romeo” with high probability.\n\nThe next input word, “Romeo”, is identical to the previous input word. However, the state input is different. The state input is now *f*(“O”, “Romeo”). The different state allows the network to output the prediction “wherefore” even though the previous input words were identical.\n\n### Expose the state of the model\n\nAdd a recurrent neural network based model to your project in Xcode to see the state of the neural network exposed as input and output features.\n\n\n\n[\/documentation\/coreml\/making_predictions_with_a_sequence_of_inputs#2937173] shows the view in Xcode for the `ShakespeareLanguageModel` that has a recurrent neural network layer, with its state input and output features listed. Other recurrent neural networks, like Long Short-Term Memory and Gated Recurrent networks, create input and output features automatically.\n\nThis network takes two inputs: the input word and the state input, which is optional. The word is a [doc:\/\/com.apple.documentation\/documentation\/Swift\/String] and the state, named `stateIn`, is a one dimensional [doc:\/\/com.apple.coreml\/documentation\/CoreML\/MLMultiArray] of 512 [doc:\/\/com.apple.documentation\/documentation\/Swift\/Double] values. The state input is optional because the beginning of a sequence has no prior state.\n\nThere are three outputs of the network: the most probable next word, a [doc:\/\/com.apple.documentation\/documentation\/Swift\/Dictionary] of possible next words paired with their probabilities, and a one dimensional [doc:\/\/com.apple.coreml\/documentation\/CoreML\/MLMultiArray] of 512 [doc:\/\/com.apple.documentation\/documentation\/Swift\/Double] values, named `stateOut`, that represent the network’s state after processing the input.\n\nThe [doc:\/\/com.apple.coreml\/documentation\/CoreML\/MLMultiArray] output represents the state of the network, which is the level of activation of its internal nodes. In order for the network to “remember” what input sequence has been processed, the previous output state must accompany the next input.\n\nIn practice, you may come across layers with default state feature names. For example, Long Short-Term Memory networks will have default state parameters named `lstm_h_in` and `lstm_c_in` for inputs and `lstm_h_out` and `lsth_c_out` for outputs. The “h” refers to the hidden state and the “c” refers to the cell state used by an LSTM network. These output states must be carried over as input states for the network to function properly across the sequence of inputs.\n\n### Start a sequence of inputs\n\nThis network was trained to generate the rest of a sentence from the play, given two prompt words from a sentence. Begin processing a sequence of inputs with this model by passing in the first word from the prompt and `nil` as the previous state.\n\nListing 1. Initializing a network by using `nil` as the first state\n\nIn this sample code the `ShakespeareLanguageModelInput` class, generated by Xcode, is used to store the two inputs for the prediction call.\n\n### Make predictions based on previous state\n\nCreate an input using the second word from the prompt and the output state from the prediction as the input state. Use that input with the model to generate a prediction for the third word of the sentence.\n\nListing 2. Predicting the third word by using the second word and the state after processing the first word\n\nWhen you initialize the network with the first two words, the output state needs to be kept to represent the sequence of inputs. The predicted words and probabilities are ignored. They are ignored because the second word (Romeo) comes from the actual text instead of the model’s prediction.\n\nHowever, once the two word prompt has been processed, the output `nextWord` is the most likely third word in the sentence. It will be used as the input word, to generate the fourth word in the sentence. Using the output as input is repeated to generate the rest of the sentence.\n\nListing 3. Using the next word prediction as the input word, to generate the rest of the sentence\n\nThe code above repeats the process of using the predicted word and state as the input word and state until the predicted word is `<\/s>`. This network uses the string `<\/s>` to signify the end of the sentence.\n\n### Verify the output and reset the input state\n\nAt this point, the model has predicted the end of the sentence. The sequence of `nextWord` values represents the model’s prediction for the entire sentence. The entire predicted sentence could be presented to the user for verification or compared to the actual text programmatically.\n\nReset the input context by using `nil` as the input state (as in Listing 1), to start making predictions on a new sentence.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/CoreML\/making-predictions-with-a-sequence-of-inputs\ncrawled: 2025-12-03T04:14:38Z\n---\n\n# Making Predictions with a Sequence of Inputs\n\n**Article**\n\nIntegrate a recurrent neural network model to process sequences of inputs.\n\n## Overview\n\nSome machine learning problems require more than a single set of inputs and need to process a sequence of inputs over time. Neural network models can process a sequence of inputs, but require some state of the neural network to be maintained between inputs. Core ML provides a straightforward way to maintain the state of the network and process a sequence of inputs.\n\n### Understand the neural network workflow\n\nProcessing natural language is a difficult task for machine learning models because the number of possible sentences is infinite, making it impossible to encode all the inputs to the model. A common approach to reducing the number of possible inputs is to use letters or words as the input to the model, instead of processing the entire sentence as a single input. However, the model then needs a way to maintain state to “remember” what letters or words it has been presented previously in the sequence.\n\nConsider a neural network model trained to generate the Shakespeare play *Romeo and Juliet*. The neural network encodes the relationship between words and their neighboring words, without using explicit rules. In the popular line, “O, Romeo, Romeo, wherefore art thou Romeo?” the word Romeo appears three times, but each occurrence has a different word following it. The model needs a way to differentiate between the uses. Recurrent neural networks are a class of neural networks that address this problem by using the state of the model after processing each word as additional input when processing a word.\n\n\n\n[\/documentation\/coreml\/making_predictions_with_a_sequence_of_inputs#2940382] shows an example workflow of a network that has learned *Romeo and Juliet*. To start the phrase, “O” and a `nil` state are provided as input. The next word is predicted, and the network also generates a representation of its state for the input “O”, referred to as *f*(“O”). The next input word “Romeo” is combined with the previous state, *f*(“O”), to create the next input. Given that input, the model again outputs “Romeo” with high probability.\n\nThe next input word, “Romeo”, is identical to the previous input word. However, the state input is different. The state input is now *f*(“O”, “Romeo”). The different state allows the network to output the prediction “wherefore” even though the previous input words were identical.\n\n### Expose the state of the model\n\nAdd a recurrent neural network based model to your project in Xcode to see the state of the neural network exposed as input and output features.\n\n\n\n[\/documentation\/coreml\/making_predictions_with_a_sequence_of_inputs#2937173] shows the view in Xcode for the `ShakespeareLanguageModel` that has a recurrent neural network layer, with its state input and output features listed. Other recurrent neural networks, like Long Short-Term Memory and Gated Recurrent networks, create input and output features automatically.\n\nThis network takes two inputs: the input word and the state input, which is optional. The word is a [doc:\/\/com.apple.documentation\/documentation\/Swift\/String] and the state, named `stateIn`, is a one dimensional [doc:\/\/com.apple.coreml\/documentation\/CoreML\/MLMultiArray] of 512 [doc:\/\/com.apple.documentation\/documentation\/Swift\/Double] values. The state input is optional because the beginning of a sequence has no prior state.\n\nThere are three outputs of the network: the most probable next word, a [doc:\/\/com.apple.documentation\/documentation\/Swift\/Dictionary] of possible next words paired with their probabilities, and a one dimensional [doc:\/\/com.apple.coreml\/documentation\/CoreML\/MLMultiArray] of 512 [doc:\/\/com.apple.documentation\/documentation\/Swift\/Double] values, named `stateOut`, that represent the network’s state after processing the input.\n\nThe [doc:\/\/com.apple.coreml\/documentation\/CoreML\/MLMultiArray] output represents the state of the network, which is the level of activation of its internal nodes. In order for the network to “remember” what input sequence has been processed, the previous output state must accompany the next input.\n\nIn practice, you may come across layers with default state feature names. For example, Long Short-Term Memory networks will have default state parameters named `lstm_h_in` and `lstm_c_in` for inputs and `lstm_h_out` and `lsth_c_out` for outputs. The “h” refers to the hidden state and the “c” refers to the cell state used by an LSTM network. These output states must be carried over as input states for the network to function properly across the sequence of inputs.\n\n### Start a sequence of inputs\n\nThis network was trained to generate the rest of a sentence from the play, given two prompt words from a sentence. Begin processing a sequence of inputs with this model by passing in the first word from the prompt and `nil` as the previous state.\n\nListing 1. Initializing a network by using `nil` as the first state\n\n```swift\n\/\/ Create the prompt to use as an example\nlet prompt = [\"O\", \"Romeo\"]\n\/\/ Use the generated input API to create the network's input, with no state\nlet modelInput = ShakespeareLanguageModelInput(previousWord: prompt[0], stateIn: nil)\n\/\/ Predict the 2nd word and generate a model state for \"O\"\nvar modelOutput = try model.prediction(input: modelInput)\n```\n\nIn this sample code the `ShakespeareLanguageModelInput` class, generated by Xcode, is used to store the two inputs for the prediction call.\n\n### Make predictions based on previous state\n\nCreate an input using the second word from the prompt and the output state from the prediction as the input state. Use that input with the model to generate a prediction for the third word of the sentence.\n\nListing 2. Predicting the third word by using the second word and the state after processing the first word\n\n```swift\n\/\/ Set up the input for the second word (ignoring the predicted words)\nmodelInput.previousWord = prompt[1]\n\/\/ Use the output model state as the input model state for the next word\nmodelInput.stateIn = modelOutput.stateOut\n\/\/ Predict the third word\nmodelOutput = try model.prediction(input: modelInput)\n\/\/ The third word is now in modelOutput.nextWord\n```\n\nWhen you initialize the network with the first two words, the output state needs to be kept to represent the sequence of inputs. The predicted words and probabilities are ignored. They are ignored because the second word (Romeo) comes from the actual text instead of the model’s prediction.\n\nHowever, once the two word prompt has been processed, the output `nextWord` is the most likely third word in the sentence. It will be used as the input word, to generate the fourth word in the sentence. Using the output as input is repeated to generate the rest of the sentence.\n\nListing 3. Using the next word prediction as the input word, to generate the rest of the sentence\n\n```swift\n\/\/ Feed the next word and output state back into the network,\n\/\/ while the predicted word isn't the end of the sentence.\nwhile modelOutput.nextWord != \"<\/s>\" {\n    \/\/ Update the inputs from the network's output\n    modelInput.previousWord = modelOutput.nextWord\n    modelInput.stateIn = modelOutput.stateOut\n    \/\/ Predict the next word\n    modelOutput = try model.prediction(input: modelInput)\n}\n```\n\nThe code above repeats the process of using the predicted word and state as the input word and state until the predicted word is `<\/s>`. This network uses the string `<\/s>` to signify the end of the sentence.\n\n### Verify the output and reset the input state\n\nAt this point, the model has predicted the end of the sentence. The sequence of `nextWord` values represents the model’s prediction for the entire sentence. The entire predicted sentence could be presented to the user for verification or compared to the actual text programmatically.\n\nReset the input context by using `nil` as the input state (as in Listing 1), to start making predictions on a new sentence.\n\n## Model inputs and outputs\n\n- **MLFeatureValue**: A generic wrapper around an underlying value and the value’s type.\n- **MLSendableFeatureValue**: A sendable feature value.\n- **MLFeatureProvider**: An interface that represents a collection of values for either a model’s input or its output.\n- **MLDictionaryFeatureProvider**: A convenience wrapper for the given dictionary of data.\n- **MLBatchProvider**: An interface that represents a collection of feature providers.\n- **MLArrayBatchProvider**: A convenience wrapper for batches of feature providers.\n- **MLModelAsset**: An abstraction of a compiled Core ML model asset.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "A generic wrapper around an underlying value and the value’s type.",
          "name" : "MLFeatureValue",
          "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/MLFeatureValue"
        },
        {
          "description" : "A sendable feature value.",
          "name" : "MLSendableFeatureValue",
          "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/MLSendableFeatureValue"
        },
        {
          "description" : "An interface that represents a collection of values for either a model’s input or its output.",
          "name" : "MLFeatureProvider",
          "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/MLFeatureProvider"
        },
        {
          "description" : "A convenience wrapper for the given dictionary of data.",
          "name" : "MLDictionaryFeatureProvider",
          "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/MLDictionaryFeatureProvider"
        },
        {
          "description" : "An interface that represents a collection of feature providers.",
          "name" : "MLBatchProvider",
          "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/MLBatchProvider"
        },
        {
          "description" : "A convenience wrapper for batches of feature providers.",
          "name" : "MLArrayBatchProvider",
          "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/MLArrayBatchProvider"
        },
        {
          "description" : "An abstraction of a compiled Core ML model asset.",
          "name" : "MLModelAsset",
          "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/MLModelAsset"
        }
      ],
      "title" : "Model inputs and outputs"
    }
  ],
  "source" : "appleJSON",
  "title" : "Making Predictions with a Sequence of Inputs",
  "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/making-predictions-with-a-sequence-of-inputs"
}