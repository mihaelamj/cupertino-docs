{
  "abstract" : "Locate people and the stance of their bodies by analyzing an image with a PoseNet model.",
  "codeExamples" : [
    {
      "code" : "if captureSession.isRunning {\n    captureSession.stopRunning()\n}\n\ncaptureSession.beginConfiguration()\n\ncaptureSession.sessionPreset = .vga640x480\n\ntry setCaptureSessionInput()\n\ntry setCaptureSessionOutput()\n\ncaptureSession.commitConfiguration()",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Attempt to lock the image buffer to gain access to its memory.\nguard CVPixelBufferLockBaseAddress(pixelBuffer, .readOnly) == kCVReturnSuccess\n    else {\n        return\n}\n\n\/\/ Create Core Graphics image placeholder.\nvar image: CGImage?\n\n\/\/ Create a Core Graphics bitmap image from the pixel buffer.\nVTCreateCGImageFromCVPixelBuffer(pixelBuffer, options: nil, imageOut: &image)\n\n\/\/ Release the image buffer.\nCVPixelBufferUnlockBaseAddress(pixelBuffer, .readOnly)\n\nDispatchQueue.main.sync {\n    delegate.videoCapture(self, didCaptureFrame: image)\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Wrap the image in an instance of PoseNetInput to have it resized\n\/\/ before being passed to the PoseNet model.\nlet input = PoseNetInput(image: image, size: self.modelInputSize)",
      "language" : "swift"
    },
    {
      "code" : "guard let prediction = try? self.poseNetMLModel.prediction(from: input) else {\n    return\n}",
      "language" : "swift"
    },
    {
      "code" : "let poseNetOutput = PoseNetOutput(prediction: prediction,\n                                  modelInputSize: self.modelInputSize,\n                                  modelOutputStride: self.outputStride)\n\nDispatchQueue.main.async {\n    self.delegate?.poseNet(self, didPredict: poseNetOutput)\n}",
      "language" : "swift"
    },
    {
      "code" : "var pose = Pose()\n\n\/\/ For each joint, find its most likely position and associated confidence\n\/\/ by querying the heatmap array for the cell with the greatest\n\/\/ confidence and using this to compute its position.\npose.joints.values.forEach { joint in\n    configure(joint: joint)\n}\n\n\/\/ Compute and assign the confidence for the pose.\npose.confidence = pose.joints.values\n    .map { $0.confidence }.reduce(0, +) \/ Double(Joint.numberOfJoints)\n\n\/\/ Map the pose joints positions back onto the original image.\npose.joints.values.forEach { joint in\n    joint.position = joint.position.applying(modelToInputTransformation)\n}\n\nreturn pose",
      "language" : "swift"
    },
    {
      "code" : "var detectedPoses = [Pose]()\n\n\/\/ Iterate through the joints with the greatest confidence, referred to here as\n\/\/ candidate roots, using each as a starting point to assemble a pose.\nfor candidateRoot in candidateRoots {\n    \/\/ Ignore any candidates that are in the proximity of joints of the\n    \/\/ same type and have already been assigned to an existing pose.\n    let maxDistance = configuration.matchingJointDistance\n    guard !detectedPoses.contains(candidateRoot, within: maxDistance) else {\n        continue\n    }\n\n    var pose = assemblePose(from: candidateRoot)\n\n    \/\/ Compute the pose's confidence by dividing the sum of all\n    \/\/ non-overlapping joints, from existing poses, by the total\n    \/\/ number of joints.\n    pose.confidence = confidence(for: pose, detectedPoses: detectedPoses)\n\n    \/\/ Ignore any pose that has a confidence less than the assigned threshold.\n    guard pose.confidence >= configuration.poseConfidenceThreshold else {\n        continue\n    }\n\n    detectedPoses.append(pose)\n\n    \/\/ Exit early if enough poses have been detected.\n    if detectedPoses.count >= configuration.maxPoseCount {\n        break\n    }\n}\n\n\/\/ Map the pose joints positions back onto the original image using\n\/\/ the pre-computed transformation matrix.\ndetectedPoses.forEach { pose in\n    pose.joints.values.forEach { joint in\n        joint.position = joint.position.applying(modelToInputTransformation)\n    }\n}\n\nreturn detectedPoses",
      "language" : "swift"
    },
    {
      "code" : "let dstImageSize = CGSize(width: frame.width, height: frame.height)\nlet dstImageFormat = UIGraphicsImageRendererFormat()\n\ndstImageFormat.scale = 1\nlet renderer = UIGraphicsImageRenderer(size: dstImageSize,\n                                       format: dstImageFormat)\n\nlet dstImage = renderer.image { rendererContext in\n    \/\/ Draw the current frame as the background for the new image.\n    draw(image: frame, in: rendererContext.cgContext)\n\n    for pose in poses {\n        \/\/ Draw the segment lines.\n        for segment in PoseImageView.jointSegments {\n            let jointA = pose[segment.jointA]\n            let jointB = pose[segment.jointB]\n\n            guard jointA.isValid, jointB.isValid else {\n                continue\n            }\n\n            drawLine(from: jointA,\n                     to: jointB,\n                     in: rendererContext.cgContext)\n        }\n\n        \/\/ Draw the joints as circles above the segment lines.\n        for joint in pose.joints.values.filter({ $0.isValid }) {\n            draw(circle: joint, in: rendererContext.cgContext)\n        }\n    }\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "928120ee2520c4e81d214679b8ae530a2830adb4b3681dddeddb961d87e8d558",
  "crawledAt" : "2025-12-02T15:48:25Z",
  "id" : "2FE49AB6-57CA-4678-8C27-8EC7957FE2E4",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "Core ML",
  "overview" : "## Overview\n\nThis sample project provides an illustrative example of using a third-party [doc:\/\/com.apple.documentation\/documentation\/CoreML] model, PoseNet, to detect human body poses from frames captured using a camera. PoseNet models detect 17 different body parts or joints: eyes, ears, nose, shoulders, hips, elbows, knees, wrists, and ankles. Collectively these joints form a pose.\n\n\n\nThe sample finds the locations of the 17 joints for each person in the image and draws a wireframe pose on top of them.\n\n### Configure the capture session\n\nThe sample starts by getting an image from the device’s built-in camera using an [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureSession] (see [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/setting-up-a-capture-session]).\n\n### Acquire the captured image\n\n### Prepare the input for the PoseNet model\n\n### Pass the input to the PoseNet model\n\nThe sample app then proceeds to pass the input to the PoseNet’s [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLModel\/prediction(from:)-9y2aa] function to obtain its outputs, which the app uses to detect poses.\n\n### Analyze the PoseNet output to locate joints\n\nThe sample uses one of two algorithms to locate the joints of either one person or multiple persons. The single-person algorithm, the simplest and fastest, inspects the model’s outputs to locate the most prominent joints in the image and uses these joints to construct a single pose.\n\nThe multiple-person algorithm first identifies a set of candidate root joints as starting points. It uses these root joints to find neighboring joints and repeats the process until it has located all 17 joints of each person. For example, the algorithm may find a left knee with a high confidence, and then search for its adjacent joints, the left ankle and left hip.\n\n### Visualize the detected poses\n\nFor each detected pose, the sample app draws a wireframe over the input image, connecting the lines between the joints and then drawing circles for the joints themselves.\n\n",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/CoreML\/detecting-human-body-poses-in-an-image\ncrawled: 2025-12-02T15:48:25Z\n---\n\n# Detecting human body poses in an image\n\n**Sample Code**\n\nLocate people and the stance of their bodies by analyzing an image with a PoseNet model.\n\n## Overview\n\nThis sample project provides an illustrative example of using a third-party [doc:\/\/com.apple.documentation\/documentation\/CoreML] model, PoseNet, to detect human body poses from frames captured using a camera. PoseNet models detect 17 different body parts or joints: eyes, ears, nose, shoulders, hips, elbows, knees, wrists, and ankles. Collectively these joints form a pose.\n\n\n\nThe sample finds the locations of the 17 joints for each person in the image and draws a wireframe pose on top of them.\n\n\n\n### Configure the capture session\n\nThe sample starts by getting an image from the device’s built-in camera using an [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureSession] (see [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/setting-up-a-capture-session]).\n\n```swift\nif captureSession.isRunning {\n    captureSession.stopRunning()\n}\n\ncaptureSession.beginConfiguration()\n\ncaptureSession.sessionPreset = .vga640x480\n\ntry setCaptureSessionInput()\n\ntry setCaptureSessionOutput()\n\ncaptureSession.commitConfiguration()\n```\n\n### Acquire the captured image\n\n```swift\n\/\/ Attempt to lock the image buffer to gain access to its memory.\nguard CVPixelBufferLockBaseAddress(pixelBuffer, .readOnly) == kCVReturnSuccess\n    else {\n        return\n}\n\n\/\/ Create Core Graphics image placeholder.\nvar image: CGImage?\n\n\/\/ Create a Core Graphics bitmap image from the pixel buffer.\nVTCreateCGImageFromCVPixelBuffer(pixelBuffer, options: nil, imageOut: &image)\n\n\/\/ Release the image buffer.\nCVPixelBufferUnlockBaseAddress(pixelBuffer, .readOnly)\n\nDispatchQueue.main.sync {\n    delegate.videoCapture(self, didCaptureFrame: image)\n}\n```\n\n### Prepare the input for the PoseNet model\n\n```swift\n\/\/ Wrap the image in an instance of PoseNetInput to have it resized\n\/\/ before being passed to the PoseNet model.\nlet input = PoseNetInput(image: image, size: self.modelInputSize)\n```\n\n### Pass the input to the PoseNet model\n\nThe sample app then proceeds to pass the input to the PoseNet’s [doc:\/\/com.apple.documentation\/documentation\/CoreML\/MLModel\/prediction(from:)-9y2aa] function to obtain its outputs, which the app uses to detect poses.\n\n```swift\nguard let prediction = try? self.poseNetMLModel.prediction(from: input) else {\n    return\n}\n```\n\n```swift\nlet poseNetOutput = PoseNetOutput(prediction: prediction,\n                                  modelInputSize: self.modelInputSize,\n                                  modelOutputStride: self.outputStride)\n\nDispatchQueue.main.async {\n    self.delegate?.poseNet(self, didPredict: poseNetOutput)\n}\n```\n\n### Analyze the PoseNet output to locate joints\n\nThe sample uses one of two algorithms to locate the joints of either one person or multiple persons. The single-person algorithm, the simplest and fastest, inspects the model’s outputs to locate the most prominent joints in the image and uses these joints to construct a single pose.\n\n```swift\nvar pose = Pose()\n\n\/\/ For each joint, find its most likely position and associated confidence\n\/\/ by querying the heatmap array for the cell with the greatest\n\/\/ confidence and using this to compute its position.\npose.joints.values.forEach { joint in\n    configure(joint: joint)\n}\n\n\/\/ Compute and assign the confidence for the pose.\npose.confidence = pose.joints.values\n    .map { $0.confidence }.reduce(0, +) \/ Double(Joint.numberOfJoints)\n\n\/\/ Map the pose joints positions back onto the original image.\npose.joints.values.forEach { joint in\n    joint.position = joint.position.applying(modelToInputTransformation)\n}\n\nreturn pose\n```\n\nThe multiple-person algorithm first identifies a set of candidate root joints as starting points. It uses these root joints to find neighboring joints and repeats the process until it has located all 17 joints of each person. For example, the algorithm may find a left knee with a high confidence, and then search for its adjacent joints, the left ankle and left hip.\n\n```swift\nvar detectedPoses = [Pose]()\n\n\/\/ Iterate through the joints with the greatest confidence, referred to here as\n\/\/ candidate roots, using each as a starting point to assemble a pose.\nfor candidateRoot in candidateRoots {\n    \/\/ Ignore any candidates that are in the proximity of joints of the\n    \/\/ same type and have already been assigned to an existing pose.\n    let maxDistance = configuration.matchingJointDistance\n    guard !detectedPoses.contains(candidateRoot, within: maxDistance) else {\n        continue\n    }\n\n    var pose = assemblePose(from: candidateRoot)\n\n    \/\/ Compute the pose's confidence by dividing the sum of all\n    \/\/ non-overlapping joints, from existing poses, by the total\n    \/\/ number of joints.\n    pose.confidence = confidence(for: pose, detectedPoses: detectedPoses)\n\n    \/\/ Ignore any pose that has a confidence less than the assigned threshold.\n    guard pose.confidence >= configuration.poseConfidenceThreshold else {\n        continue\n    }\n\n    detectedPoses.append(pose)\n\n    \/\/ Exit early if enough poses have been detected.\n    if detectedPoses.count >= configuration.maxPoseCount {\n        break\n    }\n}\n\n\/\/ Map the pose joints positions back onto the original image using\n\/\/ the pre-computed transformation matrix.\ndetectedPoses.forEach { pose in\n    pose.joints.values.forEach { joint in\n        joint.position = joint.position.applying(modelToInputTransformation)\n    }\n}\n\nreturn detectedPoses\n```\n\n### Visualize the detected poses\n\nFor each detected pose, the sample app draws a wireframe over the input image, connecting the lines between the joints and then drawing circles for the joints themselves.\n\n\n\n```swift\nlet dstImageSize = CGSize(width: frame.width, height: frame.height)\nlet dstImageFormat = UIGraphicsImageRendererFormat()\n\ndstImageFormat.scale = 1\nlet renderer = UIGraphicsImageRenderer(size: dstImageSize,\n                                       format: dstImageFormat)\n\nlet dstImage = renderer.image { rendererContext in\n    \/\/ Draw the current frame as the background for the new image.\n    draw(image: frame, in: rendererContext.cgContext)\n\n    for pose in poses {\n        \/\/ Draw the segment lines.\n        for segment in PoseImageView.jointSegments {\n            let jointA = pose[segment.jointA]\n            let jointB = pose[segment.jointB]\n\n            guard jointA.isValid, jointB.isValid else {\n                continue\n            }\n\n            drawLine(from: jointA,\n                     to: jointB,\n                     in: rendererContext.cgContext)\n        }\n\n        \/\/ Draw the joints as circles above the segment lines.\n        for joint in pose.joints.values.filter({ $0.isValid }) {\n            draw(circle: joint, in: rendererContext.cgContext)\n        }\n    }\n}\n```\n\n## Image classification models\n\n- **Using Core ML for semantic image segmentation**: Identify multiple objects in an image by using the DEtection TRansformer image-segmentation model.\n- **Classifying Images with Vision and Core ML**: Crop and scale photos using the Vision framework and classify them with a Core ML model.\n- **Understanding a Dice Roll with Vision and Object Detection**: Detect dice position and values shown in a camera frame, and determine the end of a roll by leveraging a dice detection model.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Identify multiple objects in an image by using the DEtection TRansformer image-segmentation model.",
          "name" : "Using Core ML for semantic image segmentation",
          "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/using-core-ml-for-semantic-image-segmentation"
        },
        {
          "description" : "Crop and scale photos using the Vision framework and classify them with a Core ML model.",
          "name" : "Classifying Images with Vision and Core ML",
          "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/classifying-images-with-vision-and-core-ml"
        },
        {
          "description" : "Detect dice position and values shown in a camera frame, and determine the end of a roll by leveraging a dice detection model.",
          "name" : "Understanding a Dice Roll with Vision and Object Detection",
          "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/understanding-a-dice-roll-with-vision-and-object-detection"
        }
      ],
      "title" : "Image classification models"
    }
  ],
  "source" : "appleJSON",
  "title" : "Detecting human body poses in an image",
  "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/detecting-human-body-poses-in-an-image"
}