{
  "abstract" : "Identify multiple objects in an image by using the DEtection TRansformer image-segmentation model.",
  "codeExamples" : [
    {
      "code" : "final class ViewModel: ObservableObject {\n    typealias Model = DETRResnet50SemanticSegmentationF16P8\n    typealias ModelOutput = DETRResnet50SemanticSegmentationF16P8Output\n}",
      "language" : "swift"
    },
    {
      "code" : "nonisolated func loadModel() async {\n    do {\n        let model = try Model()\n        let labels = model.model.segmentationLabels\n        await didLoadModel(model, labels: labels)\n    } catch {\n        Task { @MainActor in\n            errorMessage = \"The model failed to load: \\(error.localizedDescription)\"\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "var body: some View {\n    VStack(spacing: 20) {\n        \n        \/\/ Other existing view configuration.\n\n        \/\/ Display a load message when the sample loads the model.\n        if !viewModel.isModelLoaded {\n            ProgressView(\"Loading model...\")\n        }\n\n        \/\/ Display an error message.\n        if let message = viewModel.errorMessage, !message.isEmpty {\n            Text(\"Error: \\(message)\")\n        }\n    }\n    .photosPicker(isPresented: $viewModel.showPhotoPicker, selection: $viewModel.selectedPhoto)\n    .task {\n        \/\/ Load the model asynchronously.\n        if loadModel {\n            await viewModel.loadModel()\n        }\n    }\n",
      "language" : "swift"
    },
    {
      "code" : "extension MLModel {\n    \/\/\/ The segmentation labels specified in the metadata.\n    var segmentationLabels: [String] {\n        if let metadata = modelDescription.metadata[.creatorDefinedKey] as? [String: Any],\n           let params = metadata[\"com.apple.coreml.model.preview.params\"] as? String,\n           let data = params.data(using: .utf8),\n           let parsed = try? JSONSerialization.jsonObject(with: data) as? [String: Any],\n           let labels = parsed[\"labels\"] as? [String] {\n            return labels\n        } else {\n            return []\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Resize the input image to the target size.\nlet resizedImage = await CIImage(cgImage: inputImage.cgImage!).resized(to: targetSize)\n\n\/\/ Get the pixel buffer for the image.\nlet pixelBuffer = context.render(resizedImage, pixelFormat: kCVPixelFormatType_32ARGB)\n\n\/\/ Perform inference.\nlet result = try model.prediction(image: pixelBuffer)\n\n\/\/ Get the result.\nlet resultArray = result.semanticPredictionsShapedArray",
      "language" : "swift"
    },
    {
      "code" : "func renderMask() throws -> CGImage? {\n    guard let resultArray else {\n        return nil\n    }\n\n    \/\/ Convert the results to a mask.\n    var bitmap = resultArray.scalars.map { labelIndex in\n        let label = self.labelNames[Int(labelIndex)]\n        if label == selectedLabel {\n            return 0xFFFFFF00 as UInt32\n        } else {\n            return 0x00000000 as UInt32\n        }\n    }\n\n    \/\/ Convert the mask to an image.\n    let width = resultArray.shape[1]\n    let height = resultArray.shape[0]\n    let image = bitmap.withUnsafeMutableBytes { bytes in\n        let context = CGContext(\n            data: bytes.baseAddress,\n            width: width,\n            height: height,\n            bitsPerComponent: 8,\n            bytesPerRow: 4 * width,\n            space: CGColorSpace(name: CGColorSpace.sRGB)!,\n            bitmapInfo: CGBitmapInfo.byteOrder32Little.rawValue | CGImageAlphaInfo.noneSkipLast.rawValue  \/\/ RGB0\n        )\n        return context?.makeImage()\n    }\n\n    return image!\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "6e2d7ca55bcf2c2a4c7bfaa29f5091162a0043473f617ff939549791290fcfd2",
  "crawledAt" : "2025-12-02T15:30:20Z",
  "id" : "4052D973-BDA5-41E5-98E9-313A99CB4637",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "Core ML",
  "overview" : "## Overview\n\nSemantic image-segmentation models identify multiple objects on an input image you provide. For each object, the model output provides the precise locations of each pixel that represents it, so that you can do things like visualize the area of an image that corresponds to the object.\n\nThis sample code project shows you how to load the model, read the model’s metadata to get label information, and perform offline inference. It also shows how to create a mask — from the image-segment label — and overlay it on the original image.\n\n### Select the image-segmentation model\n\nThe sample code project includes an 8-bit palletization image-segmentation model — `DETRResnet50SemanticSegmentationF16P8` — in the `models` directory. Replace the model with a different version by dropping the model `.mlpackage` into the `models` folder in Xcode. To download a DETR model using 16-bit, see [https:\/\/developer.apple.com\/machine-learning\/models\/].\n\nSelect the model in the Xcode file navigator to view details in an Xcode model preview area. The following image shows the metadata and operations for the 8-bit model:\n\n\n\nTo use a model with a different name, but using the same semantics, replace the source code references in `ViewModel` with the model class for `Model` and `ModelOutput`:\n\n### Load the model\n\nHow long it takes to load a model depends on many factors, including the size of the model. The sample app loads the segmentation model asynchronously to avoid blocking the calling thread:\n\nIn the `MainView`, the sample loads the model using the `task` modifier, and displays a progress view to display the load time. The sample uses the `ObservableObject` protocol to observe when the loading is complete:\n\n### Read the image-segmentation model metadata\n\nThe `Core ML` framework uses optional metadata to map segment label values into strings an app reads. The metadata is in JSON format, and consists of two optional lists of strings:\n\nThe following image shows the label metadata in the Xcode model preview area:\n\n\n\nThe sample app reads the segmentation labels from the model metadata by calling the `readSegmentationLabels` method in `MLModel`. For more information about the format `Core ML` uses for metadata, see [https:\/\/apple.github.io\/coremltools\/docs-guides\/source\/xcode-model-preview-types.html]:\n\nBecause the colors from the model metadata are optional, a common practice is to use a generic set of colors that an app defines. A semantic image-segmentation model can generate a large number of labels, and may need to reuse colors or use very similar colors. To assist people with color blindness, avoid using only color to identify different objects. For more information about using inclusive colors, see [https:\/\/developer.apple.com\/design\/human-interface-guidelines\/color#Inclusive-color].\n\n### Perform inference\n\nThe 8-bit DETR model takes a `CVPixelBuffer` with the type `kCVPixelFormatType_32ARGB` as the input, and generates a `ShapedArray` of `Int32` as output. For both arrays, the model uses fixed dimensions with the size `(448, 448)`. These values are specific to the model, and can be different for another model. To perform inference on the image-segmentation model, the sample app calls the `performInference` method. After inference, the sample app gets the result array from the `semanticPredictionsShapedArray` property:\n\n### Overlay the result\n\nThe sample app provides a visualization of model output by overlaying a masked image — representing the selected segment — on top of the original, like the sky in the following image:\n\n\n\nBased on the selection, the sample app colors the labeled regions by calling the `renderMask` method in `ViewModel`:",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/CoreML\/using-core-ml-for-semantic-image-segmentation\ncrawled: 2025-12-02T15:30:20Z\n---\n\n# Using Core ML for semantic image segmentation\n\n**Sample Code**\n\nIdentify multiple objects in an image by using the DEtection TRansformer image-segmentation model.\n\n## Overview\n\nSemantic image-segmentation models identify multiple objects on an input image you provide. For each object, the model output provides the precise locations of each pixel that represents it, so that you can do things like visualize the area of an image that corresponds to the object.\n\nThis sample code project shows you how to load the model, read the model’s metadata to get label information, and perform offline inference. It also shows how to create a mask — from the image-segment label — and overlay it on the original image.\n\n### Select the image-segmentation model\n\nThe sample code project includes an 8-bit palletization image-segmentation model — `DETRResnet50SemanticSegmentationF16P8` — in the `models` directory. Replace the model with a different version by dropping the model `.mlpackage` into the `models` folder in Xcode. To download a DETR model using 16-bit, see [https:\/\/developer.apple.com\/machine-learning\/models\/].\n\nSelect the model in the Xcode file navigator to view details in an Xcode model preview area. The following image shows the metadata and operations for the 8-bit model:\n\n\n\nTo use a model with a different name, but using the same semantics, replace the source code references in `ViewModel` with the model class for `Model` and `ModelOutput`:\n\n```swift\nfinal class ViewModel: ObservableObject {\n    typealias Model = DETRResnet50SemanticSegmentationF16P8\n    typealias ModelOutput = DETRResnet50SemanticSegmentationF16P8Output\n}\n```\n\n### Load the model\n\nHow long it takes to load a model depends on many factors, including the size of the model. The sample app loads the segmentation model asynchronously to avoid blocking the calling thread:\n\n```swift\nnonisolated func loadModel() async {\n    do {\n        let model = try Model()\n        let labels = model.model.segmentationLabels\n        await didLoadModel(model, labels: labels)\n    } catch {\n        Task { @MainActor in\n            errorMessage = \"The model failed to load: \\(error.localizedDescription)\"\n        }\n    }\n}\n```\n\nIn the `MainView`, the sample loads the model using the `task` modifier, and displays a progress view to display the load time. The sample uses the `ObservableObject` protocol to observe when the loading is complete:\n\n```swift\nvar body: some View {\n    VStack(spacing: 20) {\n        \n        \/\/ Other existing view configuration.\n\n        \/\/ Display a load message when the sample loads the model.\n        if !viewModel.isModelLoaded {\n            ProgressView(\"Loading model...\")\n        }\n\n        \/\/ Display an error message.\n        if let message = viewModel.errorMessage, !message.isEmpty {\n            Text(\"Error: \\(message)\")\n        }\n    }\n    .photosPicker(isPresented: $viewModel.showPhotoPicker, selection: $viewModel.selectedPhoto)\n    .task {\n        \/\/ Load the model asynchronously.\n        if loadModel {\n            await viewModel.loadModel()\n        }\n    }\n\n```\n\n### Read the image-segmentation model metadata\n\nThe `Core ML` framework uses optional metadata to map segment label values into strings an app reads. The metadata is in JSON format, and consists of two optional lists of strings:\n\n- A `label` list that contains the user-readable names for each label\n- A `color` list for suggested colors — as hexadecimal RGB codes — an app can use\n\nThe following image shows the label metadata in the Xcode model preview area:\n\n\n\nThe sample app reads the segmentation labels from the model metadata by calling the `readSegmentationLabels` method in `MLModel`. For more information about the format `Core ML` uses for metadata, see [https:\/\/apple.github.io\/coremltools\/docs-guides\/source\/xcode-model-preview-types.html]:\n\n```swift\nextension MLModel {\n    \/\/\/ The segmentation labels specified in the metadata.\n    var segmentationLabels: [String] {\n        if let metadata = modelDescription.metadata[.creatorDefinedKey] as? [String: Any],\n           let params = metadata[\"com.apple.coreml.model.preview.params\"] as? String,\n           let data = params.data(using: .utf8),\n           let parsed = try? JSONSerialization.jsonObject(with: data) as? [String: Any],\n           let labels = parsed[\"labels\"] as? [String] {\n            return labels\n        } else {\n            return []\n        }\n    }\n}\n```\n\nBecause the colors from the model metadata are optional, a common practice is to use a generic set of colors that an app defines. A semantic image-segmentation model can generate a large number of labels, and may need to reuse colors or use very similar colors. To assist people with color blindness, avoid using only color to identify different objects. For more information about using inclusive colors, see [https:\/\/developer.apple.com\/design\/human-interface-guidelines\/color#Inclusive-color].\n\n### Perform inference\n\nThe 8-bit DETR model takes a `CVPixelBuffer` with the type `kCVPixelFormatType_32ARGB` as the input, and generates a `ShapedArray` of `Int32` as output. For both arrays, the model uses fixed dimensions with the size `(448, 448)`. These values are specific to the model, and can be different for another model. To perform inference on the image-segmentation model, the sample app calls the `performInference` method. After inference, the sample app gets the result array from the `semanticPredictionsShapedArray` property:\n\n```swift\n\/\/ Resize the input image to the target size.\nlet resizedImage = await CIImage(cgImage: inputImage.cgImage!).resized(to: targetSize)\n\n\/\/ Get the pixel buffer for the image.\nlet pixelBuffer = context.render(resizedImage, pixelFormat: kCVPixelFormatType_32ARGB)\n\n\/\/ Perform inference.\nlet result = try model.prediction(image: pixelBuffer)\n\n\/\/ Get the result.\nlet resultArray = result.semanticPredictionsShapedArray\n```\n\n### Overlay the result\n\nThe sample app provides a visualization of model output by overlaying a masked image — representing the selected segment — on top of the original, like the sky in the following image:\n\n\n\nBased on the selection, the sample app colors the labeled regions by calling the `renderMask` method in `ViewModel`:\n\n```swift\nfunc renderMask() throws -> CGImage? {\n    guard let resultArray else {\n        return nil\n    }\n\n    \/\/ Convert the results to a mask.\n    var bitmap = resultArray.scalars.map { labelIndex in\n        let label = self.labelNames[Int(labelIndex)]\n        if label == selectedLabel {\n            return 0xFFFFFF00 as UInt32\n        } else {\n            return 0x00000000 as UInt32\n        }\n    }\n\n    \/\/ Convert the mask to an image.\n    let width = resultArray.shape[1]\n    let height = resultArray.shape[0]\n    let image = bitmap.withUnsafeMutableBytes { bytes in\n        let context = CGContext(\n            data: bytes.baseAddress,\n            width: width,\n            height: height,\n            bitsPerComponent: 8,\n            bytesPerRow: 4 * width,\n            space: CGColorSpace(name: CGColorSpace.sRGB)!,\n            bitmapInfo: CGBitmapInfo.byteOrder32Little.rawValue | CGImageAlphaInfo.noneSkipLast.rawValue  \/\/ RGB0\n        )\n        return context?.makeImage()\n    }\n\n    return image!\n}\n```\n\n## Image classification models\n\n- **Classifying Images with Vision and Core ML**: Crop and scale photos using the Vision framework and classify them with a Core ML model.\n- **Detecting human body poses in an image**: Locate people and the stance of their bodies by analyzing an image with a PoseNet model.\n- **Understanding a Dice Roll with Vision and Object Detection**: Detect dice position and values shown in a camera frame, and determine the end of a roll by leveraging a dice detection model.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Crop and scale photos using the Vision framework and classify them with a Core ML model.",
          "name" : "Classifying Images with Vision and Core ML",
          "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/classifying-images-with-vision-and-core-ml"
        },
        {
          "description" : "Locate people and the stance of their bodies by analyzing an image with a PoseNet model.",
          "name" : "Detecting human body poses in an image",
          "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/detecting-human-body-poses-in-an-image"
        },
        {
          "description" : "Detect dice position and values shown in a camera frame, and determine the end of a roll by leveraging a dice detection model.",
          "name" : "Understanding a Dice Roll with Vision and Object Detection",
          "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/understanding-a-dice-roll-with-vision-and-object-detection"
        }
      ],
      "title" : "Image classification models"
    }
  ],
  "source" : "appleJSON",
  "title" : "Using Core ML for semantic image segmentation",
  "url" : "https:\/\/developer.apple.com\/documentation\/CoreML\/using-core-ml-for-semantic-image-segmentation"
}