{
  "abstract" : "Alert people before displaying images or video that might be sensitive.",
  "codeExamples" : [
    {
      "code" : "let analyzer = SCSensitivityAnalyzer()",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Check the current analysis policy. \nlet policy = analyzer.analysisPolicy\nif policy == .disabled { return } ",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Analyze an image file at a particular URL.\nlet response = try await analyzer.analyzeImage(at: url)\n\n\/\/ Analyze an image in memory.\nlet response = try await analyzer.analyzeImage(image.cgImage)",
      "language" : "swift"
    },
    {
      "code" : "let handler = analyzer.videoAnalysis(forFileAt: videoFileUrl)\nlet response = try await handler.hasSensitiveContent()",
      "language" : "swift"
    },
    {
      "code" : "let handler = analyzer.videoAnalysis(forFileAt: videoFileUrl)\n\n\/\/ Track analysis and keep the user informed on progress.\nlet progress = handler.progress",
      "language" : "swift"
    }
  ],
  "contentHash" : "a765b4fe93aaeb00ed0c030dfe6313035e0e766c71ca1924d82f791f374b867d",
  "crawledAt" : "2025-12-01T09:09:01Z",
  "id" : "FA473DCF-A102-4E6D-BD17-57BC596CFADA",
  "kind" : "article",
  "module" : "SensitiveContentAnalysis",
  "overview" : "## Overview\n\nIn iOS 17 and macOS 14, the Sensitive Content Warning user preference and Communication Safety parental control in Screen Time enable the user to indicate their desire to detect nudity in images and video. To provide people their preferred experience while those settings are active, use the SensitiveContentAnalysis framework to check incoming media before displaying it.\n\nWhen the framework flags user-provided media as sensitive, intervene by adjusting the user interface in a way that explains the situation and gives the user options to proceed. Avoid displaying flagged content until the user makes a decision.\n\nThe Sensitive Content Warning setting is for adults; Communication Safety in Screen Time is intended to protect content on the devices of children. Vary your app’s intervention strategy according to the active setting. For instance, keep interaction brief and unobstructive for adults, whereas for children, use the full view and communicate in age-appropriate language.\n\nThe SensitiveContentAnalysis framework doesn’t dictate your user interface. You can tailor your app’s experience according to the examples in apps such as Messages. The following image depicts Messages in iOS 17 when a potentially explicit image arrives from a contact while the Sensitive Content Warning setting is on. Messages blurs the image with a UI that gives the user the option to display the flagged content. It also provides a menu of options to the user, such as blocking the contact or accessing Apple-provided resources on the web for topics like grooming, harassment, and online safety.\n\n\n\n### Add the app entitlement\n\nThe OS requires the [doc:\/\/com.apple.documentation\/documentation\/BundleResources\/Entitlements\/com.apple.developer.sensitivecontentanalysis.client] entitlement in your app’s code signature to use SensitiveContentAnalysis. Calls to the framework fail to return positive results without it. You can can add this entitlement to your app by enabling the Sensitive Content Analysis capability in Xcode; see [doc:\/\/com.apple.documentation\/documentation\/Xcode\/adding-capabilities-to-your-app].\n\nAny team member of the paid App Store developer program can add the entitlement to an app after enabling the capability in Xcode and then signing the Developer Program License Agreement on the [http:\/\/developer.apple.com\/account] website.\n\n### Check images or video for unsafe content\n\nTo check a user-provided image for nudity, create an [doc:\/\/com.apple.SensitiveContentAnalysis\/documentation\/SensitiveContentAnalysis\/SCSensitivityAnalyzer] object.\n\nAn app can utilize the SensitiveContentAnalysis framework by detecting sensitive content when the system sets [doc:\/\/com.apple.SensitiveContentAnalysis\/documentation\/SensitiveContentAnalysis\/SCSensitivityAnalyzer\/analysisPolicy] to either:\n\nAn app can’t leverage the SensitiveContentAnalysis framework when the system sets [doc:\/\/com.apple.SensitiveContentAnalysis\/documentation\/SensitiveContentAnalysis\/SCSensitivityAnalyzer\/analysisPolicy] to [doc:\/\/com.apple.SensitiveContentAnalysis\/documentation\/SensitiveContentAnalysis\/SCSensitivityAnalysisPolicy\/disabled]. This indicates one or more of the following:\n\nIf [doc:\/\/com.apple.SensitiveContentAnalysis\/documentation\/SensitiveContentAnalysis\/SCSensitivityAnalyzer\/analysisPolicy] is [doc:\/\/com.apple.SensitiveContentAnalysis\/documentation\/SensitiveContentAnalysis\/SCSensitivityAnalysisPolicy\/disabled], the SensitiveContentAnalysis framework won’t detect nudity.\n\nIf [doc:\/\/com.apple.SensitiveContentAnalysis\/documentation\/SensitiveContentAnalysis\/SCSensitivityAnalyzer\/analysisPolicy] is a value other than [doc:\/\/com.apple.SensitiveContentAnalysis\/documentation\/SensitiveContentAnalysis\/SCSensitivityAnalysisPolicy\/disabled], you can check images or video for sensitive content. To check an image, call one of the `analyzeImage` functions passing in the user-provided image or a URL to an image.\n\nTo analyze a video file, pass a video URL into the `videoAnalysis` function and wait on the `hasSensitiveContent` function.\n\n### Handle performance implications\n\nAlthough sensitivity checks incur additional image processing and introduce a delay while checking video, the presence of active user preferences indicate the user’s expectation to receive protections at the cost of time to review.\n\nDepending on the amount of time that checks take to complete, adjust your app’s UI to accommodate the delay. For example, while checking video for sensitive content, use the `progress` property on the `videoAnalysis` return value to provide the user with status in a custom UI during the process.\n\n### Tailor user interface for the Sensitive Content Warning setting\n\nWhen your app detects sensitive content, present a UI that coordinates with the active `analysisPolicy`.\n\nWhen the user has enabled the Sensitive Content Warning setting, keep the user interface brief. In addition:\n\nFor example, by providing a blurred version of the potentially sensitive image in its normal location, Messages on iOS 17 and later implements *inline* intervention. Messages also keeps the information presented in the UI brief by providing a one-word button to display the image, and an icon button for more options.\n\n\n\n### Tailor user interface for the Communication Safety parental control\n\nWhen a user has enabled the Communication Safety option in Screen Time, intervene in your app as it receives sensitive content over a network, and before transmitting sensitive content over a network.\n\nDisplay the intervention in a modal view and use child-appropriate language in your UI. For example, Messages on macOS 14 and iOS 17 interrupts the normal flow of the app by presenting a modal sheet that describes flagged content with broadly understood terms, such as “a naked photo”.\n\n\n\nIn addition, use two consecutive panes. The following image depicts Messages in iOS 17 when a child attempts to view a sensitive photo sent from a contact. Tapping “I’m sure” on the left raises the second pane on the right.\n\n\n\nInline interventions for the Sensitive Content Warning setting aim to interfere minimally with an adult’s workflow while giving them quick access to help resources. The additional navigation required in the Communication Safety setting provides children with the opportunity to make a considered choice, and tries to avoid ever catching them off guard.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/SensitiveContentAnalysis\/detecting-nudity-in-media-and-providing-intervention-options\ncrawled: 2025-12-01T09:09:01Z\n---\n\n# Detecting nudity in media and providing intervention options\n\n**Article**\n\nAlert people before displaying images or video that might be sensitive.\n\n## Overview\n\nIn iOS 17 and macOS 14, the Sensitive Content Warning user preference and Communication Safety parental control in Screen Time enable the user to indicate their desire to detect nudity in images and video. To provide people their preferred experience while those settings are active, use the SensitiveContentAnalysis framework to check incoming media before displaying it.\n\nWhen the framework flags user-provided media as sensitive, intervene by adjusting the user interface in a way that explains the situation and gives the user options to proceed. Avoid displaying flagged content until the user makes a decision.\n\nThe Sensitive Content Warning setting is for adults; Communication Safety in Screen Time is intended to protect content on the devices of children. Vary your app’s intervention strategy according to the active setting. For instance, keep interaction brief and unobstructive for adults, whereas for children, use the full view and communicate in age-appropriate language.\n\nThe SensitiveContentAnalysis framework doesn’t dictate your user interface. You can tailor your app’s experience according to the examples in apps such as Messages. The following image depicts Messages in iOS 17 when a potentially explicit image arrives from a contact while the Sensitive Content Warning setting is on. Messages blurs the image with a UI that gives the user the option to display the flagged content. It also provides a menu of options to the user, such as blocking the contact or accessing Apple-provided resources on the web for topics like grooming, harassment, and online safety.\n\n\n\n\n\n### Add the app entitlement\n\nThe OS requires the [doc:\/\/com.apple.documentation\/documentation\/BundleResources\/Entitlements\/com.apple.developer.sensitivecontentanalysis.client] entitlement in your app’s code signature to use SensitiveContentAnalysis. Calls to the framework fail to return positive results without it. You can can add this entitlement to your app by enabling the Sensitive Content Analysis capability in Xcode; see [doc:\/\/com.apple.documentation\/documentation\/Xcode\/adding-capabilities-to-your-app].\n\nAny team member of the paid App Store developer program can add the entitlement to an app after enabling the capability in Xcode and then signing the Developer Program License Agreement on the [http:\/\/developer.apple.com\/account] website.\n\n\n\n### Check images or video for unsafe content\n\nTo check a user-provided image for nudity, create an [doc:\/\/com.apple.SensitiveContentAnalysis\/documentation\/SensitiveContentAnalysis\/SCSensitivityAnalyzer] object.\n\n```swift\nlet analyzer = SCSensitivityAnalyzer()\n```\n\nAn app can utilize the SensitiveContentAnalysis framework by detecting sensitive content when the system sets [doc:\/\/com.apple.SensitiveContentAnalysis\/documentation\/SensitiveContentAnalysis\/SCSensitivityAnalyzer\/analysisPolicy] to either:\n\n- [doc:\/\/com.apple.SensitiveContentAnalysis\/documentation\/SensitiveContentAnalysis\/SCSensitivityAnalysisPolicy\/simpleInterventions], which indicates that the Sensitive Content Warning user preference is enabled in Settings.\n- [doc:\/\/com.apple.SensitiveContentAnalysis\/documentation\/SensitiveContentAnalysis\/SCSensitivityAnalysisPolicy\/descriptiveInterventions], which indicates that the Communication Safety settings is active in Screen Time.\n\nAn app can’t leverage the SensitiveContentAnalysis framework when the system sets [doc:\/\/com.apple.SensitiveContentAnalysis\/documentation\/SensitiveContentAnalysis\/SCSensitivityAnalyzer\/analysisPolicy] to [doc:\/\/com.apple.SensitiveContentAnalysis\/documentation\/SensitiveContentAnalysis\/SCSensitivityAnalysisPolicy\/disabled]. This indicates one or more of the following:\n\n- The app lacks the necessary `com.apple.developer.sensitivecontentanalysis.client` entitlement.\n- Neither the Sensitive Content Warning user preference nor the Communication Safety setting in Screen Time are active.\n- One of the nudity detection settings are active, but the user disabled sensitive-content warnings for your app in Settings.\n\nIf [doc:\/\/com.apple.SensitiveContentAnalysis\/documentation\/SensitiveContentAnalysis\/SCSensitivityAnalyzer\/analysisPolicy] is [doc:\/\/com.apple.SensitiveContentAnalysis\/documentation\/SensitiveContentAnalysis\/SCSensitivityAnalysisPolicy\/disabled], the SensitiveContentAnalysis framework won’t detect nudity.\n\n```swift\n\/\/ Check the current analysis policy. \nlet policy = analyzer.analysisPolicy\nif policy == .disabled { return } \n```\n\nIf [doc:\/\/com.apple.SensitiveContentAnalysis\/documentation\/SensitiveContentAnalysis\/SCSensitivityAnalyzer\/analysisPolicy] is a value other than [doc:\/\/com.apple.SensitiveContentAnalysis\/documentation\/SensitiveContentAnalysis\/SCSensitivityAnalysisPolicy\/disabled], you can check images or video for sensitive content. To check an image, call one of the `analyzeImage` functions passing in the user-provided image or a URL to an image.\n\n```swift\n\/\/ Analyze an image file at a particular URL.\nlet response = try await analyzer.analyzeImage(at: url)\n\n\/\/ Analyze an image in memory.\nlet response = try await analyzer.analyzeImage(image.cgImage)\n```\n\nTo analyze a video file, pass a video URL into the `videoAnalysis` function and wait on the `hasSensitiveContent` function.\n\n```swift\nlet handler = analyzer.videoAnalysis(forFileAt: videoFileUrl)\nlet response = try await handler.hasSensitiveContent()\n```\n\n\n\n### Handle performance implications\n\nAlthough sensitivity checks incur additional image processing and introduce a delay while checking video, the presence of active user preferences indicate the user’s expectation to receive protections at the cost of time to review.\n\nDepending on the amount of time that checks take to complete, adjust your app’s UI to accommodate the delay. For example, while checking video for sensitive content, use the `progress` property on the `videoAnalysis` return value to provide the user with status in a custom UI during the process.\n\n```swift\nlet handler = analyzer.videoAnalysis(forFileAt: videoFileUrl)\n\n\/\/ Track analysis and keep the user informed on progress.\nlet progress = handler.progress\n```\n\n### Tailor user interface for the Sensitive Content Warning setting\n\nWhen your app detects sensitive content, present a UI that coordinates with the active `analysisPolicy`.\n\nWhen the user has enabled the Sensitive Content Warning setting, keep the user interface brief. In addition:\n\n- Display the UI in an unobstructive manner, such as inline with other content instead of using the full screen.\n- Provide intervention as the app receives sensitive content from the network but allow the app to transmit unchecked content over the network.\n\nFor example, by providing a blurred version of the potentially sensitive image in its normal location, Messages on iOS 17 and later implements *inline* intervention. Messages also keeps the information presented in the UI brief by providing a one-word button to display the image, and an icon button for more options.\n\n\n\n### Tailor user interface for the Communication Safety parental control\n\nWhen a user has enabled the Communication Safety option in Screen Time, intervene in your app as it receives sensitive content over a network, and before transmitting sensitive content over a network.\n\nDisplay the intervention in a modal view and use child-appropriate language in your UI. For example, Messages on macOS 14 and iOS 17 interrupts the normal flow of the app by presenting a modal sheet that describes flagged content with broadly understood terms, such as “a naked photo”.\n\n\n\nIn addition, use two consecutive panes. The following image depicts Messages in iOS 17 when a child attempts to view a sensitive photo sent from a contact. Tapping “I’m sure” on the left raises the second pane on the right.\n\n\n\nInline interventions for the Sensitive Content Warning setting aim to interfere minimally with an adult’s workflow while giving them quick access to help resources. The additional navigation required in the Communication Safety setting provides children with the opportunity to make a considered choice, and tries to avoid ever catching them off guard.\n\n",
  "sections" : [

  ],
  "source" : "appleJSON",
  "title" : "Detecting nudity in media and providing intervention options",
  "url" : "https:\/\/developer.apple.com\/documentation\/SensitiveContentAnalysis\/detecting-nudity-in-media-and-providing-intervention-options"
}