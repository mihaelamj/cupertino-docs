{
  "abstract" : "Enumerate the words in a string.",
  "codeExamples" : [
    {
      "code" : "let text = \"\"\"\nAll human beings are born free and equal in dignity and rights.\nThey are endowed with reason and conscience and should act towards one another in a spirit of brotherhood.\n\"\"\"\n\nlet tokenizer = NLTokenizer(unit: .word)\ntokenizer.string = text\n\ntokenizer.enumerateTokens(in: text.startIndex..<text.endIndex) { tokenRange, _ in\n    print(text[tokenRange])\n    return true\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "5240cfe993c2d0b8c560fbae006bb67827ccb764e3981c66ebcdb033741709db",
  "crawledAt" : "2025-12-09T19:51:20Z",
  "id" : "BAB57F27-5005-4369-80AD-E72A1438F9CD",
  "kind" : "article",
  "language" : "swift",
  "module" : "Natural Language",
  "overview" : "## Overview\n\nWhen you work with natural language text, it’s often useful to tokenize the text into individual words. Using [doc:\/\/com.apple.naturallanguage\/documentation\/NaturalLanguage\/NLTokenizer] to enumerate words, rather than simply splitting components by whitespace, ensures correct behavior in multiple scripts and languages. For example, neither Chinese nor Japanese uses spaces to delimit words.\n\nThe example and accompanying steps below show how you use [doc:\/\/com.apple.naturallanguage\/documentation\/NaturalLanguage\/NLTokenizer] to enumerate over the words in natural language text.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/naturallanguage\/tokenizing-natural-language-text\ncrawled: 2025-12-09T19:51:20Z\n---\n\n# Tokenizing natural language text\n\n**Article**\n\nEnumerate the words in a string.\n\n## Overview\n\nWhen you work with natural language text, it’s often useful to tokenize the text into individual words. Using [doc:\/\/com.apple.naturallanguage\/documentation\/NaturalLanguage\/NLTokenizer] to enumerate words, rather than simply splitting components by whitespace, ensures correct behavior in multiple scripts and languages. For example, neither Chinese nor Japanese uses spaces to delimit words.\n\nThe example and accompanying steps below show how you use [doc:\/\/com.apple.naturallanguage\/documentation\/NaturalLanguage\/NLTokenizer] to enumerate over the words in natural language text.\n\n```swift\nlet text = \"\"\"\nAll human beings are born free and equal in dignity and rights.\nThey are endowed with reason and conscience and should act towards one another in a spirit of brotherhood.\n\"\"\"\n\nlet tokenizer = NLTokenizer(unit: .word)\ntokenizer.string = text\n\ntokenizer.enumerateTokens(in: text.startIndex..<text.endIndex) { tokenRange, _ in\n    print(text[tokenRange])\n    return true\n}\n```\n\n1. Create an instance of [doc:\/\/com.apple.naturallanguage\/documentation\/NaturalLanguage\/NLTokenizer], specifying [doc:\/\/com.apple.naturallanguage\/documentation\/NaturalLanguage\/NLTokenUnit\/word] as the unit to tokenize.\n2. Set the [doc:\/\/com.apple.naturallanguage\/documentation\/NaturalLanguage\/NLTokenizer\/string] property of the tokenizer to the natural language text.\n3. Enumerate over the entire range of the string by calling the [doc:\/\/com.apple.naturallanguage\/documentation\/NaturalLanguage\/NLTokenizer\/enumerateTokensInRange:usingBlock:] method, specifying the entire range of the string to process.\n4. In the enumeration block, take a substring of the original text at `tokenRange` to obtain each word.\n5. Run this code to print out each word in text on a new line.\n\n## Tokenization\n\n- **NLTokenizer**: A tokenizer that segments natural language text into semantic units.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "A tokenizer that segments natural language text into semantic units.",
          "name" : "NLTokenizer",
          "url" : "https:\/\/developer.apple.com\/documentation\/NaturalLanguage\/NLTokenizer"
        }
      ],
      "title" : "Tokenization"
    }
  ],
  "source" : "appleJSON",
  "title" : "Tokenizing natural language text",
  "url" : "https:\/\/developer.apple.com\/documentation\/naturallanguage\/tokenizing-natural-language-text"
}