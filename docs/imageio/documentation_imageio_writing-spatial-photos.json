{
  "abstract" : "Create spatial photos for visionOS by packaging a pair of left- and right-eye images as a stereo HEIC file with related spatial metadata.",
  "codeExamples" : [
    {
      "code" : "init(url: URL) throws {\n\n    guard let source = CGImageSourceCreateWithURL(url as CFURL, nil) else {\n        throw ConversionError.couldNotOpenURLAsImageSource\n    }\n    self.source = source\n\n    primaryImageIndex = CGImageSourceGetPrimaryImageIndex(source)\n\n    guard let properties = CGImageSourceCopyPropertiesAtIndex(source, primaryImageIndex, nil) as? [CFString: Any] else {\n        throw ConversionError.couldNotCopyImageProperties\n    }\n    guard let width = properties[kCGImagePropertyPixelWidth] as? Int,\n        let height = properties[kCGImagePropertyPixelHeight] as? Int else {\n        throw ConversionError.unableToReadImageSize\n    }\n    self.width = width\n    self.height = height\n\n}",
      "language" : "swift"
    },
    {
      "code" : "guard leftImage.width == rightImage.width, leftImage.height == rightImage.height else {\n    throw ConversionError.leftAndRightImageSizesDoNotMatch\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Convert the baseline from millimeters to meters.\nlet baselineInMeters = baselineInMillimeters \/ 1000.0",
      "language" : "swift"
    },
    {
      "code" : "let leftPosition: [Double] = [0, 0, 0]\nlet rightPosition: [Double] = [baselineInMeters, 0, 0]",
      "language" : "swift"
    },
    {
      "code" : "let intrinsics = leftImage.intrinsics(horizontalFOV: horizontalFOV)",
      "language" : "swift"
    },
    {
      "code" : "func intrinsics(horizontalFOV: Double) -> [Double] {\n    let width = Double(width)\n    let height = Double(height)\n    let horizontalFOVInRadians = horizontalFOV \/ 180.0 * .pi\n    let focalLengthX = (width * 0.5) \/ (tan(horizontalFOVInRadians * 0.5))\n    \/\/ For a spherical pinhole camera, the focal length is the same in both X and Y.\n    let focalLengthY = focalLengthX\n    \/\/ The app assumes the principal point of the camera is located at the center of the image.\n    let principalPointX = 0.5 * width\n    let principalPointY = 0.5 * height\n    return [\n        focalLengthX, 0, principalPointX,\n        0, focalLengthY, principalPointY,\n        0, 0, 1\n    ]\n}",
      "language" : "swift"
    },
    {
      "code" : "let encodedDisparityAdjustment = Int(disparityAdjustment * 1e4)",
      "language" : "swift"
    },
    {
      "code" : "let leftProperties = propertiesDictionary(\n    isLeft: true,\n    encodedDisparityAdjustment: encodedDisparityAdjustment,\n    position: leftPosition,\n    intrinsics: intrinsics\n)\nlet rightProperties = propertiesDictionary(\n    isLeft: false,\n    encodedDisparityAdjustment: encodedDisparityAdjustment,\n    position: rightPosition,\n    intrinsics: intrinsics\n)",
      "language" : "swift"
    },
    {
      "code" : "func propertiesDictionary(\n    isLeft: Bool,\n    encodedDisparityAdjustment: Int,\n    position: [Double],\n    intrinsics: [Double]\n) -> [CFString: Any] {\n    return [\n        kCGImagePropertyGroups: [\n            kCGImagePropertyGroupIndex: 0,\n            kCGImagePropertyGroupType: kCGImagePropertyGroupTypeStereoPair,\n            (isLeft ? kCGImagePropertyGroupImageIsLeftImage : kCGImagePropertyGroupImageIsRightImage): true,\n            kCGImagePropertyGroupImageDisparityAdjustment: encodedDisparityAdjustment\n        ],\n        kCGImagePropertyHEIFDictionary: [\n            kIIOMetadata_CameraExtrinsicsKey: [\n                kIIOCameraExtrinsics_Position: position,\n                kIIOCameraExtrinsics_Rotation: Self.identityRotation\n            ],\n            kIIOMetadata_CameraModelKey: [\n                kIIOCameraModel_Intrinsics: intrinsics,\n                kIIOCameraModel_ModelType: kIIOCameraModelType_SimplifiedPinhole\n            ]\n        ],\n        kCGImagePropertyHasAlpha: false\n    ]\n}",
      "language" : "swift"
    },
    {
      "code" : "let destinationProperties: [CFString: Any] = [kCGImagePropertyPrimaryImage: 0]\nguard let destination = CGImageDestinationCreateWithURL(\n    outputImageURL as CFURL,\n    UTType.heic.description as CFString,\n    2,\n    destinationProperties as CFDictionary\n) else {\n    throw ConversionError.unableToCreateImageDestination\n}",
      "language" : "swift"
    },
    {
      "code" : "CGImageDestinationAddImageFromSource(\n    destination,\n    leftImage.source,\n    leftImage.primaryImageIndex,\n    leftProperties as CFDictionary\n)\nCGImageDestinationAddImageFromSource(\n    destination,\n    rightImage.source,\n    rightImage.primaryImageIndex,\n    rightProperties as CFDictionary\n)",
      "language" : "swift"
    },
    {
      "code" : "guard CGImageDestinationFinalize(destination) else {\n    throw ConversionError.unableToFinalizeImageDestination\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "3b3f1c0a28194d4e085322b19f450289e113ec03d4846c33f81dd8429087db68",
  "crawledAt" : "2025-12-02T15:30:41Z",
  "id" : "CBDF0E8F-671F-4D87-B985-3D75D6B055B0",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "Image I\/O",
  "overview" : "## Overview\n\nSpatial photos are *High-Efficiency Image Codec* (HEIC) files containing a pair of left- and right-eye images, together with a stereo pair group and additional spatial metadata. *Spatial metadata* describes properties of the left- and right-eye cameras that captured the stereo scene. Adding spatial metadata to a stereo HEIC prompts Apple platforms to consider the image as *spatial* instead of just stereo, and opts the image into visual treatments on Apple Vision Pro that help minimize common causes of stereo viewing discomfort.\n\nTo learn more about why you might want to package a pair of stereo images as a spatial photo and the metadata values you provide, see [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/Creating-spatial-photos-and-videos-with-spatial-metadata].\n\nThis sample app demonstrates how to use [doc:\/\/com.apple.documentation\/documentation\/ImageIO] APIs to convert two separate images into a spatial photo, saving the output as a HEIC file. The app’s `SpatialPhotoConverter` class performs this conversion process.\n\n### Configure the sample code project\n\nTo run the app in Xcode, add the input file locations, output file location, and spatial metadata for your images as arguments to the project’s scheme. Select Product > Scheme > Edit Scheme (Command-<), and add the arguments described below to Arguments Passed On Launch:\n\nBy default, the project’s scheme loads a pair of left- and right-eye images from the Xcode project folder: `Hummingbird_Left.png` and `Hummingbird_Right.png`. These images are renders of a 3D scene of a hummingbird model. The virtual cameras that created these renders were positioned 64mm apart, with a horizontal field of view of 80 degrees, which means the value for the `--baseline` argument is `64.0`, and the value of the `--fov` argument is `80.0`.\n\nFor these images, a disparity adjustment of +2% of the image width produces a comfortable depth effect when the spatial photo is presented in a window on Apple Vision Pro. This 2% disparity adjustment value positions the nearest object in the spatial photo — the hummingbird — just behind the front of the window, while still keeping an effective illusion of depth between the hummingbird and the background. The scheme’s arguments express the +2% disparity adjustment with a `--disparityAdjustment` value of `0.02`.\n\n### Load the left- and right-eye images\n\nThe app starts by creating a [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/CGImageSource] for each of the left- and right-eye images. An initializer on `StereoPairImage` performs the following steps:\n\nThe app next validates that the left and right `StereoPairImage` have the same pixel width and height.\n\n### Define an extrinsic position for each image\n\nThe app converts the provided baseline distance from millimeters to meters.\n\nThen, the app uses this baseline distance to define a left extrinsic position (at the origin) and a right extrinsic position (offset from the origin by `baselineInMeters` in the positive x-axis) to describe how the left and right cameras are positioned relative to each other in 3D space.\n\n### Compute a camera intrinsics matrix\n\nThe app uses the provided horizontal field of view, plus the known width and height of each input image, to compute a 3x3 camera intrinsics matrix that describes the characteristics of the left and right cameras.\n\nThe app calculates the 3x3 intrinsics matrix using a convenience method, `intrinsics(horizontalFOV:)`, on `StereoPairImage`. This method defines an intrinsics matrix with the same focal length in both X and Y (to represent a spherical lens), with the principal point of the camera located at the center of the image, and no shear.\n\nThe app already validated the left-eye and right-eye images to confirm that they have the same pixel size. As a result, the app calculates the intrinsics matrix for the left-eye image and uses it as the intrinsics matrix for both images below.\n\n### Encode the disparity adjustment\n\nThe app encodes the provided disparity adjustment value as a signed integer, converting it from a signed floating-point value in the range `[-1.0, +1.0]` (as a fraction of image width) to a signed integer value in the range `[-10000, +10000]`.\n\n### Define an image properties dictionary for each image\n\nBefore adding the left-eye and right-eye images to the image destination, the app defines a properties dictionary for each image, which expresses the spatial metadata for that image as expected ImageIO properties.\n\nA convenience method, `propertiesDictionary(isLeft:encodedDisparityAdjustment:position:intrinsics:)`, defines these property dictionaries.\n\nThe properties dictionary has two sub-dictionaries for each image:\n\nFor the groups dictionary, the convenience method defines a single stereo pair group with the following properties:\n\nFor the HEIF dictionary, the convenience method defines:\n\nThe camera extrinsics dictionary specifies an identity rotation to indicate that the app defines camera extrinsics with a position offset only.\n\nThe properties dictionary also specifies a [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/kCGImagePropertyHasAlpha] value of `false` to indicate that the system should ignore any alpha channel data in the source image when adding that source image to the image destination.\n\n### Write the images to an output image destination\n\nThe app calls [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/CGImageDestinationCreateWithURL(_:_:_:_:)] to create an output [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/CGImageDestination] at the provided URL for the output spatial photo. The app creates the image destination with the uniform type identifier for a HEIC image and an expected image count of `2` to indicate that the app writes both a left- and right-eye image to a single HEIC file.\n\nThe app creates an image destination with a `destinationProperties` dictionary that specifies a primary image index of `0` for the output HEIC file. This primary image index specifies which image in the output HEIC file is preferred for display when an app or system needs a single image to represent the file’s content (for example, on a nonstereo platform such as iOS). However, not all apps and operating systems use the primary image index when displaying a HEIC file, and instead display the first image in the HEIC by default. For this reason, the app sets the primary image index to `0`, so that apps that use the primary image index select the same image as apps that don’t.\n\nNext, the app calls [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/CGImageDestinationAddImageFromSource(_:_:_:_:)] to copy the left- and right-eye image sources into the image destination, passing in an appropriate properties dictionary for each image. The app adds the left-eye image first, which means the left-eye image is the primary image for the output HEIC file, because it appears at image index `0` in the output file.\n\nIt’s valid to write either the left- or right-eye image as the first image in a spatial photo. visionOS detects the appropriate images to use for left- and right-eye presentation based on the `kCGImagePropertyGroupImageIsLeftImage` or `kCGImagePropertyGroupImageIsRightImage` properties in the groups dictionary for each image, regardless of the order in which the images are added to the HEIC file. If the system should prefer the right-eye image as the primary image to display when it shows the spatial photo in a nonstereo environment, modify the app to add the right-eye image to the image destination first, so that it appears at index `0` in the output HEIC file.\n\nFinally, the app calls [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/CGImageDestinationFinalize(_:)] to write the image destination to disk as a self-contained spatial photo.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/ImageIO\/writing-spatial-photos\ncrawled: 2025-12-02T15:30:41Z\n---\n\n# Writing spatial photos\n\n**Sample Code**\n\nCreate spatial photos for visionOS by packaging a pair of left- and right-eye images as a stereo HEIC file with related spatial metadata.\n\n## Overview\n\nSpatial photos are *High-Efficiency Image Codec* (HEIC) files containing a pair of left- and right-eye images, together with a stereo pair group and additional spatial metadata. *Spatial metadata* describes properties of the left- and right-eye cameras that captured the stereo scene. Adding spatial metadata to a stereo HEIC prompts Apple platforms to consider the image as *spatial* instead of just stereo, and opts the image into visual treatments on Apple Vision Pro that help minimize common causes of stereo viewing discomfort.\n\nTo learn more about why you might want to package a pair of stereo images as a spatial photo and the metadata values you provide, see [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/Creating-spatial-photos-and-videos-with-spatial-metadata].\n\nThis sample app demonstrates how to use [doc:\/\/com.apple.documentation\/documentation\/ImageIO] APIs to convert two separate images into a spatial photo, saving the output as a HEIC file. The app’s `SpatialPhotoConverter` class performs this conversion process.\n\n\n\n### Configure the sample code project\n\nTo run the app in Xcode, add the input file locations, output file location, and spatial metadata for your images as arguments to the project’s scheme. Select Product > Scheme > Edit Scheme (Command-<), and add the arguments described below to Arguments Passed On Launch:\n\n- `--leftImage` (or `-l`) followed by a path to a local left-eye image file\n- `--rightImage` (or `-r`) followed by a path to a local right-eye image file\n- `--outputImage` (or `-o`) followed by the output path for the spatial photo, with a file extension of `.heic` (for example, `~\/Documents\/spatial_photo.heic`)\n- `--baseline` (or `-b`) to provide a baseline in millimeters (for example, `--baseline 64.0` for a 64mm baseline)\n- `--fov` (or `-f`) to provide a horizontal field of view in degrees (for example, `--fov 80.0` for an 80-degree field of view)\n- `--disparityAdjustment` (or `-d`) to provide a disparity adjustment value as a fraction of the image’s width (for example, `--disparityAdjustment 0.02` for a 2% positive disparity shift)\n\nBy default, the project’s scheme loads a pair of left- and right-eye images from the Xcode project folder: `Hummingbird_Left.png` and `Hummingbird_Right.png`. These images are renders of a 3D scene of a hummingbird model. The virtual cameras that created these renders were positioned 64mm apart, with a horizontal field of view of 80 degrees, which means the value for the `--baseline` argument is `64.0`, and the value of the `--fov` argument is `80.0`.\n\nFor these images, a disparity adjustment of +2% of the image width produces a comfortable depth effect when the spatial photo is presented in a window on Apple Vision Pro. This 2% disparity adjustment value positions the nearest object in the spatial photo — the hummingbird — just behind the front of the window, while still keeping an effective illusion of depth between the hummingbird and the background. The scheme’s arguments express the +2% disparity adjustment with a `--disparityAdjustment` value of `0.02`.\n\n### Load the left- and right-eye images\n\nThe app starts by creating a [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/CGImageSource] for each of the left- and right-eye images. An initializer on `StereoPairImage` performs the following steps:\n\n1. Opens the image as a `CGImageSource` by calling [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/CGImageSourceCreateWithURL(_:_:)]\n2. Discovers the primary image index for the image source by calling [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/CGImageSourceGetPrimaryImageIndex(_:)]\n3. Discovers the pixel width and height of the image by calling [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/CGImageSourceCopyPropertiesAtIndex(_:_:_:)] for the primary image index, and looking for [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/kCGImagePropertyPixelWidth] and [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/kCGImagePropertyPixelHeight] values in the returned properties dictionary\n4. Stores the results of the above steps as properties on the `StereoPairImage`\n\n```swift\ninit(url: URL) throws {\n\n    guard let source = CGImageSourceCreateWithURL(url as CFURL, nil) else {\n        throw ConversionError.couldNotOpenURLAsImageSource\n    }\n    self.source = source\n\n    primaryImageIndex = CGImageSourceGetPrimaryImageIndex(source)\n\n    guard let properties = CGImageSourceCopyPropertiesAtIndex(source, primaryImageIndex, nil) as? [CFString: Any] else {\n        throw ConversionError.couldNotCopyImageProperties\n    }\n    guard let width = properties[kCGImagePropertyPixelWidth] as? Int,\n        let height = properties[kCGImagePropertyPixelHeight] as? Int else {\n        throw ConversionError.unableToReadImageSize\n    }\n    self.width = width\n    self.height = height\n\n}\n```\n\nThe app next validates that the left and right `StereoPairImage` have the same pixel width and height.\n\n```swift\nguard leftImage.width == rightImage.width, leftImage.height == rightImage.height else {\n    throw ConversionError.leftAndRightImageSizesDoNotMatch\n}\n```\n\n\n\n### Define an extrinsic position for each image\n\nThe app converts the provided baseline distance from millimeters to meters.\n\n```swift\n\/\/ Convert the baseline from millimeters to meters.\nlet baselineInMeters = baselineInMillimeters \/ 1000.0\n```\n\nThen, the app uses this baseline distance to define a left extrinsic position (at the origin) and a right extrinsic position (offset from the origin by `baselineInMeters` in the positive x-axis) to describe how the left and right cameras are positioned relative to each other in 3D space.\n\n```swift\nlet leftPosition: [Double] = [0, 0, 0]\nlet rightPosition: [Double] = [baselineInMeters, 0, 0]\n```\n\n\n\n### Compute a camera intrinsics matrix\n\nThe app uses the provided horizontal field of view, plus the known width and height of each input image, to compute a 3x3 camera intrinsics matrix that describes the characteristics of the left and right cameras.\n\n```swift\nlet intrinsics = leftImage.intrinsics(horizontalFOV: horizontalFOV)\n```\n\nThe app calculates the 3x3 intrinsics matrix using a convenience method, `intrinsics(horizontalFOV:)`, on `StereoPairImage`. This method defines an intrinsics matrix with the same focal length in both X and Y (to represent a spherical lens), with the principal point of the camera located at the center of the image, and no shear.\n\n```swift\nfunc intrinsics(horizontalFOV: Double) -> [Double] {\n    let width = Double(width)\n    let height = Double(height)\n    let horizontalFOVInRadians = horizontalFOV \/ 180.0 * .pi\n    let focalLengthX = (width * 0.5) \/ (tan(horizontalFOVInRadians * 0.5))\n    \/\/ For a spherical pinhole camera, the focal length is the same in both X and Y.\n    let focalLengthY = focalLengthX\n    \/\/ The app assumes the principal point of the camera is located at the center of the image.\n    let principalPointX = 0.5 * width\n    let principalPointY = 0.5 * height\n    return [\n        focalLengthX, 0, principalPointX,\n        0, focalLengthY, principalPointY,\n        0, 0, 1\n    ]\n}\n```\n\nThe app already validated the left-eye and right-eye images to confirm that they have the same pixel size. As a result, the app calculates the intrinsics matrix for the left-eye image and uses it as the intrinsics matrix for both images below.\n\n### Encode the disparity adjustment\n\nThe app encodes the provided disparity adjustment value as a signed integer, converting it from a signed floating-point value in the range `[-1.0, +1.0]` (as a fraction of image width) to a signed integer value in the range `[-10000, +10000]`.\n\n```swift\nlet encodedDisparityAdjustment = Int(disparityAdjustment * 1e4)\n```\n\n### Define an image properties dictionary for each image\n\nBefore adding the left-eye and right-eye images to the image destination, the app defines a properties dictionary for each image, which expresses the spatial metadata for that image as expected ImageIO properties.\n\n```swift\nlet leftProperties = propertiesDictionary(\n    isLeft: true,\n    encodedDisparityAdjustment: encodedDisparityAdjustment,\n    position: leftPosition,\n    intrinsics: intrinsics\n)\nlet rightProperties = propertiesDictionary(\n    isLeft: false,\n    encodedDisparityAdjustment: encodedDisparityAdjustment,\n    position: rightPosition,\n    intrinsics: intrinsics\n)\n```\n\nA convenience method, `propertiesDictionary(isLeft:encodedDisparityAdjustment:position:intrinsics:)`, defines these property dictionaries.\n\n```swift\nfunc propertiesDictionary(\n    isLeft: Bool,\n    encodedDisparityAdjustment: Int,\n    position: [Double],\n    intrinsics: [Double]\n) -> [CFString: Any] {\n    return [\n        kCGImagePropertyGroups: [\n            kCGImagePropertyGroupIndex: 0,\n            kCGImagePropertyGroupType: kCGImagePropertyGroupTypeStereoPair,\n            (isLeft ? kCGImagePropertyGroupImageIsLeftImage : kCGImagePropertyGroupImageIsRightImage): true,\n            kCGImagePropertyGroupImageDisparityAdjustment: encodedDisparityAdjustment\n        ],\n        kCGImagePropertyHEIFDictionary: [\n            kIIOMetadata_CameraExtrinsicsKey: [\n                kIIOCameraExtrinsics_Position: position,\n                kIIOCameraExtrinsics_Rotation: Self.identityRotation\n            ],\n            kIIOMetadata_CameraModelKey: [\n                kIIOCameraModel_Intrinsics: intrinsics,\n                kIIOCameraModel_ModelType: kIIOCameraModelType_SimplifiedPinhole\n            ]\n        ],\n        kCGImagePropertyHasAlpha: false\n    ]\n}\n```\n\nThe properties dictionary has two sub-dictionaries for each image:\n\n- A groups dictionary ([doc:\/\/com.apple.documentation\/documentation\/ImageIO\/kCGImagePropertyGroups]) that defines the image as part of a stereo pair group, and specifies a disparity adjustment for that group\n- A HEIF dictionary ([doc:\/\/com.apple.documentation\/documentation\/ImageIO\/kCGImagePropertyHEIFDictionary]) that defines the extrinsics and camera model for the camera that created the image\n\nFor the groups dictionary, the convenience method defines a single stereo pair group with the following properties:\n\n- A [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/kCGImagePropertyGroupIndex] of `0`, because this is the first and only group in the output file\n- A [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/kCGImagePropertyGroupType] of [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/kCGImagePropertyGroupTypeStereoPair] to indicate that this group defines a stereo pair of images\n- A flag indicating if this image is the left-eye image ([doc:\/\/com.apple.documentation\/documentation\/ImageIO\/kCGImagePropertyGroupImageIsLeftImage]) or the right-eye image ([doc:\/\/com.apple.documentation\/documentation\/ImageIO\/kCGImagePropertyGroupImageIsRightImage]) in the stereo pair group\n- The [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/kCGImagePropertyGroupImageDisparityAdjustment] to use when presenting this stereo pair group\n\n\n\nFor the HEIF dictionary, the convenience method defines:\n\n- A camera extrinsics ([doc:\/\/com.apple.documentation\/documentation\/ImageIO\/kIIOMetadata_CameraExtrinsicsKey]) dictionary that contains the extrinsic position and rotation for the camera\n- A camera model ([doc:\/\/com.apple.documentation\/documentation\/ImageIO\/kIIOMetadata_CameraModelKey]) dictionary that contains the camera intrinsics matrix and camera model type for the camera that captured the image\n\nThe camera extrinsics dictionary specifies an identity rotation to indicate that the app defines camera extrinsics with a position offset only.\n\n\n\nThe properties dictionary also specifies a [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/kCGImagePropertyHasAlpha] value of `false` to indicate that the system should ignore any alpha channel data in the source image when adding that source image to the image destination.\n\n### Write the images to an output image destination\n\nThe app calls [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/CGImageDestinationCreateWithURL(_:_:_:_:)] to create an output [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/CGImageDestination] at the provided URL for the output spatial photo. The app creates the image destination with the uniform type identifier for a HEIC image and an expected image count of `2` to indicate that the app writes both a left- and right-eye image to a single HEIC file.\n\n```swift\nlet destinationProperties: [CFString: Any] = [kCGImagePropertyPrimaryImage: 0]\nguard let destination = CGImageDestinationCreateWithURL(\n    outputImageURL as CFURL,\n    UTType.heic.description as CFString,\n    2,\n    destinationProperties as CFDictionary\n) else {\n    throw ConversionError.unableToCreateImageDestination\n}\n```\n\nThe app creates an image destination with a `destinationProperties` dictionary that specifies a primary image index of `0` for the output HEIC file. This primary image index specifies which image in the output HEIC file is preferred for display when an app or system needs a single image to represent the file’s content (for example, on a nonstereo platform such as iOS). However, not all apps and operating systems use the primary image index when displaying a HEIC file, and instead display the first image in the HEIC by default. For this reason, the app sets the primary image index to `0`, so that apps that use the primary image index select the same image as apps that don’t.\n\nNext, the app calls [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/CGImageDestinationAddImageFromSource(_:_:_:_:)] to copy the left- and right-eye image sources into the image destination, passing in an appropriate properties dictionary for each image. The app adds the left-eye image first, which means the left-eye image is the primary image for the output HEIC file, because it appears at image index `0` in the output file.\n\n```swift\nCGImageDestinationAddImageFromSource(\n    destination,\n    leftImage.source,\n    leftImage.primaryImageIndex,\n    leftProperties as CFDictionary\n)\nCGImageDestinationAddImageFromSource(\n    destination,\n    rightImage.source,\n    rightImage.primaryImageIndex,\n    rightProperties as CFDictionary\n)\n```\n\nIt’s valid to write either the left- or right-eye image as the first image in a spatial photo. visionOS detects the appropriate images to use for left- and right-eye presentation based on the `kCGImagePropertyGroupImageIsLeftImage` or `kCGImagePropertyGroupImageIsRightImage` properties in the groups dictionary for each image, regardless of the order in which the images are added to the HEIC file. If the system should prefer the right-eye image as the primary image to display when it shows the spatial photo in a nonstereo environment, modify the app to add the right-eye image to the image destination first, so that it appears at index `0` in the output HEIC file.\n\nFinally, the app calls [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/CGImageDestinationFinalize(_:)] to write the image destination to disk as a self-contained spatial photo.\n\n```swift\nguard CGImageDestinationFinalize(destination) else {\n    throw ConversionError.unableToFinalizeImageDestination\n}\n```\n\n## Spatial Photos\n\n- **Creating spatial photos and videos with spatial metadata**: Add spatial metadata to stereo photos and videos to create spatial media for viewing on Apple Vision Pro.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Add spatial metadata to stereo photos and videos to create spatial media for viewing on Apple Vision Pro.",
          "name" : "Creating spatial photos and videos with spatial metadata",
          "url" : "https:\/\/developer.apple.com\/documentation\/ImageIO\/Creating-spatial-photos-and-videos-with-spatial-metadata"
        }
      ],
      "title" : "Spatial Photos"
    }
  ],
  "source" : "appleJSON",
  "title" : "Writing spatial photos",
  "url" : "https:\/\/developer.apple.com\/documentation\/ImageIO\/writing-spatial-photos"
}