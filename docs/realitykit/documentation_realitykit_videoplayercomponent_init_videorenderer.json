{
  "abstract" : "Creates a video player component from a sample buffer video renderer object.",
  "codeExamples" : [
    {
      "code" : "\/\/ Create an `AVSampleBufferVideoRenderer` instance to control playback of a movie.\nlet videoRenderer = AVSampleBufferVideoRenderer()\n\n\/\/ Create an `AVSampleBufferAudioRenderer` instance to control audio of the playback.\nlet audioRenderer = AVSampleBufferAudioRenderer()\n\n\/\/ Create a `AVSampleBufferRenderSynchronizer` instance to synchronize video and audio.\nlet synchronizer = AVSampleBufferRenderSynchronizer()\n\n\/\/ Add both videoRenderer and audioRenderer to the synchronizer.\nsynchronizer.addRenderer(videoRenderer)\nsynchronizer.addRenderer(audioRenderer)\n\n\/\/ Create an entity for display.\nlet videoEntity = Entity()\n\n\/\/ Create a `VideoPlayerComponent` object that supplies the `AVSampleBufferVideoRenderer` object.\nlet videoPlayerComponent = VideoPlayerComponent(videoRenderer: videoRenderer)\nvideoEntity.components[VideoPlayerComponent.self] = videoPlayerComponent\n\n\/\/ Create a URL that points to the movie file.\nif let url = Bundle.main.url(forResource: \"MyMovie\", withExtension: \"mp4\") {\n\n    let sourceAsset = AVURLAsset(url: url)\n    let sourceAssetReader = AVAssetReader(asset: sourceAsset)\n    let sourceAssetVideoTrack = sourceAsset.loadTracks(withMediaType: .video).first\n    let sourceAssetAudioTrack = sourceAsset.loadTracks(withMediaType: .audio).first\n    let sourceAssetReaderVideoTrackOutput = AVAssetReaderTrackOutput(track: sourceAssetVideoTrack!, outputSettings: nil)\n    let sourceAssetReaderAudioTrackOutput = AVAssetReaderTrackOutput(track: sourceAssetAudioTrack!, outputSettings: nil)\n    sourceAssetReader.add(sourceAssetReaderVideoTrackOutput!)\n    sourceAssetReader.add(sourceAssetReaderAudioTrackOutput!)\n\n    sourceAssetReader.startReading()\n\n    videoRenderer.requestMediaDataWhenReady(on: DispatchQueue.global()) {\n        while videoRenderer.isReadyForMoreMediaData {\n            if let sampleBuffer = sourceAssetReaderVideoTrackOutput!.copyNextSampleBuffer() {\n                videoRenderer.enqueue(sampleBuffer)\n            } else {\n                videoRenderer.stopRequestingMediaData()\n                return\n            }\n        }\n    }\n\n    audioRenderer.requestMediaDataWhenReady(on: DispatchQueue.global()) {\n        while audioRenderer.isReadyForMoreMediaData {\n            if let sampleBuffer = sourceAssetReaderAudioTrackOutput!.copyNextSampleBuffer() {\n                audioRenderer.enqueue(sampleBuffer)\n            } else {\n                audioRenderer.stopRequestingMediaData()\n                return\n            }\n        }\n    }\n\n    \/\/ Start the playback immediately.\n    synchronizer.setRate(1, time: .zero)\n\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "c6b9ce7ed9b1f22fd78431951a237d5689f6fc96697a3af1c53fd4128a860f30",
  "crawledAt" : "2025-12-01T20:03:31Z",
  "declaration" : {
    "code" : "init(videoRenderer: AVSampleBufferVideoRenderer)",
    "language" : "swift"
  },
  "id" : "6011C832-8000-400A-97E2-1D19A47CAD09",
  "kind" : "unknown",
  "module" : "RealityKit",
  "overview" : "## Discussion\n\nTo create a `VideoPlayerComponent`, first create an [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVSampleBufferVideoRenderer] instance with no parameters and then pass that to the `VideoPlayerComponent` initializer. After creating the `VideoPlayerComponent`, create an [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVAssetReader] with the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVURLAsset] and load the media (video), which you need to track as the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVAssetTrack]. Feed this `AVAssetTrack` to the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVAssetReaderTrackOutput]\nand add this output tracker to the `AVAssetReader`.\n\nWait for setup to finish and start reading. Read the sample buffers from the reader and enqueue it into the `AVSampleBufferVideoRenderer` object. `AVSampleBufferVideoRenderer` uses a push model, so feed the video sample buffers until the queue is full, and then resume feeding it when it’s ready for more. You can use `isReadyForMoreMediaData` on the `AVSampleBufferVideoRenderer` object in a while-loop to check whether you need to enqueue more sample buffers. You can’t use the same `AVSampleBufferVideoRenderer` object with more than one `VideoPlayerComponent`.\n\nYou need to synchronize the audio, captions, and playback rate separately in your app.\n\nThe following code example demonstrates how to synchronize video with audio:",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst",
    "macOS",
    "tvOS",
    "visionOS"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/RealityKit\/VideoPlayerComponent\/init(videoRenderer:)\ncrawled: 2025-12-01T20:03:31Z\n---\n\n# init(videoRenderer:)\n\n**Initializer**\n\nCreates a video player component from a sample buffer video renderer object.\n\n## Declaration\n\n```swift\ninit(videoRenderer: AVSampleBufferVideoRenderer)\n```\n\n## Parameters\n\n- **videoRenderer**: The sample buffer video renderer with the visual contents the component presents.\n\n## Discussion\n\nTo create a `VideoPlayerComponent`, first create an [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVSampleBufferVideoRenderer] instance with no parameters and then pass that to the `VideoPlayerComponent` initializer. After creating the `VideoPlayerComponent`, create an [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVAssetReader] with the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVURLAsset] and load the media (video), which you need to track as the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVAssetTrack]. Feed this `AVAssetTrack` to the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVAssetReaderTrackOutput]\nand add this output tracker to the `AVAssetReader`.\n\nWait for setup to finish and start reading. Read the sample buffers from the reader and enqueue it into the `AVSampleBufferVideoRenderer` object. `AVSampleBufferVideoRenderer` uses a push model, so feed the video sample buffers until the queue is full, and then resume feeding it when it’s ready for more. You can use `isReadyForMoreMediaData` on the `AVSampleBufferVideoRenderer` object in a while-loop to check whether you need to enqueue more sample buffers. You can’t use the same `AVSampleBufferVideoRenderer` object with more than one `VideoPlayerComponent`.\n\nYou need to synchronize the audio, captions, and playback rate separately in your app.\n\nThe following code example demonstrates how to synchronize video with audio:\n\n```swift\n\/\/ Create an `AVSampleBufferVideoRenderer` instance to control playback of a movie.\nlet videoRenderer = AVSampleBufferVideoRenderer()\n\n\/\/ Create an `AVSampleBufferAudioRenderer` instance to control audio of the playback.\nlet audioRenderer = AVSampleBufferAudioRenderer()\n\n\/\/ Create a `AVSampleBufferRenderSynchronizer` instance to synchronize video and audio.\nlet synchronizer = AVSampleBufferRenderSynchronizer()\n\n\/\/ Add both videoRenderer and audioRenderer to the synchronizer.\nsynchronizer.addRenderer(videoRenderer)\nsynchronizer.addRenderer(audioRenderer)\n\n\/\/ Create an entity for display.\nlet videoEntity = Entity()\n\n\/\/ Create a `VideoPlayerComponent` object that supplies the `AVSampleBufferVideoRenderer` object.\nlet videoPlayerComponent = VideoPlayerComponent(videoRenderer: videoRenderer)\nvideoEntity.components[VideoPlayerComponent.self] = videoPlayerComponent\n\n\/\/ Create a URL that points to the movie file.\nif let url = Bundle.main.url(forResource: \"MyMovie\", withExtension: \"mp4\") {\n\n    let sourceAsset = AVURLAsset(url: url)\n    let sourceAssetReader = AVAssetReader(asset: sourceAsset)\n    let sourceAssetVideoTrack = sourceAsset.loadTracks(withMediaType: .video).first\n    let sourceAssetAudioTrack = sourceAsset.loadTracks(withMediaType: .audio).first\n    let sourceAssetReaderVideoTrackOutput = AVAssetReaderTrackOutput(track: sourceAssetVideoTrack!, outputSettings: nil)\n    let sourceAssetReaderAudioTrackOutput = AVAssetReaderTrackOutput(track: sourceAssetAudioTrack!, outputSettings: nil)\n    sourceAssetReader.add(sourceAssetReaderVideoTrackOutput!)\n    sourceAssetReader.add(sourceAssetReaderAudioTrackOutput!)\n\n    sourceAssetReader.startReading()\n\n    videoRenderer.requestMediaDataWhenReady(on: DispatchQueue.global()) {\n        while videoRenderer.isReadyForMoreMediaData {\n            if let sampleBuffer = sourceAssetReaderVideoTrackOutput!.copyNextSampleBuffer() {\n                videoRenderer.enqueue(sampleBuffer)\n            } else {\n                videoRenderer.stopRequestingMediaData()\n                return\n            }\n        }\n    }\n\n    audioRenderer.requestMediaDataWhenReady(on: DispatchQueue.global()) {\n        while audioRenderer.isReadyForMoreMediaData {\n            if let sampleBuffer = sourceAssetReaderAudioTrackOutput!.copyNextSampleBuffer() {\n                audioRenderer.enqueue(sampleBuffer)\n            } else {\n                audioRenderer.stopRequestingMediaData()\n                return\n            }\n        }\n    }\n\n    \/\/ Start the playback immediately.\n    synchronizer.setRate(1, time: .zero)\n\n}\n```\n\n## Creating a video player component\n\n- **init(avPlayer:)**: Creates a video player component from an AV player object.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Creates a video player component from an AV player object.",
          "name" : "init(avPlayer:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/RealityKit\/VideoPlayerComponent\/init(avPlayer:)"
        }
      ],
      "title" : "Creating a video player component"
    }
  ],
  "source" : "appleJSON",
  "title" : "init(videoRenderer:)",
  "url" : "https:\/\/developer.apple.com\/documentation\/RealityKit\/VideoPlayerComponent\/init(videoRenderer:)"
}