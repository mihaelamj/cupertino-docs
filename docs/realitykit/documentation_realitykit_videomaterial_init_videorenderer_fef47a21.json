{
  "abstract" : "Creates and initializes a video material for a sample buffer video renderer object.",
  "codeExamples" : [
    {
      "code" : "\/\/ Create an `AVSampleBufferVideoRenderer` instance to control playback of a movie.\nlet videoRenderer = AVSampleBufferVideoRenderer()\n\n\/\/ Create an `AVSampleBufferAudioRenderer` instance to control audio of the playback.\nlet audioRenderer = AVSampleBufferAudioRenderer()\n\n\/\/ Create a `AVSampleBufferRenderSynchronizer` instance to synchronize video and audio.\nlet synchronizer = AVSampleBufferRenderSynchronizer()\n\n\/\/ Add both videoRenderer and audioRenderer to the synchronizer.\nsynchronizer.addRenderer(videoRenderer)\nsynchronizer.addRenderer(audioRenderer)\n\n\/\/ Create an entity for display.\nlet entity = ModelEntity()\n\n\/\/ Create a `VideoMaterial` object that supplies the `AVSampleBufferVideoRenderer` object.\nentity.model = .init(mesh: plane, materials: [VideoMaterial(videoRenderer: videoRenderer)])\n\n\/\/ Create a URL that points to the movie file.\nif let url = Bundle.main.url(forResource: \"MyMovie\", withExtension: \"mp4\") {\n\n    let sourceAsset = AVURLAsset(url: url)\n    let sourceAssetReader = AVAssetReader(asset: sourceAsset)\n    let sourceAssetVideoTrack = sourceAsset.loadTracks(withMediaType: .video).first\n    let sourceAssetAudioTrack = sourceAsset.loadTracks(withMediaType: .audio).first\n    let sourceAssetReaderVideoTrackOutput = AVAssetReaderTrackOutput(track: sourceAssetVideoTrack!, outputSettings: nil)\n    let sourceAssetReaderAudioTrackOutput = AVAssetReaderTrackOutput(track: sourceAssetAudioTrack!, outputSettings: nil)\n    sourceAssetReader.add(sourceAssetReaderVideoTrackOutput!)\n    sourceAssetReader.add(sourceAssetReaderAudioTrackOutput!)\n\n    sourceAssetReader.startReading()\n\n    videoRenderer.requestMediaDataWhenReady(on: DispatchQueue.global()) {\n        while videoRenderer.isReadyForMoreMediaData {\n            if let sampleBuffer = sourceAssetReaderVideoTrackOutput!.copyNextSampleBuffer() {\n                videoRenderer.enqueue(sampleBuffer)\n            } else {\n                videoRenderer.stopRequestingMediaData()\n                return\n            }\n        }\n    }\n\n    audioRenderer.requestMediaDataWhenReady(on: DispatchQueue.global()) {\n        while audioRenderer.isReadyForMoreMediaData {\n            if let sampleBuffer = sourceAssetReaderAudioTrackOutput!.copyNextSampleBuffer() {\n                audioRenderer.enqueue(sampleBuffer)\n            } else {\n                audioRenderer.stopRequestingMediaData()\n                return\n            }\n        }\n    }\n\n    \/\/ Start the playback immediately.\n    synchronizer.setRate(1, time: .zero)\n\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "29942fad7ed2deedb80f4c1c87efff4013169e3e9aab8475289bec7fc4300bc8",
  "crawledAt" : "2025-12-03T15:31:02Z",
  "declaration" : {
    "code" : "init(videoRenderer: AVSampleBufferVideoRenderer)",
    "language" : "swift"
  },
  "id" : "94A05465-2CAB-4E48-95F8-64214ADA3737",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "RealityKit",
  "overview" : "## Discussion\n\nTo create a `VideoMaterial`, first create an [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVSampleBufferVideoRenderer] instance with no parameters and then pass that to the `VideoMaterial` initializer. After creating the `VideoMaterial`, create a [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVAssetReader] with the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVURLAsset] and load the media (video), which you need to track as the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVAssetTrack]. Feed this `AVAssetTrack` to the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVAssetReaderTrackOutput] and add this output tracker to the `AVAssetReader` you created before. Wait for setup to finish and start reading. Read the sample buffers from the reader and enqueue it into the `AVSampleBufferVideoRenderer` object. `AVSampleBufferVideoRenderer` uses a push model, so feed the video sample buffers until the queue is full and then resume feeding it when it’s ready for more. You can use `isReadyForMoreMediaData` on the `AVSampleBufferVideoRenderer` object in a while-loop to check if you need to enqueue more sample buffers. You can’t use the same `AVSampleBufferVideoRenderer` object with more than one `VideoMaterial`.\n\nYou need to synchronize the audio, captions, and playback rate separately in your application.\n\nThe following code example demonstrates how to synchronize video with audio:",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst",
    "macOS",
    "tvOS",
    "visionOS"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/RealityKit\/VideoMaterial\/init(videoRenderer:)\ncrawled: 2025-12-03T15:31:02Z\n---\n\n# init(videoRenderer:)\n\n**Initializer**\n\nCreates and initializes a video material for a sample buffer video renderer object.\n\n## Declaration\n\n```swift\ninit(videoRenderer: AVSampleBufferVideoRenderer)\n```\n\n## Parameters\n\n- **videoRenderer**: The sample buffer video renderer whose visual contents the material presents.\n\n## Discussion\n\nTo create a `VideoMaterial`, first create an [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVSampleBufferVideoRenderer] instance with no parameters and then pass that to the `VideoMaterial` initializer. After creating the `VideoMaterial`, create a [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVAssetReader] with the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVURLAsset] and load the media (video), which you need to track as the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVAssetTrack]. Feed this `AVAssetTrack` to the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVAssetReaderTrackOutput] and add this output tracker to the `AVAssetReader` you created before. Wait for setup to finish and start reading. Read the sample buffers from the reader and enqueue it into the `AVSampleBufferVideoRenderer` object. `AVSampleBufferVideoRenderer` uses a push model, so feed the video sample buffers until the queue is full and then resume feeding it when it’s ready for more. You can use `isReadyForMoreMediaData` on the `AVSampleBufferVideoRenderer` object in a while-loop to check if you need to enqueue more sample buffers. You can’t use the same `AVSampleBufferVideoRenderer` object with more than one `VideoMaterial`.\n\nYou need to synchronize the audio, captions, and playback rate separately in your application.\n\nThe following code example demonstrates how to synchronize video with audio:\n\n```swift\n\/\/ Create an `AVSampleBufferVideoRenderer` instance to control playback of a movie.\nlet videoRenderer = AVSampleBufferVideoRenderer()\n\n\/\/ Create an `AVSampleBufferAudioRenderer` instance to control audio of the playback.\nlet audioRenderer = AVSampleBufferAudioRenderer()\n\n\/\/ Create a `AVSampleBufferRenderSynchronizer` instance to synchronize video and audio.\nlet synchronizer = AVSampleBufferRenderSynchronizer()\n\n\/\/ Add both videoRenderer and audioRenderer to the synchronizer.\nsynchronizer.addRenderer(videoRenderer)\nsynchronizer.addRenderer(audioRenderer)\n\n\/\/ Create an entity for display.\nlet entity = ModelEntity()\n\n\/\/ Create a `VideoMaterial` object that supplies the `AVSampleBufferVideoRenderer` object.\nentity.model = .init(mesh: plane, materials: [VideoMaterial(videoRenderer: videoRenderer)])\n\n\/\/ Create a URL that points to the movie file.\nif let url = Bundle.main.url(forResource: \"MyMovie\", withExtension: \"mp4\") {\n\n    let sourceAsset = AVURLAsset(url: url)\n    let sourceAssetReader = AVAssetReader(asset: sourceAsset)\n    let sourceAssetVideoTrack = sourceAsset.loadTracks(withMediaType: .video).first\n    let sourceAssetAudioTrack = sourceAsset.loadTracks(withMediaType: .audio).first\n    let sourceAssetReaderVideoTrackOutput = AVAssetReaderTrackOutput(track: sourceAssetVideoTrack!, outputSettings: nil)\n    let sourceAssetReaderAudioTrackOutput = AVAssetReaderTrackOutput(track: sourceAssetAudioTrack!, outputSettings: nil)\n    sourceAssetReader.add(sourceAssetReaderVideoTrackOutput!)\n    sourceAssetReader.add(sourceAssetReaderAudioTrackOutput!)\n\n    sourceAssetReader.startReading()\n\n    videoRenderer.requestMediaDataWhenReady(on: DispatchQueue.global()) {\n        while videoRenderer.isReadyForMoreMediaData {\n            if let sampleBuffer = sourceAssetReaderVideoTrackOutput!.copyNextSampleBuffer() {\n                videoRenderer.enqueue(sampleBuffer)\n            } else {\n                videoRenderer.stopRequestingMediaData()\n                return\n            }\n        }\n    }\n\n    audioRenderer.requestMediaDataWhenReady(on: DispatchQueue.global()) {\n        while audioRenderer.isReadyForMoreMediaData {\n            if let sampleBuffer = sourceAssetReaderAudioTrackOutput!.copyNextSampleBuffer() {\n                audioRenderer.enqueue(sampleBuffer)\n            } else {\n                audioRenderer.stopRequestingMediaData()\n                return\n            }\n        }\n    }\n\n    \/\/ Start the playback immediately.\n    synchronizer.setRate(1, time: .zero)\n\n}\n```\n\n",
  "sections" : [

  ],
  "source" : "appleJSON",
  "title" : "init(videoRenderer:)",
  "url" : "https:\/\/developer.apple.com\/documentation\/RealityKit\/VideoMaterial\/init(videoRenderer:)"
}