{
  "abstract" : "Render stereoscopic video in visionOS with RealityKit.",
  "codeExamples" : [
    {
      "code" : "\/\/\/ The main app structure.\n@main\nstruct RealityKitPlaybackApp: App {\n    \/\/\/ An object that controls the video playback behavior.\n    @State private var player = PlayerModel()\n    \n    var body: some Scene {\n        WindowGroup {\n            ContentView()\n                .environment(player)\n                .frame(width: 1600, height: 900)\n        }\n        \/\/ Expressly constrain window size to that of its content.\n        .windowResizability(.contentSize)\n        \/\/ Disable background glass.\n        .windowStyle(.plain)\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ A reference to the player.\n@Environment(PlayerModel.self) private var player",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ The root entity of the scene.\nprivate let entity = Entity()",
      "language" : "swift"
    },
    {
      "code" : "RealityView { content in\n    \/\/ Initialize the video player with the supplied renderer.\n    let videoPlayerComponent = VideoPlayerComponent(videoRenderer: player.videoRenderer)\n    entity.components.set(videoPlayerComponent)\n\n    \/\/ Scale the root entity and add it to the view.\n    entity.scale = SIMD3<Float>(repeating: Self.scaleFactor)\n    content.add(entity)\n}\n\/\/ Set the frame to 0 so that the RealityView's origin exists on the same plane as the window.\n.frame(depth: 0)",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Begin playback when ready.\n.onChange(of: player.isReadyToPlay) { _, ready in\n    if ready {\n        player.play()\n    }\n}\n\/\/ Monitor the scene phase and stop playback when entering the background.\n.onChange(of: scenePhase) { _, scenePhase in\n    if scenePhase == .background {\n        player.stop()\n    }\n}\n\/\/ Start loading the player.\n.task {\n    await player.load()\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ The synchronizer that controls the underlying video renderer.\nprivate let synchronizer = AVSampleBufferRenderSynchronizer()\n\n\/\/\/ The video renderer that enqueues individual frames for playback.\nlet videoRenderer = AVSampleBufferVideoRenderer()",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ Initializes a player with the specified asset URL.\n\/\/\/ - Parameter assetURL: A URL for the asset that the app plays.\ninit(assetURL: URL) {\n    synchronizer.addRenderer(videoRenderer)\n    asset = AVURLAsset(url: assetURL)\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ Begin loading the player.\nfunc load() async throws {\n    \/\/ Determine the duration of the underlying video asset.\n    let duration = try await asset.load(.duration)\n\n    \/\/ Use the asset duration as the boundary period with which to loop.\n    synchronizer.addBoundaryTimeObserver(forTimes: [NSValue(time: duration)], queue: nil) {\n        Task { @MainActor [weak self] in\n            guard let self else { return }\n            self.loop(rate: self.synchronizer.rate)\n        }\n    }\n\n    \/\/ Prepare the processor that the app uses for the initial playback loop.\n    enqueueProcessor()\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ Begin playback by starting the loop.\nfunc play() {\n    isLooping = true\n    loop(rate: 1)\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ Executes a logical playback loop.\n\/\/\/ - Parameter rate: The rate with which to playback content.\nprivate func loop(rate: Float) {\n    guard isLooping, let nextProcessor else {\n        return\n    }\n\n    let currentProcessor = nextProcessor\n    process(with: currentProcessor)\n    synchronizer.setRate(rate, time: .zero)\n\n    enqueueProcessor()\n    loopCount += 1\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ End playback by stopping the loop and resetting relevant state.\nfunc stop() {\n    nextProcessor = nil\n    isLooping = false\n    loopCount = 0\n    synchronizer.rate = 0\n    videoRenderer.stopRequestingMediaData()\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Load the asset.\nguard let videoTrack = try await asset.loadTracks(withMediaCharacteristic: .visual).first else {\n    fatalError(\"Error loading side-by-side video input\")\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Determine the size of the video track, which reflects frame packing.\nlet videoFrameSize = try await videoTrack.load(.naturalSize)",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Setup the asset reader.\nlet readerSettings: [String: Any] = [\n    kCVPixelBufferIOSurfacePropertiesKey as String: [String: String]()\n]\nlet videoTrackOutput = AVAssetReaderTrackOutput(track: videoTrack, outputSettings: readerSettings)\nlet assetReader = try AVAssetReader(asset: asset)\nlet videoTrackOutputProvider = assetReader.outputProvider(for: videoTrackOutput)\ntry assetReader.start()",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Setup the pixel transfer session.\nvar transferSession: VTPixelTransferSession?\nlet sessionResult = VTPixelTransferSessionCreate(\n    allocator: kCFAllocatorDefault,\n    pixelTransferSessionOut: &transferSession\n)\nguard sessionResult == kCVReturnSuccess, let transferSession else {\n    fatalError(\"Failed to create pixel transfer session: \\(sessionResult)\")\n}\nVTSessionSetProperty(transferSession, key: kVTPixelTransferPropertyKey_ScalingMode, value: kVTScalingMode_CropSourceToCleanAperture)",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Setup the pixel buffer pool.\nlet eyeFrameSize = CVImageSize(\n    width: Int(videoFrameSize.width \/ stereoMetadata.horizontalScale),\n    height: Int(videoFrameSize.height \/ stereoMetadata.verticalScale)\n)\nlet defaultAttributes = CVPixelBufferCreationAttributes(\n    pixelFormatType: CVPixelFormatType(rawValue: kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange),\n    size: eyeFrameSize\n)\nlet recommendedAttributes = videoRenderer.recommendedPixelBufferAttributes\nguard let mergedAttributes = CVPixelBufferAttributes(merging: [CVPixelBufferAttributes(defaultAttributes), recommendedAttributes]),\n      let creationAttributes = CVPixelBufferCreationAttributes(mergedAttributes),\n      let pixelBufferPool = try? CVMutablePixelBuffer.Pool(pixelBufferAttributes: creationAttributes)\nelse {\n    fatalError(\"Failed to create pixel buffer pool\")\n}",
      "language" : "swift"
    },
    {
      "code" : "Task {\n    \/\/ Prepare the renderer for processing.\n    await untilReadyForMoreMediaData()\n    isProcessing = true\n\n    \/\/ Process all read frames from the input video track.\n    while videoRenderer.isReadyForMoreMediaData && isProcessing {\n        while let sampleBuffer = try await videoTrackOutputProvider.next() {\n            if let transformedBuffer = try transform(from: sampleBuffer, with: pixelBufferPool, in: transferSession) {\n                videoRenderer.enqueue(transformedBuffer)\n            }\n        }\n\n        \/\/ Indicate that processing is substantially complete.\n        isProcessing = false\n    }\n\n    \/\/ Conclude processing.\n    assetReader.cancelReading()\n    VTPixelTransferSessionInvalidate(transferSession)\n}",
      "language" : "swift"
    },
    {
      "code" : "let layerIDs = [0, 1]\nlet eyeComponents: [CMStereoViewComponents] = [.leftEye, .rightEye]\nvar taggedBuffers = [CMTaggedDynamicBuffer]()\nfor (layerID, eye) in zip(layerIDs, eyeComponents) {\n    \/\/ ..."
    },
    {
      "code" : "\/\/ Crop the transfer region to the current eye.\nlet bufferSize = pixelBufferPool.pixelBufferAttributes.size\nlet apertureOffset = stereoMetadata.apertureOffset(for: bufferSize, layerID: layerID)\nlet cropRectDict = [\n    kCVImageBufferCleanApertureHorizontalOffsetKey: apertureOffset.horizontal,\n    kCVImageBufferCleanApertureVerticalOffsetKey: apertureOffset.vertical,\n    kCVImageBufferCleanApertureWidthKey: bufferSize.width,\n    kCVImageBufferCleanApertureHeightKey: bufferSize.height\n]\nCVBufferSetAttachment(sourceImageBuffer, kCVImageBufferCleanApertureKey, cropRectDict as CFDictionary, .shouldPropagate)\nVTSessionSetProperty(transferSession, key: kVTPixelTransferPropertyKey_ScalingMode, value: kVTScalingMode_CropSourceToCleanAperture)\n\n\/\/ Transfer the image to the pixel buffer.\npixelBuffer.withUnsafeBuffer { cvPixelBuffer in\n    let transferResult = VTPixelTransferSessionTransferImage(transferSession, from: sourceImageBuffer, to: cvPixelBuffer)\n    guard transferResult == kCVReturnSuccess else {\n        fatalError(\"Error during pixel transfer session for layer \\(layerID): \\(transferResult)\")\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Create and append a tagged buffer for this eye.\nlet tags: [CMTag] = [.videoLayerID(Int64(layerID)), .stereoView(eye), .mediaType(.video)]\ntaggedBuffers.append(CMTaggedDynamicBuffer(tags: tags, content: .pixelBuffer(CVReadOnlyPixelBuffer(pixelBuffer))))",
      "language" : "swift"
    },
    {
      "code" : "let buffer = CMReadySampleBuffer(\n    taggedBuffers: taggedBuffers,\n    formatDescription: CMTaggedBufferGroupFormatDescription(taggedBuffers: taggedBuffers),\n    presentationTimeStamp: cmSampleBuffer.presentationTimeStamp,\n    duration: cmSampleBuffer.duration\n)",
      "language" : "swift"
    }
  ],
  "contentHash" : "53117bda42d3d0ce7500f08cf96260f8e09e9fecfdff0d129301362f05aec31b",
  "crawledAt" : "2025-12-02T19:56:05Z",
  "id" : "07AC92D2-7FFF-47D6-B1CC-A64D8A1D9FD6",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "RealityKit",
  "overview" : "## Overview\n\nvisionOS offers a range of options for programmatic video playback, including:\n\n`VideoMaterial` and `VideoPlayerComponent` offer two distinct options for controlling playback.\n\nThis sample app uses an `AVSampleBufferVideoRenderer` and a `VideoPlayerComponent` to render stereoscopic video in the *Shared Space*. Its content is a *side-by-side* video, which places the left- and right-eye images next to each other as part of a single video frame. Because the duration of the video is brief, a looping mechanism supports continuous playback.\n\n\n\n## Structure the app\n\nThe structure of the app is simple. `PlayerModel` is an [doc:\/\/com.apple.documentation\/documentation\/Observation\/Observable] custom type that’s injected into the SwiftUI [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/Environment] for visibility to the root `ContentView`. This model includes a property of type `PlayerState`, which is a Swift enumeration that reflects the current player state. It also includes an instance of type `LoopingVideoPlayer`, which exposes the underlying `AVSampleBufferVideoRenderer`.\n\nFor simplicity, the following modifiers are applied to create a window with fixed size, 16x9 aspect ratio, and absent background glass:\n\n## Create an entity to render video content\n\nUse the `PlayerModel` as a property exposed from `ContentView`:\n\n`ContentView` also defines an [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/Entity] to house the [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/RealityView] at the root of this scene:\n\nIn the `RealityView` `make` closure, the app instantiates a `VideoPlayerComponent`, passing the player’s video renderer. It then adds the video player to the root entity. The sample uses `scaleFactor`, a type property set to `0.5`, to scale the root entity to half its default size.\n\nFinally, the sample applies these modifiers to initialize the player, and to begin and end playback:\n\n## Prepare for continuous playback\n\n`LoopingVideoPlayer` is a custom type that coordinates continuous playback of the sample video. To achieve this, it manages multiple instances of another custom type, `SerialProcessor`.\n\nThe player has two key properties: a video renderer and a synchronizer to control the rendering timeline:\n\nWhen the system creates the player, it adds the renderer to the synchronizer, and initializes an [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVURLAsset] with a URL to the underlying video:\n\nTo prepare for playback, the processor loads the asset duration asynchronously with [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVAsynchronousKeyValueLoading]. The sample uses duration to trigger looping at the end of each playback cycle with [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVSampleBufferRenderSynchronizer\/addBoundaryTimeObserver(forTimes:queue:using:)]. The sample initializes the first serial processor with the video renderer and asset:\n\n## Loop playback\n\nThe app begins playback by calling `loop(rate:)` for the first time. The initial rate of the render synchronizer is `0.0`, meaning that playback has stopped. Passing `1.0` starts playback at the natural rate of the media.\n\nThe app starts the loop by dequeueing a serial processor instance. It specifies the time and rate of the render synchronizer with [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVSampleBufferRenderSynchronizer\/setRate(_:time:)]. It then enqueues the next serial processor instance and increments the loop count:\n\nThe boundary time observer initiates subsequent loops. Playback continues until someone closes the scene. At that time, the sample calls `stop()` to dispose of the player’s resources:\n\n## Load the side-by-side video\n\nEach `SerialProcessor` traverses the video track from start to finish, extracting each individual video frame for processing. Processing converts these input frames from the single-layer, side-by-side input format to a multi-layer, output format.\n\nWith side-by-side input, the sample places left- and right-eye images next to each other as part of a single frame. The sample splits the frame into separate images, copies them to distinct left- and right-eye layers, and writes them as a multi-layer frame.\n\nThe processor begins when the sample calls [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVAsset\/loadTracks(withMediaCharacteristic:completionHandler:)] to load video tracks, and then selects the first available track as the side-by-side input.\n\nThe processor also loads the natural size of the side-by-side video for later use:\n\nThe processor specifies [doc:\/\/com.apple.documentation\/documentation\/IOSurface] settings in its `readerSettings` dictionary. Because the sample manages its own pixel buffer allocations, it uses an empty array as the value corresponding to the [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/kCVPixelBufferIOSurfacePropertiesKey] key. These settings create an [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVAssetReaderTrackOutput]. To finish loading the video, the sample obtains an output provider from the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVAssetReader], and starts reading.\n\n## Create the video frame-transfer session and output pixel-buffer pool\n\nTo prepare for processing the video input, the sample creates a [doc:\/\/com.apple.documentation\/documentation\/VideoToolbox\/VTPixelTransferSession] to read raw [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/cvpixelbuffer-q2e] input and write processed `CVPixelBuffers` as output.\n\nFor efficiency, the sample creates a [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/CVMutablePixelBuffer\/Pool] to allocate pixel buffers for the processed multi-layer output. It creates a pool with attributes that include the pixel format type and size of the eye frame. The sample derives the eye frame size from the natural video size, previously loaded. It then merges these specified attributes with [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVSampleBufferVideoRenderer\/recommendedPixelBufferAttributes-6zrqb].\n\n## Process input as it becomes available\n\nTo begin processing, the processor waits for the video renderer to indicate that it is ready to begin rendering. The private `untilReadyForMoreMediaData()` function achieves this with a call to [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVQueuedSampleBufferRendering\/requestMediaDataWhenReady(on:using:)]. As the sample reads the asset, the `videoTrackOutputProvider` supplies a stream of sample buffers for processing. As the sample receives these sample buffers, the processor calls `transform(from:with:in:)` to convert the side-by-side frame input into stereo-encoded output. The sample then enqueues the stereo-encoded frames to the video renderer. Processing concludes once the stream of sample buffers is exhausted.\n\n## Transform side-by-side input to multi-layer output\n\nThe processor creates individual left- and right-eye images in the transformation function. It specifies layer ID `0` for the left eye, and `1` for the right eye.\n\nThe function uses the `VTPixelTransferSession` to copy pixels from the side-by-side source pixel buffer, crop to the frame for the current eye, and place them into the destination pixel buffer.\n\nThe sample creates the individual left and right pixel buffers, adorns them with [doc:\/\/com.apple.documentation\/documentation\/CoreMedia\/CMTag-swift.class] metadata, and stores them as [doc:\/\/com.apple.documentation\/documentation\/CoreMedia\/CMTaggedDynamicBuffer] pairs.\n\nFinally, the sample combines the tagged buffers with the presentation timestamp and duration of the input sample buffer and creates the final output sample buffer.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/realitykit\/rendering-stereoscopic-video-with-realitykit\ncrawled: 2025-12-02T19:56:05Z\n---\n\n# Rendering stereoscopic video with RealityKit\n\n**Sample Code**\n\nRender stereoscopic video in visionOS with RealityKit.\n\n## Overview\n\nvisionOS offers a range of options for programmatic video playback, including:\n\n- [doc:\/\/com.apple.documentation\/documentation\/AVKit] provides a superior video playback experience in visionOS. With AVKit, you can present an interface that’s consistent with other apps on the system, with minimal adoption effort. For more information on using AVKit in visionOS, see [doc:\/\/com.apple.documentation\/documentation\/AVKit\/adopting-the-system-player-interface-in-visionos].\n- [doc:\/\/com.apple.documentation\/documentation\/RealityKit] offer a greater degree of customization, using either [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/VideoMaterial] or [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/VideoPlayerComponent].\n\n`VideoMaterial` and `VideoPlayerComponent` offer two distinct options for controlling playback.\n\n- [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVPlayer] is a versatile system component, appropriate for controlling both playback and timing of a media asset.\n- [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVSampleBufferVideoRenderer] enables greater customization — it requires that you programmatically enqueue individual video-sample buffers for rendering. You can also use an `AVSampleBufferAudioRenderer` instance with a [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVSampleBufferRenderSynchronizer], which affords I\/O control, supports preprocessing of media data, and accommodates DRM models not supported by `AVPlayer`. An `AVSampleBufferRenderSynchronizer` can optionally synchronize audio with an [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVSampleBufferAudioRenderer].\n\nThis sample app uses an `AVSampleBufferVideoRenderer` and a `VideoPlayerComponent` to render stereoscopic video in the *Shared Space*. Its content is a *side-by-side* video, which places the left- and right-eye images next to each other as part of a single video frame. Because the duration of the video is brief, a looping mechanism supports continuous playback.\n\n\n\n\n\n## Structure the app\n\nThe structure of the app is simple. `PlayerModel` is an [doc:\/\/com.apple.documentation\/documentation\/Observation\/Observable] custom type that’s injected into the SwiftUI [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/Environment] for visibility to the root `ContentView`. This model includes a property of type `PlayerState`, which is a Swift enumeration that reflects the current player state. It also includes an instance of type `LoopingVideoPlayer`, which exposes the underlying `AVSampleBufferVideoRenderer`.\n\n```swift\n\/\/\/ The main app structure.\n@main\nstruct RealityKitPlaybackApp: App {\n    \/\/\/ An object that controls the video playback behavior.\n    @State private var player = PlayerModel()\n    \n    var body: some Scene {\n        WindowGroup {\n            ContentView()\n                .environment(player)\n                .frame(width: 1600, height: 900)\n        }\n        \/\/ Expressly constrain window size to that of its content.\n        .windowResizability(.contentSize)\n        \/\/ Disable background glass.\n        .windowStyle(.plain)\n    }\n}\n```\n\nFor simplicity, the following modifiers are applied to create a window with fixed size, 16x9 aspect ratio, and absent background glass:\n\n- [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/View\/frame(width:height:alignment:)]\n- [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/Scene\/windowResizability(_:)]\n- [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/Scene\/windowStyle(_:)]\n\n## Create an entity to render video content\n\nUse the `PlayerModel` as a property exposed from `ContentView`:\n\n```swift\n\/\/\/ A reference to the player.\n@Environment(PlayerModel.self) private var player\n```\n\n`ContentView` also defines an [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/Entity] to house the [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/RealityView] at the root of this scene:\n\n```swift\n\/\/\/ The root entity of the scene.\nprivate let entity = Entity()\n```\n\nIn the `RealityView` `make` closure, the app instantiates a `VideoPlayerComponent`, passing the player’s video renderer. It then adds the video player to the root entity. The sample uses `scaleFactor`, a type property set to `0.5`, to scale the root entity to half its default size.\n\n```swift\nRealityView { content in\n    \/\/ Initialize the video player with the supplied renderer.\n    let videoPlayerComponent = VideoPlayerComponent(videoRenderer: player.videoRenderer)\n    entity.components.set(videoPlayerComponent)\n\n    \/\/ Scale the root entity and add it to the view.\n    entity.scale = SIMD3<Float>(repeating: Self.scaleFactor)\n    content.add(entity)\n}\n\/\/ Set the frame to 0 so that the RealityView's origin exists on the same plane as the window.\n.frame(depth: 0)\n```\n\nFinally, the sample applies these modifiers to initialize the player, and to begin and end playback:\n\n- [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/View\/onChange(of:initial:_:)-8wgw9]\n- [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/View\/task(priority:_:)]\n\n```swift\n\/\/ Begin playback when ready.\n.onChange(of: player.isReadyToPlay) { _, ready in\n    if ready {\n        player.play()\n    }\n}\n\/\/ Monitor the scene phase and stop playback when entering the background.\n.onChange(of: scenePhase) { _, scenePhase in\n    if scenePhase == .background {\n        player.stop()\n    }\n}\n\/\/ Start loading the player.\n.task {\n    await player.load()\n}\n```\n\n## Prepare for continuous playback\n\n`LoopingVideoPlayer` is a custom type that coordinates continuous playback of the sample video. To achieve this, it manages multiple instances of another custom type, `SerialProcessor`.\n\nThe player has two key properties: a video renderer and a synchronizer to control the rendering timeline:\n\n```swift\n\/\/\/ The synchronizer that controls the underlying video renderer.\nprivate let synchronizer = AVSampleBufferRenderSynchronizer()\n\n\/\/\/ The video renderer that enqueues individual frames for playback.\nlet videoRenderer = AVSampleBufferVideoRenderer()\n```\n\nWhen the system creates the player, it adds the renderer to the synchronizer, and initializes an [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVURLAsset] with a URL to the underlying video:\n\n```swift\n\/\/\/ Initializes a player with the specified asset URL.\n\/\/\/ - Parameter assetURL: A URL for the asset that the app plays.\ninit(assetURL: URL) {\n    synchronizer.addRenderer(videoRenderer)\n    asset = AVURLAsset(url: assetURL)\n}\n```\n\nTo prepare for playback, the processor loads the asset duration asynchronously with [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVAsynchronousKeyValueLoading]. The sample uses duration to trigger looping at the end of each playback cycle with [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVSampleBufferRenderSynchronizer\/addBoundaryTimeObserver(forTimes:queue:using:)]. The sample initializes the first serial processor with the video renderer and asset:\n\n```swift\n\/\/\/ Begin loading the player.\nfunc load() async throws {\n    \/\/ Determine the duration of the underlying video asset.\n    let duration = try await asset.load(.duration)\n\n    \/\/ Use the asset duration as the boundary period with which to loop.\n    synchronizer.addBoundaryTimeObserver(forTimes: [NSValue(time: duration)], queue: nil) {\n        Task { @MainActor [weak self] in\n            guard let self else { return }\n            self.loop(rate: self.synchronizer.rate)\n        }\n    }\n\n    \/\/ Prepare the processor that the app uses for the initial playback loop.\n    enqueueProcessor()\n}\n```\n\n## Loop playback\n\nThe app begins playback by calling `loop(rate:)` for the first time. The initial rate of the render synchronizer is `0.0`, meaning that playback has stopped. Passing `1.0` starts playback at the natural rate of the media.\n\n```swift\n\/\/\/ Begin playback by starting the loop.\nfunc play() {\n    isLooping = true\n    loop(rate: 1)\n}\n```\n\nThe app starts the loop by dequeueing a serial processor instance. It specifies the time and rate of the render synchronizer with [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVSampleBufferRenderSynchronizer\/setRate(_:time:)]. It then enqueues the next serial processor instance and increments the loop count:\n\n```swift\n\/\/\/ Executes a logical playback loop.\n\/\/\/ - Parameter rate: The rate with which to playback content.\nprivate func loop(rate: Float) {\n    guard isLooping, let nextProcessor else {\n        return\n    }\n\n    let currentProcessor = nextProcessor\n    process(with: currentProcessor)\n    synchronizer.setRate(rate, time: .zero)\n\n    enqueueProcessor()\n    loopCount += 1\n}\n```\n\nThe boundary time observer initiates subsequent loops. Playback continues until someone closes the scene. At that time, the sample calls `stop()` to dispose of the player’s resources:\n\n```swift\n\/\/\/ End playback by stopping the loop and resetting relevant state.\nfunc stop() {\n    nextProcessor = nil\n    isLooping = false\n    loopCount = 0\n    synchronizer.rate = 0\n    videoRenderer.stopRequestingMediaData()\n}\n```\n\n## Load the side-by-side video\n\nEach `SerialProcessor` traverses the video track from start to finish, extracting each individual video frame for processing. Processing converts these input frames from the single-layer, side-by-side input format to a multi-layer, output format.\n\nWith side-by-side input, the sample places left- and right-eye images next to each other as part of a single frame. The sample splits the frame into separate images, copies them to distinct left- and right-eye layers, and writes them as a multi-layer frame.\n\nThe processor begins when the sample calls [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVAsset\/loadTracks(withMediaCharacteristic:completionHandler:)] to load video tracks, and then selects the first available track as the side-by-side input.\n\n```swift\n\/\/ Load the asset.\nguard let videoTrack = try await asset.loadTracks(withMediaCharacteristic: .visual).first else {\n    fatalError(\"Error loading side-by-side video input\")\n}\n```\n\nThe processor also loads the natural size of the side-by-side video for later use:\n\n```swift\n\/\/ Determine the size of the video track, which reflects frame packing.\nlet videoFrameSize = try await videoTrack.load(.naturalSize)\n```\n\nThe processor specifies [doc:\/\/com.apple.documentation\/documentation\/IOSurface] settings in its `readerSettings` dictionary. Because the sample manages its own pixel buffer allocations, it uses an empty array as the value corresponding to the [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/kCVPixelBufferIOSurfacePropertiesKey] key. These settings create an [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVAssetReaderTrackOutput]. To finish loading the video, the sample obtains an output provider from the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVAssetReader], and starts reading.\n\n```swift\n\/\/ Setup the asset reader.\nlet readerSettings: [String: Any] = [\n    kCVPixelBufferIOSurfacePropertiesKey as String: [String: String]()\n]\nlet videoTrackOutput = AVAssetReaderTrackOutput(track: videoTrack, outputSettings: readerSettings)\nlet assetReader = try AVAssetReader(asset: asset)\nlet videoTrackOutputProvider = assetReader.outputProvider(for: videoTrackOutput)\ntry assetReader.start()\n```\n\n## Create the video frame-transfer session and output pixel-buffer pool\n\nTo prepare for processing the video input, the sample creates a [doc:\/\/com.apple.documentation\/documentation\/VideoToolbox\/VTPixelTransferSession] to read raw [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/cvpixelbuffer-q2e] input and write processed `CVPixelBuffers` as output.\n\n```swift\n\/\/ Setup the pixel transfer session.\nvar transferSession: VTPixelTransferSession?\nlet sessionResult = VTPixelTransferSessionCreate(\n    allocator: kCFAllocatorDefault,\n    pixelTransferSessionOut: &transferSession\n)\nguard sessionResult == kCVReturnSuccess, let transferSession else {\n    fatalError(\"Failed to create pixel transfer session: \\(sessionResult)\")\n}\nVTSessionSetProperty(transferSession, key: kVTPixelTransferPropertyKey_ScalingMode, value: kVTScalingMode_CropSourceToCleanAperture)\n```\n\nFor efficiency, the sample creates a [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/CVMutablePixelBuffer\/Pool] to allocate pixel buffers for the processed multi-layer output. It creates a pool with attributes that include the pixel format type and size of the eye frame. The sample derives the eye frame size from the natural video size, previously loaded. It then merges these specified attributes with [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVSampleBufferVideoRenderer\/recommendedPixelBufferAttributes-6zrqb].\n\n```swift\n\/\/ Setup the pixel buffer pool.\nlet eyeFrameSize = CVImageSize(\n    width: Int(videoFrameSize.width \/ stereoMetadata.horizontalScale),\n    height: Int(videoFrameSize.height \/ stereoMetadata.verticalScale)\n)\nlet defaultAttributes = CVPixelBufferCreationAttributes(\n    pixelFormatType: CVPixelFormatType(rawValue: kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange),\n    size: eyeFrameSize\n)\nlet recommendedAttributes = videoRenderer.recommendedPixelBufferAttributes\nguard let mergedAttributes = CVPixelBufferAttributes(merging: [CVPixelBufferAttributes(defaultAttributes), recommendedAttributes]),\n      let creationAttributes = CVPixelBufferCreationAttributes(mergedAttributes),\n      let pixelBufferPool = try? CVMutablePixelBuffer.Pool(pixelBufferAttributes: creationAttributes)\nelse {\n    fatalError(\"Failed to create pixel buffer pool\")\n}\n```\n\n## Process input as it becomes available\n\nTo begin processing, the processor waits for the video renderer to indicate that it is ready to begin rendering. The private `untilReadyForMoreMediaData()` function achieves this with a call to [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVQueuedSampleBufferRendering\/requestMediaDataWhenReady(on:using:)]. As the sample reads the asset, the `videoTrackOutputProvider` supplies a stream of sample buffers for processing. As the sample receives these sample buffers, the processor calls `transform(from:with:in:)` to convert the side-by-side frame input into stereo-encoded output. The sample then enqueues the stereo-encoded frames to the video renderer. Processing concludes once the stream of sample buffers is exhausted.\n\n```swift\nTask {\n    \/\/ Prepare the renderer for processing.\n    await untilReadyForMoreMediaData()\n    isProcessing = true\n\n    \/\/ Process all read frames from the input video track.\n    while videoRenderer.isReadyForMoreMediaData && isProcessing {\n        while let sampleBuffer = try await videoTrackOutputProvider.next() {\n            if let transformedBuffer = try transform(from: sampleBuffer, with: pixelBufferPool, in: transferSession) {\n                videoRenderer.enqueue(transformedBuffer)\n            }\n        }\n\n        \/\/ Indicate that processing is substantially complete.\n        isProcessing = false\n    }\n\n    \/\/ Conclude processing.\n    assetReader.cancelReading()\n    VTPixelTransferSessionInvalidate(transferSession)\n}\n```\n\n## Transform side-by-side input to multi-layer output\n\nThe processor creates individual left- and right-eye images in the transformation function. It specifies layer ID `0` for the left eye, and `1` for the right eye.\n\n```\nlet layerIDs = [0, 1]\nlet eyeComponents: [CMStereoViewComponents] = [.leftEye, .rightEye]\nvar taggedBuffers = [CMTaggedDynamicBuffer]()\nfor (layerID, eye) in zip(layerIDs, eyeComponents) {\n    \/\/ ...\n```\n\nThe function uses the `VTPixelTransferSession` to copy pixels from the side-by-side source pixel buffer, crop to the frame for the current eye, and place them into the destination pixel buffer.\n\n```swift\n\/\/ Crop the transfer region to the current eye.\nlet bufferSize = pixelBufferPool.pixelBufferAttributes.size\nlet apertureOffset = stereoMetadata.apertureOffset(for: bufferSize, layerID: layerID)\nlet cropRectDict = [\n    kCVImageBufferCleanApertureHorizontalOffsetKey: apertureOffset.horizontal,\n    kCVImageBufferCleanApertureVerticalOffsetKey: apertureOffset.vertical,\n    kCVImageBufferCleanApertureWidthKey: bufferSize.width,\n    kCVImageBufferCleanApertureHeightKey: bufferSize.height\n]\nCVBufferSetAttachment(sourceImageBuffer, kCVImageBufferCleanApertureKey, cropRectDict as CFDictionary, .shouldPropagate)\nVTSessionSetProperty(transferSession, key: kVTPixelTransferPropertyKey_ScalingMode, value: kVTScalingMode_CropSourceToCleanAperture)\n\n\/\/ Transfer the image to the pixel buffer.\npixelBuffer.withUnsafeBuffer { cvPixelBuffer in\n    let transferResult = VTPixelTransferSessionTransferImage(transferSession, from: sourceImageBuffer, to: cvPixelBuffer)\n    guard transferResult == kCVReturnSuccess else {\n        fatalError(\"Error during pixel transfer session for layer \\(layerID): \\(transferResult)\")\n    }\n}\n```\n\nThe sample creates the individual left and right pixel buffers, adorns them with [doc:\/\/com.apple.documentation\/documentation\/CoreMedia\/CMTag-swift.class] metadata, and stores them as [doc:\/\/com.apple.documentation\/documentation\/CoreMedia\/CMTaggedDynamicBuffer] pairs.\n\n```swift\n\/\/ Create and append a tagged buffer for this eye.\nlet tags: [CMTag] = [.videoLayerID(Int64(layerID)), .stereoView(eye), .mediaType(.video)]\ntaggedBuffers.append(CMTaggedDynamicBuffer(tags: tags, content: .pixelBuffer(CVReadOnlyPixelBuffer(pixelBuffer))))\n```\n\nFinally, the sample combines the tagged buffers with the presentation timestamp and duration of the input sample buffer and creates the final output sample buffer.\n\n```swift\nlet buffer = CMReadySampleBuffer(\n    taggedBuffers: taggedBuffers,\n    formatDescription: CMTaggedBufferGroupFormatDescription(taggedBuffers: taggedBuffers),\n    presentationTimeStamp: cmSampleBuffer.presentationTimeStamp,\n    duration: cmSampleBuffer.duration\n)\n```\n\n",
  "sections" : [

  ],
  "source" : "appleJSON",
  "title" : "Rendering stereoscopic video with RealityKit",
  "url" : "https:\/\/developer.apple.com\/documentation\/realitykit\/rendering-stereoscopic-video-with-realitykit"
}