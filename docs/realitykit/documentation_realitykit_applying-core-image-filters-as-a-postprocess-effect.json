{
  "abstract" : "Create special rendering effects for your RealityKit scenes using Core Image.",
  "codeExamples" : [
    {
      "code" : "func setupCoreImage(device: MTLDevice) {\n    \/\/ Create a CIContext and store it in a property.\n    ciContext = CIContext(mtlDevice: device)\n\n    \/\/ Do other expensive tasks, like loading images, here.\n}",
      "language" : "swift"
    },
    {
      "code" : "arView.renderCallbacks.prepareWithDevice = setupCoreImage",
      "language" : "swift"
    },
    {
      "code" : "func postProcessWithCoreImage(context: ARView.PostProcessContext) {\n\n    \/\/ Create and configure the Core Image filter.\n    let filter = CIFilter.falseColor()\n    filter.color0 = CIColor.blue\n    filter.color1 = CIColor.yellow\n\n    \/\/ Convert the frame buffer from a Metal texture to a CIImage, and \n    \/\/ set the CIImage as the filter's input image.\n    guard let input = CIImage(mtlTexture: context.sourceColorTexture) else {\n        fatalError(\"Unable to create a CIImage from sourceColorTexture.\")\n    }\n    filter.setValue(input, forKey: kCIInputImageKey)\n\n    \/\/ Get a reference to the filter's output image.\n    guard let output = filter.outputImage else {\n        fatalError(\"Error applying filter.\")\n    }\n\n    \/\/ Create a render destination and render the filter to the context's command buffer.\n    let destination = CIRenderDestination(mtlTexture: context.compatibleTargetTexture,\n                                          commandBuffer: context.commandBuffer)\n    destination.isFlipped = false\n    _ = try? self.ciContext.startTask(toRender: output, to: destination)\n}",
      "language" : "swift"
    },
    {
      "code" : "arView.renderCallbacks.postProcess = postProcessWithCoreImage",
      "language" : "swift"
    }
  ],
  "contentHash" : "e3a1ae01509f605c97254d907a92b3542012f386a34b0c9dc64499bd91890af9",
  "crawledAt" : "2025-12-01T20:27:16Z",
  "id" : "3ECA23EE-0378-4262-A458-1D27A6584F75",
  "kind" : "article",
  "module" : "RealityKit",
  "overview" : "## Overview\n\nIn iOS 15 and later, and macOS 12 and later, you can apply postprocess effects to a RealityKit scene after RealityKit renders it, but before RealityKit displays it. If you register a postprocess callback function, RealityKit passes that function the complete, rendered frame so you can modify it before the viewer sees it. You can use any image processing or drawing APIs on the rendered frame but, as a practical matter, only APIs that execute on the GPU are fast enough to use every frame and maintain a good framerate.\n\nOne option for implementing postprocess effects is to apply [doc:\/\/com.apple.documentation\/documentation\/CoreImage] filters to the rendered frame. Core Image provides a wide variety of filters that implement different image effects and execute on the GPU, making them a good choice for postprocessing effects.\n\nYou may also wish to look at the Metal Performance Shaders framework as an alternative. The Metal Performance Shaders framework offers a smaller set of image filters than Core Image, but they operate directly on Metal textures and can use an existing [doc:\/\/com.apple.documentation\/documentation\/Metal\/MTLComputeCommandEncoder]. That means they take less code to implement and are more efficient than Core Image filters because you don’t have to convert the [doc:\/\/com.apple.documentation\/documentation\/Metal\/MTLTexture] containing the rendered frame into a [doc:\/\/com.apple.documentation\/documentation\/CoreImage\/CIImage] before applying the filter.\n\nFor information on using Image Filters from the Metal Performance Shaders framework, see [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/using-metal-performance-shaders-to-create-custom-postprocess-effects]. If neither Core Image nor the Metal Performance Shaders framework provide the effect you need, you can also write custom compute functions to implement postprocess effects. For more information about postprocessing with compute functions, see [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/implementing-postprocess-effects-using-metal-compute-functions].\n\n### Set up the core image context\n\nIn order to apply a [doc:\/\/com.apple.documentation\/documentation\/CoreImage\/CIFilter-swift.class], you need to create a [doc:\/\/com.apple.documentation\/documentation\/CoreImage\/CIContext] during app launch and store it in a property so it’s available to your postprocess callback. You can use a single Core Image context regardless of the number or types of filters you use. The [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/ARView\/RenderCallbacks-swift.struct\/prepareWithDevice] render callback is a good place to create your context and do other setup work. RealityKit calls the [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/ARView\/RenderCallbacks-swift.struct\/prepareWithDevice] function once, after does its own setup, but before it renders the next frame.\n\nIf you set the callback at launch, RealityKit calls it before it draws the first frame. This is a good place to do setup tasks, especially processor-intensive or time-consuming tasks, such as loading images, or tasks that require access to the [doc:\/\/com.apple.documentation\/documentation\/Metal\/MTLDevice], which RealityKit passes to the callback function.\n\nTo create a [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/ARView\/RenderCallbacks-swift.struct\/prepareWithDevice] callback, write a function that takes a single [doc:\/\/com.apple.documentation\/documentation\/Metal\/MTLDevice] parameter and has no return value. In that function, create and store a [doc:\/\/com.apple.documentation\/documentation\/CoreImage\/CIContext] and do any other necessary setup tasks, such as loading textures needed by the filter.\n\nNext, register the function as a callback by assigning it to the [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/ARView\/RenderCallbacks-swift.struct\/prepareWithDevice] property of the view’s [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/ARView\/renderCallbacks-swift.property] property.\n\nTo make sure your setup code fires before rendering begins, assign the [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/ARView\/RenderCallbacks-swift.struct\/prepareWithDevice] callback in code that executes during app startup, such as in your main view controller’s [doc:\/\/com.apple.documentation\/documentation\/UIKit\/UIViewController\/viewWillAppear(_:)] method or the main SwiftUI view’s [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/UIViewRepresentable\/makeUIView(context:)] method. RealityKit does call a [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/ARView\/RenderCallbacks-swift.struct\/prepareWithDevice] callback if it’s registered after RealityKit starts rendering the scene, but doing setup tasks after rendering has started can cause a rendering hitch.\n\n### Check the output texture pixel format\n\nSome device GPUs require that the output texture be in a specific pixel format. If the device your code is running on doesn’t support [doc:\/\/com.apple.documentation\/documentation\/Metal\/MTLGPUFamily\/apple2], convert the output texture to the [doc:\/\/com.apple.documentation\/documentation\/Metal\/MTLPixelFormat\/bgra8Unorm] before using it. For more information, see [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/checking-the-pixel-format-of-a-postprocess-effect-s-output-texture].\n\n### Create a postprocess callback function\n\nNext, create the render callback function. Once you register it, RealityKit calls it every frame before displaying the rendered scene. In the callback, configure your [doc:\/\/com.apple.documentation\/documentation\/CoreImage\/CIFilter-swift.class], convert the source color texture — which contains the frame buffer — into a [doc:\/\/com.apple.documentation\/documentation\/CoreImage\/CIImage], and set it as the filter’s image input. Then, create a [doc:\/\/com.apple.documentation\/documentation\/CoreImage\/CIRenderDestination], and render the filter to it.\n\n### Register the callback function\n\nTo apply the effect, register the function as the [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/ARView\/RenderCallbacks-swift.struct\/postProcess] render callback for the [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/ARView].",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/RealityKit\/applying-core-image-filters-as-a-postprocess-effect\ncrawled: 2025-12-01T20:27:16Z\n---\n\n# Applying core image filters as a postprocess effect\n\n**Article**\n\nCreate special rendering effects for your RealityKit scenes using Core Image.\n\n## Overview\n\nIn iOS 15 and later, and macOS 12 and later, you can apply postprocess effects to a RealityKit scene after RealityKit renders it, but before RealityKit displays it. If you register a postprocess callback function, RealityKit passes that function the complete, rendered frame so you can modify it before the viewer sees it. You can use any image processing or drawing APIs on the rendered frame but, as a practical matter, only APIs that execute on the GPU are fast enough to use every frame and maintain a good framerate.\n\nOne option for implementing postprocess effects is to apply [doc:\/\/com.apple.documentation\/documentation\/CoreImage] filters to the rendered frame. Core Image provides a wide variety of filters that implement different image effects and execute on the GPU, making them a good choice for postprocessing effects.\n\nYou may also wish to look at the Metal Performance Shaders framework as an alternative. The Metal Performance Shaders framework offers a smaller set of image filters than Core Image, but they operate directly on Metal textures and can use an existing [doc:\/\/com.apple.documentation\/documentation\/Metal\/MTLComputeCommandEncoder]. That means they take less code to implement and are more efficient than Core Image filters because you don’t have to convert the [doc:\/\/com.apple.documentation\/documentation\/Metal\/MTLTexture] containing the rendered frame into a [doc:\/\/com.apple.documentation\/documentation\/CoreImage\/CIImage] before applying the filter.\n\nFor information on using Image Filters from the Metal Performance Shaders framework, see [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/using-metal-performance-shaders-to-create-custom-postprocess-effects]. If neither Core Image nor the Metal Performance Shaders framework provide the effect you need, you can also write custom compute functions to implement postprocess effects. For more information about postprocessing with compute functions, see [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/implementing-postprocess-effects-using-metal-compute-functions].\n\n### Set up the core image context\n\nIn order to apply a [doc:\/\/com.apple.documentation\/documentation\/CoreImage\/CIFilter-swift.class], you need to create a [doc:\/\/com.apple.documentation\/documentation\/CoreImage\/CIContext] during app launch and store it in a property so it’s available to your postprocess callback. You can use a single Core Image context regardless of the number or types of filters you use. The [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/ARView\/RenderCallbacks-swift.struct\/prepareWithDevice] render callback is a good place to create your context and do other setup work. RealityKit calls the [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/ARView\/RenderCallbacks-swift.struct\/prepareWithDevice] function once, after does its own setup, but before it renders the next frame.\n\nIf you set the callback at launch, RealityKit calls it before it draws the first frame. This is a good place to do setup tasks, especially processor-intensive or time-consuming tasks, such as loading images, or tasks that require access to the [doc:\/\/com.apple.documentation\/documentation\/Metal\/MTLDevice], which RealityKit passes to the callback function.\n\nTo create a [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/ARView\/RenderCallbacks-swift.struct\/prepareWithDevice] callback, write a function that takes a single [doc:\/\/com.apple.documentation\/documentation\/Metal\/MTLDevice] parameter and has no return value. In that function, create and store a [doc:\/\/com.apple.documentation\/documentation\/CoreImage\/CIContext] and do any other necessary setup tasks, such as loading textures needed by the filter.\n\n```swift\nfunc setupCoreImage(device: MTLDevice) {\n    \/\/ Create a CIContext and store it in a property.\n    ciContext = CIContext(mtlDevice: device)\n\n    \/\/ Do other expensive tasks, like loading images, here.\n}\n```\n\nNext, register the function as a callback by assigning it to the [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/ARView\/RenderCallbacks-swift.struct\/prepareWithDevice] property of the view’s [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/ARView\/renderCallbacks-swift.property] property.\n\n```swift\narView.renderCallbacks.prepareWithDevice = setupCoreImage\n```\n\nTo make sure your setup code fires before rendering begins, assign the [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/ARView\/RenderCallbacks-swift.struct\/prepareWithDevice] callback in code that executes during app startup, such as in your main view controller’s [doc:\/\/com.apple.documentation\/documentation\/UIKit\/UIViewController\/viewWillAppear(_:)] method or the main SwiftUI view’s [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/UIViewRepresentable\/makeUIView(context:)] method. RealityKit does call a [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/ARView\/RenderCallbacks-swift.struct\/prepareWithDevice] callback if it’s registered after RealityKit starts rendering the scene, but doing setup tasks after rendering has started can cause a rendering hitch.\n\n### Check the output texture pixel format\n\nSome device GPUs require that the output texture be in a specific pixel format. If the device your code is running on doesn’t support [doc:\/\/com.apple.documentation\/documentation\/Metal\/MTLGPUFamily\/apple2], convert the output texture to the [doc:\/\/com.apple.documentation\/documentation\/Metal\/MTLPixelFormat\/bgra8Unorm] before using it. For more information, see [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/checking-the-pixel-format-of-a-postprocess-effect-s-output-texture].\n\n### Create a postprocess callback function\n\nNext, create the render callback function. Once you register it, RealityKit calls it every frame before displaying the rendered scene. In the callback, configure your [doc:\/\/com.apple.documentation\/documentation\/CoreImage\/CIFilter-swift.class], convert the source color texture — which contains the frame buffer — into a [doc:\/\/com.apple.documentation\/documentation\/CoreImage\/CIImage], and set it as the filter’s image input. Then, create a [doc:\/\/com.apple.documentation\/documentation\/CoreImage\/CIRenderDestination], and render the filter to it.\n\n```swift\nfunc postProcessWithCoreImage(context: ARView.PostProcessContext) {\n\n    \/\/ Create and configure the Core Image filter.\n    let filter = CIFilter.falseColor()\n    filter.color0 = CIColor.blue\n    filter.color1 = CIColor.yellow\n\n    \/\/ Convert the frame buffer from a Metal texture to a CIImage, and \n    \/\/ set the CIImage as the filter's input image.\n    guard let input = CIImage(mtlTexture: context.sourceColorTexture) else {\n        fatalError(\"Unable to create a CIImage from sourceColorTexture.\")\n    }\n    filter.setValue(input, forKey: kCIInputImageKey)\n\n    \/\/ Get a reference to the filter's output image.\n    guard let output = filter.outputImage else {\n        fatalError(\"Error applying filter.\")\n    }\n\n    \/\/ Create a render destination and render the filter to the context's command buffer.\n    let destination = CIRenderDestination(mtlTexture: context.compatibleTargetTexture,\n                                          commandBuffer: context.commandBuffer)\n    destination.isFlipped = false\n    _ = try? self.ciContext.startTask(toRender: output, to: destination)\n}\n```\n\n\n\n### Register the callback function\n\nTo apply the effect, register the function as the [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/ARView\/RenderCallbacks-swift.struct\/postProcess] render callback for the [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/ARView].\n\n```swift\narView.renderCallbacks.postProcess = postProcessWithCoreImage\n```\n\n\n\n",
  "sections" : [

  ],
  "source" : "appleJSON",
  "title" : "Applying core image filters as a postprocess effect",
  "url" : "https:\/\/developer.apple.com\/documentation\/RealityKit\/applying-core-image-filters-as-a-postprocess-effect"
}