{
  "abstract" : "Use attachments to place 2D content relative to 3D content in your visionOS app.",
  "codeExamples" : [
    {
      "code" : "\/\/\/ Creates an entity from the data model for each Reality Composer Pro asset.\nfunc createEntity(for item: EntityData) async -> Entity {\n    \n    \/\/ Load the entity from Reality Composer Pro.\n    let realityComposerEntity = try! await Entity(named: item.title, in: realityKitContentBundle)\n    \n    \/\/ Find the model component entity and model component.\n    guard\n        let modelEntity = realityComposerEntity.findEntity(named: item.title),\n        var modelComponent = modelEntity.components[ModelComponent.self]\n    else {\n        return Entity()\n    }\n    \n    \/\/ Set the material if it has a simple material.\n    if let material = item.simpleMaterial {\n        modelComponent.materials = [material]\n    }\n    \n    \/\/ Set the model component.\n    modelEntity.components.set(modelComponent)\n    \n    return modelEntity\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Iterate over the attachments array and create the various arches.\nForEach(rainbowModel.archAttachments) { entity in\n    \/\/ Create an attachment with an ID that the `update` closure references.\n    Attachment(id: \"\\(entity.title.rawValue)ArchAttachmentEntity\") {\n        createArchAttachment(for: entity.title)\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ Creates the arch view for each attachment based on the color.\n@ViewBuilder func createArchAttachment(for arch: ArchAttachmentColor) -> some View {\n        switch arch {\n        case .blue:\n            SwiftUIArcView(color: .blue)\n        case .orange:\n            UIViewArcViewRep(color: .orange)\n        case .pink:\n            SwiftUIArcView(color: .pink)\n        case .red:\n            CALayerArcViewRep(color: .red)\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Add and configure attachments.\nfor viewAttachmentEntity in rainbowModel.archAttachments {\n    \n    \/\/ Check whether there's an attachment.\n    if let attachment = attachments.entity(for: \"\\(viewAttachmentEntity.title)ArchAttachmentEntity\") {\n        \n        attachment.name = viewAttachmentEntity.title.rawValue\n        \n        \/\/ Add it as a subentity of the plane.\n        plane?.addChild(attachment)\n        \n        \/\/ Set the scale and position.\n        attachment.scale = viewAttachmentEntity.scale\n        attachment.setPosition(viewAttachmentEntity.position, relativeTo: yellowArch)\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ Updates the array containing the scale and position for each attachment entity.\nfunc scaleAndPositionArches(yellowArchSize: BoundingBox) {\n    \/\/ MARK: - Scaling properties\n    \n    \/\/ Set the x scale to be the same as the yellow arch.\n    \/\/ Set the y scale to be double the yellow arch to account for the larger frame due to the SwiftUI view.\n    var archScale = SIMD3(x: yellowArchSize.extents.x, y: yellowArchSize.max.y * 2, z: 1)\n    \n    \/\/ MARK: - Positioning properties\n    \n    \/\/ Set the y position to be the same as the yellow arch.\n    let yPosition = yellowArchSize.min.y\n    \n    \/\/ Set the z position to be 0.1 meters back.\n    var zPosition: Float = -0.1\n    var position = SIMD3(x: 0, y: yPosition, z: zPosition)\n    \n    for (index, attachment) in rainbowModel.archAttachments.enumerated() {\n        \n        \/\/ Push the arch back by 0.1 meters.\n        zPosition -= 0.1\n        position.z = zPosition\n        \n        \/\/ Update the attachments in the view attachment array to include position and scale.\n        rainbowModel.archAttachments[index] = ArchAttachment(title: attachment.title, position: position, scale: archScale)\n        \n        \/\/ Scale the next attachment to be 75% of the size of the previous arch.\n        archScale *= 3 \/ 4\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ Sets the components necessary for hover and tap gestures.\nfunc configureForTapGesture(entity: Entity) async {\n    \/\/ Set the hover effect component.\n    entity.components.set(HoverEffectComponent())\n    \n    \/\/ Find the `ModelComponent` to get the mesh and create a static mesh in the shape of the entity.\n    guard let modelComponent = entity.components[ModelComponent.self] else { return }\n    let entityMesh = modelComponent.mesh\n    let shapeResource = try! await ShapeResource.generateStaticMesh(from: entityMesh)\n    entity.components.set(CollisionComponent(shapes: [shapeResource]))\n    \n    \/\/ Set the input target component.\n    entity.components.set(InputTargetComponent())\n}",
      "language" : "swift"
    },
    {
      "code" : ".simultaneousGesture(\n    SpatialTapGesture()\n        .targetedToAnyEntity()\n        .onEnded { value in\n            \/\/ Convert the tap location to the scene's coordinate space.\n            var location3D = value.convert(value.location3D, from: .local, to: .scene)\n            \/\/ Move the z index forward to ensure it doesn't overlap with the entity.\n            location3D.z += 0.02\n            \n            \/\/ You don't need to set the position of attachments on entities relative to the root entity, so pass `nil` here.\n            \/\/ The system handles this with the location conversion.\n            rainbowModel.tapAttachments.append(CloudTapAttachment(position: location3D, parent: nil))\n        }\n)",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Iterate over the tap attachments and provide content for each.\nForEach(rainbowModel.tapAttachments) { cloud in\n    Attachment(id: cloud.position) {\n        Image(systemName: \"cloud.fill\")\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "for cloud in rainbowModel.tapAttachments {\n    if let cloudEntity = attachments.entity(for: cloud.position) {\n        \/\/ Scale the attachment larger and add it.\n        cloudEntity.scale = [5, 5, 5]\n        cloudEntity.name = \"\\(cloud.position)tapEntity\"\n        root.addChild(cloudEntity)\n        \n        \/\/ Set the position of the attachment.\n        cloudEntity.setPosition(cloud.position, relativeTo: cloud.parent)\n    }\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "3b5db49fba342e0755883452719f18d5c6af08682c4d508a2572c4f5eb3de11a",
  "crawledAt" : "2025-12-02T19:56:13Z",
  "id" : "C9690BC9-7DFE-4A29-97F2-1C33101BF170",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "RealityKit",
  "overview" : "## Overview\n\nTo demonstrate how you can integrate any 2D content into your 3D app, this sample code project uses a variety of frameworks to create both 2D and 3D views, and places them relative to each other in an immersive space. It also illustrates how to position an attachment at the location of a tap gesture.\n\nThe rainbow that appears in the sample app contains two USDZ models and four [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/RealityViewAttachments].\n\nThe app loads the 3D assets from Reality Composer Pro as a [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/ModelEntity] in a [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/RealityView], and creates a reality view attachment for each of the 2D arches to attach them to the view.\n\nThe cloud attachments at the locations of tap gestures are `RealityViewAttachments` containing [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/Text] with an SF Symbols image.\n\n\n\n### Load and configure entities from Reality Composer Pro\n\nThis sample creates 3D assets in an asset creator and imports them into Reality Composer Pro as `.usdc` files. See doc:\/\/com.apple.documentation\/documentation\/visionos\/designing-realitykit-content-with-reality-composer-pro for more information.\n\nThe app then configures the appearance of the [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/ModelEntity] by setting the material of the [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/ModelComponent], which is the [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/Component] that affects appearance. The following code example demonstrates loading a model and configuring the material:\n\n### Create attachments that contain SwiftUI views\n\nThe sample includes the remaining four arches as reality view attachments by creating attachments of various types in the `attachments` closure of a reality view instance. These types include both SwiftUI and UIKit to exemplify how to use any framework in your visionOS app.\n\nAttachments can contain views from other frameworks that adopt the `UIViewRepresentable` protocol.\n\n### Add and position entity attachments\n\nThe sample loads the attachments as reality view attachments in the `update` closure of the reality view. If there’s an existing attachment for an `id`, the sample adds the attachment entity as a subentity of the plane entity to display it in the scene, and then configures the scale and position.\n\nThis method sets the scales and positions for each attachment by using the yellow arch’s bounding box. This ensures each arch is smaller and further back than the previous. The app applies these scale and position values to each entity in the `update` closure as the code example below shows:\n\n### Position attachments at the tapped location\n\nFollow these steps to add attachments to RealityKit entities and position them at the tapped location. Ensure that your entities have both an [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/InputTargetComponent] and a [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/CollisionComponent].\n\nAdd a [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/SpatialTapGesture] to the `RealityView` and make sure it uses [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/Gesture\/targetedToAnyEntity()], or specify which entities to target with [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/Gesture\/targetedToEntity(_:)]. Then use [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/RealityCoordinateSpaceConverting\/convert(_:from:to:)-6uv65] to convert the location of the tap gesture from the local coordinate space of the entity to the scene’s coordinate space.\n\nCreate the attachment in the `attachments` closure by iterating over the array of attachments.\n\nFinally, add each attachment in the `update` closure by iterating over the array of attachments and setting their stored position and root entity.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/realitykit\/combining-2d-and-3d-views-in-an-immersive-app\ncrawled: 2025-12-02T19:56:13Z\n---\n\n# Combining 2D and 3D views in an immersive app\n\n**Sample Code**\n\nUse attachments to place 2D content relative to 3D content in your visionOS app.\n\n## Overview\n\nTo demonstrate how you can integrate any 2D content into your 3D app, this sample code project uses a variety of frameworks to create both 2D and 3D views, and places them relative to each other in an immersive space. It also illustrates how to position an attachment at the location of a tap gesture.\n\nThe rainbow that appears in the sample app contains two USDZ models and four [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/RealityViewAttachments].\n\n- The green arch is a USDZ file from Reality Composer Pro with a custom shader graph material.\n- The yellow arch is a USDZ file from Reality Composer Pro with a programmatically created simple material.\n- The orange arch is a reality view attachment containing a [doc:\/\/com.apple.documentation\/documentation\/UIKit\/UIView] in a [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/UIViewRepresentable] with a custom 2D arc shape.\n- The red arch is a reality view attachment containing a `UIView` in a `UIViewRepresentable` with a custom 2D arc shape.\n- The pink arch is a reality view attachment containing a SwiftUI [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/View] with a custom 2D arc shape.\n- The blue arch is a reality view attachment containing a SwiftUI `View` with a custom 2D arc shape.\n\nThe app loads the 3D assets from Reality Composer Pro as a [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/ModelEntity] in a [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/RealityView], and creates a reality view attachment for each of the 2D arches to attach them to the view.\n\nThe cloud attachments at the locations of tap gestures are `RealityViewAttachments` containing [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/Text] with an SF Symbols image.\n\n\n\n### Load and configure entities from Reality Composer Pro\n\nThis sample creates 3D assets in an asset creator and imports them into Reality Composer Pro as `.usdc` files. See doc:\/\/com.apple.documentation\/documentation\/visionos\/designing-realitykit-content-with-reality-composer-pro for more information.\n\nThe app then configures the appearance of the [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/ModelEntity] by setting the material of the [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/ModelComponent], which is the [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/Component] that affects appearance. The following code example demonstrates loading a model and configuring the material:\n\n```swift\n\/\/\/ Creates an entity from the data model for each Reality Composer Pro asset.\nfunc createEntity(for item: EntityData) async -> Entity {\n    \n    \/\/ Load the entity from Reality Composer Pro.\n    let realityComposerEntity = try! await Entity(named: item.title, in: realityKitContentBundle)\n    \n    \/\/ Find the model component entity and model component.\n    guard\n        let modelEntity = realityComposerEntity.findEntity(named: item.title),\n        var modelComponent = modelEntity.components[ModelComponent.self]\n    else {\n        return Entity()\n    }\n    \n    \/\/ Set the material if it has a simple material.\n    if let material = item.simpleMaterial {\n        modelComponent.materials = [material]\n    }\n    \n    \/\/ Set the model component.\n    modelEntity.components.set(modelComponent)\n    \n    return modelEntity\n}\n```\n\n### Create attachments that contain SwiftUI views\n\nThe sample includes the remaining four arches as reality view attachments by creating attachments of various types in the `attachments` closure of a reality view instance. These types include both SwiftUI and UIKit to exemplify how to use any framework in your visionOS app.\n\n```swift\n\/\/ Iterate over the attachments array and create the various arches.\nForEach(rainbowModel.archAttachments) { entity in\n    \/\/ Create an attachment with an ID that the `update` closure references.\n    Attachment(id: \"\\(entity.title.rawValue)ArchAttachmentEntity\") {\n        createArchAttachment(for: entity.title)\n    }\n}\n```\n\n```swift\n\/\/\/ Creates the arch view for each attachment based on the color.\n@ViewBuilder func createArchAttachment(for arch: ArchAttachmentColor) -> some View {\n        switch arch {\n        case .blue:\n            SwiftUIArcView(color: .blue)\n        case .orange:\n            UIViewArcViewRep(color: .orange)\n        case .pink:\n            SwiftUIArcView(color: .pink)\n        case .red:\n            CALayerArcViewRep(color: .red)\n    }\n}\n```\n\nAttachments can contain views from other frameworks that adopt the `UIViewRepresentable` protocol.\n\n### Add and position entity attachments\n\nThe sample loads the attachments as reality view attachments in the `update` closure of the reality view. If there’s an existing attachment for an `id`, the sample adds the attachment entity as a subentity of the plane entity to display it in the scene, and then configures the scale and position.\n\n```swift\n\/\/ Add and configure attachments.\nfor viewAttachmentEntity in rainbowModel.archAttachments {\n    \n    \/\/ Check whether there's an attachment.\n    if let attachment = attachments.entity(for: \"\\(viewAttachmentEntity.title)ArchAttachmentEntity\") {\n        \n        attachment.name = viewAttachmentEntity.title.rawValue\n        \n        \/\/ Add it as a subentity of the plane.\n        plane?.addChild(attachment)\n        \n        \/\/ Set the scale and position.\n        attachment.scale = viewAttachmentEntity.scale\n        attachment.setPosition(viewAttachmentEntity.position, relativeTo: yellowArch)\n    }\n}\n```\n\nThis method sets the scales and positions for each attachment by using the yellow arch’s bounding box. This ensures each arch is smaller and further back than the previous. The app applies these scale and position values to each entity in the `update` closure as the code example below shows:\n\n```swift\n\/\/\/ Updates the array containing the scale and position for each attachment entity.\nfunc scaleAndPositionArches(yellowArchSize: BoundingBox) {\n    \/\/ MARK: - Scaling properties\n    \n    \/\/ Set the x scale to be the same as the yellow arch.\n    \/\/ Set the y scale to be double the yellow arch to account for the larger frame due to the SwiftUI view.\n    var archScale = SIMD3(x: yellowArchSize.extents.x, y: yellowArchSize.max.y * 2, z: 1)\n    \n    \/\/ MARK: - Positioning properties\n    \n    \/\/ Set the y position to be the same as the yellow arch.\n    let yPosition = yellowArchSize.min.y\n    \n    \/\/ Set the z position to be 0.1 meters back.\n    var zPosition: Float = -0.1\n    var position = SIMD3(x: 0, y: yPosition, z: zPosition)\n    \n    for (index, attachment) in rainbowModel.archAttachments.enumerated() {\n        \n        \/\/ Push the arch back by 0.1 meters.\n        zPosition -= 0.1\n        position.z = zPosition\n        \n        \/\/ Update the attachments in the view attachment array to include position and scale.\n        rainbowModel.archAttachments[index] = ArchAttachment(title: attachment.title, position: position, scale: archScale)\n        \n        \/\/ Scale the next attachment to be 75% of the size of the previous arch.\n        archScale *= 3 \/ 4\n    }\n}\n```\n\n### Position attachments at the tapped location\n\nFollow these steps to add attachments to RealityKit entities and position them at the tapped location. Ensure that your entities have both an [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/InputTargetComponent] and a [doc:\/\/com.apple.RealityKit\/documentation\/RealityKit\/CollisionComponent].\n\n```swift\n\/\/\/ Sets the components necessary for hover and tap gestures.\nfunc configureForTapGesture(entity: Entity) async {\n    \/\/ Set the hover effect component.\n    entity.components.set(HoverEffectComponent())\n    \n    \/\/ Find the `ModelComponent` to get the mesh and create a static mesh in the shape of the entity.\n    guard let modelComponent = entity.components[ModelComponent.self] else { return }\n    let entityMesh = modelComponent.mesh\n    let shapeResource = try! await ShapeResource.generateStaticMesh(from: entityMesh)\n    entity.components.set(CollisionComponent(shapes: [shapeResource]))\n    \n    \/\/ Set the input target component.\n    entity.components.set(InputTargetComponent())\n}\n```\n\nAdd a [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/SpatialTapGesture] to the `RealityView` and make sure it uses [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/Gesture\/targetedToAnyEntity()], or specify which entities to target with [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/Gesture\/targetedToEntity(_:)]. Then use [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/RealityCoordinateSpaceConverting\/convert(_:from:to:)-6uv65] to convert the location of the tap gesture from the local coordinate space of the entity to the scene’s coordinate space.\n\n```swift\n.simultaneousGesture(\n    SpatialTapGesture()\n        .targetedToAnyEntity()\n        .onEnded { value in\n            \/\/ Convert the tap location to the scene's coordinate space.\n            var location3D = value.convert(value.location3D, from: .local, to: .scene)\n            \/\/ Move the z index forward to ensure it doesn't overlap with the entity.\n            location3D.z += 0.02\n            \n            \/\/ You don't need to set the position of attachments on entities relative to the root entity, so pass `nil` here.\n            \/\/ The system handles this with the location conversion.\n            rainbowModel.tapAttachments.append(CloudTapAttachment(position: location3D, parent: nil))\n        }\n)\n```\n\nCreate the attachment in the `attachments` closure by iterating over the array of attachments.\n\n```swift\n\/\/ Iterate over the tap attachments and provide content for each.\nForEach(rainbowModel.tapAttachments) { cloud in\n    Attachment(id: cloud.position) {\n        Image(systemName: \"cloud.fill\")\n    }\n}\n```\n\nFinally, add each attachment in the `update` closure by iterating over the array of attachments and setting their stored position and root entity.\n\n```swift\nfor cloud in rainbowModel.tapAttachments {\n    if let cloudEntity = attachments.entity(for: cloud.position) {\n        \/\/ Scale the attachment larger and add it.\n        cloudEntity.scale = [5, 5, 5]\n        cloudEntity.name = \"\\(cloud.position)tapEntity\"\n        root.addChild(cloudEntity)\n        \n        \/\/ Set the position of the attachment.\n        cloudEntity.setPosition(cloud.position, relativeTo: cloud.parent)\n    }\n}\n```\n\n## Scene content\n\n- **Hello World**: Use windows, volumes, and immersive spaces to teach people about the Earth.\n- **Enabling video reflections in an immersive environment**: Create a more immersive experience by adding video reflections in a custom environment.\n- **Creating a spatial drawing app with RealityKit**: Use low-level mesh and texture APIs to achieve fast updates to a person’s brush strokes by integrating RealityKit with ARKit and SwiftUI.\n- **Generating interactive geometry with RealityKit**: Create an interactive mesh with low-level mesh and low-level texture.\n- **Transforming RealityKit entities using gestures**: Build a RealityKit component to support standard visionOS gestures on any entity.\n- **Responding to gestures on an entity**: Respond to gestures performed on RealityKit entities using input target and collision components.\n- **Models and meshes**: Display virtual objects in your scene with mesh-based models.\n- **Materials, textures, and shaders**: Apply textures to the surface of your scene’s 3D objects to give each object a unique appearance.\n- **Anchors**: Lock virtual content to the real world.\n- **Lights and cameras**: Control the lighting and point of view for a scene.\n- **Content synchronization**: Synchronize the contents of entities locally or across the network.\n- **Audio**: Create personalized and realistic spatial audio experiences.\n- **Videos**: Present videos in your RealityKit experiences.\n- **Images**: Present images and spatial scenes in your RealityKit experiences.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Use windows, volumes, and immersive spaces to teach people about the Earth.",
          "name" : "Hello World",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/World"
        },
        {
          "description" : "Create a more immersive experience by adding video reflections in a custom environment.",
          "name" : "Enabling video reflections in an immersive environment",
          "url" : "https:\/\/developer.apple.com\/documentation\/visionOS\/enabling-video-reflections-in-an-immersive-environment"
        },
        {
          "description" : "Use low-level mesh and texture APIs to achieve fast updates to a person’s brush strokes by integrating RealityKit with ARKit and SwiftUI.",
          "name" : "Creating a spatial drawing app with RealityKit",
          "url" : "https:\/\/developer.apple.com\/documentation\/RealityKit\/creating-a-spatial-drawing-app-with-realitykit"
        },
        {
          "description" : "Create an interactive mesh with low-level mesh and low-level texture.",
          "name" : "Generating interactive geometry with RealityKit",
          "url" : "https:\/\/developer.apple.com\/documentation\/RealityKit\/generating-interactive-geometry-with-realitykit"
        },
        {
          "description" : "Build a RealityKit component to support standard visionOS gestures on any entity.",
          "name" : "Transforming RealityKit entities using gestures",
          "url" : "https:\/\/developer.apple.com\/documentation\/RealityKit\/transforming-realitykit-entities-with-gestures"
        },
        {
          "description" : "Respond to gestures performed on RealityKit entities using input target and collision components.",
          "name" : "Responding to gestures on an entity",
          "url" : "https:\/\/developer.apple.com\/documentation\/RealityKit\/responding-to-gestures-on-an-entity"
        },
        {
          "description" : "Display virtual objects in your scene with mesh-based models.",
          "name" : "Models and meshes",
          "url" : "https:\/\/developer.apple.com\/documentation\/RealityKit\/scene-content-models-and-meshes"
        },
        {
          "description" : "Apply textures to the surface of your scene’s 3D objects to give each object a unique appearance.",
          "name" : "Materials, textures, and shaders",
          "url" : "https:\/\/developer.apple.com\/documentation\/RealityKit\/scene-content-materials-and-shaders"
        },
        {
          "description" : "Lock virtual content to the real world.",
          "name" : "Anchors",
          "url" : "https:\/\/developer.apple.com\/documentation\/RealityKit\/scene-content-anchors"
        },
        {
          "description" : "Control the lighting and point of view for a scene.",
          "name" : "Lights and cameras",
          "url" : "https:\/\/developer.apple.com\/documentation\/RealityKit\/scene-content-lights-and-cameras"
        },
        {
          "description" : "Synchronize the contents of entities locally or across the network.",
          "name" : "Content synchronization",
          "url" : "https:\/\/developer.apple.com\/documentation\/RealityKit\/scene-content-content-synchronization"
        },
        {
          "description" : "Create personalized and realistic spatial audio experiences.",
          "name" : "Audio",
          "url" : "https:\/\/developer.apple.com\/documentation\/RealityKit\/scene-content-audio"
        },
        {
          "description" : "Present videos in your RealityKit experiences.",
          "name" : "Videos",
          "url" : "https:\/\/developer.apple.com\/documentation\/RealityKit\/scene-content-videos"
        },
        {
          "description" : "Present images and spatial scenes in your RealityKit experiences.",
          "name" : "Images",
          "url" : "https:\/\/developer.apple.com\/documentation\/RealityKit\/scene-content-images"
        }
      ],
      "title" : "Scene content"
    }
  ],
  "source" : "appleJSON",
  "title" : "Combining 2D and 3D views in an immersive app",
  "url" : "https:\/\/developer.apple.com\/documentation\/realitykit\/combining-2d-and-3d-views-in-an-immersive-app"
}