{
  "abstract" : "Place virtual content at tracked, real-world locations, and enable the user to interact with virtual content by using gestures.",
  "codeExamples" : [
    {
      "code" : "func setGoal() {\n    coachingOverlay.goal = .horizontalPlane\n}",
      "language" : "swift"
    },
    {
      "code" : "func setActivatesAutomatically() {\n    coachingOverlay.activatesAutomatically = true\n}",
      "language" : "swift"
    },
    {
      "code" : "func coachingOverlayViewWillActivate(_ coachingOverlayView: ARCoachingOverlayView) {\n    upperControlsView.isHidden = true\n}",
      "language" : "swift"
    },
    {
      "code" : "func coachingOverlayViewDidDeactivate(_ coachingOverlayView: ARCoachingOverlayView) {\n    upperControlsView.isHidden = false\n}",
      "language" : "swift"
    },
    {
      "code" : "func getRaycastQuery(for alignment: ARRaycastQuery.TargetAlignment = .any) -> ARRaycastQuery? {\n    return raycastQuery(from: screenCenter, allowing: .estimatedPlane, alignment: alignment)\n}",
      "language" : "swift"
    },
    {
      "code" : "func castRay(for query: ARRaycastQuery) -> [ARRaycastResult] {\n    return session.raycast(query)\n}",
      "language" : "swift"
    },
    {
      "code" : "func setPosition(with raycastResult: ARRaycastResult, _ camera: ARCamera?) {\n    let position = raycastResult.worldTransform.translation\n    recentFocusSquarePositions.append(position)\n    updateTransform(for: raycastResult, camera: camera)\n}",
      "language" : "swift"
    },
    {
      "code" : "func updateOrientation(basedOn raycastResult: ARRaycastResult) {\n    self.simdOrientation = raycastResult.worldTransform.orientation\n}",
      "language" : "swift"
    },
    {
      "code" : "func placeVirtualObject(_ virtualObject: VirtualObject) {\n    guard focusSquare.state != .initializing, let query = virtualObject.raycastQuery else {\n        self.statusViewController.showMessage(\"CANNOT PLACE OBJECT\\nTry moving left or right.\")\n        if let controller = self.objectsViewController {\n            self.virtualObjectSelectionViewController(controller, didDeselectObject: virtualObject)\n        }\n        return\n    }\n   \n    let trackedRaycast = createTrackedRaycastAndSet3DPosition(of: virtualObject, from: query,\n                                                              withInitialResult: virtualObject.mostRecentInitialPlacementResult)\n    \n    virtualObject.raycast = trackedRaycast\n    virtualObjectInteraction.selectedObject = virtualObject\n    virtualObject.isHidden = false\n}",
      "language" : "swift"
    },
    {
      "code" : "func createTrackedRaycastAndSet3DPosition(of virtualObject: VirtualObject, from query: ARRaycastQuery,\n                                          withInitialResult initialResult: ARRaycastResult? = nil) -> ARTrackedRaycast? {\n    if let initialResult = initialResult {\n        self.setTransform(of: virtualObject, with: initialResult)\n    }\n    \n    return session.trackedRaycast(query) { (results) in\n        self.setVirtualObject3DPosition(results, with: virtualObject)\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "private func setVirtualObject3DPosition(_ results: [ARRaycastResult], with virtualObject: VirtualObject) {\n    \n    guard let result = results.first else {\n        fatalError(\"Unexpected case: the update handler is always supposed to return at least one result.\")\n    }\n    \n    self.setTransform(of: virtualObject, with: result)\n    \n    \/\/ If the virtual object is not yet in the scene, add it.\n    if virtualObject.parent == nil {\n        self.sceneView.scene.rootNode.addChildNode(virtualObject)\n        virtualObject.shouldUpdateAnchor = true\n    }\n    \n    if virtualObject.shouldUpdateAnchor {\n        virtualObject.shouldUpdateAnchor = false\n        self.updateQueue.async {\n            self.sceneView.addOrUpdateAnchor(for: virtualObject)\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "func removeVirtualObject(at index: Int) {\n    guard loadedObjects.indices.contains(index) else { return }\n    \n    \/\/ Stop the object's tracked ray cast.\n    loadedObjects[index].stopTrackedRaycast()\n    \n    \/\/ Remove the visual node from the scene graph.\n    loadedObjects[index].removeFromParentNode()\n    \/\/ Recoup resources allocated by the object.\n    loadedObjects[index].unload()\n    loadedObjects.remove(at: index)\n}",
      "language" : "swift"
    },
    {
      "code" : "func stopTrackedRaycast() {\n    raycast?.stopTracking()\n    raycast = nil\n}",
      "language" : "swift"
    },
    {
      "code" : "func createPanGestureRecognizer(_ sceneView: VirtualObjectARView) {\n    let panGesture = ThresholdPanGesture(target: self, action: #selector(didPan(_:)))\n    panGesture.delegate = self\n    sceneView.addGestureRecognizer(panGesture)\n}",
      "language" : "swift"
    },
    {
      "code" : "func translate(_ object: VirtualObject, basedOn screenPos: CGPoint) {\n    object.stopTrackedRaycast()\n    \n    \/\/ Update the object by using a one-time position request.\n    if let query = sceneView.raycastQuery(from: screenPos, allowing: .estimatedPlane, alignment: object.allowedAlignment) {\n        viewController.createRaycastAndUpdate3DPosition(of: object, from: query)\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "@objc\nfunc didRotate(_ gesture: UIRotationGestureRecognizer) {\n    guard gesture.state == .changed else { return }\n    \n    trackedObject?.objectRotation -= Float(gesture.rotation)\n    \n    gesture.rotation = 0\n}",
      "language" : "swift"
    },
    {
      "code" : "func hideVirtualContent() {\n    virtualObjectLoader.loadedObjects.forEach { $0.isHidden = true }\n}",
      "language" : "swift"
    },
    {
      "code" : "func session(_ session: ARSession, cameraDidChangeTrackingState camera: ARCamera) {\n    statusViewController.showTrackingQualityInfo(for: camera.trackingState, autoHide: true)\n    switch camera.trackingState {\n    case .notAvailable, .limited:\n        statusViewController.escalateFeedback(for: camera.trackingState, inSeconds: 3.0)\n    case .normal:\n        statusViewController.cancelScheduledMessage(for: .trackingStateEscalation)\n        showVirtualContent()\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "func sessionShouldAttemptRelocalization(_ session: ARSession) -> Bool {\n    return true\n}",
      "language" : "swift"
    },
    {
      "code" : "func coachingOverlayViewWillActivate(_ coachingOverlayView: ARCoachingOverlayView) {\n    upperControlsView.isHidden = true\n}",
      "language" : "swift"
    },
    {
      "code" : "func coachingOverlayViewDidDeactivate(_ coachingOverlayView: ARCoachingOverlayView) {\n    upperControlsView.isHidden = false\n}",
      "language" : "swift"
    },
    {
      "code" : "func coachingOverlayViewDidRequestSessionReset(_ coachingOverlayView: ARCoachingOverlayView) {\n    restartExperience()\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "0f25c0d2392c2aa054967c83f6f048848794a6f700d867e15aeafd4d564a051e",
  "crawledAt" : "2025-12-02T15:29:20Z",
  "id" : "A2B8DA49-A440-4A5D-8ADB-8A9770C831E9",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "ARKit",
  "overview" : "## Overview\n\nThe key facet of an AR experience is the ability to intermix virtual and real-world objects. A flat surface is the optimum location for setting a virtual object. To assist ARKit with finding surfaces, you tell the user to move their device in ways that help ARKit prepare the experience. ARKit provides a view that tailors its instructions to the user, guiding them to the surface that your app needs.\n\nTo enable the user to put a virtual item on the real-world surface when they tap the screen, ARKit incorporates ray casting, which provides a 3D location in physical space that corresponds to the screen’s touch location. When the user rotates or otherwise moves the virtual items they place, you respond to the respective touch gestures and correlate that input to the virtual content’s look in the physical environment.\n\n## Set a goal to coach the user’s movement\n\nTo enable your app to detect real-world surfaces, you use a world tracking configuration. For ARKit to establish tracking, the user must physically move their device to allow ARKit to get a sense of perspective. To communicate this need to the user, you use a view provided by ARKit that presents the user with instructional diagrams and verbal guidance, called [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARCoachingOverlayView] For example, when you start the app, the first thing the user sees is a message and animation from the coaching overlay telling them to move their device left and right, repeatedly, in order to get started.\n\nTo enable the user to place virtual content on a horizontal surface, you set the coaching overlay goal accordingly.\n\nThe coaching overlay then tailors its instructions according to the goal you choose. After ARKit gets a sense of perspective, the coaching overlay instructs the user to find a surface.\n\n## Respond to coaching events\n\nTo make sure the coaching overlay provides guidance to the user whenever ARKit determines it’s necessary, you set [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARCoachingOverlayView\/activatesAutomatically] to `true`.\n\nThe coaching overlay activates automatically when the app starts, or when tracking degrades past a certain threshold. In those situations, ARKit notifies your delegate by calling [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARCoachingOverlayViewDelegate\/coachingOverlayViewWillActivate(_:)]. In response to this event, hide your app’s UI to enable the user to focus on the instructions that the coaching overlay provides.\n\nWhen the coaching overlay determines that the goal has been met, it disappears from the user’s view. ARKit notifies your delegate that the coaching process has ended, which is when you show your app’s main user interface.\n\n## Place virtual content\n\nTo give the user an idea of where they can place virtual content, annotate the environment to give them a preview. The sample app draws a square that gives the user visual confirmation of the shape and alignment of the surfaces that ARKit is aware of.\n\nTo figure out where to put the square in the real world, you use an [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARRaycastQuery] to ask ARKit where any surfaces exist in the real world. First, you create a ray-cast query that defines the 2D point on the screen you’re interested in. Because the focus square is aligned with the center of the screen, you create a query for the screen center.\n\nThen, you execute the ray-cast query by asking the session to cast it.\n\nARKit returns a position in the `results` parameter that includes the depth of where that point lies on a surface in the real world. To give the user a preview of where on the real-world surface a user can place their virtual content, update the focus square’s position using the ray-cast result’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARRaycastResult\/worldTransform]:\n\nThe ray-cast result also indicates how the surface is angled with respect to gravity. To preview the angle at which the user’s virtual content can be placed on the surface, update the focus square’s [doc:\/\/com.apple.documentation\/documentation\/SceneKit\/SCNNode\/simdWorldTransform] with the result’s orientation.\n\nIf your app offers different types of virtual content, give the user an interface to choose from. The sample app exposes a selection menu when the user taps the plus button. When the user chooses an item from the list, you instantiate the corresponding 3D model and anchor it in the world at the focus square’s current position.\n\n## Refine the position of virtual content over time\n\nAs the session runs, ARKit analyzes each camera image and learns more about the layout of the physical environment. When ARKit updates its estimated size and position of real-world surfaces, you may need to update the position of your app’s virtual content to match. To help make it easy, ARKit notifies you when it corrects its understanding of the scene by way of an [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARTrackedRaycast].\n\nARKit successively repeats the query you provide to a tracked ray cast, and it calls the closure you provide only when the results differ from prior results. The code you provide in the closure is your response to ARKit’s updated scene understanding. In this case, you check your ray-cast intersections against the updated planes and apply those positions to your app’s virtual content.\n\n## Manage tracked ray casts\n\nBecause ARKit continues to call them, tracked ray casts can increasingly consume resources as the user places more virtual content. Stop the tracked ray cast when you no longer need refined positions over time, such as when a virtual balloon takes flight, or when you remove a virtual object from your scene.\n\nTo stop a tracked ray cast, you call its [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARTrackedRaycast\/stopTracking()] function:\n\n## Enable user interaction with virtual content\n\nTo allow users to move virtual content in the world after they’ve placed it, implement a pan gesture recognizer.\n\nWhen the user pans an object, you request its position along the object’s path across the plane. Because the object’s position is transitory, use a [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSession\/raycast(_:)] instead of using a tracked ray cast. In this case, a one-time hit test is appropriate because you don’t need refined position results over time for these requests.\n\nRay casting gives you orientation information about the surface at a given screen point. While dragging, you avoid quick changes in orientation by subtracting the gesture’s rotation from the current object rotation.\n\n## Handle interruption in tracking\n\nIn cases where tracking conditions are poor, ARKit invokes your delegate’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSessionObserver\/sessionWasInterrupted(_:)]. In these circumstances, the positions of your app’s virtual content may be inaccurate with respect to the camera feed, so hide your virtual content.\n\nRestore your app’s virtual content when tracking conditions improve. To notify you of improved conditions, ARKit calls your delegate’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSessionObserver\/session(_:cameraDidChangeTrackingState:)]function, passing in a camera [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARCamera\/trackingState-6i3pt] equal to [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARTrackingState\/ARTrackingStateNormal].\n\n## Restore an interrupted AR experience\n\nWhen a session is interrupted, ARKit asks if you want to try to restore the AR experience. You do that by opting in to *relocalization*, by overriding [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSessionObserver\/sessionShouldAttemptRelocalization(_:)] and returning `true`.\n\nDuring relocalization, the coaching overlay displays tailored instructions to the user. To allow the user to focus on the coaching process, hide your app’s UI when coaching is enabled.\n\nWhen ARKit succeeds in restoring the experience, show your app’s UI again so everything appears the way it was before the interruption. When the coaching overlay disappears from the user’s view, ARKit invokes your [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARCoachingOverlayViewDelegate\/coachingOverlayViewDidDeactivate(_:)] callback, which is where you restore your app’s UI.\n\n## Enable the user to start over rather than restore\n\nIf the user decides to give up on restoring the session, you restart the experience in your delegate’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARCoachingOverlayViewDelegate\/coachingOverlayViewDidRequestSessionReset(_:)] function. ARKit invokes this callback when the user taps the coaching overlay’s Start Over button.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/ARKit\/placing-objects-and-handling-3d-interaction\ncrawled: 2025-12-02T15:29:20Z\n---\n\n# Placing objects and handling 3D interaction\n\n**Sample Code**\n\nPlace virtual content at tracked, real-world locations, and enable the user to interact with virtual content by using gestures.\n\n## Overview\n\nThe key facet of an AR experience is the ability to intermix virtual and real-world objects. A flat surface is the optimum location for setting a virtual object. To assist ARKit with finding surfaces, you tell the user to move their device in ways that help ARKit prepare the experience. ARKit provides a view that tailors its instructions to the user, guiding them to the surface that your app needs.\n\nTo enable the user to put a virtual item on the real-world surface when they tap the screen, ARKit incorporates ray casting, which provides a 3D location in physical space that corresponds to the screen’s touch location. When the user rotates or otherwise moves the virtual items they place, you respond to the respective touch gestures and correlate that input to the virtual content’s look in the physical environment.\n\n\n\n## Set a goal to coach the user’s movement\n\nTo enable your app to detect real-world surfaces, you use a world tracking configuration. For ARKit to establish tracking, the user must physically move their device to allow ARKit to get a sense of perspective. To communicate this need to the user, you use a view provided by ARKit that presents the user with instructional diagrams and verbal guidance, called [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARCoachingOverlayView] For example, when you start the app, the first thing the user sees is a message and animation from the coaching overlay telling them to move their device left and right, repeatedly, in order to get started.\n\nTo enable the user to place virtual content on a horizontal surface, you set the coaching overlay goal accordingly.\n\n```swift\nfunc setGoal() {\n    coachingOverlay.goal = .horizontalPlane\n}\n```\n\nThe coaching overlay then tailors its instructions according to the goal you choose. After ARKit gets a sense of perspective, the coaching overlay instructs the user to find a surface.\n\n## Respond to coaching events\n\nTo make sure the coaching overlay provides guidance to the user whenever ARKit determines it’s necessary, you set [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARCoachingOverlayView\/activatesAutomatically] to `true`.\n\n```swift\nfunc setActivatesAutomatically() {\n    coachingOverlay.activatesAutomatically = true\n}\n```\n\nThe coaching overlay activates automatically when the app starts, or when tracking degrades past a certain threshold. In those situations, ARKit notifies your delegate by calling [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARCoachingOverlayViewDelegate\/coachingOverlayViewWillActivate(_:)]. In response to this event, hide your app’s UI to enable the user to focus on the instructions that the coaching overlay provides.\n\n```swift\nfunc coachingOverlayViewWillActivate(_ coachingOverlayView: ARCoachingOverlayView) {\n    upperControlsView.isHidden = true\n}\n```\n\nWhen the coaching overlay determines that the goal has been met, it disappears from the user’s view. ARKit notifies your delegate that the coaching process has ended, which is when you show your app’s main user interface.\n\n```swift\nfunc coachingOverlayViewDidDeactivate(_ coachingOverlayView: ARCoachingOverlayView) {\n    upperControlsView.isHidden = false\n}\n```\n\n## Place virtual content\n\nTo give the user an idea of where they can place virtual content, annotate the environment to give them a preview. The sample app draws a square that gives the user visual confirmation of the shape and alignment of the surfaces that ARKit is aware of.\n\nTo figure out where to put the square in the real world, you use an [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARRaycastQuery] to ask ARKit where any surfaces exist in the real world. First, you create a ray-cast query that defines the 2D point on the screen you’re interested in. Because the focus square is aligned with the center of the screen, you create a query for the screen center.\n\n```swift\nfunc getRaycastQuery(for alignment: ARRaycastQuery.TargetAlignment = .any) -> ARRaycastQuery? {\n    return raycastQuery(from: screenCenter, allowing: .estimatedPlane, alignment: alignment)\n}\n```\n\nThen, you execute the ray-cast query by asking the session to cast it.\n\n```swift\nfunc castRay(for query: ARRaycastQuery) -> [ARRaycastResult] {\n    return session.raycast(query)\n}\n```\n\nARKit returns a position in the `results` parameter that includes the depth of where that point lies on a surface in the real world. To give the user a preview of where on the real-world surface a user can place their virtual content, update the focus square’s position using the ray-cast result’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARRaycastResult\/worldTransform]:\n\n```swift\nfunc setPosition(with raycastResult: ARRaycastResult, _ camera: ARCamera?) {\n    let position = raycastResult.worldTransform.translation\n    recentFocusSquarePositions.append(position)\n    updateTransform(for: raycastResult, camera: camera)\n}\n```\n\nThe ray-cast result also indicates how the surface is angled with respect to gravity. To preview the angle at which the user’s virtual content can be placed on the surface, update the focus square’s [doc:\/\/com.apple.documentation\/documentation\/SceneKit\/SCNNode\/simdWorldTransform] with the result’s orientation.\n\n```swift\nfunc updateOrientation(basedOn raycastResult: ARRaycastResult) {\n    self.simdOrientation = raycastResult.worldTransform.orientation\n}\n```\n\nIf your app offers different types of virtual content, give the user an interface to choose from. The sample app exposes a selection menu when the user taps the plus button. When the user chooses an item from the list, you instantiate the corresponding 3D model and anchor it in the world at the focus square’s current position.\n\n```swift\nfunc placeVirtualObject(_ virtualObject: VirtualObject) {\n    guard focusSquare.state != .initializing, let query = virtualObject.raycastQuery else {\n        self.statusViewController.showMessage(\"CANNOT PLACE OBJECT\\nTry moving left or right.\")\n        if let controller = self.objectsViewController {\n            self.virtualObjectSelectionViewController(controller, didDeselectObject: virtualObject)\n        }\n        return\n    }\n   \n    let trackedRaycast = createTrackedRaycastAndSet3DPosition(of: virtualObject, from: query,\n                                                              withInitialResult: virtualObject.mostRecentInitialPlacementResult)\n    \n    virtualObject.raycast = trackedRaycast\n    virtualObjectInteraction.selectedObject = virtualObject\n    virtualObject.isHidden = false\n}\n```\n\n## Refine the position of virtual content over time\n\nAs the session runs, ARKit analyzes each camera image and learns more about the layout of the physical environment. When ARKit updates its estimated size and position of real-world surfaces, you may need to update the position of your app’s virtual content to match. To help make it easy, ARKit notifies you when it corrects its understanding of the scene by way of an [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARTrackedRaycast].\n\n```swift\nfunc createTrackedRaycastAndSet3DPosition(of virtualObject: VirtualObject, from query: ARRaycastQuery,\n                                          withInitialResult initialResult: ARRaycastResult? = nil) -> ARTrackedRaycast? {\n    if let initialResult = initialResult {\n        self.setTransform(of: virtualObject, with: initialResult)\n    }\n    \n    return session.trackedRaycast(query) { (results) in\n        self.setVirtualObject3DPosition(results, with: virtualObject)\n    }\n}\n```\n\nARKit successively repeats the query you provide to a tracked ray cast, and it calls the closure you provide only when the results differ from prior results. The code you provide in the closure is your response to ARKit’s updated scene understanding. In this case, you check your ray-cast intersections against the updated planes and apply those positions to your app’s virtual content.\n\n```swift\nprivate func setVirtualObject3DPosition(_ results: [ARRaycastResult], with virtualObject: VirtualObject) {\n    \n    guard let result = results.first else {\n        fatalError(\"Unexpected case: the update handler is always supposed to return at least one result.\")\n    }\n    \n    self.setTransform(of: virtualObject, with: result)\n    \n    \/\/ If the virtual object is not yet in the scene, add it.\n    if virtualObject.parent == nil {\n        self.sceneView.scene.rootNode.addChildNode(virtualObject)\n        virtualObject.shouldUpdateAnchor = true\n    }\n    \n    if virtualObject.shouldUpdateAnchor {\n        virtualObject.shouldUpdateAnchor = false\n        self.updateQueue.async {\n            self.sceneView.addOrUpdateAnchor(for: virtualObject)\n        }\n    }\n}\n```\n\n## Manage tracked ray casts\n\nBecause ARKit continues to call them, tracked ray casts can increasingly consume resources as the user places more virtual content. Stop the tracked ray cast when you no longer need refined positions over time, such as when a virtual balloon takes flight, or when you remove a virtual object from your scene.\n\n```swift\nfunc removeVirtualObject(at index: Int) {\n    guard loadedObjects.indices.contains(index) else { return }\n    \n    \/\/ Stop the object's tracked ray cast.\n    loadedObjects[index].stopTrackedRaycast()\n    \n    \/\/ Remove the visual node from the scene graph.\n    loadedObjects[index].removeFromParentNode()\n    \/\/ Recoup resources allocated by the object.\n    loadedObjects[index].unload()\n    loadedObjects.remove(at: index)\n}\n```\n\nTo stop a tracked ray cast, you call its [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARTrackedRaycast\/stopTracking()] function:\n\n```swift\nfunc stopTrackedRaycast() {\n    raycast?.stopTracking()\n    raycast = nil\n}\n```\n\n## Enable user interaction with virtual content\n\nTo allow users to move virtual content in the world after they’ve placed it, implement a pan gesture recognizer.\n\n```swift\nfunc createPanGestureRecognizer(_ sceneView: VirtualObjectARView) {\n    let panGesture = ThresholdPanGesture(target: self, action: #selector(didPan(_:)))\n    panGesture.delegate = self\n    sceneView.addGestureRecognizer(panGesture)\n}\n```\n\nWhen the user pans an object, you request its position along the object’s path across the plane. Because the object’s position is transitory, use a [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSession\/raycast(_:)] instead of using a tracked ray cast. In this case, a one-time hit test is appropriate because you don’t need refined position results over time for these requests.\n\n```swift\nfunc translate(_ object: VirtualObject, basedOn screenPos: CGPoint) {\n    object.stopTrackedRaycast()\n    \n    \/\/ Update the object by using a one-time position request.\n    if let query = sceneView.raycastQuery(from: screenPos, allowing: .estimatedPlane, alignment: object.allowedAlignment) {\n        viewController.createRaycastAndUpdate3DPosition(of: object, from: query)\n    }\n}\n```\n\nRay casting gives you orientation information about the surface at a given screen point. While dragging, you avoid quick changes in orientation by subtracting the gesture’s rotation from the current object rotation.\n\n```swift\n@objc\nfunc didRotate(_ gesture: UIRotationGestureRecognizer) {\n    guard gesture.state == .changed else { return }\n    \n    trackedObject?.objectRotation -= Float(gesture.rotation)\n    \n    gesture.rotation = 0\n}\n```\n\n## Handle interruption in tracking\n\nIn cases where tracking conditions are poor, ARKit invokes your delegate’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSessionObserver\/sessionWasInterrupted(_:)]. In these circumstances, the positions of your app’s virtual content may be inaccurate with respect to the camera feed, so hide your virtual content.\n\n```swift\nfunc hideVirtualContent() {\n    virtualObjectLoader.loadedObjects.forEach { $0.isHidden = true }\n}\n```\n\nRestore your app’s virtual content when tracking conditions improve. To notify you of improved conditions, ARKit calls your delegate’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSessionObserver\/session(_:cameraDidChangeTrackingState:)]function, passing in a camera [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARCamera\/trackingState-6i3pt] equal to [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARTrackingState\/ARTrackingStateNormal].\n\n```swift\nfunc session(_ session: ARSession, cameraDidChangeTrackingState camera: ARCamera) {\n    statusViewController.showTrackingQualityInfo(for: camera.trackingState, autoHide: true)\n    switch camera.trackingState {\n    case .notAvailable, .limited:\n        statusViewController.escalateFeedback(for: camera.trackingState, inSeconds: 3.0)\n    case .normal:\n        statusViewController.cancelScheduledMessage(for: .trackingStateEscalation)\n        showVirtualContent()\n    }\n}\n```\n\n## Restore an interrupted AR experience\n\nWhen a session is interrupted, ARKit asks if you want to try to restore the AR experience. You do that by opting in to *relocalization*, by overriding [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSessionObserver\/sessionShouldAttemptRelocalization(_:)] and returning `true`.\n\n```swift\nfunc sessionShouldAttemptRelocalization(_ session: ARSession) -> Bool {\n    return true\n}\n```\n\nDuring relocalization, the coaching overlay displays tailored instructions to the user. To allow the user to focus on the coaching process, hide your app’s UI when coaching is enabled.\n\n```swift\nfunc coachingOverlayViewWillActivate(_ coachingOverlayView: ARCoachingOverlayView) {\n    upperControlsView.isHidden = true\n}\n```\n\nWhen ARKit succeeds in restoring the experience, show your app’s UI again so everything appears the way it was before the interruption. When the coaching overlay disappears from the user’s view, ARKit invokes your [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARCoachingOverlayViewDelegate\/coachingOverlayViewDidDeactivate(_:)] callback, which is where you restore your app’s UI.\n\n```swift\nfunc coachingOverlayViewDidDeactivate(_ coachingOverlayView: ARCoachingOverlayView) {\n    upperControlsView.isHidden = false\n}\n```\n\n## Enable the user to start over rather than restore\n\nIf the user decides to give up on restoring the session, you restart the experience in your delegate’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARCoachingOverlayViewDelegate\/coachingOverlayViewDidRequestSessionReset(_:)] function. ARKit invokes this callback when the user taps the coaching overlay’s Start Over button.\n\n```swift\nfunc coachingOverlayViewDidRequestSessionReset(_ coachingOverlayView: ARCoachingOverlayView) {\n    restartExperience()\n}\n```\n\n## Raycasting\n\n- **ARRaycastQuery**: A mathematical ray you use to find 3D positions on real-world surfaces.\n- **ARTrackedRaycast**: A raycast query that ARKit repeats in succession to give you refined results over time.\n- **ARRaycastResult**: Information about a real-world surface found by examining a point on the screen.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "A mathematical ray you use to find 3D positions on real-world surfaces.",
          "name" : "ARRaycastQuery",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARRaycastQuery"
        },
        {
          "description" : "A raycast query that ARKit repeats in succession to give you refined results over time.",
          "name" : "ARTrackedRaycast",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARTrackedRaycast"
        },
        {
          "description" : "Information about a real-world surface found by examining a point on the screen.",
          "name" : "ARRaycastResult",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARRaycastResult"
        }
      ],
      "title" : "Raycasting"
    }
  ],
  "source" : "appleJSON",
  "title" : "Placing objects and handling 3D interaction",
  "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/placing-objects-and-handling-3d-interaction"
}