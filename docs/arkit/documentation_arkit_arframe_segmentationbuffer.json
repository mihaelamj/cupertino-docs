{
  "abstract" : "A buffer that contains pixel information identifying the shape of objects from the camera feed that you use to occlude virtual content.",
  "codeExamples" : [

  ],
  "contentHash" : "93d56a1ca583ace797d28dc97e4b0fd7fd14a97190aa7d7986a8dc7c34b28250",
  "crawledAt" : "2025-12-02T18:53:03Z",
  "declaration" : {
    "code" : "var segmentationBuffer: CVPixelBuffer? { get }",
    "language" : "swift"
  },
  "id" : "91305A9F-97C3-44D3-B0CE-AACF4B2F72BC",
  "kind" : "property",
  "language" : "swift",
  "module" : "ARKit",
  "overview" : "## Discussion\n\nARKit generates the contents of this buffer by processing the camera feed.\n\nIf you implement a custom renderer, you apply this property by using alpha and depth mattes provided with [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARMatteGenerator].\n\nApps using one of the standard renderers don’t need this this property to occlude virtual content with people. The standard renderers ([doc:\/\/com.apple.documentation\/documentation\/RealityKit\/ARView], [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNView], and [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSKView]) enable people occlusion when you add [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/FrameSemantics-swift.struct\/personSegmentation] or [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/FrameSemantics-swift.struct\/personSegmentationWithDepth] to your configuration’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/frameSemantics-swift.property].",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/segmentationBuffer\ncrawled: 2025-12-02T18:53:03Z\n---\n\n# segmentationBuffer\n\n**Instance Property**\n\nA buffer that contains pixel information identifying the shape of objects from the camera feed that you use to occlude virtual content.\n\n## Declaration\n\n```swift\nvar segmentationBuffer: CVPixelBuffer? { get }\n```\n\n## Discussion\n\nARKit generates the contents of this buffer by processing the camera feed.\n\nIf you implement a custom renderer, you apply this property by using alpha and depth mattes provided with [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARMatteGenerator].\n\nApps using one of the standard renderers don’t need this this property to occlude virtual content with people. The standard renderers ([doc:\/\/com.apple.documentation\/documentation\/RealityKit\/ARView], [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNView], and [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSKView]) enable people occlusion when you add [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/FrameSemantics-swift.struct\/personSegmentation] or [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/FrameSemantics-swift.struct\/personSegmentationWithDepth] to your configuration’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/frameSemantics-swift.property].\n\n## Checking for people\n\n- **detectedBody**: The screen position information of a body that ARKit recognizes in the camera image.\n- **ARBody2D**: The screen-space representation of a person ARKit recognizes in the camera feed.\n- **estimatedDepthData**: A buffer that represents the estimated depth values from the camera feed that you use to occlude virtual content.\n- **ARFrame.SegmentationClass**: A categorization of a pixel that defines a type of content you use to occlude your app’s virtual content.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "The screen position information of a body that ARKit recognizes in the camera image.",
          "name" : "detectedBody",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/detectedBody"
        },
        {
          "description" : "The screen-space representation of a person ARKit recognizes in the camera feed.",
          "name" : "ARBody2D",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARBody2D"
        },
        {
          "description" : "A buffer that represents the estimated depth values from the camera feed that you use to occlude virtual content.",
          "name" : "estimatedDepthData",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/estimatedDepthData"
        },
        {
          "description" : "A categorization of a pixel that defines a type of content you use to occlude your app’s virtual content.",
          "name" : "ARFrame.SegmentationClass",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/SegmentationClass"
        }
      ],
      "title" : "Checking for people"
    }
  ],
  "source" : "appleJSON",
  "title" : "segmentationBuffer",
  "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/segmentationBuffer"
}