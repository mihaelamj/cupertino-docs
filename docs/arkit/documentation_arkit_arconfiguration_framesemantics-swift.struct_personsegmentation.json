{
  "abstract" : "An option that indicates that people occlude your app’s virtual content.",
  "codeExamples" : [

  ],
  "contentHash" : "9bfbd77b7a1c38b2979d5d09684c678e46703bf364d9752e89c112a01fa16397",
  "crawledAt" : "2025-12-05T15:18:19Z",
  "declaration" : {
    "code" : "static var personSegmentation: ARConfiguration.FrameSemantics { get }",
    "language" : "swift"
  },
  "id" : "D2EFC574-B9F2-43EB-B2C2-09428BC89DA4",
  "kind" : "property",
  "language" : "swift",
  "module" : "ARKit",
  "overview" : "## Discussion\n\nThe [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/FrameSemantics-swift.struct\/personSegmentation] frame semantic specifies that any person ARKit detects in the camera feed occludes virtual content, regardless of the person’s depth in the scene.\n\nWhen this option is enabled, ARKit sets the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFrame\/estimatedDepthData] and [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFrame\/segmentationBuffer] properties to serve as a foundation for people occlusion. The standard renderers ([doc:\/\/com.apple.documentation\/documentation\/RealityKit\/ARView], and [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNView]) use those properties to implement people occlusion for you. See [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/frameSemantics-swift.property] for more information.",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/ARKit\/ARConfiguration\/FrameSemantics-swift.struct\/personSegmentation\ncrawled: 2025-12-05T15:18:19Z\n---\n\n# personSegmentation\n\n**Type Property**\n\nAn option that indicates that people occlude your app’s virtual content.\n\n## Declaration\n\n```swift\nstatic var personSegmentation: ARConfiguration.FrameSemantics { get }\n```\n\n## Discussion\n\nThe [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/FrameSemantics-swift.struct\/personSegmentation] frame semantic specifies that any person ARKit detects in the camera feed occludes virtual content, regardless of the person’s depth in the scene.\n\nWhen this option is enabled, ARKit sets the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFrame\/estimatedDepthData] and [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFrame\/segmentationBuffer] properties to serve as a foundation for people occlusion. The standard renderers ([doc:\/\/com.apple.documentation\/documentation\/RealityKit\/ARView], and [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNView]) use those properties to implement people occlusion for you. See [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/frameSemantics-swift.property] for more information.\n\n## Occluding Virtual Content with People\n\n- **personSegmentationWithDepth**: An option that indicates that people occlude your app’s virtual content depending on depth.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "An option that indicates that people occlude your app’s virtual content depending on depth.",
          "name" : "personSegmentationWithDepth",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARConfiguration\/FrameSemantics-swift.struct\/personSegmentationWithDepth"
        }
      ],
      "title" : "Occluding Virtual Content with People"
    }
  ],
  "source" : "appleJSON",
  "title" : "personSegmentation",
  "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARConfiguration\/FrameSemantics-swift.struct\/personSegmentation"
}