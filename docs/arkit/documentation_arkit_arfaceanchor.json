{
  "abstract" : "An anchor for a unique face that is visible in the front-facing camera.",
  "codeExamples" : [

  ],
  "conformsTo" : [
    "ARAnchorCopying",
    "ARTrackable",
    "CVarArg",
    "CustomDebugStringConvertible",
    "CustomStringConvertible",
    "Equatable",
    "Hashable",
    "NSCoding",
    "NSCopying",
    "NSObjectProtocol",
    "NSSecureCoding",
    "Sendable",
    "SendableMetatype"
  ],
  "contentHash" : "fba149097dcdd01386ae17e530526da525b04c8cc8c31bd2e7e5b8cc27d0688d",
  "crawledAt" : "2025-12-02T16:08:37Z",
  "declaration" : {
    "code" : "class ARFaceAnchor",
    "language" : "swift"
  },
  "id" : "0C2C52D6-14D8-4E95-9C56-6462DFC8E6FD",
  "kind" : "class",
  "language" : "swift",
  "module" : "ARKit",
  "overview" : "## Overview\n\nThe session automatically adds to its list of anchors an ARFaceAnchor object when it detects a unique face in the front camera feed.\n\nWhen you track faces using [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFaceTrackingConfiguration], ARKit can track multiple faces simultaneously.\n\nAlternatively, you can enable face tracking with a world tracking configuration by setting .\n\n### Tracking Face Position and Orientation\n\nThe inherited [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARAnchor\/transform] property describes the face’s current position and orientation in world coordinates; that is, in a coordinate space relative to that specified by the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/worldAlignment-swift.property] property of the session configuration. Use this transform matrix to position virtual content you want to “attach” to the face in your AR scene.\n\nThis transform matrix creates a face coordinates system for positioning other elements relative to the face. Units of face coordinate space are in meters, with the origin centered behind the face as indicated in the figure below.\n\n\n\nThe coordinate system is right-handed—the positive x direction points to the viewer’s right (that is, the face’s own left), the positive y direction points up (relative to the face itself, not to the world), and the positive z direction points outward from the face (toward the viewer).\n\n### Using Face Topology\n\nThe [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFaceAnchor\/geometry] property provides an [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFaceGeometry] object representing detailed topology for the face, which conforms a generic face model to match the dimensions, shape, and current expression of the detected face.\n\nYou can use this model as the basis for overlaying content that follows the shape of the user’s face—for example, to apply virtual makeup or tattoos. You can also use this model to create *occlusion geometry*—a 3D model that doesn’t render any visible content (allowing the camera image to show through), but that obstructs the camera’s view of other virtual content in the scene.\n\n### Tracking Facial Expressions\n\nThe [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFaceAnchor\/blendShapes] property provides a high-level model of the current facial expression, described via a series of many named coefficients that represent the movement of specific facial features relative to their neutral configurations. You can use blend shape coefficients to animate 2D or 3D content, such as a character or avatar, in ways that follow the user’s facial expressions.",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/ARKit\/ARFaceAnchor\ncrawled: 2025-12-02T16:08:37Z\n---\n\n# ARFaceAnchor\n\n**Class**\n\nAn anchor for a unique face that is visible in the front-facing camera.\n\n## Declaration\n\n```swift\nclass ARFaceAnchor\n```\n\n## Overview\n\nThe session automatically adds to its list of anchors an ARFaceAnchor object when it detects a unique face in the front camera feed.\n\nWhen you track faces using [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFaceTrackingConfiguration], ARKit can track multiple faces simultaneously.\n\nAlternatively, you can enable face tracking with a world tracking configuration by setting .\n\n### Tracking Face Position and Orientation\n\nThe inherited [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARAnchor\/transform] property describes the face’s current position and orientation in world coordinates; that is, in a coordinate space relative to that specified by the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/worldAlignment-swift.property] property of the session configuration. Use this transform matrix to position virtual content you want to “attach” to the face in your AR scene.\n\nThis transform matrix creates a face coordinates system for positioning other elements relative to the face. Units of face coordinate space are in meters, with the origin centered behind the face as indicated in the figure below.\n\n\n\nThe coordinate system is right-handed—the positive x direction points to the viewer’s right (that is, the face’s own left), the positive y direction points up (relative to the face itself, not to the world), and the positive z direction points outward from the face (toward the viewer).\n\n### Using Face Topology\n\nThe [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFaceAnchor\/geometry] property provides an [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFaceGeometry] object representing detailed topology for the face, which conforms a generic face model to match the dimensions, shape, and current expression of the detected face.\n\nYou can use this model as the basis for overlaying content that follows the shape of the user’s face—for example, to apply virtual makeup or tattoos. You can also use this model to create *occlusion geometry*—a 3D model that doesn’t render any visible content (allowing the camera image to show through), but that obstructs the camera’s view of other virtual content in the scene.\n\n### Tracking Facial Expressions\n\nThe [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFaceAnchor\/blendShapes] property provides a high-level model of the current facial expression, described via a series of many named coefficients that represent the movement of specific facial features relative to their neutral configurations. You can use blend shape coefficients to animate 2D or 3D content, such as a character or avatar, in ways that follow the user’s facial expressions.\n\n## Using Face Geometry\n\n- **geometry**: A coarse triangle mesh representing the topology of the detected face.\n\n## Using Blend Shapes\n\n- **blendShapes**: A dictionary of named coefficients representing the detected facial expression in terms of the movement of specific facial features.\n- **ARFaceAnchor.BlendShapeLocation**: Identifiers for specific facial features, for use with coefficients describing the relative movements of those features.\n\n## Tracking Eye Movement\n\n- **leftEyeTransform**: A transform matrix indicating the position and orientation of the face’s left eye.\n- **rightEyeTransform**: A transform matrix indicating the position and orientation of the face’s right eye.\n- **lookAtPoint**: A position in face coordinate space estimating the direction of the face’s gaze.\n\n## Face Tracking\n\n- **Tracking and visualizing faces**: Detect faces in a front-camera AR experience, overlay virtual content, and animate facial expressions in real-time.\n- **Combining user face-tracking and world tracking**: Track the user’s face in an app that displays an AR experience with the rear camera.\n\n## Inherits From\n\n- ARAnchor\n\n## Conforms To\n\n- ARAnchorCopying\n- ARTrackable\n- CVarArg\n- CustomDebugStringConvertible\n- CustomStringConvertible\n- Equatable\n- Hashable\n- NSCoding\n- NSCopying\n- NSObjectProtocol\n- NSSecureCoding\n- Sendable\n- SendableMetatype\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "A coarse triangle mesh representing the topology of the detected face.",
          "name" : "geometry",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFaceAnchor\/geometry"
        }
      ],
      "title" : "Using Face Geometry"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "A dictionary of named coefficients representing the detected facial expression in terms of the movement of specific facial features.",
          "name" : "blendShapes",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFaceAnchor\/blendShapes"
        },
        {
          "description" : "Identifiers for specific facial features, for use with coefficients describing the relative movements of those features.",
          "name" : "ARFaceAnchor.BlendShapeLocation",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFaceAnchor\/BlendShapeLocation"
        }
      ],
      "title" : "Using Blend Shapes"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "A transform matrix indicating the position and orientation of the face’s left eye.",
          "name" : "leftEyeTransform",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFaceAnchor\/leftEyeTransform"
        },
        {
          "description" : "A transform matrix indicating the position and orientation of the face’s right eye.",
          "name" : "rightEyeTransform",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFaceAnchor\/rightEyeTransform"
        },
        {
          "description" : "A position in face coordinate space estimating the direction of the face’s gaze.",
          "name" : "lookAtPoint",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFaceAnchor\/lookAtPoint"
        }
      ],
      "title" : "Tracking Eye Movement"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "Detect faces in a front-camera AR experience, overlay virtual content, and animate facial expressions in real-time.",
          "name" : "Tracking and visualizing faces",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/tracking-and-visualizing-faces"
        },
        {
          "description" : "Track the user’s face in an app that displays an AR experience with the rear camera.",
          "name" : "Combining user face-tracking and world tracking",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/combining-user-face-tracking-and-world-tracking"
        }
      ],
      "title" : "Face Tracking"
    },
    {
      "content" : "",
      "items" : [
        {
          "name" : "ARAnchor"
        }
      ],
      "title" : "Inherits From"
    }
  ],
  "source" : "appleJSON",
  "title" : "ARFaceAnchor",
  "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFaceAnchor"
}