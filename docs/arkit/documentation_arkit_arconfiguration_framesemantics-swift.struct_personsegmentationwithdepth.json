{
  "abstract" : "An option that indicates that people occlude your app’s virtual content depending on depth.",
  "codeExamples" : [

  ],
  "contentHash" : "89167078f28d3990eb7984adeaf4db821f7f58cd29fa9b0c0d78715b044afe19",
  "crawledAt" : "2025-12-05T15:18:19Z",
  "declaration" : {
    "code" : "static var personSegmentationWithDepth: ARConfiguration.FrameSemantics { get }",
    "language" : "swift"
  },
  "id" : "B4C9B690-F90E-406B-A747-23B6F5931F40",
  "kind" : "property",
  "language" : "swift",
  "module" : "ARKit",
  "overview" : "## Discussion\n\nThe [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/FrameSemantics-swift.struct\/personSegmentationWithDepth] frame semantic specifies that any person ARKit detects in the camera feed should occlude virtual content, depending on the person’s depth in the scene.\n\nWhen this option is enabled, ARKit sets the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFrame\/estimatedDepthData] and [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFrame\/segmentationBuffer] properties to serve as a foundation for people occlusion. The standard renderers ([doc:\/\/com.apple.documentation\/documentation\/RealityKit\/ARView], and [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNView]) use those properties to implement people occlusion for you. See [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/frameSemantics-swift.property] for more information.",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/ARKit\/ARConfiguration\/FrameSemantics-swift.struct\/personSegmentationWithDepth\ncrawled: 2025-12-05T15:18:19Z\n---\n\n# personSegmentationWithDepth\n\n**Type Property**\n\nAn option that indicates that people occlude your app’s virtual content depending on depth.\n\n## Declaration\n\n```swift\nstatic var personSegmentationWithDepth: ARConfiguration.FrameSemantics { get }\n```\n\n## Discussion\n\nThe [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/FrameSemantics-swift.struct\/personSegmentationWithDepth] frame semantic specifies that any person ARKit detects in the camera feed should occlude virtual content, depending on the person’s depth in the scene.\n\nWhen this option is enabled, ARKit sets the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFrame\/estimatedDepthData] and [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFrame\/segmentationBuffer] properties to serve as a foundation for people occlusion. The standard renderers ([doc:\/\/com.apple.documentation\/documentation\/RealityKit\/ARView], and [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNView]) use those properties to implement people occlusion for you. See [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/frameSemantics-swift.property] for more information.\n\n## Occluding Virtual Content with People\n\n- **personSegmentation**: An option that indicates that people occlude your app’s virtual content.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "An option that indicates that people occlude your app’s virtual content.",
          "name" : "personSegmentation",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARConfiguration\/FrameSemantics-swift.struct\/personSegmentation"
        }
      ],
      "title" : "Occluding Virtual Content with People"
    }
  ],
  "source" : "appleJSON",
  "title" : "personSegmentationWithDepth",
  "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARConfiguration\/FrameSemantics-swift.struct\/personSegmentationWithDepth"
}