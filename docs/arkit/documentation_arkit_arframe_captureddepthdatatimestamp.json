{
  "abstract" : "The time at which depth data for the frame (if any) was captured.",
  "codeExamples" : [

  ],
  "contentHash" : "ee7c409950dc1f23e6db92e00dc4b0f622b3842dab91196939613d78040fe58c",
  "crawledAt" : "2025-12-02T18:34:13Z",
  "declaration" : {
    "code" : "var capturedDepthDataTimestamp: TimeInterval { get }",
    "language" : "swift"
  },
  "id" : "5C6C294B-4ADB-48CE-AFC3-F9EE8338796C",
  "kind" : "property",
  "language" : "swift",
  "module" : "ARKit",
  "overview" : "## Discussion\n\nFace-based AR (see [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFaceTrackingConfiguration]) uses the front-facing, depth-sensing camera on compatible devices. When running such a configuration, frames vended by the session contain a depth map captured by the depth camera in addition to the color pixel buffer (see capturedImage) captured by the color camera. This property’s value is always zero when running other AR configurations.\n\nThe depth-sensing camera provides data at a different frame rate than the color camera, so this property’s value may not exactly match the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFrame\/timestamp] property for the image captured by the color camera, and can also be zero if no depth data was captured at the same time as the current color image.",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/capturedDepthDataTimestamp\ncrawled: 2025-12-02T18:34:13Z\n---\n\n# capturedDepthDataTimestamp\n\n**Instance Property**\n\nThe time at which depth data for the frame (if any) was captured.\n\n## Declaration\n\n```swift\nvar capturedDepthDataTimestamp: TimeInterval { get }\n```\n\n## Discussion\n\nFace-based AR (see [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFaceTrackingConfiguration]) uses the front-facing, depth-sensing camera on compatible devices. When running such a configuration, frames vended by the session contain a depth map captured by the depth camera in addition to the color pixel buffer (see capturedImage) captured by the color camera. This property’s value is always zero when running other AR configurations.\n\nThe depth-sensing camera provides data at a different frame rate than the color camera, so this property’s value may not exactly match the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFrame\/timestamp] property for the image captured by the color camera, and can also be zero if no depth data was captured at the same time as the current color image.\n\n## Accessing scene data\n\n- **lightEstimate**: An estimate of lighting conditions based on the camera image.\n- **displayTransform(for:viewportSize:)**: Returns an affine transform for converting between normalized image coordinates and a coordinate space appropriate for rendering the camera image onscreen.\n- **rawFeaturePoints**: The current intermediate results of the scene analysis ARKit uses to perform world tracking.\n- **capturedDepthData**: Depth data captured in front-camera experiences.\n- **sceneDepth**: Data on the distance between a device’s rear camera and real-world objects in an AR experience.\n- **smoothedSceneDepth**: An average of distance measurements between a device’s rear camera and real-world objects that creates smoother visuals in an AR experience.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "An estimate of lighting conditions based on the camera image.",
          "name" : "lightEstimate",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/lightEstimate"
        },
        {
          "description" : "Returns an affine transform for converting between normalized image coordinates and a coordinate space appropriate for rendering the camera image onscreen.",
          "name" : "displayTransform(for:viewportSize:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/displayTransform(for:viewportSize:)"
        },
        {
          "description" : "The current intermediate results of the scene analysis ARKit uses to perform world tracking.",
          "name" : "rawFeaturePoints",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/rawFeaturePoints"
        },
        {
          "description" : "Depth data captured in front-camera experiences.",
          "name" : "capturedDepthData",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/capturedDepthData"
        },
        {
          "description" : "Data on the distance between a device’s rear camera and real-world objects in an AR experience.",
          "name" : "sceneDepth",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/sceneDepth"
        },
        {
          "description" : "An average of distance measurements between a device’s rear camera and real-world objects that creates smoother visuals in an AR experience.",
          "name" : "smoothedSceneDepth",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/smoothedSceneDepth"
        }
      ],
      "title" : "Accessing scene data"
    }
  ],
  "source" : "appleJSON",
  "title" : "capturedDepthDataTimestamp",
  "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/capturedDepthDataTimestamp"
}