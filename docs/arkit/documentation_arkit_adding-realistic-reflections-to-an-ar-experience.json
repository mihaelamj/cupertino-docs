{
  "abstract" : "Use ARKit to generate environment probe textures from camera imagery and render reflective virtual objects.",
  "codeExamples" : [
    {
      "code" : "let configuration = ARWorldTrackingConfiguration()\nconfiguration.planeDetection = .horizontal\nconfiguration.environmentTexturing = .automatic\nsceneView.session.run(configuration)",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Make sure the probe encompasses the object and provides some surrounding area to appear in reflections.\nvar extent = object.extents * object.simdScale\nextent.x *= 3 \/\/ Reflect an area 3x the width of the object.\nextent.z *= 3 \/\/ Reflect an area 3x the depth of the object.\n\n\/\/ Also include some vertical area around the object, but keep the bottom of the probe at the\n\/\/ bottom of the object so that it captures the real-world surface underneath.\nlet verticalOffset = SIMD3<Float>(0, extent.y, 0)\nlet transform = float4x4(translation: object.simdPosition + verticalOffset)\nextent.y *= 2\n\n\/\/ Create the new environment probe anchor and add it to the session.\nlet probeAnchor = AREnvironmentProbeAnchor(transform: transform, extent: extent)\nsceneView.session.add(anchor: probeAnchor)",
      "language" : "swift"
    }
  ],
  "contentHash" : "047550a4daeb86b978391bc9d09c3a03fb10d4975dd7e30aca592d9c55f3dfdb",
  "crawledAt" : "2025-12-02T15:47:17Z",
  "id" : "DD34436D-6C8E-4CA9-8FB8-34BCE3FC5612",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "ARKit",
  "overview" : "## Overview\n\nThis app provides a simple AR experience demonstrating the environment texturing features in ARKit 2 and SceneKit. After you build and run the app, explore your surroundings in the camera view. Then, tap a nearby horizontal surface to place a virtual object: a mirror-finish sphere. After you place the object, you can drag it around or tap to move it to another location. You can also pinch to make the object bigger or smaller.\n\n\n\nNotice the surface of the virtual sphere shows a generally realistic (if not perfectly accurate) reflection of its real-world surroundings. To create reflective virtual surfaces, a renderer (such as SceneKit) needs an  *environment texture*—an image that captures the view in all directions from a certain point in the scene (called an *environment probe*). Realistically rendering reflections for multiple objects, or moving objects, may require multiple environment textures, each capturing the scene from a different point of view.\n\nARKit generates environment textures by collecting camera imagery during the AR session. Because ARKit cannot see the scene in all directions, it uses machine learning to extrapolate a realistic environment from available imagery.\n\n## Getting started\n\nBefore you can run the sample code project, you’ll need:\n\n## Set up environment texturing\n\nAs with any AR experience, you run a session with a world tracking configuration and whatever other options you want to enable. (For example, this app allows you to place virtual objects on flat surfaces, so it enables horizontal plane detection.) To generate environment textures, also set the configuration’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARWorldTrackingConfiguration\/EnvironmentTexturing-swift.enum\/automatic] property:\n\nWith [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARWorldTrackingConfiguration\/EnvironmentTexturing-swift.enum\/automatic] environment texturing (the default for this app) ARKit automatically chooses when and where to generate textures.\n\n## Render virtual objects with reflection\n\nBecause this app also uses [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNView] to display AR content, SceneKit automatically uses the appropriate environment texture to render each virtual object in the scene. In SceneKit, any asset using [doc:\/\/com.apple.documentation\/documentation\/SceneKit\/SCNMaterial\/LightingModel-swift.struct] materials automatically uses *environmental lighting*. With environmental lighting, the shading for each point on a surface depends on nearby light probe textures or the global lighting environment in the direction that point faces.\n\nThe visual effect of environment texturing depends on how you configure the properties of a physically based material. For example, materials with a high [doc:\/\/com.apple.documentation\/documentation\/SceneKit\/SCNMaterial\/roughness] pick up some diffuse color from the texture, and materials with low [doc:\/\/com.apple.documentation\/documentation\/SceneKit\/SCNMaterial\/roughness] and high [doc:\/\/com.apple.documentation\/documentation\/SceneKit\/SCNMaterial\/metalness] reflect their surroundings with a mirror-like finish.\n\n\n\n## Place environment probes manually for enhanced results\n\nAutomatic environment texturing is all you need for basic environmental lighting or reflection effects. To render reflections more realistically, however, each reflective object needs an environment probe texture that accurately captures the area close to that object. For example, in the images above, the virtual sphere reflects the real cup when the cup is close to the sphere’s real-world position.\n\nTo more precisely define environment probes, choose [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARWorldTrackingConfiguration\/EnvironmentTexturing-swift.enum\/manual] environment texturing when you configure your AR session, then create your own [doc:\/\/com.apple.arkit\/documentation\/ARKit\/AREnvironmentProbeAnchor] instance for each virtual object you want to use environmental lighting with. Initialize each probe’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/AREnvironmentProbeAnchor\/extent] and position (using [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARAnchor\/transform] based on the size of the corresponding virtual object:\n\nThis code applies the rules below to optimally capture the area around each virtual object:\n\n\n\n## Use environment texturing wisely\n\nFollow these tips to keep your app’s use of environment texturing realistic and efficient:\n\n**Avoid virtual content that requires accurate reflections, such as mirror-finish surfaces.**\n\nIn general, an AR experience doesn’t have all the information needed to produce a perfect imitation of reality. Good AR experiences carefully design content to hide limitations in realism, preserving the illusion that virtual objects inhabit the user’s real-world surroundings.\n\nARKit environment textures don’t image the environment in all directions around the user, and don’t update in real time, so some kinds of content aren’t well suited for use in AR. For example, a user encountering a virtual mirror may expect to see their own reflection. Design virtual content to use fully-reflective surfaces only in small or highly-detailed parts, and use less reflectivity in large flat surfaces.\n\n**Handle moving objects.**\n\nRendering a virtual object with realistic reflections require an environment probe that captures a small area around that object. If the object changes position, the corresponding environment probe needs to change to reflect the object’s new surroundings. When manually placing probes, consider one or more of these strategies for handling objects that move:\n\nWhen you display AR content with [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNView], SceneKit automatically interpolates between environment textures for any objects that overlap the extents of multiple environment probes.\n\n**Don’t generate environment textures too often.**\n\nARKit requires some time to collect camera imagery, and combining and extrapolating that imagery to produce environment textures requires computational resources. Frequently adding new [doc:\/\/com.apple.arkit\/documentation\/ARKit\/AREnvironmentProbeAnchor] instances to your AR session may not produce noticeable changes in the displayed scene, but does cost battery power and reduce the performance overhead available for other aspects of your AR experience.\n\nThis app creates new environment probes whenever the user moves or resizes the virtual object, but limits such updates to occur no more often than once per second. (See the sample `updateEnvironmentProbe(atTime:)` function.)\n\n**Avoid abrupt transitions between different environment textures.**\n\nWith [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNView], if the environment probe texture(s) affecting an object change (either because the object moves or because a new texture becomes available for its current position), SceneKit automatically uses a short fade-in animation to transition to the new result. Depending on what environment textures are in use before and after the transition, that change may be jarring to the user.\n\nTo avoid unrealistic transitions, this sample app waits until the first environment texture becomes available before allowing the user to place virtual content. In automatic mode, as soon as the session begins, ARKit automatically begins generating a fallback environment texture covering a large area. In manual mode, the app creates its own fallback environment probe. (See the sample `updateSceneEnvironmentProbe(for:)` function.) Waiting until this environment texture is available ensures that virtual objects always reflect an environment appropriate to the session.\n\nAlternatively, your app may include a static environment-map texture for use as a fallback when environment texturing is not available (for example, to support earlier iOS versions). In this case, try to design or select a texture that appears realistic in a wide variety of situations.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/ARKit\/adding-realistic-reflections-to-an-ar-experience\ncrawled: 2025-12-02T15:47:17Z\n---\n\n# Adding realistic reflections to an AR experience\n\n**Sample Code**\n\nUse ARKit to generate environment probe textures from camera imagery and render reflective virtual objects.\n\n## Overview\n\nThis app provides a simple AR experience demonstrating the environment texturing features in ARKit 2 and SceneKit. After you build and run the app, explore your surroundings in the camera view. Then, tap a nearby horizontal surface to place a virtual object: a mirror-finish sphere. After you place the object, you can drag it around or tap to move it to another location. You can also pinch to make the object bigger or smaller.\n\n\n\nNotice the surface of the virtual sphere shows a generally realistic (if not perfectly accurate) reflection of its real-world surroundings. To create reflective virtual surfaces, a renderer (such as SceneKit) needs an  *environment texture*—an image that captures the view in all directions from a certain point in the scene (called an *environment probe*). Realistically rendering reflections for multiple objects, or moving objects, may require multiple environment textures, each capturing the scene from a different point of view.\n\nARKit generates environment textures by collecting camera imagery during the AR session. Because ARKit cannot see the scene in all directions, it uses machine learning to extrapolate a realistic environment from available imagery.\n\n## Getting started\n\nBefore you can run the sample code project, you’ll need:\n\n- Xcode 10 or later.\n- iOS 12 or later.\n- An iOS device with an A9 processor or later.\n\n## Set up environment texturing\n\nAs with any AR experience, you run a session with a world tracking configuration and whatever other options you want to enable. (For example, this app allows you to place virtual objects on flat surfaces, so it enables horizontal plane detection.) To generate environment textures, also set the configuration’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARWorldTrackingConfiguration\/EnvironmentTexturing-swift.enum\/automatic] property:\n\n```swift\nlet configuration = ARWorldTrackingConfiguration()\nconfiguration.planeDetection = .horizontal\nconfiguration.environmentTexturing = .automatic\nsceneView.session.run(configuration)\n```\n\nWith [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARWorldTrackingConfiguration\/EnvironmentTexturing-swift.enum\/automatic] environment texturing (the default for this app) ARKit automatically chooses when and where to generate textures.\n\n## Render virtual objects with reflection\n\nBecause this app also uses [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNView] to display AR content, SceneKit automatically uses the appropriate environment texture to render each virtual object in the scene. In SceneKit, any asset using [doc:\/\/com.apple.documentation\/documentation\/SceneKit\/SCNMaterial\/LightingModel-swift.struct] materials automatically uses *environmental lighting*. With environmental lighting, the shading for each point on a surface depends on nearby light probe textures or the global lighting environment in the direction that point faces.\n\nThe visual effect of environment texturing depends on how you configure the properties of a physically based material. For example, materials with a high [doc:\/\/com.apple.documentation\/documentation\/SceneKit\/SCNMaterial\/roughness] pick up some diffuse color from the texture, and materials with low [doc:\/\/com.apple.documentation\/documentation\/SceneKit\/SCNMaterial\/roughness] and high [doc:\/\/com.apple.documentation\/documentation\/SceneKit\/SCNMaterial\/metalness] reflect their surroundings with a mirror-like finish.\n\n\n\n\n\n## Place environment probes manually for enhanced results\n\nAutomatic environment texturing is all you need for basic environmental lighting or reflection effects. To render reflections more realistically, however, each reflective object needs an environment probe texture that accurately captures the area close to that object. For example, in the images above, the virtual sphere reflects the real cup when the cup is close to the sphere’s real-world position.\n\nTo more precisely define environment probes, choose [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARWorldTrackingConfiguration\/EnvironmentTexturing-swift.enum\/manual] environment texturing when you configure your AR session, then create your own [doc:\/\/com.apple.arkit\/documentation\/ARKit\/AREnvironmentProbeAnchor] instance for each virtual object you want to use environmental lighting with. Initialize each probe’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/AREnvironmentProbeAnchor\/extent] and position (using [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARAnchor\/transform] based on the size of the corresponding virtual object:\n\n```swift\n\/\/ Make sure the probe encompasses the object and provides some surrounding area to appear in reflections.\nvar extent = object.extents * object.simdScale\nextent.x *= 3 \/\/ Reflect an area 3x the width of the object.\nextent.z *= 3 \/\/ Reflect an area 3x the depth of the object.\n\n\/\/ Also include some vertical area around the object, but keep the bottom of the probe at the\n\/\/ bottom of the object so that it captures the real-world surface underneath.\nlet verticalOffset = SIMD3<Float>(0, extent.y, 0)\nlet transform = float4x4(translation: object.simdPosition + verticalOffset)\nextent.y *= 2\n\n\/\/ Create the new environment probe anchor and add it to the session.\nlet probeAnchor = AREnvironmentProbeAnchor(transform: transform, extent: extent)\nsceneView.session.add(anchor: probeAnchor)\n```\n\nThis code applies the rules below to optimally capture the area around each virtual object:\n\n- The probe’s position should be at the top center of the virtual object, and the `y` component of its extent should be twice the height of the object. This ensures that the bottom of the probe extent aligns with the bottom of the virtual object, accurately capturing the real surface the object sits on.\n- The `x` and `z` components of the probe’s extent should be three times the width and depth of the object, ensuring that the probe captures the area beneath and around the object.\n\n\n\n## Use environment texturing wisely\n\nFollow these tips to keep your app’s use of environment texturing realistic and efficient:\n\n**Avoid virtual content that requires accurate reflections, such as mirror-finish surfaces.**\n\nIn general, an AR experience doesn’t have all the information needed to produce a perfect imitation of reality. Good AR experiences carefully design content to hide limitations in realism, preserving the illusion that virtual objects inhabit the user’s real-world surroundings.\n\nARKit environment textures don’t image the environment in all directions around the user, and don’t update in real time, so some kinds of content aren’t well suited for use in AR. For example, a user encountering a virtual mirror may expect to see their own reflection. Design virtual content to use fully-reflective surfaces only in small or highly-detailed parts, and use less reflectivity in large flat surfaces.\n\n**Handle moving objects.**\n\nRendering a virtual object with realistic reflections require an environment probe that captures a small area around that object. If the object changes position, the corresponding environment probe needs to change to reflect the object’s new surroundings. When manually placing probes, consider one or more of these strategies for handling objects that move:\n\n- If the path of an object’s movement is known ahead of time, create multiple environment probe anchors and place them along that path.\n- Create a global environment probe with an extra-large extent to fall back to when rendering objects that have moved outside the extent of nearby probes.\n- After an object moves, create a new probe to capture the area around its new position, and remove environment probes associated with earlier positions.\n\nWhen you display AR content with [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNView], SceneKit automatically interpolates between environment textures for any objects that overlap the extents of multiple environment probes.\n\n**Don’t generate environment textures too often.**\n\nARKit requires some time to collect camera imagery, and combining and extrapolating that imagery to produce environment textures requires computational resources. Frequently adding new [doc:\/\/com.apple.arkit\/documentation\/ARKit\/AREnvironmentProbeAnchor] instances to your AR session may not produce noticeable changes in the displayed scene, but does cost battery power and reduce the performance overhead available for other aspects of your AR experience.\n\nThis app creates new environment probes whenever the user moves or resizes the virtual object, but limits such updates to occur no more often than once per second. (See the sample `updateEnvironmentProbe(atTime:)` function.)\n\n**Avoid abrupt transitions between different environment textures.**\n\nWith [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNView], if the environment probe texture(s) affecting an object change (either because the object moves or because a new texture becomes available for its current position), SceneKit automatically uses a short fade-in animation to transition to the new result. Depending on what environment textures are in use before and after the transition, that change may be jarring to the user.\n\nTo avoid unrealistic transitions, this sample app waits until the first environment texture becomes available before allowing the user to place virtual content. In automatic mode, as soon as the session begins, ARKit automatically begins generating a fallback environment texture covering a large area. In manual mode, the app creates its own fallback environment probe. (See the sample `updateSceneEnvironmentProbe(for:)` function.) Waiting until this environment texture is available ensures that virtual objects always reflect an environment appropriate to the session.\n\nAlternatively, your app may include a static environment-map texture for use as a fallback when environment texturing is not available (for example, to support earlier iOS versions). In this case, try to design or select a texture that appears realistic in a wide variety of situations.\n\n## Lighting Effects\n\n- **AREnvironmentProbeAnchor**: An object that provides environmental lighting information for a specific area of space in a world-tracking AR session.\n- **ARLightEstimate**: Estimated scene lighting information associated with a captured video frame in an AR session.\n- **ARDirectionalLightEstimate**: Estimated environmental lighting information associated with a captured video frame in a face-tracking AR session.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "An object that provides environmental lighting information for a specific area of space in a world-tracking AR session.",
          "name" : "AREnvironmentProbeAnchor",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/AREnvironmentProbeAnchor"
        },
        {
          "description" : "Estimated scene lighting information associated with a captured video frame in an AR session.",
          "name" : "ARLightEstimate",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARLightEstimate"
        },
        {
          "description" : "Estimated environmental lighting information associated with a captured video frame in a face-tracking AR session.",
          "name" : "ARDirectionalLightEstimate",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARDirectionalLightEstimate"
        }
      ],
      "title" : "Lighting Effects"
    }
  ],
  "source" : "appleJSON",
  "title" : "Adding realistic reflections to an AR experience",
  "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/adding-realistic-reflections-to-an-ar-experience"
}