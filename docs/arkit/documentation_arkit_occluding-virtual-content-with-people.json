{
  "abstract" : "Cover your app’s virtual content with people that ARKit perceives in the camera feed.",
  "codeExamples" : [
    {
      "code" : "guard ARWorldTrackingConfiguration.supportsFrameSemantics(.personSegmentationWithDepth) else {\n    fatalError(\"People occlusion is not supported on this device.\")\n}",
      "language" : "swift"
    },
    {
      "code" : "config.frameSemantics.insert(.personSegmentationWithDepth)",
      "language" : "swift"
    },
    {
      "code" : "arView.session.run(config)",
      "language" : "swift"
    },
    {
      "code" : "config.frameSemantics.remove(.personSegmentationWithDepth)",
      "language" : "swift"
    },
    {
      "code" : "arView.session.run(config)",
      "language" : "swift"
    }
  ],
  "contentHash" : "8d50930192c3f2876fe8470cb23d8a7b1a5be1e637dcbb58de04c0b5e984e2b9",
  "crawledAt" : "2025-12-02T15:29:19Z",
  "id" : "C3C32D6F-C353-4AB0-9C79-92EDB7AFFB89",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "ARKit",
  "overview" : "## Overview\n\nBy default, virtual content covers anything in the camera feed. For example, when a person passes in front of a virtual object, the object is drawn on top of the person, which can break the illusion of the AR experience.\n\n\n\nTo cover your app’s virtual content with people that ARKit perceives in the camera feed, you enable *people occlusion*. Your app can then render a virtual object behind people who pass in front of the camera. ARKit accomplishes the occlusion by identifying regions in the camera feed where people reside, and preventing virtual content from drawing into that region’s pixels.\n\n\n\nThis sample renders its graphics using RealityKit, but you can follow the same steps to use people occlusion with SceneKit. To enable people occlusion in Metal apps, see [doc:\/\/com.apple.arkit\/documentation\/ARKit\/effecting-people-occlusion-in-custom-renderers].\n\n## Verify device support for people occlusion\n\nPeople occlusion is supported on Apple A12 and later devices. Before attempting to enable people occlusion, verify that the user’s device supports it.\n\n## Enable people occlusion\n\nIf the user’s device supports people occlusion, enable it by adding the\n\narconfiguration\/framesemantics\/3194576-personsegmentationwithdepth [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/FrameSemantics-swift.struct\/personSegmentationWithDepth] option to your configuration’s frame semantics.\n\nAny time you change your session’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSession\/configuration], rerun the session to effect the configuration change.\n\nThe [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/FrameSemantics-swift.struct\/personSegmentationWithDepth] option specifies that a person occludes a virtual object only when the person is closer to the camera than the virtual object.\n\n\n\nAlternatively, the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/FrameSemantics-swift.struct\/personSegmentation] frame semantic gives you the option of always occluding virtual content with any people that ARKit perceives in the camera feed irrespective of depth. This technique is useful, for example, in green-screen scenarios.\n\n\n\n## Disable people occlusion\n\nYou might choose to disable people occlusion for performance reasons if, for example, no virtual content is present in the scene, or if the device has reached a serious or critical [doc:\/\/com.apple.documentation\/documentation\/Foundation\/ProcessInfo\/thermalState-swift.property] (see [doc:\/\/com.apple.documentation\/documentation\/Foundation\/ProcessInfo\/ThermalState-swift.enum]). To temporarily disable people occlusion, remove that option from your app’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/frameSemantics-swift.property].\n\nThen, rerun your session to effect the configuration change.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/ARKit\/occluding-virtual-content-with-people\ncrawled: 2025-12-02T15:29:19Z\n---\n\n# Occluding virtual content with people\n\n**Sample Code**\n\nCover your app’s virtual content with people that ARKit perceives in the camera feed.\n\n## Overview\n\nBy default, virtual content covers anything in the camera feed. For example, when a person passes in front of a virtual object, the object is drawn on top of the person, which can break the illusion of the AR experience.\n\n\n\nTo cover your app’s virtual content with people that ARKit perceives in the camera feed, you enable *people occlusion*. Your app can then render a virtual object behind people who pass in front of the camera. ARKit accomplishes the occlusion by identifying regions in the camera feed where people reside, and preventing virtual content from drawing into that region’s pixels.\n\n\n\nThis sample renders its graphics using RealityKit, but you can follow the same steps to use people occlusion with SceneKit. To enable people occlusion in Metal apps, see [doc:\/\/com.apple.arkit\/documentation\/ARKit\/effecting-people-occlusion-in-custom-renderers].\n\n## Verify device support for people occlusion\n\nPeople occlusion is supported on Apple A12 and later devices. Before attempting to enable people occlusion, verify that the user’s device supports it.\n\n```swift\nguard ARWorldTrackingConfiguration.supportsFrameSemantics(.personSegmentationWithDepth) else {\n    fatalError(\"People occlusion is not supported on this device.\")\n}\n```\n\n\n\n## Enable people occlusion\n\nIf the user’s device supports people occlusion, enable it by adding the\n\narconfiguration\/framesemantics\/3194576-personsegmentationwithdepth [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/FrameSemantics-swift.struct\/personSegmentationWithDepth] option to your configuration’s frame semantics.\n\n```swift\nconfig.frameSemantics.insert(.personSegmentationWithDepth)\n```\n\nAny time you change your session’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSession\/configuration], rerun the session to effect the configuration change.\n\n```swift\narView.session.run(config)\n```\n\nThe [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/FrameSemantics-swift.struct\/personSegmentationWithDepth] option specifies that a person occludes a virtual object only when the person is closer to the camera than the virtual object.\n\n\n\nAlternatively, the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/FrameSemantics-swift.struct\/personSegmentation] frame semantic gives you the option of always occluding virtual content with any people that ARKit perceives in the camera feed irrespective of depth. This technique is useful, for example, in green-screen scenarios.\n\n\n\n## Disable people occlusion\n\nYou might choose to disable people occlusion for performance reasons if, for example, no virtual content is present in the scene, or if the device has reached a serious or critical [doc:\/\/com.apple.documentation\/documentation\/Foundation\/ProcessInfo\/thermalState-swift.property] (see [doc:\/\/com.apple.documentation\/documentation\/Foundation\/ProcessInfo\/ThermalState-swift.enum]). To temporarily disable people occlusion, remove that option from your app’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/frameSemantics-swift.property].\n\n```swift\nconfig.frameSemantics.remove(.personSegmentationWithDepth)\n```\n\nThen, rerun your session to effect the configuration change.\n\n```swift\narView.session.run(config)\n```\n\n## Occlusion\n\n- **Effecting People Occlusion in Custom Renderers**: Occlude your app’s virtual content where ARKit recognizes people in the camera feed by using matte generator.\n- **Visualizing and interacting with a reconstructed scene**: Estimate the shape of the physical environment using a polygonal mesh.\n- **ARMatteGenerator**: An object that creates matte textures you use to occlude your app’s virtual content with people, that ARKit recognizes in the camera feed.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Occlude your app’s virtual content where ARKit recognizes people in the camera feed by using matte generator.",
          "name" : "Effecting People Occlusion in Custom Renderers",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/effecting-people-occlusion-in-custom-renderers"
        },
        {
          "description" : "Estimate the shape of the physical environment using a polygonal mesh.",
          "name" : "Visualizing and interacting with a reconstructed scene",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/visualizing-and-interacting-with-a-reconstructed-scene"
        },
        {
          "description" : "An object that creates matte textures you use to occlude your app’s virtual content with people, that ARKit recognizes in the camera feed.",
          "name" : "ARMatteGenerator",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARMatteGenerator"
        }
      ],
      "title" : "Occlusion"
    }
  ],
  "source" : "appleJSON",
  "title" : "Occluding virtual content with people",
  "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/occluding-virtual-content-with-people"
}