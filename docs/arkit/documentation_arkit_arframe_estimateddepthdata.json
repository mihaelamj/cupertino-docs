{
  "abstract" : "A buffer that represents the estimated depth values from the camera feed that you use to occlude virtual content.",
  "codeExamples" : [

  ],
  "contentHash" : "1aa5d253f8911b6055a8777bb90bc13af73e01d0e4c2ac3f0d85816bccbe2812",
  "crawledAt" : "2025-12-02T18:34:23Z",
  "declaration" : {
    "code" : "var estimatedDepthData: CVPixelBuffer? { get }",
    "language" : "swift"
  },
  "id" : "6EED2FA8-5E17-4C33-82C6-08534DDDEA9D",
  "kind" : "property",
  "language" : "swift",
  "module" : "ARKit",
  "overview" : "## Discussion\n\nEach non-background pixel in the segmentation buffer maps to a depth value in this buffer. Use this to occlude people from the rendering of your app’s virtual content.\n\nIf you implement a custom renderer, you apply this property by using alpha and depth mattes provided with [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARMatteGenerator].\n\nApps using one of the standard renderers don’t need this property to occlude virtual content with people. The standard renderers ([doc:\/\/com.apple.documentation\/documentation\/RealityKit\/ARView], [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNView], and [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSKView]) enable people occlusion when you add [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/FrameSemantics-swift.struct\/personSegmentation] or [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/FrameSemantics-swift.struct\/personSegmentationWithDepth] to your configuration’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/frameSemantics-swift.property].",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/estimatedDepthData\ncrawled: 2025-12-02T18:34:23Z\n---\n\n# estimatedDepthData\n\n**Instance Property**\n\nA buffer that represents the estimated depth values from the camera feed that you use to occlude virtual content.\n\n## Declaration\n\n```swift\nvar estimatedDepthData: CVPixelBuffer? { get }\n```\n\n## Discussion\n\nEach non-background pixel in the segmentation buffer maps to a depth value in this buffer. Use this to occlude people from the rendering of your app’s virtual content.\n\nIf you implement a custom renderer, you apply this property by using alpha and depth mattes provided with [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARMatteGenerator].\n\nApps using one of the standard renderers don’t need this property to occlude virtual content with people. The standard renderers ([doc:\/\/com.apple.documentation\/documentation\/RealityKit\/ARView], [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNView], and [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSKView]) enable people occlusion when you add [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/FrameSemantics-swift.struct\/personSegmentation] or [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/FrameSemantics-swift.struct\/personSegmentationWithDepth] to your configuration’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/frameSemantics-swift.property].\n\n## Checking for people\n\n- **detectedBody**: The screen position information of a body that ARKit recognizes in the camera image.\n- **ARBody2D**: The screen-space representation of a person ARKit recognizes in the camera feed.\n- **segmentationBuffer**: A buffer that contains pixel information identifying the shape of objects from the camera feed that you use to occlude virtual content.\n- **ARFrame.SegmentationClass**: A categorization of a pixel that defines a type of content you use to occlude your app’s virtual content.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "The screen position information of a body that ARKit recognizes in the camera image.",
          "name" : "detectedBody",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/detectedBody"
        },
        {
          "description" : "The screen-space representation of a person ARKit recognizes in the camera feed.",
          "name" : "ARBody2D",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARBody2D"
        },
        {
          "description" : "A buffer that contains pixel information identifying the shape of objects from the camera feed that you use to occlude virtual content.",
          "name" : "segmentationBuffer",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/segmentationBuffer"
        },
        {
          "description" : "A categorization of a pixel that defines a type of content you use to occlude your app’s virtual content.",
          "name" : "ARFrame.SegmentationClass",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/SegmentationClass"
        }
      ],
      "title" : "Checking for people"
    }
  ],
  "source" : "appleJSON",
  "title" : "estimatedDepthData",
  "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/estimatedDepthData"
}