{
  "abstract" : "Record spatial features of real-world objects, then use the results to find those objects in the user’s environment and trigger AR content.",
  "codeExamples" : [
    {
      "code" : "let configuration = ARWorldTrackingConfiguration()\nguard let referenceObjects = ARReferenceObject.referenceObjects(inGroupNamed: \"gallery\", bundle: nil) else {\n    fatalError(\"Missing expected asset catalog resources.\")\n}\nconfiguration.detectionObjects = referenceObjects\nsceneView.session.run(configuration)",
      "language" : "swift"
    },
    {
      "code" : "func renderer(_ renderer: SCNSceneRenderer, didAdd node: SCNNode, for anchor: ARAnchor) {\n    if let objectAnchor = anchor as? ARObjectAnchor {\n        node.addChildNode(self.model)\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "let configuration = ARObjectScanningConfiguration()\nconfiguration.planeDetection = .horizontal\nsceneView.session.run(configuration, options: .resetTracking)",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Extract the reference object based on the position & orientation of the bounding box.\nsceneView.session.createReferenceObject(\n    transform: boundingBox.simdWorldTransform,\n    center: SIMD3<Float>(), extent: boundingBox.extent,\n    completionHandler: { object, error in\n        if let referenceObject = object {\n            \/\/ Adjust the object's origin with the user-provided transform.\n            self.scannedReferenceObject = referenceObject.applyingTransform(origin.simdTransform)\n            self.scannedReferenceObject!.name = self.scannedObject.scanName\n            \n            if let referenceObjectToMerge = ViewController.instance?.referenceObjectToMerge {\n                ViewController.instance?.referenceObjectToMerge = nil\n                \n                \/\/ Show activity indicator during the merge.\n                ViewController.instance?.showAlert(title: \"\", message: \"Merging previous scan into this scan...\", buttonTitle: nil)\n                \n                \/\/ Try to merge the object which was just scanned with the existing one.\n                self.scannedReferenceObject?.mergeInBackground(with: referenceObjectToMerge, completion: { (mergedObject, error) in\n\n                    if let mergedObject = mergedObject {\n                        self.scannedReferenceObject = mergedObject\n                        ViewController.instance?.showAlert(title: \"Merge successful\",\n                                                           message: \"The previous scan has been merged into this scan.\", buttonTitle: \"OK\")\n                        creationFinished(self.scannedReferenceObject)\n\n                    } else {\n                        print(\"Error: Failed to merge scans. \\(error?.localizedDescription ?? \"\")\")\n                        let message = \"\"\"\n                                Merging the previous scan into this scan failed. Please make sure that\n                                there is sufficient overlap between both scans and that the lighting\n                                environment hasn't changed drastically.\n                                Which scan do you want to use for testing?\n                                \"\"\"\n                        let thisScan = UIAlertAction(title: \"Use This Scan\", style: .default) { _ in\n                            creationFinished(self.scannedReferenceObject)\n                        }\n                        let previousScan = UIAlertAction(title: \"Use Previous Scan\", style: .default) { _ in\n                            self.scannedReferenceObject = referenceObjectToMerge\n                            creationFinished(self.scannedReferenceObject)\n                        }\n                        ViewController.instance?.showAlert(title: \"Merge failed\", message: message, actions: [thisScan, previousScan])\n                    }\n                })\n            } else {\n                creationFinished(self.scannedReferenceObject)\n            }\n        } else {\n            print(\"Error: Failed to create reference object. \\(error!.localizedDescription)\")\n            creationFinished(nil)\n        }\n    })",
      "language" : "swift"
    }
  ],
  "contentHash" : "d5b20c677777522e8c7d1efd9b153887084408cb8bb71c26e3e6723418205438",
  "crawledAt" : "2025-12-02T15:47:29Z",
  "id" : "DFDA58AB-4112-4305-A2AE-224F4C5EC82D",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "ARKit",
  "overview" : "## Overview\n\nOne way to build compelling AR experiences is to recognize features of the user’s environment and use them to trigger the appearance of virtual content. For example, a museum app might add interactive 3D visualizations when the user points their device at a displayed sculpture or artifact.\n\nIn iOS 12, you can create such AR experiences by enabling *object detection* in ARKit: Your app provides *reference objects*, which encode three-dimensional spatial features of known real-world objects, and ARKit tells your app when and where it detects the corresponding real-world objects during an AR session.\n\nThis sample code project provides multiple ways to make use of object detection:\n\n## Configure your physical environment to enhance object scanning\n\nSet up your physical environment according to the following guidelines. Use these recommendations as a target configuration even if it’s unreachable in the specific circumstances of your scanning environment. You can scan objects outside of these specifications if necessary, but they provide ARKit with the conditions most conducive to object scanning.\n\n## Scan real-world objects with an ios app\n\nThe programming steps to scan and define a reference object that ARKit can use for detection are simple. (See “Create a Reference Object in an AR Session” below.) However, the fidelity of the reference object you create, and thus your success at detecting that reference object in your own apps, depends on your physical interactions with the object when scanning. Build and run this app on your iOS device to walk through a series of steps for getting high-quality scan data, resulting in reference object files that you can use for detection in your own apps.\n\n\n\n## Detect reference objects in an ar experience\n\nYou can use an Xcode asset catalog to bundle reference objects in an app for use in detection:\n\nTo enable object detection in an AR session, load the reference objects you want to detect as [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARReferenceObject] instances, provide those objects for the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARWorldTrackingConfiguration\/detectionObjects] property of an [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARWorldTrackingConfiguration], and run an [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSession] with that configuration:\n\nWhen ARKit detects one of your reference objects, the session automatically adds a corresponding [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARObjectAnchor] to its list of anchors. To respond to an object being recognized, implement an appropriate [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSessionDelegate], [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSKViewDelegate], or [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNViewDelegate] method that reports the new anchor being added to the session. For example, in a SceneKit-based app you can implement [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNViewDelegate\/renderer(_:didAdd:for:)] to add a 3D asset to the scene, automatically matching the position and orientation of the anchor:\n\nFor best results with object scanning and detection, follow these tips:\n\n## Create a reference object in an AR session\n\nThis sample app provides one way to create reference objects. You can also scan reference objects in your own app—for example, to build asset management tools for defining AR content that goes into other apps you create.\n\nA reference object encodes a slice of the internal spatial-mapping data that ARKit uses to track a device’s position and orientation. To enable the high-quality data collection required for object scanning, run a session with [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARObjectScanningConfiguration]:\n\nDuring your object-scanning AR session, scan the object from various angles to make sure you collect enough spatial data to recognize it. (If you’re building your own object-scanning tools, help users walk through the same steps this sample app provides.)\n\nAfter scanning, call [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSession\/createReferenceObject(transform:center:extent:completionHandler:)] to produce an [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARReferenceObject] from a region of the user environment mapped by the session:\n\nWhen detecting a reference object, ARKit reports its position based on the origin the reference object defines. If you want to place virtual content that appears to sit on the same surface as the real-world object, make sure the reference object’s origin is placed at the point where the real-world object sits. To adjust the origin after capturing an [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARReferenceObject], use the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARReferenceObject\/applyingTransform(_:)] method.\n\nAfter you obtain an [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARReferenceObject], you can either use it immediately for detection (see “Detect Reference Objects in an AR Experience” above) or save it as an `.arobject` file for use in later sessions or other ARKit-based apps. To save an object to a file, use the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARReferenceObject\/export(to:previewImage:)] method. In that method, you can provide a picture of the real-world object for Xcode to use as a preview image.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/ARKit\/scanning-and-detecting-3d-objects\ncrawled: 2025-12-02T15:47:29Z\n---\n\n# Scanning and Detecting 3D Objects\n\n**Sample Code**\n\nRecord spatial features of real-world objects, then use the results to find those objects in the user’s environment and trigger AR content.\n\n## Overview\n\nOne way to build compelling AR experiences is to recognize features of the user’s environment and use them to trigger the appearance of virtual content. For example, a museum app might add interactive 3D visualizations when the user points their device at a displayed sculpture or artifact.\n\nIn iOS 12, you can create such AR experiences by enabling *object detection* in ARKit: Your app provides *reference objects*, which encode three-dimensional spatial features of known real-world objects, and ARKit tells your app when and where it detects the corresponding real-world objects during an AR session.\n\nThis sample code project provides multiple ways to make use of object detection:\n\n- Run the app to scan a real-world object and export a reference object file, which you can use in your own apps to detect that object.\n- Use the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARObjectScanningConfiguration]and [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARReferenceObject] classes as demonstrated in this sample app to record reference objects as part of your own asset production pipeline.\n- Use [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARWorldTrackingConfiguration\/detectionObjects] in a world-tracking AR session to recognize a reference object and create AR interactions.\n\n\n\n## Configure your physical environment to enhance object scanning\n\nSet up your physical environment according to the following guidelines. Use these recommendations as a target configuration even if it’s unreachable in the specific circumstances of your scanning environment. You can scan objects outside of these specifications if necessary, but they provide ARKit with the conditions most conducive to object scanning.\n\n- Light the object with an illuminance of 250 to 400 lux, and ensure that it’s well-lit from all sides.\n- Provide a light temperature of around ~6500 Kelvin (D65)––similar with daylight. Avoid warm or any other colored light sources.\n- Set the object in front of a matte, middle gray background.\n\n## Scan real-world objects with an ios app\n\nThe programming steps to scan and define a reference object that ARKit can use for detection are simple. (See “Create a Reference Object in an AR Session” below.) However, the fidelity of the reference object you create, and thus your success at detecting that reference object in your own apps, depends on your physical interactions with the object when scanning. Build and run this app on your iOS device to walk through a series of steps for getting high-quality scan data, resulting in reference object files that you can use for detection in your own apps.\n\n\n\n1. **Choose an iOS Device.** For easy object scanning, use a recent, high-performance iOS device. Scanned objects can be detected on any ARKit-supported device, but the process of creating a high-quality scan is faster and smoother on a high-performance device.\n2. **Position the object.** When first run, the app displays a box that roughly estimates the size of whatever real-world objects appear centered in the camera view. Position the object you want to scan on a surface free of other objects (like an empty tabletop). Then move your device so that the object appears centered in the box, and tap the Next button.\n3. **Define bounding box.** Before scanning, you need to tell the app what region of the world contains the object you want to scan. Drag to move the box around in 3D, or press and hold on a side of the box and then drag to resize it. (Or, if you leave the box untouched, you can move around the object and the app will attempt to automatically fit a box around it.) Make sure the bounding box contains only features of the object you want to scan (not those from the environment it’s in), then tap the Scan button.\n4. **Scan the object.** Move around to look at the object from different angles. For best results, move slowly and avoid abrupt motions. The app highlights parts of the bounding box to indicate when you’ve scanned enough to recognize the object from the corresponding direction. Be sure to scan on all sides from which you want users of your app to be able to recognize the object. The app automatically proceeds to the next step when a scan is complete, or you can tap the Stop button to proceed manually.\n5. **Adjust origin.** The app displays x, y, and z coordinate axis lines showing the object’s anchor point, or *origin*. Drag the circles to move the origin relative to the object. In this step you can also use the Add (+) button to load a 3D model in USDZ format. The app displays the model as it would appear in AR upon detecting the real-world object, and uses the model’s size to adjust the scale of the reference object. Tap the Test button when done.\n6. **Test and export.** The app has now created an [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARReferenceObject] and has reconfigured its session to detect it. Look at the real-world object from different angles, in various environments and lighting conditions, to verify that ARKit reliably recognizes its position and orientation. Tap the Export button to open a share sheet for saving the finished `.arobject` file. For example, you can easily send it to your development Mac using AirDrop, or send it to the Files app to save it to iCloud Drive.\n\n\n\n## Detect reference objects in an ar experience\n\nYou can use an Xcode asset catalog to bundle reference objects in an app for use in detection:\n\n1. Open your project’s asset catalog, then use the Add button (+) to add a new AR resource group.\n2. Drag `.arobject` files from the Finder into the newly created resource group.\n3. Optionally, for each reference object, use the inspector to provide a descriptive name for your own use.\n\n\n\nTo enable object detection in an AR session, load the reference objects you want to detect as [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARReferenceObject] instances, provide those objects for the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARWorldTrackingConfiguration\/detectionObjects] property of an [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARWorldTrackingConfiguration], and run an [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSession] with that configuration:\n\n```swift\nlet configuration = ARWorldTrackingConfiguration()\nguard let referenceObjects = ARReferenceObject.referenceObjects(inGroupNamed: \"gallery\", bundle: nil) else {\n    fatalError(\"Missing expected asset catalog resources.\")\n}\nconfiguration.detectionObjects = referenceObjects\nsceneView.session.run(configuration)\n```\n\nWhen ARKit detects one of your reference objects, the session automatically adds a corresponding [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARObjectAnchor] to its list of anchors. To respond to an object being recognized, implement an appropriate [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSessionDelegate], [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSKViewDelegate], or [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNViewDelegate] method that reports the new anchor being added to the session. For example, in a SceneKit-based app you can implement [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNViewDelegate\/renderer(_:didAdd:for:)] to add a 3D asset to the scene, automatically matching the position and orientation of the anchor:\n\n```swift\nfunc renderer(_ renderer: SCNSceneRenderer, didAdd node: SCNNode, for anchor: ARAnchor) {\n    if let objectAnchor = anchor as? ARObjectAnchor {\n        node.addChildNode(self.model)\n    }\n}\n```\n\nFor best results with object scanning and detection, follow these tips:\n\n- ARKit looks for areas of clear, stable visual detail when scanning and detecting objects. Detailed, textured objects work better for detection than plain or reflective objects.\n- Object scanning and detection is optimized for objects small enough to fit on a tabletop.\n- An object to be detected must have the same shape as the scanned reference object. Rigid objects work better for detection than soft bodies or items that bend, twist, fold, or otherwise change shape.\n- Detection works best when the lighting conditions for the real-world object to be detected are similar to those in which the original object was scanned. Consistent indoor lighting works best.\n\n## Create a reference object in an AR session\n\nThis sample app provides one way to create reference objects. You can also scan reference objects in your own app—for example, to build asset management tools for defining AR content that goes into other apps you create.\n\nA reference object encodes a slice of the internal spatial-mapping data that ARKit uses to track a device’s position and orientation. To enable the high-quality data collection required for object scanning, run a session with [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARObjectScanningConfiguration]:\n\n```swift\nlet configuration = ARObjectScanningConfiguration()\nconfiguration.planeDetection = .horizontal\nsceneView.session.run(configuration, options: .resetTracking)\n```\n\nDuring your object-scanning AR session, scan the object from various angles to make sure you collect enough spatial data to recognize it. (If you’re building your own object-scanning tools, help users walk through the same steps this sample app provides.)\n\nAfter scanning, call [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSession\/createReferenceObject(transform:center:extent:completionHandler:)] to produce an [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARReferenceObject] from a region of the user environment mapped by the session:\n\n```swift\n\/\/ Extract the reference object based on the position & orientation of the bounding box.\nsceneView.session.createReferenceObject(\n    transform: boundingBox.simdWorldTransform,\n    center: SIMD3<Float>(), extent: boundingBox.extent,\n    completionHandler: { object, error in\n        if let referenceObject = object {\n            \/\/ Adjust the object's origin with the user-provided transform.\n            self.scannedReferenceObject = referenceObject.applyingTransform(origin.simdTransform)\n            self.scannedReferenceObject!.name = self.scannedObject.scanName\n            \n            if let referenceObjectToMerge = ViewController.instance?.referenceObjectToMerge {\n                ViewController.instance?.referenceObjectToMerge = nil\n                \n                \/\/ Show activity indicator during the merge.\n                ViewController.instance?.showAlert(title: \"\", message: \"Merging previous scan into this scan...\", buttonTitle: nil)\n                \n                \/\/ Try to merge the object which was just scanned with the existing one.\n                self.scannedReferenceObject?.mergeInBackground(with: referenceObjectToMerge, completion: { (mergedObject, error) in\n\n                    if let mergedObject = mergedObject {\n                        self.scannedReferenceObject = mergedObject\n                        ViewController.instance?.showAlert(title: \"Merge successful\",\n                                                           message: \"The previous scan has been merged into this scan.\", buttonTitle: \"OK\")\n                        creationFinished(self.scannedReferenceObject)\n\n                    } else {\n                        print(\"Error: Failed to merge scans. \\(error?.localizedDescription ?? \"\")\")\n                        let message = \"\"\"\n                                Merging the previous scan into this scan failed. Please make sure that\n                                there is sufficient overlap between both scans and that the lighting\n                                environment hasn't changed drastically.\n                                Which scan do you want to use for testing?\n                                \"\"\"\n                        let thisScan = UIAlertAction(title: \"Use This Scan\", style: .default) { _ in\n                            creationFinished(self.scannedReferenceObject)\n                        }\n                        let previousScan = UIAlertAction(title: \"Use Previous Scan\", style: .default) { _ in\n                            self.scannedReferenceObject = referenceObjectToMerge\n                            creationFinished(self.scannedReferenceObject)\n                        }\n                        ViewController.instance?.showAlert(title: \"Merge failed\", message: message, actions: [thisScan, previousScan])\n                    }\n                })\n            } else {\n                creationFinished(self.scannedReferenceObject)\n            }\n        } else {\n            print(\"Error: Failed to create reference object. \\(error!.localizedDescription)\")\n            creationFinished(nil)\n        }\n    })\n```\n\nWhen detecting a reference object, ARKit reports its position based on the origin the reference object defines. If you want to place virtual content that appears to sit on the same surface as the real-world object, make sure the reference object’s origin is placed at the point where the real-world object sits. To adjust the origin after capturing an [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARReferenceObject], use the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARReferenceObject\/applyingTransform(_:)] method.\n\nAfter you obtain an [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARReferenceObject], you can either use it immediately for detection (see “Detect Reference Objects in an AR Experience” above) or save it as an `.arobject` file for use in later sessions or other ARKit-based apps. To save an object to a file, use the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARReferenceObject\/export(to:previewImage:)] method. In that method, you can provide a picture of the real-world object for Xcode to use as a preview image.\n\n## Physical Objects\n\n- **Visualizing and interacting with a reconstructed scene**: Estimate the shape of the physical environment using a polygonal mesh.\n- **ARObjectAnchor**: An anchor for a real-world 3D object that ARKit detects in the physical environment.\n- **ARReferenceObject**: The description of a 3D object that you want ARKit to detect in the physical environment.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Estimate the shape of the physical environment using a polygonal mesh.",
          "name" : "Visualizing and interacting with a reconstructed scene",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/visualizing-and-interacting-with-a-reconstructed-scene"
        },
        {
          "description" : "An anchor for a real-world 3D object that ARKit detects in the physical environment.",
          "name" : "ARObjectAnchor",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARObjectAnchor"
        },
        {
          "description" : "The description of a 3D object that you want ARKit to detect in the physical environment.",
          "name" : "ARReferenceObject",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARReferenceObject"
        }
      ],
      "title" : "Physical Objects"
    }
  ],
  "source" : "appleJSON",
  "title" : "Scanning and Detecting 3D Objects",
  "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/scanning-and-detecting-3d-objects"
}