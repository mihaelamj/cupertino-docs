{
  "abstract" : "The current intermediate results of the scene analysis ARKit uses to perform world tracking.",
  "codeExamples" : [

  ],
  "contentHash" : "cefd59dbc2ee3172e58dfb81acd7d8977a7f8953a70259b70f99b83300569703",
  "crawledAt" : "2025-12-02T18:52:53Z",
  "declaration" : {
    "code" : "var rawFeaturePoints: ARPointCloud? { get }",
    "language" : "swift"
  },
  "id" : "B8A10239-AA3D-483E-8CDD-B9613C17BBB0",
  "kind" : "property",
  "language" : "swift",
  "module" : "ARKit",
  "overview" : "## Discussion\n\nThese points represent notable features detected in the camera image. Their positions in 3D world coordinate space are extrapolated as part of the image analysis that ARKit performs in order to accurately track the device’s position, orientation, and movement. Taken together, these points loosely correlate to the contours of real-world objects in view of the camera.\n\nARKit does not guarantee that the number and arrangement of raw feature points will remain stable between software releases, or even between subsequent frames in the same session. Regardless, the point cloud can sometimes prove useful when debugging your app’s placement of virtual objects into the real-world scene.\n\nIf you display AR content with SceneKit using the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNView] class, you can display this point cloud with the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNDebugOptionShowFeaturePoints] debug option.\n\nFeature point detection requires a [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARWorldTrackingConfiguration] session.",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/rawFeaturePoints\ncrawled: 2025-12-02T18:52:53Z\n---\n\n# rawFeaturePoints\n\n**Instance Property**\n\nThe current intermediate results of the scene analysis ARKit uses to perform world tracking.\n\n## Declaration\n\n```swift\nvar rawFeaturePoints: ARPointCloud? { get }\n```\n\n## Discussion\n\nThese points represent notable features detected in the camera image. Their positions in 3D world coordinate space are extrapolated as part of the image analysis that ARKit performs in order to accurately track the device’s position, orientation, and movement. Taken together, these points loosely correlate to the contours of real-world objects in view of the camera.\n\nARKit does not guarantee that the number and arrangement of raw feature points will remain stable between software releases, or even between subsequent frames in the same session. Regardless, the point cloud can sometimes prove useful when debugging your app’s placement of virtual objects into the real-world scene.\n\nIf you display AR content with SceneKit using the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNView] class, you can display this point cloud with the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNDebugOptionShowFeaturePoints] debug option.\n\nFeature point detection requires a [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARWorldTrackingConfiguration] session.\n\n## Accessing scene data\n\n- **lightEstimate**: An estimate of lighting conditions based on the camera image.\n- **displayTransform(for:viewportSize:)**: Returns an affine transform for converting between normalized image coordinates and a coordinate space appropriate for rendering the camera image onscreen.\n- **capturedDepthData**: Depth data captured in front-camera experiences.\n- **capturedDepthDataTimestamp**: The time at which depth data for the frame (if any) was captured.\n- **sceneDepth**: Data on the distance between a device’s rear camera and real-world objects in an AR experience.\n- **smoothedSceneDepth**: An average of distance measurements between a device’s rear camera and real-world objects that creates smoother visuals in an AR experience.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "An estimate of lighting conditions based on the camera image.",
          "name" : "lightEstimate",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/lightEstimate"
        },
        {
          "description" : "Returns an affine transform for converting between normalized image coordinates and a coordinate space appropriate for rendering the camera image onscreen.",
          "name" : "displayTransform(for:viewportSize:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/displayTransform(for:viewportSize:)"
        },
        {
          "description" : "Depth data captured in front-camera experiences.",
          "name" : "capturedDepthData",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/capturedDepthData"
        },
        {
          "description" : "The time at which depth data for the frame (if any) was captured.",
          "name" : "capturedDepthDataTimestamp",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/capturedDepthDataTimestamp"
        },
        {
          "description" : "Data on the distance between a device’s rear camera and real-world objects in an AR experience.",
          "name" : "sceneDepth",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/sceneDepth"
        },
        {
          "description" : "An average of distance measurements between a device’s rear camera and real-world objects that creates smoother visuals in an AR experience.",
          "name" : "smoothedSceneDepth",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/smoothedSceneDepth"
        }
      ],
      "title" : "Accessing scene data"
    }
  ],
  "source" : "appleJSON",
  "title" : "rawFeaturePoints",
  "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/rawFeaturePoints"
}