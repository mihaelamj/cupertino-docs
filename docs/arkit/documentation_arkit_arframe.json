{
  "abstract" : "A video image captured as part of a session with position-tracking information.",
  "codeExamples" : [

  ],
  "conformsTo" : [
    "CVarArg",
    "CustomDebugStringConvertible",
    "CustomStringConvertible",
    "Equatable",
    "Hashable",
    "NSCopying",
    "NSObjectProtocol",
    "Sendable",
    "SendableMetatype"
  ],
  "contentHash" : "e2949ce85ab787fc85934d647d785907773532a7719c6bac5252a97ad58aa80e",
  "crawledAt" : "2025-12-04T20:51:30Z",
  "declaration" : {
    "code" : "class ARFrame",
    "language" : "swift"
  },
  "id" : "6E1BA12F-784B-4ABA-AEE7-E01278C4474F",
  "kind" : "class",
  "language" : "swift",
  "module" : "ARKit",
  "overview" : "## Overview\n\nA running session continuously captures video frames from the device’s camera while ARKit analyzes the captures to determine the user’s position in the world. ARKit can provide this information to you in the form of an [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFrame] in two ways:\n\nTo automatically receive all frames as ARKit captures them, make one of your objects the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSession\/delegate] of your app’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSession].\n\nEach frame can contain additional data, for example, EXIF ([doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFrame\/exifData]), or data based on any particular [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/frameSemantics-swift.property] that you enable.",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/arkit\/arframe\ncrawled: 2025-12-04T20:51:30Z\n---\n\n# ARFrame\n\n**Class**\n\nA video image captured as part of a session with position-tracking information.\n\n## Declaration\n\n```swift\nclass ARFrame\n```\n\n## Overview\n\nA running session continuously captures video frames from the device’s camera while ARKit analyzes the captures to determine the user’s position in the world. ARKit can provide this information to you in the form of an [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFrame] in two ways:\n\n- Occasionally, by accessing an [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSession] object’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSession\/currentFrame]\n- Constantly, as a stream of frames through the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSessionDelegate\/session(_:didUpdate:)-9v2kw] callback\n\nTo automatically receive all frames as ARKit captures them, make one of your objects the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSession\/delegate] of your app’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSession].\n\nEach frame can contain additional data, for example, EXIF ([doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFrame\/exifData]), or data based on any particular [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARConfiguration\/frameSemantics-swift.property] that you enable.\n\n## Accessing camera data\n\n- **camera**: Information about the camera position, orientation, and imaging parameters used to capture the frame.\n- **capturedImage**: A pixel buffer containing the image captured by the camera.\n- **timestamp**: The time at which the frame was captured.\n- **cameraGrainIntensity**: A value that specifies the amount of grain present in the camera grain texture.\n- **cameraGrainTexture**: A tileable Metal texture created by ARKit to match the visual characteristics of the current video stream.\n- **exifData**: Auxiliary data for the captured image.\n\n## Accessing scene data\n\n- **lightEstimate**: An estimate of lighting conditions based on the camera image.\n- **displayTransform(for:viewportSize:)**: Returns an affine transform for converting between normalized image coordinates and a coordinate space appropriate for rendering the camera image onscreen.\n- **rawFeaturePoints**: The current intermediate results of the scene analysis ARKit uses to perform world tracking.\n- **capturedDepthData**: Depth data captured in front-camera experiences.\n- **capturedDepthDataTimestamp**: The time at which depth data for the frame (if any) was captured.\n- **sceneDepth**: Data on the distance between a device’s rear camera and real-world objects in an AR experience.\n- **smoothedSceneDepth**: An average of distance measurements between a device’s rear camera and real-world objects that creates smoother visuals in an AR experience.\n\n## Tracking and interacting with the real world\n\n- **anchors**: The list of anchors representing positions tracked or objects detected in the scene.\n- **raycastQuery(from:allowing:alignment:)**: Get a ray-cast query for a screen point.\n- **hitTest(_:types:)**: Searches for real-world objects or AR anchors in the captured camera image.\n\n## Checking world-mapping status\n\n- **worldMappingStatus**: The feasibility of generating or relocalizing a world map for this frame.\n- **ARFrame.WorldMappingStatus**: A value describing the world mapping status for the area visible in a given frame.\n\n## Checking for people\n\n- **detectedBody**: The screen position information of a body that ARKit recognizes in the camera image.\n- **ARBody2D**: The screen-space representation of a person ARKit recognizes in the camera feed.\n- **segmentationBuffer**: A buffer that contains pixel information identifying the shape of objects from the camera feed that you use to occlude virtual content.\n- **estimatedDepthData**: A buffer that represents the estimated depth values from the camera feed that you use to occlude virtual content.\n- **ARFrame.SegmentationClass**: A categorization of a pixel that defines a type of content you use to occlude your app’s virtual content.\n\n## Assessing geo-tracking condition\n\n- **geoTrackingStatus**: The session’s condition with respect to geographic tracking at the time the session captured the frame.\n- **ARGeoTrackingStatus**: The state, accuracy, and reason that are possible for geo-tracking’s current condition.\n\n## Accessing the camera frame\n\n- **currentFrame**: The most recent still frame captured by the active camera feed, including ARKit’s interpretation of it.\n- **captureHighResolutionFrame(completion:)**: Requests a frame outside of the normal frequency that contains a high-resolution captured image.\n\n## Inherits From\n\n- NSObject\n\n## Conforms To\n\n- CVarArg\n- CustomDebugStringConvertible\n- CustomStringConvertible\n- Equatable\n- Hashable\n- NSCopying\n- NSObjectProtocol\n- Sendable\n- SendableMetatype\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Information about the camera position, orientation, and imaging parameters used to capture the frame.",
          "name" : "camera",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/camera"
        },
        {
          "description" : "A pixel buffer containing the image captured by the camera.",
          "name" : "capturedImage",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/capturedImage"
        },
        {
          "description" : "The time at which the frame was captured.",
          "name" : "timestamp",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/timestamp"
        },
        {
          "description" : "A value that specifies the amount of grain present in the camera grain texture.",
          "name" : "cameraGrainIntensity",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/cameraGrainIntensity"
        },
        {
          "description" : "A tileable Metal texture created by ARKit to match the visual characteristics of the current video stream.",
          "name" : "cameraGrainTexture",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/cameraGrainTexture"
        },
        {
          "description" : "Auxiliary data for the captured image.",
          "name" : "exifData",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/exifData"
        }
      ],
      "title" : "Accessing camera data"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "An estimate of lighting conditions based on the camera image.",
          "name" : "lightEstimate",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/lightEstimate"
        },
        {
          "description" : "Returns an affine transform for converting between normalized image coordinates and a coordinate space appropriate for rendering the camera image onscreen.",
          "name" : "displayTransform(for:viewportSize:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/displayTransform(for:viewportSize:)"
        },
        {
          "description" : "The current intermediate results of the scene analysis ARKit uses to perform world tracking.",
          "name" : "rawFeaturePoints",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/rawFeaturePoints"
        },
        {
          "description" : "Depth data captured in front-camera experiences.",
          "name" : "capturedDepthData",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/capturedDepthData"
        },
        {
          "description" : "The time at which depth data for the frame (if any) was captured.",
          "name" : "capturedDepthDataTimestamp",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/capturedDepthDataTimestamp"
        },
        {
          "description" : "Data on the distance between a device’s rear camera and real-world objects in an AR experience.",
          "name" : "sceneDepth",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/sceneDepth"
        },
        {
          "description" : "An average of distance measurements between a device’s rear camera and real-world objects that creates smoother visuals in an AR experience.",
          "name" : "smoothedSceneDepth",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/smoothedSceneDepth"
        }
      ],
      "title" : "Accessing scene data"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "The list of anchors representing positions tracked or objects detected in the scene.",
          "name" : "anchors",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/anchors"
        },
        {
          "description" : "Get a ray-cast query for a screen point.",
          "name" : "raycastQuery(from:allowing:alignment:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/raycastQuery(from:allowing:alignment:)"
        },
        {
          "description" : "Searches for real-world objects or AR anchors in the captured camera image.",
          "name" : "hitTest(_:types:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/hitTest(_:types:)"
        }
      ],
      "title" : "Tracking and interacting with the real world"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "The feasibility of generating or relocalizing a world map for this frame.",
          "name" : "worldMappingStatus",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/worldMappingStatus-swift.property"
        },
        {
          "description" : "A value describing the world mapping status for the area visible in a given frame.",
          "name" : "ARFrame.WorldMappingStatus",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/WorldMappingStatus-swift.enum"
        }
      ],
      "title" : "Checking world-mapping status"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "The screen position information of a body that ARKit recognizes in the camera image.",
          "name" : "detectedBody",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/detectedBody"
        },
        {
          "description" : "The screen-space representation of a person ARKit recognizes in the camera feed.",
          "name" : "ARBody2D",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARBody2D"
        },
        {
          "description" : "A buffer that contains pixel information identifying the shape of objects from the camera feed that you use to occlude virtual content.",
          "name" : "segmentationBuffer",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/segmentationBuffer"
        },
        {
          "description" : "A buffer that represents the estimated depth values from the camera feed that you use to occlude virtual content.",
          "name" : "estimatedDepthData",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/estimatedDepthData"
        },
        {
          "description" : "A categorization of a pixel that defines a type of content you use to occlude your app’s virtual content.",
          "name" : "ARFrame.SegmentationClass",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/SegmentationClass"
        }
      ],
      "title" : "Checking for people"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "The session’s condition with respect to geographic tracking at the time the session captured the frame.",
          "name" : "geoTrackingStatus",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/geoTrackingStatus"
        },
        {
          "description" : "The state, accuracy, and reason that are possible for geo-tracking’s current condition.",
          "name" : "ARGeoTrackingStatus",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARGeoTrackingStatus"
        }
      ],
      "title" : "Assessing geo-tracking condition"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "The most recent still frame captured by the active camera feed, including ARKit’s interpretation of it.",
          "name" : "currentFrame",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARSession\/currentFrame"
        },
        {
          "description" : "Requests a frame outside of the normal frequency that contains a high-resolution captured image.",
          "name" : "captureHighResolutionFrame(completion:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARSession\/captureHighResolutionFrame(completion:)"
        }
      ],
      "title" : "Accessing the camera frame"
    },
    {
      "content" : "",
      "items" : [
        {
          "name" : "NSObject"
        }
      ],
      "title" : "Inherits From"
    }
  ],
  "source" : "appleJSON",
  "title" : "ARFrame",
  "url" : "https:\/\/developer.apple.com\/documentation\/arkit\/arframe"
}