{
  "abstract" : "Determine the camera position and lighting for the current session, and apply effects, such as occlusion, to elements of the environment.",
  "codeExamples" : [

  ],
  "contentHash" : "78a55a624566d7ebd0069432ed3da8fa94da0763d637acf68b68caecc60872f7",
  "crawledAt" : "2025-12-03T15:48:13Z",
  "id" : "FE16BA00-04CC-4EB6-96FB-64EF3B0F38C2",
  "kind" : "collection",
  "language" : "swift",
  "module" : "ARKit",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/ARKit\/camera-lighting-and-effects\ncrawled: 2025-12-03T15:48:13Z\n---\n\n# Camera, Lighting, and Effects\n\n**API Collection**\n\nDetermine the camera position and lighting for the current session, and apply effects, such as occlusion, to elements of the environment.\n\n## Camera\n\n- **ARCamera**: Information about the camera position and imaging characteristics for a given frame.\n\n## Lighting Effects\n\n- **Adding realistic reflections to an AR experience**: Use ARKit to generate environment probe textures from camera imagery and render reflective virtual objects.\n- **AREnvironmentProbeAnchor**: An object that provides environmental lighting information for a specific area of space in a world-tracking AR session.\n- **ARLightEstimate**: Estimated scene lighting information associated with a captured video frame in an AR session.\n- **ARDirectionalLightEstimate**: Estimated environmental lighting information associated with a captured video frame in a face-tracking AR session.\n\n## Occlusion\n\n- **Occluding virtual content with people**: Cover your app’s virtual content with people that ARKit perceives in the camera feed.\n- **Effecting People Occlusion in Custom Renderers**: Occlude your app’s virtual content where ARKit recognizes people in the camera feed by using matte generator.\n- **Visualizing and interacting with a reconstructed scene**: Estimate the shape of the physical environment using a polygonal mesh.\n- **ARMatteGenerator**: An object that creates matte textures you use to occlude your app’s virtual content with people, that ARKit recognizes in the camera feed.\n\n## Virtual Content\n\n- **Content Anchors**: Identify items in the physical environment, including planar surfaces, images, physical objects, body positions, and faces.\n- **Environmental Analysis**: Analyze the video from the cameras and the accompanying data, and use ray-casting and depth-map information to determine the location of items.\n- **Data Management**: Obtain detailed information about skeletal and face geometry, and saved world data.\n- **Creating USD files for Apple devices**: Generate 3D assets that render as expected.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Information about the camera position and imaging characteristics for a given frame.",
          "name" : "ARCamera",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARCamera"
        }
      ],
      "title" : "Camera"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "Use ARKit to generate environment probe textures from camera imagery and render reflective virtual objects.",
          "name" : "Adding realistic reflections to an AR experience",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/adding-realistic-reflections-to-an-ar-experience"
        },
        {
          "description" : "An object that provides environmental lighting information for a specific area of space in a world-tracking AR session.",
          "name" : "AREnvironmentProbeAnchor",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/AREnvironmentProbeAnchor"
        },
        {
          "description" : "Estimated scene lighting information associated with a captured video frame in an AR session.",
          "name" : "ARLightEstimate",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARLightEstimate"
        },
        {
          "description" : "Estimated environmental lighting information associated with a captured video frame in a face-tracking AR session.",
          "name" : "ARDirectionalLightEstimate",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARDirectionalLightEstimate"
        }
      ],
      "title" : "Lighting Effects"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "Cover your app’s virtual content with people that ARKit perceives in the camera feed.",
          "name" : "Occluding virtual content with people",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/occluding-virtual-content-with-people"
        },
        {
          "description" : "Occlude your app’s virtual content where ARKit recognizes people in the camera feed by using matte generator.",
          "name" : "Effecting People Occlusion in Custom Renderers",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/effecting-people-occlusion-in-custom-renderers"
        },
        {
          "description" : "Estimate the shape of the physical environment using a polygonal mesh.",
          "name" : "Visualizing and interacting with a reconstructed scene",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/visualizing-and-interacting-with-a-reconstructed-scene"
        },
        {
          "description" : "An object that creates matte textures you use to occlude your app’s virtual content with people, that ARKit recognizes in the camera feed.",
          "name" : "ARMatteGenerator",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARMatteGenerator"
        }
      ],
      "title" : "Occlusion"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "Identify items in the physical environment, including planar surfaces, images, physical objects, body positions, and faces.",
          "name" : "Content Anchors",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/content-anchors"
        },
        {
          "description" : "Analyze the video from the cameras and the accompanying data, and use ray-casting and depth-map information to determine the location of items.",
          "name" : "Environmental Analysis",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/environmental-analysis"
        },
        {
          "description" : "Obtain detailed information about skeletal and face geometry, and saved world data.",
          "name" : "Data Management",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/data-management"
        },
        {
          "description" : "Generate 3D assets that render as expected.",
          "name" : "Creating USD files for Apple devices",
          "url" : "https:\/\/developer.apple.com\/documentation\/USD\/creating-usd-files-for-apple-devices"
        }
      ],
      "title" : "Virtual Content"
    }
  ],
  "source" : "appleJSON",
  "title" : "Camera, Lighting, and Effects",
  "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/camera-lighting-and-effects"
}