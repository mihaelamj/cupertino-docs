{
  "abstract" : "React to known 2D images in the user’s environment, and use their positions to place AR content.",
  "codeExamples" : [
    {
      "code" : "guard let referenceImages = ARReferenceImage.referenceImages(inGroupNamed: \"AR Resources\", \n                                                             bundle: nil) else {\n    fatalError(\"Missing expected asset catalog resources.\")\n}\n\nlet configuration = ARWorldTrackingConfiguration()\nconfiguration.detectionImages = referenceImages\nsession.run(configuration, options: [.resetTracking, .removeExistingAnchors])",
      "language" : "swift"
    },
    {
      "code" : "func renderer(_ renderer: SCNSceneRenderer, didAdd node: SCNNode, for anchor: ARAnchor) {\n    guard let imageAnchor = anchor as? ARImageAnchor else { return }\n    let referenceImage = imageAnchor.referenceImage\n    updateQueue.async {\n        \n        \/\/ Create a plane to visualize the initial position of the detected image.\n        let plane = SCNPlane(width: referenceImage.physicalSize.width,\n                                height: referenceImage.physicalSize.height)\n        let planeNode = SCNNode(geometry: plane)\n        planeNode.opacity = 0.25\n        \n        \/*\n            `SCNPlane` is vertically oriented in its local coordinate space, but\n            `ARImageAnchor` assumes the image is horizontal in its local space, so\n            rotate the plane to match.\n            *\/\n        planeNode.eulerAngles.x = -.pi \/ 2\n        \n        \/*\n            Image anchors are not tracked after initial detection, so create an\n            animation that limits the duration for which the plane visualization appears.\n            *\/\n        planeNode.runAction(self.imageHighlightAction)\n        \n        \/\/ Add the plane visualization to the scene.\n        node.addChildNode(planeNode)\n    }\n        DispatchQueue.main.async {\n        let imageName = referenceImage.name ?? \"\"\n        self.statusViewController.cancelAllScheduledMessages()\n        self.statusViewController.showMessage(\"Detected image “\\(imageName)”\")\n    }\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "f650971f49f82dc0da58d384bc1caf453f83045e3a0271ebf06323cdad648eb5",
  "crawledAt" : "2025-12-02T15:29:17Z",
  "id" : "DCFB0E6F-B6A9-4E5A-BAD2-1C116833AA80",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "ARKit",
  "overview" : "## Overview\n\nMany AR experiences can be enhanced by using known features of the user’s environment to trigger the appearance of virtual content. For example, a museum app might show a virtual curator when the user points their device at a painting, or a board game might place virtual pieces when the player points their device at a game board. In iOS 11.3 and later, you can add such features to your AR experience by enabling image detection in ARKit: Your app provides known 2D images, and ARKit tells you when and where those images are detected during an AR session.\n\nThis example app looks for any of the several reference images included in the app’s asset catalog. When ARKit detects one of those images, the app shows a message identifying the detected image and a brief animation showing its position in the scene.\n\n### Enable Image Detection\n\nImage detection is an add-on feature for world-tracking AR sessions. (For more details on world tracking, see [doc:\/\/com.apple.arkit\/documentation\/ARKit\/tracking-and-visualizing-planes].)\n\nTo enable image detection:\n\nThe code below shows how the sample app performs these steps when starting or restarting the AR experience.\n\n### Visualize Image Detection Results\n\nWhen ARKit detects one of your reference images, the session automatically adds a corresponding [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARImageAnchor] to its list of anchors. To respond to an image being detected, implement an appropriate [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSessionDelegate], [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSKViewDelegate], or [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNViewDelegate] method that reports the new anchor being added to the session. (This example app uses the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNViewDelegate\/renderer(_:didAdd:for:)] method for the code shown below.)\n\nTo use the detected image as a trigger for AR content, you’ll need to know its position and orientation, its size, and which reference image it is. The anchor’s inherited [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARAnchor\/transform] property provides position and orientation, and its [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARImageAnchor\/referenceImage] property tells you which [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARReferenceImage] object was detected. If your AR content depends on the extent of the image in the scene, you can then use the reference image’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARReferenceImage\/physicalSize] to set up your content, as shown in the code below.\n\n### Provide Your Own Reference Images\n\nTo use your own images for detection (in this sample or in your own project), you’ll need to add them to your asset catalog in Xcode.\n\n**Be aware of image detection capabilities.** Choose, design, and configure reference images for optimal reliability and performance:\n\n### Apply Best Practices\n\nThis example app simply visualizes where ARKit detects each reference image in the user’s environment, but your app can do much more. Follow the tips below to design AR experiences that use image detection well.\n\n**Use detected images to set a frame of reference for the AR scene.** Instead of requiring the user to choose a place for virtual content, or arbitrarily placing content in the user’s environment, use detected images to anchor the virtual scene. You can even use multiple detected images. For example, an app for a retail store could make a virtual character appear to emerge from a store’s front door by detecting posters placed on either side of the door and then calculating a position for the character directly between the posters.\n\n**Design your AR experience to use detected images as a starting point for virtual content.** ARKit doesn’t track changes to the position or orientation of each detected image. If you try to place virtual content that stays attached to a detected image, that content may not appear to stay in place correctly. Instead, use detected images as a frame of reference for starting a dynamic scene. For example, your app might detect theater posters for a sci-fi film and then have virtual spaceships appear to emerge from the posters and fly around the environment.\n\n**Consider when to allow detection of each image to trigger (or repeat) AR interactions.** ARKit adds an image anchor to a session exactly once for each reference image in the session configuration’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARWorldTrackingConfiguration\/detectionImages] array. If your AR experience adds virtual content to the scene when an image is detected, that action will by default happen only once. To allow the user to experience that content again without restarting your app, call the session’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSession\/remove(anchor:)] method to remove the corresponding [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARImageAnchor]. After the anchor is removed, ARKit will add a new anchor the next time it detects the image.\n\nFor example, in the case described above, where spaceships appear to fly out of a movie poster, you might not want an extra copy of that animation to appear while the first one is still playing. Wait until the animation ends to remove the anchor, so that the user can trigger it again by pointing their device at the image.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/ARKit\/detecting-images-in-an-ar-experience\ncrawled: 2025-12-02T15:29:17Z\n---\n\n# Detecting Images in an AR Experience\n\n**Sample Code**\n\nReact to known 2D images in the user’s environment, and use their positions to place AR content.\n\n## Overview\n\nMany AR experiences can be enhanced by using known features of the user’s environment to trigger the appearance of virtual content. For example, a museum app might show a virtual curator when the user points their device at a painting, or a board game might place virtual pieces when the player points their device at a game board. In iOS 11.3 and later, you can add such features to your AR experience by enabling image detection in ARKit: Your app provides known 2D images, and ARKit tells you when and where those images are detected during an AR session.\n\nThis example app looks for any of the several reference images included in the app’s asset catalog. When ARKit detects one of those images, the app shows a message identifying the detected image and a brief animation showing its position in the scene.\n\n\n\n### Enable Image Detection\n\nImage detection is an add-on feature for world-tracking AR sessions. (For more details on world tracking, see [doc:\/\/com.apple.arkit\/documentation\/ARKit\/tracking-and-visualizing-planes].)\n\nTo enable image detection:\n\n1. Load one or more [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARReferenceImage] resources from your app’s asset catalog.\n2. Create a world-tracking configuration and pass those reference images to its [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARWorldTrackingConfiguration\/detectionImages] property.\n3. Use the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSession\/run(_:options:)] method to run a session with your configuration.\n\nThe code below shows how the sample app performs these steps when starting or restarting the AR experience.\n\n```swift\nguard let referenceImages = ARReferenceImage.referenceImages(inGroupNamed: \"AR Resources\", \n                                                             bundle: nil) else {\n    fatalError(\"Missing expected asset catalog resources.\")\n}\n\nlet configuration = ARWorldTrackingConfiguration()\nconfiguration.detectionImages = referenceImages\nsession.run(configuration, options: [.resetTracking, .removeExistingAnchors])\n```\n\n### Visualize Image Detection Results\n\nWhen ARKit detects one of your reference images, the session automatically adds a corresponding [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARImageAnchor] to its list of anchors. To respond to an image being detected, implement an appropriate [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSessionDelegate], [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSKViewDelegate], or [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNViewDelegate] method that reports the new anchor being added to the session. (This example app uses the [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSCNViewDelegate\/renderer(_:didAdd:for:)] method for the code shown below.)\n\nTo use the detected image as a trigger for AR content, you’ll need to know its position and orientation, its size, and which reference image it is. The anchor’s inherited [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARAnchor\/transform] property provides position and orientation, and its [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARImageAnchor\/referenceImage] property tells you which [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARReferenceImage] object was detected. If your AR content depends on the extent of the image in the scene, you can then use the reference image’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARReferenceImage\/physicalSize] to set up your content, as shown in the code below.\n\n```swift\nfunc renderer(_ renderer: SCNSceneRenderer, didAdd node: SCNNode, for anchor: ARAnchor) {\n    guard let imageAnchor = anchor as? ARImageAnchor else { return }\n    let referenceImage = imageAnchor.referenceImage\n    updateQueue.async {\n        \n        \/\/ Create a plane to visualize the initial position of the detected image.\n        let plane = SCNPlane(width: referenceImage.physicalSize.width,\n                                height: referenceImage.physicalSize.height)\n        let planeNode = SCNNode(geometry: plane)\n        planeNode.opacity = 0.25\n        \n        \/*\n            `SCNPlane` is vertically oriented in its local coordinate space, but\n            `ARImageAnchor` assumes the image is horizontal in its local space, so\n            rotate the plane to match.\n            *\/\n        planeNode.eulerAngles.x = -.pi \/ 2\n        \n        \/*\n            Image anchors are not tracked after initial detection, so create an\n            animation that limits the duration for which the plane visualization appears.\n            *\/\n        planeNode.runAction(self.imageHighlightAction)\n        \n        \/\/ Add the plane visualization to the scene.\n        node.addChildNode(planeNode)\n    }\n        DispatchQueue.main.async {\n        let imageName = referenceImage.name ?? \"\"\n        self.statusViewController.cancelAllScheduledMessages()\n        self.statusViewController.showMessage(\"Detected image “\\(imageName)”\")\n    }\n}\n```\n\n### Provide Your Own Reference Images\n\nTo use your own images for detection (in this sample or in your own project), you’ll need to add them to your asset catalog in Xcode.\n\n1. Open your project’s asset catalog, then use the Add button (+) to add a new AR resource group.\n2. Drag image files from the Finder into the newly created resource group.\n3. For each image, use the inspector to describe the physical size of the image as you’d expect to find it in the user’s real-world environment, and optionally include a descriptive name for your own use.\n\n\n\n**Be aware of image detection capabilities.** Choose, design, and configure reference images for optimal reliability and performance:\n\n- Enter the physical size of the image in Xcode as accurately as possible. ARKit relies on this information to determine the distance of the image from the camera. Entering an incorrect physical size will result in an [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARImageAnchor] that’s the wrong distance from the camera.\n- When you add reference images to your asset catalog in Xcode, pay attention to the quality estimation warnings Xcode provides. Images with high contrast work best for image detection.\n- Use only images on flat surfaces for detection. If an image to be detected is on a nonplanar surface, like a label on a wine bottle, ARKit might not detect it at all, or might create an image anchor at the wrong location.\n- Consider how your image appears under different lighting conditions. If an image is printed on glossy paper or displayed on a device screen, reflections on those surfaces can interfere with detection.\n\n### Apply Best Practices\n\nThis example app simply visualizes where ARKit detects each reference image in the user’s environment, but your app can do much more. Follow the tips below to design AR experiences that use image detection well.\n\n**Use detected images to set a frame of reference for the AR scene.** Instead of requiring the user to choose a place for virtual content, or arbitrarily placing content in the user’s environment, use detected images to anchor the virtual scene. You can even use multiple detected images. For example, an app for a retail store could make a virtual character appear to emerge from a store’s front door by detecting posters placed on either side of the door and then calculating a position for the character directly between the posters.\n\n\n\n**Design your AR experience to use detected images as a starting point for virtual content.** ARKit doesn’t track changes to the position or orientation of each detected image. If you try to place virtual content that stays attached to a detected image, that content may not appear to stay in place correctly. Instead, use detected images as a frame of reference for starting a dynamic scene. For example, your app might detect theater posters for a sci-fi film and then have virtual spaceships appear to emerge from the posters and fly around the environment.\n\n**Consider when to allow detection of each image to trigger (or repeat) AR interactions.** ARKit adds an image anchor to a session exactly once for each reference image in the session configuration’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARWorldTrackingConfiguration\/detectionImages] array. If your AR experience adds virtual content to the scene when an image is detected, that action will by default happen only once. To allow the user to experience that content again without restarting your app, call the session’s [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARSession\/remove(anchor:)] method to remove the corresponding [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARImageAnchor]. After the anchor is removed, ARKit will add a new anchor the next time it detects the image.\n\nFor example, in the case described above, where spaceships appear to fly out of a movie poster, you might not want an extra copy of that animation to appear while the first one is still playing. Wait until the animation ends to remove the anchor, so that the user can trigger it again by pointing their device at the image.\n\n",
  "sections" : [

  ],
  "source" : "appleJSON",
  "title" : "Detecting Images in an AR Experience",
  "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/detecting-images-in-an-ar-experience"
}