{
  "abstract" : "Returns an affine transform for converting between normalized image coordinates and a coordinate space appropriate for rendering the camera image onscreen.",
  "codeExamples" : [

  ],
  "contentHash" : "f88955231628cdb5833ec9496d9aa4483a0578df9fb3df689994c8c6a46a1ace",
  "crawledAt" : "2025-12-02T18:34:10Z",
  "declaration" : {
    "code" : "func displayTransform(for orientation: UIInterfaceOrientation, viewportSize: CGSize) -> CGAffineTransform",
    "language" : "swift"
  },
  "id" : "1DE37BF2-76C6-41B6-8078-1C82F36CFFD2",
  "kind" : "method",
  "language" : "swift",
  "module" : "ARKit",
  "overview" : "## Return Value\n\nA transform matrix that converts from normalized image coordinates in the captured image to normalized image coordinates that account for the specified parameters.\n\n## Discussion\n\nNormalized image coordinates range from `(0,0)` in the upper left corner of the image to `(1,1)` in the lower right corner.\n\nThis method creates an affine transform representing the rotation and aspect-fit crop operations necessary to adapt the camera image to the specified orientation and to the aspect ratio of the specified viewport. The affine transform does not scale to the viewport’s pixel size.\n\nThe [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFrame\/capturedImage] pixel buffer is the original image captured by the device camera, and thus not adjusted for device orientation or view aspect ratio.",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/displayTransform(for:viewportSize:)\ncrawled: 2025-12-02T18:34:10Z\n---\n\n# displayTransform(for:viewportSize:)\n\n**Instance Method**\n\nReturns an affine transform for converting between normalized image coordinates and a coordinate space appropriate for rendering the camera image onscreen.\n\n## Declaration\n\n```swift\nfunc displayTransform(for orientation: UIInterfaceOrientation, viewportSize: CGSize) -> CGAffineTransform\n```\n\n## Parameters\n\n- **orientation**: The orientation intended for presenting the view.\n- **viewportSize**: The size, in points, of the view intended for rendering the camera image.\n\n## Return Value\n\nA transform matrix that converts from normalized image coordinates in the captured image to normalized image coordinates that account for the specified parameters.\n\n## Discussion\n\nNormalized image coordinates range from `(0,0)` in the upper left corner of the image to `(1,1)` in the lower right corner.\n\nThis method creates an affine transform representing the rotation and aspect-fit crop operations necessary to adapt the camera image to the specified orientation and to the aspect ratio of the specified viewport. The affine transform does not scale to the viewport’s pixel size.\n\nThe [doc:\/\/com.apple.arkit\/documentation\/ARKit\/ARFrame\/capturedImage] pixel buffer is the original image captured by the device camera, and thus not adjusted for device orientation or view aspect ratio.\n\n## Accessing scene data\n\n- **lightEstimate**: An estimate of lighting conditions based on the camera image.\n- **rawFeaturePoints**: The current intermediate results of the scene analysis ARKit uses to perform world tracking.\n- **capturedDepthData**: Depth data captured in front-camera experiences.\n- **capturedDepthDataTimestamp**: The time at which depth data for the frame (if any) was captured.\n- **sceneDepth**: Data on the distance between a device’s rear camera and real-world objects in an AR experience.\n- **smoothedSceneDepth**: An average of distance measurements between a device’s rear camera and real-world objects that creates smoother visuals in an AR experience.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "An estimate of lighting conditions based on the camera image.",
          "name" : "lightEstimate",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/lightEstimate"
        },
        {
          "description" : "The current intermediate results of the scene analysis ARKit uses to perform world tracking.",
          "name" : "rawFeaturePoints",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/rawFeaturePoints"
        },
        {
          "description" : "Depth data captured in front-camera experiences.",
          "name" : "capturedDepthData",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/capturedDepthData"
        },
        {
          "description" : "The time at which depth data for the frame (if any) was captured.",
          "name" : "capturedDepthDataTimestamp",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/capturedDepthDataTimestamp"
        },
        {
          "description" : "Data on the distance between a device’s rear camera and real-world objects in an AR experience.",
          "name" : "sceneDepth",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/sceneDepth"
        },
        {
          "description" : "An average of distance measurements between a device’s rear camera and real-world objects that creates smoother visuals in an AR experience.",
          "name" : "smoothedSceneDepth",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/smoothedSceneDepth"
        }
      ],
      "title" : "Accessing scene data"
    }
  ],
  "source" : "appleJSON",
  "title" : "displayTransform(for:viewportSize:)",
  "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/displayTransform(for:viewportSize:)"
}