{
  "abstract" : "A pixel buffer containing the image captured by the camera.",
  "codeExamples" : [
    {
      "code" : "const float4x4 ycbcrToRGBTransform = float4x4(\n    float4(+1.0000f, +1.0000f, +1.0000f, +0.0000f),\n    float4(+0.0000f, -0.3441f, +1.7720f, +0.0000f),\n    float4(+1.4020f, -0.7141f, +0.0000f, +0.0000f),\n    float4(-0.7010f, +0.5291f, -0.8860f, +1.0000f)\n);",
      "language" : "cpp"
    }
  ],
  "contentHash" : "37968918eba9fcc1287387f687b47ef0b37e3354bce65b61a43b32fed132e908",
  "crawledAt" : "2025-12-02T18:52:48Z",
  "declaration" : {
    "code" : "var capturedImage: CVPixelBuffer { get }",
    "language" : "swift"
  },
  "id" : "476B8391-5901-452B-906A-0DB0E02CBA87",
  "kind" : "property",
  "language" : "swift",
  "module" : "ARKit",
  "overview" : "## Discussion\n\nARKit captures pixel buffers in a full-range planar YCbCr format (also known as YUV) format according to the ITU R. 601-4 standard. (You can verify this by checking the [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/kCVImageBufferYCbCrMatrixKey] pixel buffer attachment.)\n\nUnlike some uses of that standard, ARKit captures full-range color space values, not video-range values. To correctly render these images on a device display, you’ll need to access the luma and chroma planes of the pixel buffer and convert full-range YCbCr values to an sRGB (or ITU R. 709) format according to the ITU-T T.871 specification.\n\nThe following matrix (shown in Metal shader syntax) performs this conversion when multiplied by a 4-element vector (containing Y’, Cb, Cr values and an “alpha” value of 1.0):\n\nFor more details, see [doc:\/\/com.apple.arkit\/documentation\/ARKit\/displaying-an-ar-experience-with-metal], or use the Metal variant of the AR app template when creating a new project in Xcode.",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/capturedImage\ncrawled: 2025-12-02T18:52:48Z\n---\n\n# capturedImage\n\n**Instance Property**\n\nA pixel buffer containing the image captured by the camera.\n\n## Declaration\n\n```swift\nvar capturedImage: CVPixelBuffer { get }\n```\n\n## Discussion\n\nARKit captures pixel buffers in a full-range planar YCbCr format (also known as YUV) format according to the ITU R. 601-4 standard. (You can verify this by checking the [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/kCVImageBufferYCbCrMatrixKey] pixel buffer attachment.)\n\nUnlike some uses of that standard, ARKit captures full-range color space values, not video-range values. To correctly render these images on a device display, you’ll need to access the luma and chroma planes of the pixel buffer and convert full-range YCbCr values to an sRGB (or ITU R. 709) format according to the ITU-T T.871 specification.\n\nThe following matrix (shown in Metal shader syntax) performs this conversion when multiplied by a 4-element vector (containing Y’, Cb, Cr values and an “alpha” value of 1.0):\n\n```cpp\nconst float4x4 ycbcrToRGBTransform = float4x4(\n    float4(+1.0000f, +1.0000f, +1.0000f, +0.0000f),\n    float4(+0.0000f, -0.3441f, +1.7720f, +0.0000f),\n    float4(+1.4020f, -0.7141f, +0.0000f, +0.0000f),\n    float4(-0.7010f, +0.5291f, -0.8860f, +1.0000f)\n);\n```\n\nFor more details, see [doc:\/\/com.apple.arkit\/documentation\/ARKit\/displaying-an-ar-experience-with-metal], or use the Metal variant of the AR app template when creating a new project in Xcode.\n\n## Accessing camera data\n\n- **camera**: Information about the camera position, orientation, and imaging parameters used to capture the frame.\n- **timestamp**: The time at which the frame was captured.\n- **cameraGrainIntensity**: A value that specifies the amount of grain present in the camera grain texture.\n- **cameraGrainTexture**: A tileable Metal texture created by ARKit to match the visual characteristics of the current video stream.\n- **exifData**: Auxiliary data for the captured image.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Information about the camera position, orientation, and imaging parameters used to capture the frame.",
          "name" : "camera",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/camera"
        },
        {
          "description" : "The time at which the frame was captured.",
          "name" : "timestamp",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/timestamp"
        },
        {
          "description" : "A value that specifies the amount of grain present in the camera grain texture.",
          "name" : "cameraGrainIntensity",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/cameraGrainIntensity"
        },
        {
          "description" : "A tileable Metal texture created by ARKit to match the visual characteristics of the current video stream.",
          "name" : "cameraGrainTexture",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/cameraGrainTexture"
        },
        {
          "description" : "Auxiliary data for the captured image.",
          "name" : "exifData",
          "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/exifData"
        }
      ],
      "title" : "Accessing camera data"
    }
  ],
  "source" : "appleJSON",
  "title" : "capturedImage",
  "url" : "https:\/\/developer.apple.com\/documentation\/ARKit\/ARFrame\/capturedImage"
}