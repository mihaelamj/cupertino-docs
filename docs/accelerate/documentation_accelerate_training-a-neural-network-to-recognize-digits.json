{
  "abstract" : "Build a simple neural network and train it to recognize randomly generated numbers.",
  "codeExamples" : [
    {
      "code" : "static let three: [Float] = [0, 1, 1, 1, 1, 0,  \/\/ ⚪️⚫️⚫️⚫️⚫️⚪️\n                             0, 0, 0, 0, 1, 0,  \/\/ ⚪️⚪️⚪️⚪️⚫️⚪️\n                             0, 1, 1, 1, 1, 0,  \/\/ ⚪️⚫️⚫️⚫️⚫️⚪️\n                             0, 0, 0, 0, 1, 0,  \/\/ ⚪️⚪️⚪️⚪️⚫️⚪️\n                             0, 1, 1, 1, 1, 0,  \/\/ ⚪️⚫️⚫️⚫️⚫️⚪️\n                             0, 0, 0, 0, 0, 0]  \/\/ ⚪️⚪️⚪️⚪️⚪️⚪️",
      "language" : "swift"
    },
    {
      "code" : "static var filterParameters = BNNSFilterParameters(\n    flags: BNNSFlags.useClientPointer.rawValue,\n    n_threads: 1,\n    alloc_memory: nil,\n    free_memory: nil)",
      "language" : "swift"
    },
    {
      "code" : "static let convolutionWeights: BNNSNDArrayDescriptor = {\n    let convolutionKernelSize = 3\n    \n    let convolutionWeightsShape = BNNS.Shape.convolutionWeightsOIHW(\n        convolutionKernelSize,\n        convolutionKernelSize,\n        convolutionInputImageChannels,\n        convolutionOutputImageChannels)\n    \n    guard let desc = BNNSNDArrayDescriptor.allocate(\n        randomUniformUsing: randomGenerator,\n        range: Float(-0.5)...Float(0.5),\n        shape: convolutionWeightsShape) else {\n        fatalError(\"Unable to create `convolutionWeightsArray`.\")\n    }\n    return desc\n}()",
      "language" : "swift"
    },
    {
      "code" : "static let convolutionBias = BNNSNDArrayDescriptor.allocate(\n    repeating: Float(0),\n    shape: .vector(convolutionOutputImageChannels))\n\nstatic let featureMaps = convolutionOutputImageChannels\n\nstatic let batchNormBeta = BNNSNDArrayDescriptor.allocate(\n    repeating: Float(0),\n    shape: .vector(featureMaps),\n    batchSize: batchSize)\n\nstatic let batchNormGamma = BNNSNDArrayDescriptor.allocate(\n    repeating: Float(1),\n    shape: .vector(featureMaps),\n    batchSize: batchSize)",
      "language" : "swift"
    },
    {
      "code" : "static let fusedConvBatchNormLayer: BNNS.FusedParametersLayer = {\n    \n    let convolutionParameters = BNNS.FusedConvolutionParameters(\n        type: .standard,\n        weights: convolutionWeights,\n        bias: convolutionBias,\n        stride: (1, 1),\n        dilationStride: (1, 1),\n        groupSize: 1,\n        padding: .symmetric(x: convolutionPadding,\n                            y: convolutionPadding))\n    \n    let normalizationParameters = BNNS.FusedNormalizationParameters(\n        type: .batch(movingMean: batchNormMovingMean,\n                     movingVariance: batchNormMovingVariance),\n        beta: batchNormBeta,\n        gamma: batchNormGamma,\n        momentum: 0.9,\n        epsilon: 1e-07,\n        activation: .rectifiedLinear)\n    \n    guard let layer = BNNS.FusedParametersLayer(\n        input: input,\n        output: batchNormOutput,\n        fusedLayerParameters: [convolutionParameters, normalizationParameters],\n        filterParameters: filterParameters) else {\n        fatalError(\"unable to create fusedConvBatchnormLayer\")\n    }\n    \n    return layer\n}()",
      "language" : "swift"
    },
    {
      "code" : "static var poolingLayer: BNNS.PoolingLayer = {\n    guard let poolingLayer = BNNS.PoolingLayer(\n        type: .max(xDilationStride: 1, yDilationStride: 1),\n        input: batchNormOutput,\n        output: poolingOutput,\n        bias: nil,\n        activation: .identity,\n        kernelSize: (2, 2),\n        stride: (2, 2),\n        padding: .zero,\n        filterParameters: filterParameters) else {\n        fatalError(\"Unable to create `poolingLayer`.\")\n    }\n    \n    return poolingLayer\n}()",
      "language" : "swift"
    },
    {
      "code" : "static let fullyConnectedWeights: BNNSNDArrayDescriptor = {\n    guard let desc = BNNSNDArrayDescriptor.allocate(\n        randomUniformUsing: randomGenerator,\n        range: Float(-0.5)...Float(0.5),\n        shape: .matrixRowMajor(poolingOutputSize,\n                               fullyConnectedOutputWidth)) else {\n        fatalError(\"Unable to create `fullyConnectedWeightsArray`.\")\n    }\n    return desc\n}()",
      "language" : "swift"
    },
    {
      "code" : "static var fullyConnectedLayer: BNNS.FullyConnectedLayer = {\n    \n    let desc = BNNSNDArrayDescriptor(dataType: .float,\n                                     shape: .vector(poolingOutputSize))\n    \n    guard let fullyConnectedLayer = BNNS.FullyConnectedLayer(\n            input: desc,\n            output: fullyConnectedOutput,\n            weights: fullyConnectedWeights,\n            bias: nil,\n            activation: .identity,\n            filterParameters: filterParameters) else {\n        fatalError(\"Unable to create `fullyConnectedLayer`.\")\n    }\n    \n    return fullyConnectedLayer\n}()",
      "language" : "swift"
    },
    {
      "code" : "static var lossLayer: BNNS.LossLayer = {\n    \n    guard let lossLayer = BNNS.LossLayer(input: fullyConnectedOutput,\n                                         output: lossOutput,\n                                         lossFunction: .softmaxCrossEntropy(labelSmoothing: 0),\n                                         lossReduction: .reductionMean,\n                                         filterParameters: filterParameters) else {\n        fatalError(\"Unable to create `lossLayer`.\")\n    }\n    \n    return lossLayer\n}()",
      "language" : "swift"
    },
    {
      "code" : "⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚫️⚫️⚫️⚫️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚫️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚫️⚫️⚫️⚫️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚫️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚫️⚫️⚫️⚫️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️"
    },
    {
      "code" : "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]"
    },
    {
      "code" : "try fusedConvBatchNormLayer.apply(batchSize: batchSize,\n                                  input: input,\n                                  output: batchNormOutput,\n                                  for: .training)\n\ntry poolingLayer.apply(batchSize: batchSize,\n                       input: batchNormOutput,\n                       output: poolingOutput)\n\ntry fullyConnectedLayer.apply(batchSize: batchSize,\n                              input: poolingOutput,\n                              output: fullyConnectedOutput)",
      "language" : "swift"
    },
    {
      "code" : "try lossLayer.apply(batchSize: batchSize,\n                    input: fullyConnectedOutput,\n                    labels: oneHotLabels,\n                    output: lossOutput,\n                    generatingInputGradient: lossInputGradient)",
      "language" : "swift"
    },
    {
      "code" : "static var adam = BNNS.AdamOptimizer(learningRate: 0.01,\n                                     timeStep: 1,\n                                     gradientScale: 1,\n                                     regularizationScale: 0.01,\n                                     gradientClipping: .byValue(bounds: -0.5 ... 0.5),\n                                     regularizationFunction: BNNSOptimizerRegularizationL2)",
      "language" : "swift"
    },
    {
      "code" : "try fullyConnectedLayer.applyBackward(\n    batchSize: batchSize,\n    input: poolingOutput,\n    output: fullyConnectedOutput,\n    outputGradient: lossInputGradient,\n    generatingInputGradient: fullyConnectedInputGradient,\n    generatingWeightsGradient: fullyConnectedWeightGradient)",
      "language" : "swift"
    },
    {
      "code" : "try poolingLayer.applyBackward(\n    batchSize: batchSize,\n    input: batchNormOutput,\n    output: poolingOutput,\n    outputGradient: fullyConnectedInputGradient,\n    generatingInputGradient: poolingInputGradientArray)",
      "language" : "swift"
    },
    {
      "code" : "let gradientParameters = [convolutionWeightGradient,\n                          convolutionBiasGradient,\n                          batchNormBetaGradient,\n                          batchNormGammaGradient]\n\ntry fusedConvBatchNormLayer.applyBackward(\n    batchSize: batchSize,\n    input: input,\n    output: batchNormOutput,\n    outputGradient: poolingInputGradientArray,\n    generatingInputGradient: convolutionInputGradient,\n    generatingParameterGradients: gradientParameters)",
      "language" : "swift"
    },
    {
      "code" : "try adam.step(\n    parameters: [fullyConnectedWeights,\n                 convolutionWeights, convolutionBias,\n                 batchNormBeta, batchNormGamma],\n    gradients: [fullyConnectedWeightGradient,\n                convolutionWeightGradient, convolutionBiasGradient,\n                batchNormBetaGradient, batchNormGammaGradient],\n    accumulators: [fullyConnectedWeightAccumulator1,\n                   convolutionWeightAccumulator1, convolutionBiasAccumulator1,\n                   batchNormBetaAccumulator1, batchNormGammaAccumulator1,\n                   fullyConnectedWeightAccumulator2,\n                   convolutionWeightAccumulator2, convolutionBiasAccumulator2,\n                   batchNormBetaAccumulator2, batchNormGammaAccumulator2],\n    filterParameters: filterParameters)",
      "language" : "swift"
    },
    {
      "code" : "adam.timeStep += 1",
      "language" : "swift"
    },
    {
      "code" : "let maximumIterationCount = 1000\n\n\/\/ The `recentLosses` array contains the last `recentLossesCount` losses.\nlet recentLossesCount = 20\nvar recentLosses = [Float]()\n\n\/\/ The `averageRecentLossThreshold` constant defines the loss threshold\n\/\/ at which to consider the training phase complete.\nlet averageRecentLossThreshold = Float(0.125)\n\nfor epoch in 0 ..< maximumIterationCount {\n    if epoch == 500 {\n        adam.learningRate \/= 10\n    }\n    \n    generateInputAndLabels()\n    forwardPass()\n    computeLoss()\n    \n    guard let loss = lossOutput.makeArray(of: Float.self,\n                                          batchSize: 1)?.first else {\n        print(\"Unable to calculate loss.\")\n        return\n    }\n    \n    if recentLosses.isEmpty {\n        recentLosses = [Float](repeating: loss,\n                               count: recentLossesCount)\n    }\n    \n    recentLosses[epoch % recentLossesCount] = loss\n    let averageRecentLoss = vDSP.mean(recentLosses)\n    \n    if epoch % 10 == 0 {\n        print(\"Epoch \\(epoch): \\(loss) : \\(averageRecentLoss)\")\n    }\n    \n    if averageRecentLoss < averageRecentLossThreshold {\n        print(\"Recent average loss: \\(averageRecentLoss), breaking at epoch \\(epoch).\")\n        break\n    }\n    \n    backwardPass()\n}",
      "language" : "swift"
    },
    {
      "code" : "try fusedConvBatchNormLayer.apply(batchSize: batchSize,\n                                  input: input,\n                                  output: batchNormOutput,\n                                  for: .inference)\n\ntry poolingLayer.apply(batchSize: batchSize,\n                       input: batchNormOutput,\n                       output: poolingOutput)\n\ntry fullyConnectedLayer.apply(batchSize: batchSize,\n                              input: poolingOutput,\n                              output: fullyConnectedOutput)",
      "language" : "swift"
    },
    {
      "code" : "[-2.51, -3.62, -0.10, 8.52, -0.42, 5.11, -1.65, 1.34,  0.82, -2.77]\n[-3.94, -2.74, -0.30, 8.39, -1.45, 6.02, -0.66, 3.25,  0.49, -3.19]\n[-2.51, -2.77, -0.77, 8.41, -0.82, 4.87, -0.37, 2.32, -0.49, -3.05]\n[-3.01, -2.79,  0.48, 7.95, -2.57, 4.55, -1.05, 1.67,  1.38, -1.43]\n[-2.48, -1.59, -0.97, 7.59, -2.52, 4.00,  0.95, 4.02, -2.10, -1.62]"
    },
    {
      "code" : "guard\n    let fullyConnected = fullyConnectedOutput.makeArray(\n        of: Float.self,\n        batchSize: batchSize),\n    let labels = oneHotLabels.makeArray(\n        of: Float.self,\n        batchSize: batchSize) else {\n    fatalError(\"Unable to create arrays for evaluation.\")\n}\n\nvar correctCount = 0\n\nfor sample in 0 ..< batchSize {\n    let offset = fullyConnectedOutputWidth * sample\n    \n    let fullyConnectedBatch = fullyConnected[offset ..< offset + fullyConnectedOutputWidth]\n    let predictedDigit = vDSP.indexOfMaximum(fullyConnectedBatch).0\n    \n    let oneHotLabelsBatch = labels[offset ..< offset + fullyConnectedOutputWidth]\n    let label = vDSP.indexOfMaximum(oneHotLabelsBatch).0\n    \n    if label == predictedDigit {\n        correctCount += 1\n    }\n    \n    print(\"Sample \\(sample) — digit: \\(label) | prediction: \\(predictedDigit)\")\n}",
      "language" : "swift"
    },
    {
      "code" : "Sample 0 — digit: 7 | prediction: 7\nSample 1 — digit: 5 | prediction: 5\nSample 2 — digit: 7 | prediction: 7\nSample 3 — digit: 7 | prediction: 7\nSample 4 — digit: 0 | prediction: 0\nSample 5 — digit: 8 | prediction: 8\nSample 6 — digit: 3 | prediction: 3\nSample 7 — digit: 6 | prediction: 6\nSample 8 — digit: 2 | prediction: 2\nSample 9 — digit: 7 | prediction: 7\n[ ... ]"
    }
  ],
  "contentHash" : "33b57840dcbd59c9e859595710c78711361920e8a703051e5bcd598a20a435ea",
  "crawledAt" : "2025-12-02T16:25:32Z",
  "id" : "EA8A3F2B-25A5-45CA-B983-9C3496243A89",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "Accelerate",
  "overview" : "## Overview\n\nThis sample code project uses the Basic Neural Network Subroutines (BNNS) library to create a simple neural network that’s capable of recognizing digits.\n\nThe sample iterates over randomly generated digits in the training phase, incrementally improving its ability to recognize numbers. After the code completes the training phase, it evaluates its accuracy at recognizing numbers, and returns a score.\n\nA 6 x 6 matrix represents each digit. For example, the code below represents the number 3:\n\nThe network consists of the three layers below:\n\nAfter the code completes a forward pass, it calculates its loss, which is a score that indicates how the predicted values deviate from the labels. The sample code project uses the gradients that the loss generates as the basis for the backward pass, where it backward-applies the three layers in reverse order.\n\nThe backward passes generate gradient values that an optimizer uses to update the parameters below:\n\nThe optimizer’s gradual changes to the weights, bias, beta, and gamma increases the network’s efficacy in recognizing digits with each iteration.\n\nThe image below shows the relationships between the layers:\n\n\n\n### Define the filter parameters\n\nThe sample creates a [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/BNNSFilterParameters] structure with [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/BNNSFlags\/useClientPointer]. This flag instructs the layers to keep the provided pointers at creation time, and to work directly from that data rather than use internal copies of the data.\n\n### Create the fused convolution-batch normalization layer\n\nThe convolution-batch normalization layer consists of two sublayers.\n\nThe app initializes the convolution weights array with random values.\n\nThe app initializes the convolution bias and the batch normalization beta and gamma arrays with a repeated scalar value.\n\nThe code below creates the fused layer that applies convolution and normalization to the input:\n\n### Create the pooling layer\n\nPooling layers downscale their input while preserving the most important information and produce an output that, in the case of this sample code project, consists of the maximum value in each input pixel’s local neighborhood.\n\nThe following code creates the pooling layer:\n\n### Create the fully connected layer\n\nFully connected layers compute the matrix-vector product of a weights matrix and its input, and flatten the data to predict the correct label.\n\nThe app initializes the fully connected weights array with random values.\n\nThe code below creates the fully connected layer:\n\n### Create the loss layer\n\nThe loss layer is responsible for quantifying a score that indicates how the predicted values deviate from the labels.\n\nThe code below creates the loss layer:\n\n### Create the candidate input\n\nFor each iteration of the training phase, the sample creates a matrix that represents a random digit, and a *one-hot* encoded tensor of the same digit. The sample places digits randomly in a 20 x 20 matrix, so a `3` might appear in the matrix as the image below. This example renders `0` as `⚪️`, and `1` as `⚫️`.\n\nThe one-hot encoded tensor contains a `1` at the zero-based index of `3`.\n\nThe sample code project uses a batch size of 32, so each iteration generates 32 random digits in random positions in the 20 x 20 grid.\n\n### Perform the forward pass\n\nTo perform the forward pass, the sample code calls `apply` on the fused, pooling, and fully connected layers.\n\n### Calculate the loss and loss gradient\n\nCalculating the loss evaluates the efficacy of the neural network. The loss layer generates its output, `lossOutput`, which contains a score that indicates how the predicted values deviate from the labels, and `lossInputGradient`, which is the output gradient parameter to the backward application of the fully connected layer.\n\n### Create the optimizer\n\nThe optimizer is responsible for updating the weights, biases, beta, and gamma. In the code below, the sample code project creates an optimizer using the Adam algorithm:\n\n### Perform a backward pass and optimization step on the fully connected layer\n\nThe sample code project performs the backward pass in reverse order to the forward pass. Therefore, the sample’s first step is to call `applyBackward` on the fully connected layer, and perform an optimization step on its weights.\n\nThe `applyBackward` call on the fully connected layer generates an input gradient that acts as the output gradient for the pooling layer’s backward apply, and a weights gradient that passes to the fully connected optimizer step.\n\n### Perform a backward pass on the pooling layer\n\nThe backward pass on the pooling layer generates an input gradient that’s the output gradient to the backward apply of the fused layer.\n\n### Perform a backward pass and optimization step on the fused layer\n\nThe sample calls `applyBackward` on the fused layer. This performs an optimization step on the convolution layer’s weights and bias, and the normalization layer’s beta and gamma.\n\nThe code below performs the optimization step:\n\nAfter the app completes all the optimization steps for this iteration, it increments the optimizer time step.\n\n### Evaluate the neural network\n\nThe sample iterates over the forward, loss, backward, and optimization steps, and with each iteration, the trend of the loss is to reduce. The following graph shows the loss, as a solid stroke, decreasing during training:\n\n\n\nThe code in the sample defines a maximum number of iterations. Additionally, it calculates a moving average of recent loss values, which appear as a dashed stroke in the graph above. At each iteration, the sample checks whether the recent average loss is below that threshold, and, if it is, it breaks from the training phase early.\n\nAfter the training phase completes, the sample calculates the accuracy of the network over a new dataset. It then creates a new batch of random digits and runs a forward pass of the network.\n\nFinally, the app evaluates the accuracy of the network by comparing the values in the fully connected layer’s output to the one-hot labels. For example, when the recognized digit is `3`, one-hot labels contain the values `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`, and values in the fully connected layer’s output might be as follows:\n\nNote that in each case, the highest value in the fully connected layer’s output is at index `3`.\n\nThe following code performs that evaluation for each digit in the batch:\n\nThe evaluation function prints out something like the following:\n\nIn this case, the neural network accurately predicts each ground truth digit.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/accelerate\/training-a-neural-network-to-recognize-digits\ncrawled: 2025-12-02T16:25:32Z\n---\n\n# Training a neural network to recognize digits\n\n**Sample Code**\n\nBuild a simple neural network and train it to recognize randomly generated numbers.\n\n## Overview\n\nThis sample code project uses the Basic Neural Network Subroutines (BNNS) library to create a simple neural network that’s capable of recognizing digits.\n\nThe sample iterates over randomly generated digits in the training phase, incrementally improving its ability to recognize numbers. After the code completes the training phase, it evaluates its accuracy at recognizing numbers, and returns a score.\n\nA 6 x 6 matrix represents each digit. For example, the code below represents the number 3:\n\n```swift\nstatic let three: [Float] = [0, 1, 1, 1, 1, 0,  \/\/ ⚪️⚫️⚫️⚫️⚫️⚪️\n                             0, 0, 0, 0, 1, 0,  \/\/ ⚪️⚪️⚪️⚪️⚫️⚪️\n                             0, 1, 1, 1, 1, 0,  \/\/ ⚪️⚫️⚫️⚫️⚫️⚪️\n                             0, 0, 0, 0, 1, 0,  \/\/ ⚪️⚪️⚪️⚪️⚫️⚪️\n                             0, 1, 1, 1, 1, 0,  \/\/ ⚪️⚫️⚫️⚫️⚫️⚪️\n                             0, 0, 0, 0, 0, 0]  \/\/ ⚪️⚪️⚪️⚪️⚪️⚪️\n```\n\nThe network consists of the three layers below:\n\n- Fused convolution-batch normalization layer\n- Pooling layer\n- Fully connected layer\n\nAfter the code completes a forward pass, it calculates its loss, which is a score that indicates how the predicted values deviate from the labels. The sample code project uses the gradients that the loss generates as the basis for the backward pass, where it backward-applies the three layers in reverse order.\n\nThe backward passes generate gradient values that an optimizer uses to update the parameters below:\n\n- Convolution weights that the app initializes with random values, and bias\n- Normalization beta (offset) and gamma (scale)\n- Fully connected weights\n\nThe optimizer’s gradual changes to the weights, bias, beta, and gamma increases the network’s efficacy in recognizing digits with each iteration.\n\nThe image below shows the relationships between the layers:\n\n\n\n### Define the filter parameters\n\nThe sample creates a [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/BNNSFilterParameters] structure with [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/BNNSFlags\/useClientPointer]. This flag instructs the layers to keep the provided pointers at creation time, and to work directly from that data rather than use internal copies of the data.\n\n```swift\nstatic var filterParameters = BNNSFilterParameters(\n    flags: BNNSFlags.useClientPointer.rawValue,\n    n_threads: 1,\n    alloc_memory: nil,\n    free_memory: nil)\n```\n\n### Create the fused convolution-batch normalization layer\n\nThe convolution-batch normalization layer consists of two sublayers.\n\n- Convolution layers that generate their output by multiplying each input value and its neighbors by corresponding values in an array of weights, and then adding a corresponding bias. Each output value is the sum of each of those operations. Convolution layers are fundamental to convolutional neural networks and, with the correct weights and bias values, can identify features, such as horizontal and vertical lines.\n- Normalization layers that rescale their data so that all of the batches of data have the same standard deviation.\n\nThe app initializes the convolution weights array with random values.\n\n```swift\nstatic let convolutionWeights: BNNSNDArrayDescriptor = {\n    let convolutionKernelSize = 3\n    \n    let convolutionWeightsShape = BNNS.Shape.convolutionWeightsOIHW(\n        convolutionKernelSize,\n        convolutionKernelSize,\n        convolutionInputImageChannels,\n        convolutionOutputImageChannels)\n    \n    guard let desc = BNNSNDArrayDescriptor.allocate(\n        randomUniformUsing: randomGenerator,\n        range: Float(-0.5)...Float(0.5),\n        shape: convolutionWeightsShape) else {\n        fatalError(\"Unable to create `convolutionWeightsArray`.\")\n    }\n    return desc\n}()\n```\n\nThe app initializes the convolution bias and the batch normalization beta and gamma arrays with a repeated scalar value.\n\n```swift\nstatic let convolutionBias = BNNSNDArrayDescriptor.allocate(\n    repeating: Float(0),\n    shape: .vector(convolutionOutputImageChannels))\n\nstatic let featureMaps = convolutionOutputImageChannels\n\nstatic let batchNormBeta = BNNSNDArrayDescriptor.allocate(\n    repeating: Float(0),\n    shape: .vector(featureMaps),\n    batchSize: batchSize)\n\nstatic let batchNormGamma = BNNSNDArrayDescriptor.allocate(\n    repeating: Float(1),\n    shape: .vector(featureMaps),\n    batchSize: batchSize)\n```\n\nThe code below creates the fused layer that applies convolution and normalization to the input:\n\n```swift\nstatic let fusedConvBatchNormLayer: BNNS.FusedParametersLayer = {\n    \n    let convolutionParameters = BNNS.FusedConvolutionParameters(\n        type: .standard,\n        weights: convolutionWeights,\n        bias: convolutionBias,\n        stride: (1, 1),\n        dilationStride: (1, 1),\n        groupSize: 1,\n        padding: .symmetric(x: convolutionPadding,\n                            y: convolutionPadding))\n    \n    let normalizationParameters = BNNS.FusedNormalizationParameters(\n        type: .batch(movingMean: batchNormMovingMean,\n                     movingVariance: batchNormMovingVariance),\n        beta: batchNormBeta,\n        gamma: batchNormGamma,\n        momentum: 0.9,\n        epsilon: 1e-07,\n        activation: .rectifiedLinear)\n    \n    guard let layer = BNNS.FusedParametersLayer(\n        input: input,\n        output: batchNormOutput,\n        fusedLayerParameters: [convolutionParameters, normalizationParameters],\n        filterParameters: filterParameters) else {\n        fatalError(\"unable to create fusedConvBatchnormLayer\")\n    }\n    \n    return layer\n}()\n```\n\n### Create the pooling layer\n\nPooling layers downscale their input while preserving the most important information and produce an output that, in the case of this sample code project, consists of the maximum value in each input pixel’s local neighborhood.\n\nThe following code creates the pooling layer:\n\n```swift\nstatic var poolingLayer: BNNS.PoolingLayer = {\n    guard let poolingLayer = BNNS.PoolingLayer(\n        type: .max(xDilationStride: 1, yDilationStride: 1),\n        input: batchNormOutput,\n        output: poolingOutput,\n        bias: nil,\n        activation: .identity,\n        kernelSize: (2, 2),\n        stride: (2, 2),\n        padding: .zero,\n        filterParameters: filterParameters) else {\n        fatalError(\"Unable to create `poolingLayer`.\")\n    }\n    \n    return poolingLayer\n}()\n```\n\n### Create the fully connected layer\n\nFully connected layers compute the matrix-vector product of a weights matrix and its input, and flatten the data to predict the correct label.\n\nThe app initializes the fully connected weights array with random values.\n\n```swift\nstatic let fullyConnectedWeights: BNNSNDArrayDescriptor = {\n    guard let desc = BNNSNDArrayDescriptor.allocate(\n        randomUniformUsing: randomGenerator,\n        range: Float(-0.5)...Float(0.5),\n        shape: .matrixRowMajor(poolingOutputSize,\n                               fullyConnectedOutputWidth)) else {\n        fatalError(\"Unable to create `fullyConnectedWeightsArray`.\")\n    }\n    return desc\n}()\n```\n\nThe code below creates the fully connected layer:\n\n```swift\nstatic var fullyConnectedLayer: BNNS.FullyConnectedLayer = {\n    \n    let desc = BNNSNDArrayDescriptor(dataType: .float,\n                                     shape: .vector(poolingOutputSize))\n    \n    guard let fullyConnectedLayer = BNNS.FullyConnectedLayer(\n            input: desc,\n            output: fullyConnectedOutput,\n            weights: fullyConnectedWeights,\n            bias: nil,\n            activation: .identity,\n            filterParameters: filterParameters) else {\n        fatalError(\"Unable to create `fullyConnectedLayer`.\")\n    }\n    \n    return fullyConnectedLayer\n}()\n```\n\n### Create the loss layer\n\nThe loss layer is responsible for quantifying a score that indicates how the predicted values deviate from the labels.\n\nThe code below creates the loss layer:\n\n```swift\nstatic var lossLayer: BNNS.LossLayer = {\n    \n    guard let lossLayer = BNNS.LossLayer(input: fullyConnectedOutput,\n                                         output: lossOutput,\n                                         lossFunction: .softmaxCrossEntropy(labelSmoothing: 0),\n                                         lossReduction: .reductionMean,\n                                         filterParameters: filterParameters) else {\n        fatalError(\"Unable to create `lossLayer`.\")\n    }\n    \n    return lossLayer\n}()\n```\n\n### Create the candidate input\n\nFor each iteration of the training phase, the sample creates a matrix that represents a random digit, and a *one-hot* encoded tensor of the same digit. The sample places digits randomly in a 20 x 20 matrix, so a `3` might appear in the matrix as the image below. This example renders `0` as `⚪️`, and `1` as `⚫️`.\n\n```\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚫️⚫️⚫️⚫️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚫️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚫️⚫️⚫️⚫️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚫️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚫️⚫️⚫️⚫️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️⚪️\n```\n\nThe one-hot encoded tensor contains a `1` at the zero-based index of `3`.\n\n```\n[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n```\n\nThe sample code project uses a batch size of 32, so each iteration generates 32 random digits in random positions in the 20 x 20 grid.\n\n### Perform the forward pass\n\nTo perform the forward pass, the sample code calls `apply` on the fused, pooling, and fully connected layers.\n\n```swift\ntry fusedConvBatchNormLayer.apply(batchSize: batchSize,\n                                  input: input,\n                                  output: batchNormOutput,\n                                  for: .training)\n\ntry poolingLayer.apply(batchSize: batchSize,\n                       input: batchNormOutput,\n                       output: poolingOutput)\n\ntry fullyConnectedLayer.apply(batchSize: batchSize,\n                              input: poolingOutput,\n                              output: fullyConnectedOutput)\n```\n\n### Calculate the loss and loss gradient\n\nCalculating the loss evaluates the efficacy of the neural network. The loss layer generates its output, `lossOutput`, which contains a score that indicates how the predicted values deviate from the labels, and `lossInputGradient`, which is the output gradient parameter to the backward application of the fully connected layer.\n\n```swift\ntry lossLayer.apply(batchSize: batchSize,\n                    input: fullyConnectedOutput,\n                    labels: oneHotLabels,\n                    output: lossOutput,\n                    generatingInputGradient: lossInputGradient)\n```\n\n### Create the optimizer\n\nThe optimizer is responsible for updating the weights, biases, beta, and gamma. In the code below, the sample code project creates an optimizer using the Adam algorithm:\n\n```swift\nstatic var adam = BNNS.AdamOptimizer(learningRate: 0.01,\n                                     timeStep: 1,\n                                     gradientScale: 1,\n                                     regularizationScale: 0.01,\n                                     gradientClipping: .byValue(bounds: -0.5 ... 0.5),\n                                     regularizationFunction: BNNSOptimizerRegularizationL2)\n```\n\n### Perform a backward pass and optimization step on the fully connected layer\n\nThe sample code project performs the backward pass in reverse order to the forward pass. Therefore, the sample’s first step is to call `applyBackward` on the fully connected layer, and perform an optimization step on its weights.\n\nThe `applyBackward` call on the fully connected layer generates an input gradient that acts as the output gradient for the pooling layer’s backward apply, and a weights gradient that passes to the fully connected optimizer step.\n\n```swift\ntry fullyConnectedLayer.applyBackward(\n    batchSize: batchSize,\n    input: poolingOutput,\n    output: fullyConnectedOutput,\n    outputGradient: lossInputGradient,\n    generatingInputGradient: fullyConnectedInputGradient,\n    generatingWeightsGradient: fullyConnectedWeightGradient)\n```\n\n### Perform a backward pass on the pooling layer\n\nThe backward pass on the pooling layer generates an input gradient that’s the output gradient to the backward apply of the fused layer.\n\n```swift\ntry poolingLayer.applyBackward(\n    batchSize: batchSize,\n    input: batchNormOutput,\n    output: poolingOutput,\n    outputGradient: fullyConnectedInputGradient,\n    generatingInputGradient: poolingInputGradientArray)\n```\n\n### Perform a backward pass and optimization step on the fused layer\n\nThe sample calls `applyBackward` on the fused layer. This performs an optimization step on the convolution layer’s weights and bias, and the normalization layer’s beta and gamma.\n\n```swift\nlet gradientParameters = [convolutionWeightGradient,\n                          convolutionBiasGradient,\n                          batchNormBetaGradient,\n                          batchNormGammaGradient]\n\ntry fusedConvBatchNormLayer.applyBackward(\n    batchSize: batchSize,\n    input: input,\n    output: batchNormOutput,\n    outputGradient: poolingInputGradientArray,\n    generatingInputGradient: convolutionInputGradient,\n    generatingParameterGradients: gradientParameters)\n```\n\nThe code below performs the optimization step:\n\n```swift\ntry adam.step(\n    parameters: [fullyConnectedWeights,\n                 convolutionWeights, convolutionBias,\n                 batchNormBeta, batchNormGamma],\n    gradients: [fullyConnectedWeightGradient,\n                convolutionWeightGradient, convolutionBiasGradient,\n                batchNormBetaGradient, batchNormGammaGradient],\n    accumulators: [fullyConnectedWeightAccumulator1,\n                   convolutionWeightAccumulator1, convolutionBiasAccumulator1,\n                   batchNormBetaAccumulator1, batchNormGammaAccumulator1,\n                   fullyConnectedWeightAccumulator2,\n                   convolutionWeightAccumulator2, convolutionBiasAccumulator2,\n                   batchNormBetaAccumulator2, batchNormGammaAccumulator2],\n    filterParameters: filterParameters)\n```\n\nAfter the app completes all the optimization steps for this iteration, it increments the optimizer time step.\n\n```swift\nadam.timeStep += 1\n```\n\n### Evaluate the neural network\n\nThe sample iterates over the forward, loss, backward, and optimization steps, and with each iteration, the trend of the loss is to reduce. The following graph shows the loss, as a solid stroke, decreasing during training:\n\n\n\nThe code in the sample defines a maximum number of iterations. Additionally, it calculates a moving average of recent loss values, which appear as a dashed stroke in the graph above. At each iteration, the sample checks whether the recent average loss is below that threshold, and, if it is, it breaks from the training phase early.\n\n```swift\nlet maximumIterationCount = 1000\n\n\/\/ The `recentLosses` array contains the last `recentLossesCount` losses.\nlet recentLossesCount = 20\nvar recentLosses = [Float]()\n\n\/\/ The `averageRecentLossThreshold` constant defines the loss threshold\n\/\/ at which to consider the training phase complete.\nlet averageRecentLossThreshold = Float(0.125)\n\nfor epoch in 0 ..< maximumIterationCount {\n    if epoch == 500 {\n        adam.learningRate \/= 10\n    }\n    \n    generateInputAndLabels()\n    forwardPass()\n    computeLoss()\n    \n    guard let loss = lossOutput.makeArray(of: Float.self,\n                                          batchSize: 1)?.first else {\n        print(\"Unable to calculate loss.\")\n        return\n    }\n    \n    if recentLosses.isEmpty {\n        recentLosses = [Float](repeating: loss,\n                               count: recentLossesCount)\n    }\n    \n    recentLosses[epoch % recentLossesCount] = loss\n    let averageRecentLoss = vDSP.mean(recentLosses)\n    \n    if epoch % 10 == 0 {\n        print(\"Epoch \\(epoch): \\(loss) : \\(averageRecentLoss)\")\n    }\n    \n    if averageRecentLoss < averageRecentLossThreshold {\n        print(\"Recent average loss: \\(averageRecentLoss), breaking at epoch \\(epoch).\")\n        break\n    }\n    \n    backwardPass()\n}\n```\n\nAfter the training phase completes, the sample calculates the accuracy of the network over a new dataset. It then creates a new batch of random digits and runs a forward pass of the network.\n\n```swift\ntry fusedConvBatchNormLayer.apply(batchSize: batchSize,\n                                  input: input,\n                                  output: batchNormOutput,\n                                  for: .inference)\n\ntry poolingLayer.apply(batchSize: batchSize,\n                       input: batchNormOutput,\n                       output: poolingOutput)\n\ntry fullyConnectedLayer.apply(batchSize: batchSize,\n                              input: poolingOutput,\n                              output: fullyConnectedOutput)\n```\n\nFinally, the app evaluates the accuracy of the network by comparing the values in the fully connected layer’s output to the one-hot labels. For example, when the recognized digit is `3`, one-hot labels contain the values `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`, and values in the fully connected layer’s output might be as follows:\n\n```\n[-2.51, -3.62, -0.10, 8.52, -0.42, 5.11, -1.65, 1.34,  0.82, -2.77]\n[-3.94, -2.74, -0.30, 8.39, -1.45, 6.02, -0.66, 3.25,  0.49, -3.19]\n[-2.51, -2.77, -0.77, 8.41, -0.82, 4.87, -0.37, 2.32, -0.49, -3.05]\n[-3.01, -2.79,  0.48, 7.95, -2.57, 4.55, -1.05, 1.67,  1.38, -1.43]\n[-2.48, -1.59, -0.97, 7.59, -2.52, 4.00,  0.95, 4.02, -2.10, -1.62]\n```\n\nNote that in each case, the highest value in the fully connected layer’s output is at index `3`.\n\nThe following code performs that evaluation for each digit in the batch:\n\n```swift\nguard\n    let fullyConnected = fullyConnectedOutput.makeArray(\n        of: Float.self,\n        batchSize: batchSize),\n    let labels = oneHotLabels.makeArray(\n        of: Float.self,\n        batchSize: batchSize) else {\n    fatalError(\"Unable to create arrays for evaluation.\")\n}\n\nvar correctCount = 0\n\nfor sample in 0 ..< batchSize {\n    let offset = fullyConnectedOutputWidth * sample\n    \n    let fullyConnectedBatch = fullyConnected[offset ..< offset + fullyConnectedOutputWidth]\n    let predictedDigit = vDSP.indexOfMaximum(fullyConnectedBatch).0\n    \n    let oneHotLabelsBatch = labels[offset ..< offset + fullyConnectedOutputWidth]\n    let label = vDSP.indexOfMaximum(oneHotLabelsBatch).0\n    \n    if label == predictedDigit {\n        correctCount += 1\n    }\n    \n    print(\"Sample \\(sample) — digit: \\(label) | prediction: \\(predictedDigit)\")\n}\n```\n\nThe evaluation function prints out something like the following:\n\n```\nSample 0 — digit: 7 | prediction: 7\nSample 1 — digit: 5 | prediction: 5\nSample 2 — digit: 7 | prediction: 7\nSample 3 — digit: 7 | prediction: 7\nSample 4 — digit: 0 | prediction: 0\nSample 5 — digit: 8 | prediction: 8\nSample 6 — digit: 3 | prediction: 3\nSample 7 — digit: 6 | prediction: 6\nSample 8 — digit: 2 | prediction: 2\nSample 9 — digit: 7 | prediction: 7\n[ ... ]\n```\n\nIn this case, the neural network accurately predicts each ground truth digit.\n\n## Neural Networks\n\n- **BNNS**: Implement and run neural networks for training and inference.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Implement and run neural networks for training and inference.",
          "name" : "BNNS",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/bnns-library"
        }
      ],
      "title" : "Neural Networks"
    }
  ],
  "source" : "appleJSON",
  "title" : "Training a neural network to recognize digits",
  "url" : "https:\/\/developer.apple.com\/documentation\/accelerate\/training-a-neural-network-to-recognize-digits"
}