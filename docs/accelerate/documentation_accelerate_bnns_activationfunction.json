{
  "abstract" : "Constants that describe activation functions.",
  "codeExamples" : [

  ],
  "contentHash" : "25d34b397da57ff2c19aae5f8ca1e604feb6ade0b6d21a01d769e60daa206c17",
  "crawledAt" : "2025-12-01T02:55:47Z",
  "declaration" : {
    "code" : "enum ActivationFunction",
    "language" : "swift"
  },
  "id" : "8B258F40-FFD4-4C69-BDE9-BA3E530D07CC",
  "kind" : "enum",
  "module" : "Accelerate",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst",
    "macOS",
    "tvOS",
    "visionOS",
    "watchOS"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\ncrawled: 2025-12-01T02:55:47Z\n---\n\n# BNNS.ActivationFunction\n\n**Enumeration**\n\nConstants that describe activation functions.\n\n## Declaration\n\n```swift\nenum ActivationFunction\n```\n\n## Activation Functions\n\n- **BNNS.ActivationFunction.abs**: An activation function that returns the absolute value of its input.\n- **BNNS.ActivationFunction.celu(alpha:)**: An activation function that evaluates the continuously differentiable exponential linear units (CELU) on its input.\n- **BNNS.ActivationFunction.clamp(bounds:)**: An activation function that returns its input clamped to the specified range.\n- **BNNS.ActivationFunction.clampedLeakyRectifiedLinear(alpha:beta:)**: An activation function that returns its input clamped to beta when that is greater than or equal to zero, otherwise it returns its input multiplied by alpha clamped to beta.\n- **BNNS.ActivationFunction.elu(alpha:)**: An activation function that evaluates the exponential linear units (ELU) on its input.\n- **BNNS.ActivationFunction.geluApproximation(alpha:beta:)**: An activation function that evaluates the Gaussian error linear units (GELU) approximation on its input.\n- **BNNS.ActivationFunction.geluApproximation2(alpha:beta:)**: An activation function that provides a fast evaluation of the Gaussian error linear units (GELU) approximation on its input.\n- **BNNS.ActivationFunction.gumbel(alpha:beta:)**: An activation function that returns random numbers from the Gumbel distribution.\n- **BNNS.ActivationFunction.gumbelMax(alpha:beta:)**: An activation function that returns random numbers from the Gumbel distribution.\n- **BNNS.ActivationFunction.hardShrink(alpha:)**: An activation function that returns zero when the absolute input is less than alpha, otherwise it returns its input.\n- **BNNS.ActivationFunction.hardSigmoid(alpha:beta:)**: An activation function that returns the hard sigmoid function of its input.\n- **BNNS.ActivationFunction.hardSwish(alpha:beta:)**: An activation function that returns the hard swish function of its input.\n- **BNNS.ActivationFunction.identity**: An activation function that returns its input.\n- **BNNS.ActivationFunction.leakyRectifiedLinear(alpha:)**: An activation function that returns its input when that is greater than or equal to zero, otherwise it returns its input multiplied by a specified value.\n- **BNNS.ActivationFunction.linear(alpha:)**: An activation function that returns its input multiplied by a specified value.\n- **BNNS.ActivationFunction.linearWithBias(alpha:beta:)**: An activation function that returns its input multiplied by a scale and added to a bias.\n- **BNNS.ActivationFunction.logSigmoid**: An activation function that returns the logarithm of the sigmoid function of its input.\n- **BNNS.ActivationFunction.logSoftmax**: An activation function that returns the logarithm of the softmax function of its input.\n- **BNNS.ActivationFunction.rectifiedLinear**: An activation function that returns its input when that is greater than or equal to zero, otherwise it returns zero.\n- **BNNS.ActivationFunction.scaledTanh(alpha:beta:)**: An activation function that returns the scaled hyperbolic tangent of its input.\n- **BNNS.ActivationFunction.selu**: An activation function that evaluates the scaled exponential linear units (SELU) on its input.\n- **BNNS.ActivationFunction.sigmoid**: An activation function that returns the sigmoid function of its input.\n- **BNNS.ActivationFunction.silu**: An activation function that returns the sigmoid linear unit (SiLU) function of its input.\n- **BNNS.ActivationFunction.softShrink(alpha:)**: An activation function that returns zero when the absolute input is less than alpha, otherwise it returns its input minus alpha.\n- **BNNS.ActivationFunction.softmax**: An activation function that returns the softmax function of its input.\n- **BNNS.ActivationFunction.softplus(alpha:beta:)**: An activation function that returns the softplus function of its input.\n- **BNNS.ActivationFunction.softsign**: An activation function that returns the softsign function of its input.\n- **BNNS.ActivationFunction.tanh**: An activation function that returns the hyperbolic tangent of its input.\n- **BNNS.ActivationFunction.tanhShrink**: An activation function that returns its input minus the hyperbolic tangent of its input.\n- **BNNS.ActivationFunction.threshold(alpha:beta:)**: An activation function that returns beta if its input is less than a specified threshold, otherwise it returns its input.\n\n## Instance Properties\n\n- **bnnsActivation**: The underlying activation function.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "An activation function that returns the absolute value of its input.",
          "name" : "BNNS.ActivationFunction.abs",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/abs"
        },
        {
          "description" : "An activation function that evaluates the continuously differentiable exponential linear units (CELU) on its input.",
          "name" : "BNNS.ActivationFunction.celu(alpha:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/celu(alpha:)"
        },
        {
          "description" : "An activation function that returns its input clamped to the specified range.",
          "name" : "BNNS.ActivationFunction.clamp(bounds:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/clamp(bounds:)"
        },
        {
          "description" : "An activation function that returns its input clamped to beta when that is greater than or equal to zero, otherwise it returns its input multiplied by alpha clamped to beta.",
          "name" : "BNNS.ActivationFunction.clampedLeakyRectifiedLinear(alpha:beta:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/clampedLeakyRectifiedLinear(alpha:beta:)"
        },
        {
          "description" : "An activation function that evaluates the exponential linear units (ELU) on its input.",
          "name" : "BNNS.ActivationFunction.elu(alpha:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/elu(alpha:)"
        },
        {
          "description" : "An activation function that evaluates the Gaussian error linear units (GELU) approximation on its input.",
          "name" : "BNNS.ActivationFunction.geluApproximation(alpha:beta:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/geluApproximation(alpha:beta:)"
        },
        {
          "description" : "An activation function that provides a fast evaluation of the Gaussian error linear units (GELU) approximation on its input.",
          "name" : "BNNS.ActivationFunction.geluApproximation2(alpha:beta:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/geluApproximation2(alpha:beta:)"
        },
        {
          "description" : "An activation function that returns random numbers from the Gumbel distribution.",
          "name" : "BNNS.ActivationFunction.gumbel(alpha:beta:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/gumbel(alpha:beta:)"
        },
        {
          "description" : "An activation function that returns random numbers from the Gumbel distribution.",
          "name" : "BNNS.ActivationFunction.gumbelMax(alpha:beta:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/gumbelMax(alpha:beta:)"
        },
        {
          "description" : "An activation function that returns zero when the absolute input is less than alpha, otherwise it returns its input.",
          "name" : "BNNS.ActivationFunction.hardShrink(alpha:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/hardShrink(alpha:)"
        },
        {
          "description" : "An activation function that returns the hard sigmoid function of its input.",
          "name" : "BNNS.ActivationFunction.hardSigmoid(alpha:beta:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/hardSigmoid(alpha:beta:)"
        },
        {
          "description" : "An activation function that returns the hard swish function of its input.",
          "name" : "BNNS.ActivationFunction.hardSwish(alpha:beta:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/hardSwish(alpha:beta:)"
        },
        {
          "description" : "An activation function that returns its input.",
          "name" : "BNNS.ActivationFunction.identity",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/identity"
        },
        {
          "description" : "An activation function that returns its input when that is greater than or equal to zero, otherwise it returns its input multiplied by a specified value.",
          "name" : "BNNS.ActivationFunction.leakyRectifiedLinear(alpha:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/leakyRectifiedLinear(alpha:)"
        },
        {
          "description" : "An activation function that returns its input multiplied by a specified value.",
          "name" : "BNNS.ActivationFunction.linear(alpha:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/linear(alpha:)"
        },
        {
          "description" : "An activation function that returns its input multiplied by a scale and added to a bias.",
          "name" : "BNNS.ActivationFunction.linearWithBias(alpha:beta:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/linearWithBias(alpha:beta:)"
        },
        {
          "description" : "An activation function that returns the logarithm of the sigmoid function of its input.",
          "name" : "BNNS.ActivationFunction.logSigmoid",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/logSigmoid"
        },
        {
          "description" : "An activation function that returns the logarithm of the softmax function of its input.",
          "name" : "BNNS.ActivationFunction.logSoftmax",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/logSoftmax"
        },
        {
          "description" : "An activation function that returns its input when that is greater than or equal to zero, otherwise it returns zero.",
          "name" : "BNNS.ActivationFunction.rectifiedLinear",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/rectifiedLinear"
        },
        {
          "description" : "An activation function that returns the scaled hyperbolic tangent of its input.",
          "name" : "BNNS.ActivationFunction.scaledTanh(alpha:beta:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/scaledTanh(alpha:beta:)"
        },
        {
          "description" : "An activation function that evaluates the scaled exponential linear units (SELU) on its input.",
          "name" : "BNNS.ActivationFunction.selu",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/selu"
        },
        {
          "description" : "An activation function that returns the sigmoid function of its input.",
          "name" : "BNNS.ActivationFunction.sigmoid",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/sigmoid"
        },
        {
          "description" : "An activation function that returns the sigmoid linear unit (SiLU) function of its input.",
          "name" : "BNNS.ActivationFunction.silu",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/silu"
        },
        {
          "description" : "An activation function that returns zero when the absolute input is less than alpha, otherwise it returns its input minus alpha.",
          "name" : "BNNS.ActivationFunction.softShrink(alpha:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/softShrink(alpha:)"
        },
        {
          "description" : "An activation function that returns the softmax function of its input.",
          "name" : "BNNS.ActivationFunction.softmax",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/softmax"
        },
        {
          "description" : "An activation function that returns the softplus function of its input.",
          "name" : "BNNS.ActivationFunction.softplus(alpha:beta:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/softplus(alpha:beta:)"
        },
        {
          "description" : "An activation function that returns the softsign function of its input.",
          "name" : "BNNS.ActivationFunction.softsign",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/softsign"
        },
        {
          "description" : "An activation function that returns the hyperbolic tangent of its input.",
          "name" : "BNNS.ActivationFunction.tanh",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/tanh"
        },
        {
          "description" : "An activation function that returns its input minus the hyperbolic tangent of its input.",
          "name" : "BNNS.ActivationFunction.tanhShrink",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/tanhShrink"
        },
        {
          "description" : "An activation function that returns beta if its input is less than a specified threshold, otherwise it returns its input.",
          "name" : "BNNS.ActivationFunction.threshold(alpha:beta:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/threshold(alpha:beta:)"
        }
      ],
      "title" : "Activation Functions"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "The underlying activation function.",
          "name" : "bnnsActivation",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction\/bnnsActivation"
        }
      ],
      "title" : "Instance Properties"
    }
  ],
  "source" : "appleJSON",
  "title" : "BNNS.ActivationFunction",
  "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationFunction"
}