{
  "abstract" : "Returns a new Adam optimizer object with gradient clipped by value or clipped by norm.",
  "codeExamples" : [

  ],
  "contentHash" : "8f0df64762452693cfccd2c6a1d2385449ad9d1cfc8d6b35b889e69b929c2f0e",
  "crawledAt" : "2025-12-02T01:23:41Z",
  "declaration" : {
    "code" : "init(learningRate: Float = 0.001, beta1: Float = 0.9, beta2: Float = 0.999, timeStep: Float, epsilon: Float = 1e-8, gradientScale: Float, regularizationScale: Float, gradientClipping: BNNS.GradientClipping, regularizationFunction: BNNSOptimizerRegularizationFunction, usesAMSGrad: Bool = false)",
    "language" : "swift"
  },
  "id" : "294B4936-D5FE-49FA-9742-640723394328",
  "kind" : "unknown",
  "module" : "Accelerate",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst",
    "macOS",
    "tvOS",
    "visionOS",
    "watchOS"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/AdamOptimizer\/init(learningRate:beta1:beta2:timeStep:epsilon:gradientScale:regularizationScale:gradientClipping:regularizationFunction:usesAMSGrad:)\ncrawled: 2025-12-02T01:23:41Z\n---\n\n# init(learningRate:beta1:beta2:timeStep:epsilon:gradientScale:regularizationScale:gradientClipping:regularizationFunction:usesAMSGrad:)\n\n**Initializer**\n\nReturns a new Adam optimizer object with gradient clipped by value or clipped by norm.\n\n## Declaration\n\n```swift\ninit(learningRate: Float = 0.001, beta1: Float = 0.9, beta2: Float = 0.999, timeStep: Float, epsilon: Float = 1e-8, gradientScale: Float, regularizationScale: Float, gradientClipping: BNNS.GradientClipping, regularizationFunction: BNNSOptimizerRegularizationFunction, usesAMSGrad: Bool = false)\n```\n\n## Parameters\n\n- **learningRate**: A value that specifies the learning rate.\n- **beta1**: A value that specifies the first-moment constant, in the range `0` to `1`.\n- **beta2**: A value that specifies the second-moment constant, in the range `0` to `1`.\n- **timeStep**: A value that’s at least `1` and represents the optimizer’s current time.\n- **epsilon**: The epsilon value you use to improve numerical stability.\n- **gradientScale**: A value that specifies the gradient scaling factor.\n- **regularizationScale**: A value that specifies the regularization scaling factor.\n- **gradientClipping**: The gradient clipping function and bounds.\n- **regularizationFunction**: A value that specifies the regularization function.\n- **usesAMSGrad**: A Boolean value that specifies whether the optimizer should use the AMSGrad variant.\n\n## Creating an Adam Optimizer\n\n- **init(learningRate:beta1:beta2:timeStep:epsilon:gradientScale:regularizationScale:clipsGradientsTo:regularizationFunction:)**: Returns a new Adam optimizer object.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Returns a new Adam optimizer object.",
          "name" : "init(learningRate:beta1:beta2:timeStep:epsilon:gradientScale:regularizationScale:clipsGradientsTo:regularizationFunction:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/AdamOptimizer\/init(learningRate:beta1:beta2:timeStep:epsilon:gradientScale:regularizationScale:clipsGradientsTo:regularizationFunction:)"
        }
      ],
      "title" : "Creating an Adam Optimizer"
    }
  ],
  "source" : "appleJSON",
  "title" : "init(learningRate:beta1:beta2:timeStep:epsilon:gradientScale:regularizationScale:gradientClipping:regularizationFunction:usesAMSGrad:)",
  "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/AdamOptimizer\/init(learningRate:beta1:beta2:timeStep:epsilon:gradientScale:regularizationScale:gradientClipping:regularizationFunction:usesAMSGrad:)"
}