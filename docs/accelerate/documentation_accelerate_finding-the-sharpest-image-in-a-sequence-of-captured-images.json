{
  "abstract" : "Share image data between vDSP and vImage to compute the sharpest image from a bracketed photo sequence.",
  "codeExamples" : [
    {
      "code" : "captureSession.sessionPreset = .hd1280x720",
      "language" : "swift"
    },
    {
      "code" : "let pixelFormat: FourCharCode = {\n    if photoOutput.availablePhotoPixelFormatTypes\n        .contains(kCVPixelFormatType_420YpCbCr8BiPlanarFullRange) {\n        return kCVPixelFormatType_420YpCbCr8BiPlanarFullRange\n    } else if photoOutput.availablePhotoPixelFormatTypes\n        .contains(kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange) {\n        return kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange\n    } else {\n        fatalError(\"No available YpCbCr formats.\")\n    }\n}()",
      "language" : "swift"
    },
    {
      "code" : "let exposureSettings = (0 ..< photoOutput.maxBracketedCapturePhotoCount).map { _ in\n    AVCaptureAutoExposureBracketedStillImageSettings.autoExposureSettings(\n        exposureTargetBias: AVCaptureDevice.currentExposureTargetBias)\n}",
      "language" : "swift"
    },
    {
      "code" : "let photoSettings = AVCapturePhotoBracketSettings(\n    rawPixelFormatType: 0,\n    processedFormat: [kCVPixelBufferPixelFormatTypeKey as String: pixelFormat],\n    bracketedSettings: exposureSettings)",
      "language" : "swift"
    },
    {
      "code" : "photoOutput.capturePhoto(with: photoSettings,\n                         delegate: self)",
      "language" : "swift"
    },
    {
      "code" : "guard let pixelBuffer = photo.pixelBuffer else {\n    fatalError(\"Error acquiring pixel buffer.\")\n}\n\nCVPixelBufferLockBaseAddress(pixelBuffer,\n                             CVPixelBufferLockFlags.readOnly)",
      "language" : "swift"
    },
    {
      "code" : "let width = CVPixelBufferGetWidthOfPlane(pixelBuffer, 0)\nlet height = CVPixelBufferGetHeightOfPlane(pixelBuffer, 0)\nlet count = width * height\n\nlet lumaBaseAddress = CVPixelBufferGetBaseAddressOfPlane(pixelBuffer, 0)\nlet lumaRowBytes = CVPixelBufferGetBytesPerRowOfPlane(pixelBuffer, 0)\n\nlet lumaCopy = UnsafeMutableRawPointer.allocate(\n    byteCount: count,\n    alignment: MemoryLayout<Pixel_8>.alignment)\nlumaCopy.copyMemory(from: lumaBaseAddress!,\n                    byteCount: count)",
      "language" : "swift"
    },
    {
      "code" : "CVPixelBufferUnlockBaseAddress(pixelBuffer,\n                               CVPixelBufferLockFlags.readOnly)\n\nTask(priority: .utility) {\n    self.processImage(data: lumaCopy,\n                      rowBytes: lumaRowBytes,\n                      width: width,\n                      height: height,\n                      sequenceCount: photo.sequenceCount,\n                      expectedCount: photo.resolvedSettings.expectedPhotoCount,\n                      orientation: photo.metadata[ String(kCGImagePropertyOrientation) ] as? UInt32)\n    \n    lumaCopy.deallocate()\n}",
      "language" : "swift"
    },
    {
      "code" : "let imageBuffer = vImage.PixelBuffer(data: data,\n                                     width: width,\n                                     height: height,\n                                     byteCountPerRow: rowBytes,\n                                     pixelFormat: vImage.Planar8.self)",
      "language" : "swift"
    },
    {
      "code" : "let buffer0 = try? vImage_Buffer(width: 10,\n                                 height: 5,\n                                 bitsPerPixel: 8)\n\nlet buffer1 = vImage.PixelBuffer(width: 10,\n                                 height: 5,\n                                 pixelFormat: vImage.Planar8.self)                               "
    },
    {
      "code" : "var laplacianStorage = UnsafeMutableBufferPointer<Float>.allocate(capacity: width * height)\nlet laplacianBuffer = vImage.PixelBuffer(data: laplacianStorage.baseAddress!,\n                                         width: width,\n                                         height: height,\n                                         byteCountPerRow: width * MemoryLayout<Float>.stride,\n                                         pixelFormat: vImage.PlanarF.self)\ndefer {\n    laplacianStorage.deallocate()\n}\n\nimageBuffer.convert(to: laplacianBuffer)",
      "language" : "swift"
    },
    {
      "code" : "let laplacian: [Float] = [-1, -1, -1,\n                          -1,  8, -1,\n                          -1, -1, -1]",
      "language" : "swift"
    },
    {
      "code" : "vDSP.convolve(laplacianStorage,\n              rowCount: height,\n              columnCount: width,\n              with3x3Kernel: laplacian,\n              result: &laplacianStorage)",
      "language" : "swift"
    },
    {
      "code" : "extension AccelerateMutableBuffer where Element == Float {\n    var variance: Float {\n        \n        var mean = Float.nan\n        var standardDeviation = Float.nan\n        \n        self.withUnsafeBufferPointer {\n            vDSP_normalize($0.baseAddress!, 1,\n                           nil, 1,\n                           &mean, &standardDeviation,\n                           vDSP_Length(self.count))\n        }\n        \n        return standardDeviation * standardDeviation\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "var outputBuffer: vImage.PixelBuffer<Format>\nvar outputRotation: Int\n\nif orientation == .right || orientation == .left {\n    outputBuffer = vImage.PixelBuffer<Format>(width: sourceBuffer.height,\n                                              height: sourceBuffer.width)\n    \n    outputRotation = orientation == .right ?\n            kRotate90DegreesClockwise : kRotate90DegreesCounterClockwise\n} else if orientation == .up || orientation == .down {\n    outputBuffer = vImage.PixelBuffer<Format>(width: sourceBuffer.width,\n                                              height: sourceBuffer.height)\n    outputRotation = orientation == .down ?\n            kRotate180DegreesClockwise : kRotate0DegreesClockwise\n} else {\n    return nil\n}",
      "language" : "swift"
    },
    {
      "code" : "let imageFormat: vImage_CGImageFormat\n\nlet rotateFunction: (UnsafePointer<vImage_Buffer>,\n                     UnsafePointer<vImage_Buffer>,\n                     UInt8) -> vImage_Error\n\nif Format.self == vImage.Planar8.self {\n    imageFormat = vImage_CGImageFormat(\n        bitsPerComponent: 8,\n        bitsPerPixel: 8,\n        colorSpace: CGColorSpaceCreateDeviceGray(),\n        bitmapInfo: .init(rawValue: CGImageAlphaInfo.none.rawValue))!\n    \n    func rotate (src: UnsafePointer<vImage_Buffer>,\n                 dst: UnsafePointer<vImage_Buffer>,\n                 rotation: UInt8) -> vImage_Error {\n        vImageRotate90_Planar8(src, dst, rotation, 0, 0)\n    }\n    rotateFunction = rotate\n} else if Format.self == vImage.PlanarF.self {\n    imageFormat = vImage_CGImageFormat(\n        bitsPerComponent: 32,\n        bitsPerPixel: 32,\n        colorSpace: CGColorSpaceCreateDeviceGray(),\n        bitmapInfo: CGBitmapInfo(rawValue:\n                                    kCGBitmapByteOrder32Host.rawValue |\n                                    CGBitmapInfo.floatComponents.rawValue |\n                                    CGImageAlphaInfo.none.rawValue))!\n    \n    func rotate (src: UnsafePointer<vImage_Buffer>,\n                 dst: UnsafePointer<vImage_Buffer>,\n                 rotation: UInt8) -> vImage_Error {\n        vImageRotate90_PlanarF(src, dst, rotation, 0, 0)\n    }\n    rotateFunction = rotate\n} else {\n    fatalError(\"This function only supports Planar8 and PlanarF formats.\")\n}\n\nsourceBuffer.withUnsafePointerToVImageBuffer { src in\n    outputBuffer.withUnsafePointerToVImageBuffer { dst in\n        _ = rotateFunction(src, dst, UInt8(outputRotation))\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "return outputBuffer.makeCGImage(cgImageFormat: imageFormat)",
      "language" : "swift"
    }
  ],
  "contentHash" : "fcfbd1dc8efaf24d1191c36f4ec8e8b25d42e80a3d7a980116f9c70c6ed4c1f5",
  "crawledAt" : "2025-12-02T15:27:57Z",
  "id" : "FC65239B-A065-4D66-8E9B-E2854BEE50F6",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "Accelerate",
  "overview" : "## Overview\n\nThis sample code project captures a sequence of photographs and uses a combination of routines from vImage and vDSP to order the images by their relative sharpness. This technique is useful in applications such as an image scanner, where your user requires the least blurry captured image. After applying the routines, the app displays the images in a list, with the sharpest image at the top.\n\n\n\nThis project uses [doc:\/\/com.apple.documentation\/documentation\/SwiftUI] to build the user interface,  [doc:\/\/com.apple.documentation\/documentation\/AVFoundation] to capture a sequence of images, and a method known as *the variance of the Laplacian* to determine the sharpness of each image.\n\nBefore exploring the code, try building and running the app, and taking photographs of subjects such as documents and signs.\n\n### Configure the capture session\n\nThe 3 x 3 Laplacian kernel that this sample uses reports a lot of noise if applied to a full-resolution image. To reduce this noise, the sample uses a downscaled image and defines the capture session’s preset to a size that’s smaller than the camera’s native resolution:\n\nTo learn more about configuring a capture session, see [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/setting-up-a-capture-session].\n\n### Define the photo settings\n\nThe sample defines the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCapturePhotoBracketSettings] object, which specifies the capture features and settings, in the `BlurDetector.takePhoto()` function.\n\nThe sharpness detection algorithm in this sample works on a grayscale image. The camera’s YpCbCr pixel formats, either [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange] or [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/kCVPixelFormatType_420YpCbCr8BiPlanarFullRange], represent the luminance of the image using one plane and represent color information on separate planes. The code converts the luminance plane to a grayscale image.\n\nThe following code checks that the current device supports one or both of these formats:\n\nThe `exposureSettings` array contains [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureAutoExposureBracketedStillImageSettings] instances and defines the exposure target bias of each as [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureDevice\/currentExposureTargetBias]. The [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCapturePhotoOutput\/maxBracketedCapturePhotoCount] property of the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCapturePhotoOutput] object defines the maximum number of items in the array.\n\nThe following code uses the array of exposure settings and the first available YpCbCr format type to define the bracketed settings:\n\nThe `BlurDetector.takePhoto()` function passes the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCapturePhotoBracketSettings] instance to capture the sequence of images:\n\n### Acquire the captured image\n\nFor each captured image, AVFoundation calls the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCapturePhotoCaptureDelegate\/photoOutput(_:didFinishProcessingPhoto:error:)] method.\n\nThe sample uses the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCapturePhoto\/pixelBuffer] property of the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCapturePhoto] instance that AVFoundation supplies to acquire the uncompressed [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/cvpixelbuffer-q2e] that contains the captured photograph. While the code is accessing the pixel data of the pixel buffer, it calls [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/CVPixelBufferLockBaseAddress(_:_:)] to lock the base address:\n\nThe pixel buffer that AVFoundation vends contains two planes; the plane at index zero contains the luminance data. Because the sample app runs the sharpness detection code in a background thread, it calls [doc:\/\/com.apple.documentation\/documentation\/Swift\/UnsafeMutableRawPointer\/copyMemory(from:byteCount:)] to create a copy of the luminance data:\n\nAfter the code has copied the luminance data, it unlocks the pixel buffer’s base address and passes the copied luminance data to the processing function in a background thread:\n\n### Initialize grayscale source pixel buffer\n\nThe following code creates a pixel buffer from data passed to the `BlurDetector.processImage(data:rowBytes:width:height:sequenceCount:expectedCount:orientation:)` function:\n\nOn return, `sourceBuffer` contains a grayscale representation of the captured image.\n\n### Create floating point pixels to use with vDSP\n\nvImage buffers store their image data in row-major format. However, when you pass data between vImage and vDSP, be aware that, in some cases, vImage will add extra bytes at the end of each row. For example, the following code declares two 8-bit-per-pixel buffers that are 10 pixels wide:\n\nAlthough the code defines buffers with 10 bytes per row, to maximize performance, [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vImageBuffer_Init(_:_:_:_:_:)] and [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vImage\/PixelBuffer\/init(width:height:pixelFormat:)] both initialize a buffer with 16 bytes per row.\n\n\n\nIn some cases, this disparity between the row bytes used to hold image data and the buffer’s actual row bytes may not affect an app’s results. However, the sample app declares a [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vImage\/PixelBuffer] structure with external memory that has no additional padding. This ensures that the uninitialized data in the row padding doesn’t affect the blur detection algorithm.\n\nOn return, `laplacianStorage` and `laplacianBuffer` share the same memory that contains a 32-bit version of the image data in the `imageBuffer`.\n\n### Perform the convolution\n\nThe Laplacian kernel finds edges in the single-precision pixel values:\n\nThe vDSP convolve function performs the convolution in place on the `laplacianStorage` memory:\n\nAfter the convolution, edges in the image have high values. The following image shows the result after convolution using the Laplacian kernel:\n\n\n\n### Calculate the variance\n\nThe [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vDSP_normalize] function calculates the standard deviation of the pixel values after the edge detection. The following computed property returns the variance of a single-precision [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/AccelerateMutableBuffer] instance:\n\nThe sample app uses this value as a measure of relative sharpness. Images with more variance have more detail than those with less variance, and that difference is used to derive the relative sharpness.\n\n### Create a display image with the correct orientation\n\nThe sample app uses the vImage 90º rotation functions in conjunction with the [doc:\/\/com.apple.documentation\/documentation\/CoreGraphics\/CGImage] object’s orientation to create Core Graphics images that are suitable for displaying in the app. The `static BlurDetector.makeImage(fromPlanarBuffer:orientation:)` function accepts a planar buffer (either the grayscale representation of the captured image or the result of the convolution) and the orientation, and returns a `CGImage` instance.\n\nFor landscape images, meaning images with an orientation of [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/CGImagePropertyOrientation\/left] or [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/CGImagePropertyOrientation\/right], the function creates a destination buffer with a width equal to the height, and a height equal to the width of the supplied buffer. For portrait images, meaning images with an orientation of [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/CGImagePropertyOrientation\/up] or [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/CGImagePropertyOrientation\/down], the function creates a destination buffer with the same dimensions as the supplied buffer.\n\nThe following code populates the destination buffer using either  [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vImageRotate90_Planar8(_:_:_:_:_:)] or [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vImageRotate90_PlanarF(_:_:_:_:_:)]\n\nFinally, the function returns a [doc:\/\/com.apple.documentation\/documentation\/CoreGraphics\/CGImage] from the destination buffer:",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/Accelerate\/finding-the-sharpest-image-in-a-sequence-of-captured-images\ncrawled: 2025-12-02T15:27:57Z\n---\n\n# Finding the sharpest image in a sequence of captured images\n\n**Sample Code**\n\nShare image data between vDSP and vImage to compute the sharpest image from a bracketed photo sequence.\n\n## Overview\n\nThis sample code project captures a sequence of photographs and uses a combination of routines from vImage and vDSP to order the images by their relative sharpness. This technique is useful in applications such as an image scanner, where your user requires the least blurry captured image. After applying the routines, the app displays the images in a list, with the sharpest image at the top.\n\n\n\nThis project uses [doc:\/\/com.apple.documentation\/documentation\/SwiftUI] to build the user interface,  [doc:\/\/com.apple.documentation\/documentation\/AVFoundation] to capture a sequence of images, and a method known as *the variance of the Laplacian* to determine the sharpness of each image.\n\nBefore exploring the code, try building and running the app, and taking photographs of subjects such as documents and signs.\n\n### Configure the capture session\n\nThe 3 x 3 Laplacian kernel that this sample uses reports a lot of noise if applied to a full-resolution image. To reduce this noise, the sample uses a downscaled image and defines the capture session’s preset to a size that’s smaller than the camera’s native resolution:\n\n```swift\ncaptureSession.sessionPreset = .hd1280x720\n```\n\nTo learn more about configuring a capture session, see [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/setting-up-a-capture-session].\n\n### Define the photo settings\n\nThe sample defines the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCapturePhotoBracketSettings] object, which specifies the capture features and settings, in the `BlurDetector.takePhoto()` function.\n\nThe sharpness detection algorithm in this sample works on a grayscale image. The camera’s YpCbCr pixel formats, either [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange] or [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/kCVPixelFormatType_420YpCbCr8BiPlanarFullRange], represent the luminance of the image using one plane and represent color information on separate planes. The code converts the luminance plane to a grayscale image.\n\nThe following code checks that the current device supports one or both of these formats:\n\n```swift\nlet pixelFormat: FourCharCode = {\n    if photoOutput.availablePhotoPixelFormatTypes\n        .contains(kCVPixelFormatType_420YpCbCr8BiPlanarFullRange) {\n        return kCVPixelFormatType_420YpCbCr8BiPlanarFullRange\n    } else if photoOutput.availablePhotoPixelFormatTypes\n        .contains(kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange) {\n        return kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange\n    } else {\n        fatalError(\"No available YpCbCr formats.\")\n    }\n}()\n```\n\nThe `exposureSettings` array contains [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureAutoExposureBracketedStillImageSettings] instances and defines the exposure target bias of each as [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCaptureDevice\/currentExposureTargetBias]. The [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCapturePhotoOutput\/maxBracketedCapturePhotoCount] property of the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCapturePhotoOutput] object defines the maximum number of items in the array.\n\n```swift\nlet exposureSettings = (0 ..< photoOutput.maxBracketedCapturePhotoCount).map { _ in\n    AVCaptureAutoExposureBracketedStillImageSettings.autoExposureSettings(\n        exposureTargetBias: AVCaptureDevice.currentExposureTargetBias)\n}\n```\n\nThe following code uses the array of exposure settings and the first available YpCbCr format type to define the bracketed settings:\n\n```swift\nlet photoSettings = AVCapturePhotoBracketSettings(\n    rawPixelFormatType: 0,\n    processedFormat: [kCVPixelBufferPixelFormatTypeKey as String: pixelFormat],\n    bracketedSettings: exposureSettings)\n```\n\nThe `BlurDetector.takePhoto()` function passes the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCapturePhotoBracketSettings] instance to capture the sequence of images:\n\n```swift\nphotoOutput.capturePhoto(with: photoSettings,\n                         delegate: self)\n```\n\n### Acquire the captured image\n\nFor each captured image, AVFoundation calls the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCapturePhotoCaptureDelegate\/photoOutput(_:didFinishProcessingPhoto:error:)] method.\n\nThe sample uses the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCapturePhoto\/pixelBuffer] property of the [doc:\/\/com.apple.documentation\/documentation\/AVFoundation\/AVCapturePhoto] instance that AVFoundation supplies to acquire the uncompressed [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/cvpixelbuffer-q2e] that contains the captured photograph. While the code is accessing the pixel data of the pixel buffer, it calls [doc:\/\/com.apple.documentation\/documentation\/CoreVideo\/CVPixelBufferLockBaseAddress(_:_:)] to lock the base address:\n\n```swift\nguard let pixelBuffer = photo.pixelBuffer else {\n    fatalError(\"Error acquiring pixel buffer.\")\n}\n\nCVPixelBufferLockBaseAddress(pixelBuffer,\n                             CVPixelBufferLockFlags.readOnly)\n```\n\nThe pixel buffer that AVFoundation vends contains two planes; the plane at index zero contains the luminance data. Because the sample app runs the sharpness detection code in a background thread, it calls [doc:\/\/com.apple.documentation\/documentation\/Swift\/UnsafeMutableRawPointer\/copyMemory(from:byteCount:)] to create a copy of the luminance data:\n\n```swift\nlet width = CVPixelBufferGetWidthOfPlane(pixelBuffer, 0)\nlet height = CVPixelBufferGetHeightOfPlane(pixelBuffer, 0)\nlet count = width * height\n\nlet lumaBaseAddress = CVPixelBufferGetBaseAddressOfPlane(pixelBuffer, 0)\nlet lumaRowBytes = CVPixelBufferGetBytesPerRowOfPlane(pixelBuffer, 0)\n\nlet lumaCopy = UnsafeMutableRawPointer.allocate(\n    byteCount: count,\n    alignment: MemoryLayout<Pixel_8>.alignment)\nlumaCopy.copyMemory(from: lumaBaseAddress!,\n                    byteCount: count)\n```\n\nAfter the code has copied the luminance data, it unlocks the pixel buffer’s base address and passes the copied luminance data to the processing function in a background thread:\n\n```swift\nCVPixelBufferUnlockBaseAddress(pixelBuffer,\n                               CVPixelBufferLockFlags.readOnly)\n\nTask(priority: .utility) {\n    self.processImage(data: lumaCopy,\n                      rowBytes: lumaRowBytes,\n                      width: width,\n                      height: height,\n                      sequenceCount: photo.sequenceCount,\n                      expectedCount: photo.resolvedSettings.expectedPhotoCount,\n                      orientation: photo.metadata[ String(kCGImagePropertyOrientation) ] as? UInt32)\n    \n    lumaCopy.deallocate()\n}\n```\n\n### Initialize grayscale source pixel buffer\n\nThe following code creates a pixel buffer from data passed to the `BlurDetector.processImage(data:rowBytes:width:height:sequenceCount:expectedCount:orientation:)` function:\n\n```swift\nlet imageBuffer = vImage.PixelBuffer(data: data,\n                                     width: width,\n                                     height: height,\n                                     byteCountPerRow: rowBytes,\n                                     pixelFormat: vImage.Planar8.self)\n```\n\nOn return, `sourceBuffer` contains a grayscale representation of the captured image.\n\n### Create floating point pixels to use with vDSP\n\nvImage buffers store their image data in row-major format. However, when you pass data between vImage and vDSP, be aware that, in some cases, vImage will add extra bytes at the end of each row. For example, the following code declares two 8-bit-per-pixel buffers that are 10 pixels wide:\n\n```\nlet buffer0 = try? vImage_Buffer(width: 10,\n                                 height: 5,\n                                 bitsPerPixel: 8)\n\nlet buffer1 = vImage.PixelBuffer(width: 10,\n                                 height: 5,\n                                 pixelFormat: vImage.Planar8.self)                               \n```\n\nAlthough the code defines buffers with 10 bytes per row, to maximize performance, [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vImageBuffer_Init(_:_:_:_:_:)] and [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vImage\/PixelBuffer\/init(width:height:pixelFormat:)] both initialize a buffer with 16 bytes per row.\n\n\n\nIn some cases, this disparity between the row bytes used to hold image data and the buffer’s actual row bytes may not affect an app’s results. However, the sample app declares a [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vImage\/PixelBuffer] structure with external memory that has no additional padding. This ensures that the uninitialized data in the row padding doesn’t affect the blur detection algorithm.\n\n```swift\nvar laplacianStorage = UnsafeMutableBufferPointer<Float>.allocate(capacity: width * height)\nlet laplacianBuffer = vImage.PixelBuffer(data: laplacianStorage.baseAddress!,\n                                         width: width,\n                                         height: height,\n                                         byteCountPerRow: width * MemoryLayout<Float>.stride,\n                                         pixelFormat: vImage.PlanarF.self)\ndefer {\n    laplacianStorage.deallocate()\n}\n\nimageBuffer.convert(to: laplacianBuffer)\n```\n\nOn return, `laplacianStorage` and `laplacianBuffer` share the same memory that contains a 32-bit version of the image data in the `imageBuffer`.\n\n### Perform the convolution\n\nThe Laplacian kernel finds edges in the single-precision pixel values:\n\n```swift\nlet laplacian: [Float] = [-1, -1, -1,\n                          -1,  8, -1,\n                          -1, -1, -1]\n```\n\nThe vDSP convolve function performs the convolution in place on the `laplacianStorage` memory:\n\n```swift\nvDSP.convolve(laplacianStorage,\n              rowCount: height,\n              columnCount: width,\n              with3x3Kernel: laplacian,\n              result: &laplacianStorage)\n```\n\nAfter the convolution, edges in the image have high values. The following image shows the result after convolution using the Laplacian kernel:\n\n\n\n### Calculate the variance\n\nThe [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vDSP_normalize] function calculates the standard deviation of the pixel values after the edge detection. The following computed property returns the variance of a single-precision [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/AccelerateMutableBuffer] instance:\n\n```swift\nextension AccelerateMutableBuffer where Element == Float {\n    var variance: Float {\n        \n        var mean = Float.nan\n        var standardDeviation = Float.nan\n        \n        self.withUnsafeBufferPointer {\n            vDSP_normalize($0.baseAddress!, 1,\n                           nil, 1,\n                           &mean, &standardDeviation,\n                           vDSP_Length(self.count))\n        }\n        \n        return standardDeviation * standardDeviation\n    }\n}\n```\n\nThe sample app uses this value as a measure of relative sharpness. Images with more variance have more detail than those with less variance, and that difference is used to derive the relative sharpness.\n\n### Create a display image with the correct orientation\n\nThe sample app uses the vImage 90º rotation functions in conjunction with the [doc:\/\/com.apple.documentation\/documentation\/CoreGraphics\/CGImage] object’s orientation to create Core Graphics images that are suitable for displaying in the app. The `static BlurDetector.makeImage(fromPlanarBuffer:orientation:)` function accepts a planar buffer (either the grayscale representation of the captured image or the result of the convolution) and the orientation, and returns a `CGImage` instance.\n\nFor landscape images, meaning images with an orientation of [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/CGImagePropertyOrientation\/left] or [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/CGImagePropertyOrientation\/right], the function creates a destination buffer with a width equal to the height, and a height equal to the width of the supplied buffer. For portrait images, meaning images with an orientation of [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/CGImagePropertyOrientation\/up] or [doc:\/\/com.apple.documentation\/documentation\/ImageIO\/CGImagePropertyOrientation\/down], the function creates a destination buffer with the same dimensions as the supplied buffer.\n\n```swift\nvar outputBuffer: vImage.PixelBuffer<Format>\nvar outputRotation: Int\n\nif orientation == .right || orientation == .left {\n    outputBuffer = vImage.PixelBuffer<Format>(width: sourceBuffer.height,\n                                              height: sourceBuffer.width)\n    \n    outputRotation = orientation == .right ?\n            kRotate90DegreesClockwise : kRotate90DegreesCounterClockwise\n} else if orientation == .up || orientation == .down {\n    outputBuffer = vImage.PixelBuffer<Format>(width: sourceBuffer.width,\n                                              height: sourceBuffer.height)\n    outputRotation = orientation == .down ?\n            kRotate180DegreesClockwise : kRotate0DegreesClockwise\n} else {\n    return nil\n}\n```\n\nThe following code populates the destination buffer using either  [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vImageRotate90_Planar8(_:_:_:_:_:)] or [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vImageRotate90_PlanarF(_:_:_:_:_:)]\n\n```swift\nlet imageFormat: vImage_CGImageFormat\n\nlet rotateFunction: (UnsafePointer<vImage_Buffer>,\n                     UnsafePointer<vImage_Buffer>,\n                     UInt8) -> vImage_Error\n\nif Format.self == vImage.Planar8.self {\n    imageFormat = vImage_CGImageFormat(\n        bitsPerComponent: 8,\n        bitsPerPixel: 8,\n        colorSpace: CGColorSpaceCreateDeviceGray(),\n        bitmapInfo: .init(rawValue: CGImageAlphaInfo.none.rawValue))!\n    \n    func rotate (src: UnsafePointer<vImage_Buffer>,\n                 dst: UnsafePointer<vImage_Buffer>,\n                 rotation: UInt8) -> vImage_Error {\n        vImageRotate90_Planar8(src, dst, rotation, 0, 0)\n    }\n    rotateFunction = rotate\n} else if Format.self == vImage.PlanarF.self {\n    imageFormat = vImage_CGImageFormat(\n        bitsPerComponent: 32,\n        bitsPerPixel: 32,\n        colorSpace: CGColorSpaceCreateDeviceGray(),\n        bitmapInfo: CGBitmapInfo(rawValue:\n                                    kCGBitmapByteOrder32Host.rawValue |\n                                    CGBitmapInfo.floatComponents.rawValue |\n                                    CGImageAlphaInfo.none.rawValue))!\n    \n    func rotate (src: UnsafePointer<vImage_Buffer>,\n                 dst: UnsafePointer<vImage_Buffer>,\n                 rotation: UInt8) -> vImage_Error {\n        vImageRotate90_PlanarF(src, dst, rotation, 0, 0)\n    }\n    rotateFunction = rotate\n} else {\n    fatalError(\"This function only supports Planar8 and PlanarF formats.\")\n}\n\nsourceBuffer.withUnsafePointerToVImageBuffer { src in\n    outputBuffer.withUnsafePointerToVImageBuffer { dst in\n        _ = rotateFunction(src, dst, UInt8(outputRotation))\n    }\n}\n```\n\nFinally, the function returns a [doc:\/\/com.apple.documentation\/documentation\/CoreGraphics\/CGImage] from the destination buffer:\n\n```swift\nreturn outputBuffer.makeCGImage(cgImageFormat: imageFormat)\n```\n\n## vImage \/ vDSP Interoperability\n\n- **Visualizing sound as an audio spectrogram**: Share image data between vDSP and vImage to visualize audio that a device microphone captures.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Share image data between vDSP and vImage to visualize audio that a device microphone captures.",
          "name" : "Visualizing sound as an audio spectrogram",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/visualizing-sound-as-an-audio-spectrogram"
        }
      ],
      "title" : "vImage \/ vDSP Interoperability"
    }
  ],
  "source" : "appleJSON",
  "title" : "Finding the sharpest image in a sequence of captured images",
  "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/finding-the-sharpest-image-in-a-sequence-of-captured-images"
}