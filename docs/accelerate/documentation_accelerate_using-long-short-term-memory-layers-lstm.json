{
  "abstract" : "Add long short-term memory (LSTM) layers to recurrent neural networks to avoid long-term dependency problems.",
  "codeExamples" : [
    {
      "code" : "let inputDescriptor = BNNSLSTMDataDescriptor(data_desc: inputDataDescriptor,\n                                             hidden_desc: inputHiddenDescriptor,\n                                             cell_state_desc: inputCellStateDescriptor)\n\nlet outputDescriptor = BNNSLSTMDataDescriptor(data_desc: outputDataDescriptor,\n                                              hidden_desc: outputHiddenDescriptor,\n                                              cell_state_desc: outputCellStateDescriptor)",
      "language" : "swift"
    },
    {
      "code" : "let forgetGate = BNNSLSTMGateDescriptor(iw_desc: (forgetGateInputWeightsDescriptor,\n                                                  forgetGateInputWeightsDescriptor),\n                                        hw_desc: forgetGateHiddenWeightsDescriptor,\n                                        cw_desc: forgetGateCellStateWeightsDescriptor,\n                                        b_desc: forgetGateBiasDescriptor,\n                                        activation: BNNSActivation(function: .sigmoid))\n\nlet inputGate = BNNSLSTMGateDescriptor(iw_desc: (inputGateInputWeightsDescriptor,\n                                                 inputGateInputWeightsDescriptor),\n                                       hw_desc: inputGateHiddenWeightsDescriptor,\n                                       cw_desc: inputGateCellStateWeightsDescriptor,\n                                       b_desc: inputGateBiasDescriptor,\n                                       activation: BNNSActivation(function: .sigmoid))\n\nlet candidateGate = BNNSLSTMGateDescriptor(iw_desc: (candidateGateInputWeightsDescriptor,\n                                                     candidateGateInputWeightsDescriptor),\n                                           hw_desc: candidateGateHiddenWeightsDescriptor,\n                                           cw_desc: candidateGateCellStateWeightsDescriptor,\n                                           b_desc: candidateGateBiasDescriptor,\n                                           activation: BNNSActivation(function: .tanh))\n\nlet outputGate = BNNSLSTMGateDescriptor(iw_desc: (outputGateInputWeightsDescriptor,\n                                                  outputGateInputWeightsDescriptor),\n                                        hw_desc: outputGateHiddenWeightsDescriptor,\n                                        cw_desc: outputGateCellStateWeightsDescriptor,\n                                        b_desc: outputGateBiasDescriptor,\n                                        activation: BNNSActivation(function: .sigmoid))",
      "language" : "swift"
    },
    {
      "code" : " for (size_t o = 0; o < iw_desc.size[1]; o++)\n {\n  float res = bias[o]; \/\/ init with bias value\n  for (size_t i = 0; i < iw_desc.size[0]; i++) \/\/ matrix vector multiply\n   res += input[i] * input_weights[o][i];\n  for (size_t i = 0; i < hw_desc.size[0]; i++) \/\/ matrix vector multiply\n   res += hidden[i] * hidden_weights[o][i];\n  for (size_t i = 0; i < cw_desc.size[0]; i++) \/\/ matrix vector multiply\n   res += cell[i] * cell_weights[o][i];\n  out[i] = activation.func(res); \/\/ apply activation function\n }",
      "language" : "objc"
    },
    {
      "code" : "var layerParams = BNNSLayerParametersLSTM(input_size:  ... ,\n                                          hidden_size:  ... ,\n                                          batch_size: ... ,\n                                          num_layers: ... ,\n                                          seq_len: ... ,\n                                          dropout: ... ,\n                                          lstm_flags: BNNSLayerFlagsLSTMDefaultActivations.rawValue,\n                                          sequence_descriptor: BNNSNDArrayDescriptor(),\n                                          input_descriptor: inputDescriptor,\n                                          output_descriptor: outputDescriptor,\n                                          input_gate: inputGate,\n                                          forget_gate: forgetGate,\n                                          candidate_gate: candidateGate,\n                                          output_gate: outputGate,\n                                          hidden_activation: BNNSActivation(function: .identity))",
      "language" : "swift"
    },
    {
      "code" : "let trainingCacheBufferBytes = BNNSComputeLSTMTrainingCacheCapacity(&layerParams)\n\nlet trainingCache = UnsafeMutableRawPointer.allocate(byteCount: trainingCacheBufferBytes,\n                                                     alignment: 1)\ndefer {\n    trainingCache.deallocate()\n}",
      "language" : "swift"
    },
    {
      "code" : "BNNSDirectApplyLSTMBatchTrainingCaching(&layerParams,\n                                        nil,\n                                        trainingCache,\n                                        trainingCacheBufferBytes)",
      "language" : "swift"
    }
  ],
  "contentHash" : "5a29d2c2fea96ab8f3b97a95c4eeb526672be73dc6ab6a5e706cffbbadb5af43",
  "crawledAt" : "2025-12-02T01:53:06Z",
  "id" : "65850EDF-6CC3-4F7F-9531-924822E7C0EE",
  "kind" : "article",
  "module" : "Accelerate",
  "overview" : "## Overview\n\nAn LSTM layer consists of four gates that manipulate cell-state data:\n\nThe following figure illustrates the components of an LSTM layer. The inputs are the cell-state (c), the hidden state (h), and the input data (x). The outputs are the updated cell-state (c) and hidden state (h):\n\n\n\nNote that the default activation function for the forget, input, and output gates is [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/BNNSActivationFunction\/sigmoid]; the default activation function for the candidate gate is [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/BNNSActivationFunction\/tanh].\n\nBNNS provides direct apply functions for forward and backward LSTM passes, that is, you don’t need to create an explicit LSTM layer. Rather, you create descriptors of the input, output, and gates to create a parameters structure, and pass the parameters structure to the apply function.\n\nThe input and out descriptors require n-dimensional array descriptors for the data, hidden state, and cell-state:\n\nDefine each gate by specifying input, hidden, and cell-state weights, and bias:\n\nThe following code shows how each gate computes its output:\n\nGive the descriptors for the input, output, and gates, you’re ready to create the parameters structure:\n\nLSTM provides the option to define a training cache that stores intermediate results to accelerate backward computation. Use [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/BNNSComputeLSTMTrainingCacheCapacity(_:)] to compute the size, in bytes, of the training cache:\n\nFinally, call [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/BNNSDirectApplyLSTMBatchTrainingCaching(_:_:_:_:)] to apply the LSTM layer:",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/Accelerate\/using-long-short-term-memory-layers-lstm\ncrawled: 2025-12-02T01:53:06Z\n---\n\n# Using Long Short-Term Memory Layers (LSTM)\n\n**Article**\n\nAdd long short-term memory (LSTM) layers to recurrent neural networks to avoid long-term dependency problems.\n\n## Overview\n\nAn LSTM layer consists of four gates that manipulate cell-state data:\n\n\n\nThe following figure illustrates the components of an LSTM layer. The inputs are the cell-state (c), the hidden state (h), and the input data (x). The outputs are the updated cell-state (c) and hidden state (h):\n\n\n\nNote that the default activation function for the forget, input, and output gates is [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/BNNSActivationFunction\/sigmoid]; the default activation function for the candidate gate is [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/BNNSActivationFunction\/tanh].\n\nBNNS provides direct apply functions for forward and backward LSTM passes, that is, you don’t need to create an explicit LSTM layer. Rather, you create descriptors of the input, output, and gates to create a parameters structure, and pass the parameters structure to the apply function.\n\nThe input and out descriptors require n-dimensional array descriptors for the data, hidden state, and cell-state:\n\n```swift\nlet inputDescriptor = BNNSLSTMDataDescriptor(data_desc: inputDataDescriptor,\n                                             hidden_desc: inputHiddenDescriptor,\n                                             cell_state_desc: inputCellStateDescriptor)\n\nlet outputDescriptor = BNNSLSTMDataDescriptor(data_desc: outputDataDescriptor,\n                                              hidden_desc: outputHiddenDescriptor,\n                                              cell_state_desc: outputCellStateDescriptor)\n```\n\nDefine each gate by specifying input, hidden, and cell-state weights, and bias:\n\n```swift\nlet forgetGate = BNNSLSTMGateDescriptor(iw_desc: (forgetGateInputWeightsDescriptor,\n                                                  forgetGateInputWeightsDescriptor),\n                                        hw_desc: forgetGateHiddenWeightsDescriptor,\n                                        cw_desc: forgetGateCellStateWeightsDescriptor,\n                                        b_desc: forgetGateBiasDescriptor,\n                                        activation: BNNSActivation(function: .sigmoid))\n\nlet inputGate = BNNSLSTMGateDescriptor(iw_desc: (inputGateInputWeightsDescriptor,\n                                                 inputGateInputWeightsDescriptor),\n                                       hw_desc: inputGateHiddenWeightsDescriptor,\n                                       cw_desc: inputGateCellStateWeightsDescriptor,\n                                       b_desc: inputGateBiasDescriptor,\n                                       activation: BNNSActivation(function: .sigmoid))\n\nlet candidateGate = BNNSLSTMGateDescriptor(iw_desc: (candidateGateInputWeightsDescriptor,\n                                                     candidateGateInputWeightsDescriptor),\n                                           hw_desc: candidateGateHiddenWeightsDescriptor,\n                                           cw_desc: candidateGateCellStateWeightsDescriptor,\n                                           b_desc: candidateGateBiasDescriptor,\n                                           activation: BNNSActivation(function: .tanh))\n\nlet outputGate = BNNSLSTMGateDescriptor(iw_desc: (outputGateInputWeightsDescriptor,\n                                                  outputGateInputWeightsDescriptor),\n                                        hw_desc: outputGateHiddenWeightsDescriptor,\n                                        cw_desc: outputGateCellStateWeightsDescriptor,\n                                        b_desc: outputGateBiasDescriptor,\n                                        activation: BNNSActivation(function: .sigmoid))\n```\n\nThe following code shows how each gate computes its output:\n\n```objc\n for (size_t o = 0; o < iw_desc.size[1]; o++)\n {\n  float res = bias[o]; \/\/ init with bias value\n  for (size_t i = 0; i < iw_desc.size[0]; i++) \/\/ matrix vector multiply\n   res += input[i] * input_weights[o][i];\n  for (size_t i = 0; i < hw_desc.size[0]; i++) \/\/ matrix vector multiply\n   res += hidden[i] * hidden_weights[o][i];\n  for (size_t i = 0; i < cw_desc.size[0]; i++) \/\/ matrix vector multiply\n   res += cell[i] * cell_weights[o][i];\n  out[i] = activation.func(res); \/\/ apply activation function\n }\n```\n\nGive the descriptors for the input, output, and gates, you’re ready to create the parameters structure:\n\n```swift\nvar layerParams = BNNSLayerParametersLSTM(input_size:  ... ,\n                                          hidden_size:  ... ,\n                                          batch_size: ... ,\n                                          num_layers: ... ,\n                                          seq_len: ... ,\n                                          dropout: ... ,\n                                          lstm_flags: BNNSLayerFlagsLSTMDefaultActivations.rawValue,\n                                          sequence_descriptor: BNNSNDArrayDescriptor(),\n                                          input_descriptor: inputDescriptor,\n                                          output_descriptor: outputDescriptor,\n                                          input_gate: inputGate,\n                                          forget_gate: forgetGate,\n                                          candidate_gate: candidateGate,\n                                          output_gate: outputGate,\n                                          hidden_activation: BNNSActivation(function: .identity))\n```\n\nLSTM provides the option to define a training cache that stores intermediate results to accelerate backward computation. Use [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/BNNSComputeLSTMTrainingCacheCapacity(_:)] to compute the size, in bytes, of the training cache:\n\n```swift\nlet trainingCacheBufferBytes = BNNSComputeLSTMTrainingCacheCapacity(&layerParams)\n\nlet trainingCache = UnsafeMutableRawPointer.allocate(byteCount: trainingCacheBufferBytes,\n                                                     alignment: 1)\ndefer {\n    trainingCache.deallocate()\n}\n```\n\nFinally, call [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/BNNSDirectApplyLSTMBatchTrainingCaching(_:_:_:_:)] to apply the LSTM layer:\n\n```swift\nBNNSDirectApplyLSTMBatchTrainingCaching(&layerParams,\n                                        nil,\n                                        trainingCache,\n                                        trainingCacheBufferBytes)\n```\n\n## Recurrent layers\n\n- **BNNSLSTMDataDescriptor**: A structure that contains the input-output, hidden, and cell state n-dimensional array descriptors for a long short-term memory (LSTM) layer.\n- **BNNSLSTMGateDescriptor**: A structure that describes a long short-term memory (LSTM) gate layer.\n- **BNNSLayerFlags**: Options that control the behavior of a long short-term memory (LSTM) layer.\n- **BNNSLayerParametersLSTM**: A structure that contains the parameters of a long short-term memory (LSTM) layer.\n- **BNNSComputeLSTMTrainingCacheCapacity(_:)**: Returns the minimum bytes capacity of the training cache buffer a long short-term memory (LSTM) layer uses when it’s applied.\n- **BNNSDirectApplyLSTMBatchTrainingCaching(_:_:_:_:)**: Applies a long short-term memory (LSTM) layer directly to an input.\n- **BNNSDirectApplyLSTMBatchBackward(_:_:_:_:_:)**: Applies a long short-term memory (LSTM) filter backward to generate gradients.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "A structure that contains the input-output, hidden, and cell state n-dimensional array descriptors for a long short-term memory (LSTM) layer.",
          "name" : "BNNSLSTMDataDescriptor",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSLSTMDataDescriptor"
        },
        {
          "description" : "A structure that describes a long short-term memory (LSTM) gate layer.",
          "name" : "BNNSLSTMGateDescriptor",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSLSTMGateDescriptor"
        },
        {
          "description" : "Options that control the behavior of a long short-term memory (LSTM) layer.",
          "name" : "BNNSLayerFlags",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSLayerFlags"
        },
        {
          "description" : "A structure that contains the parameters of a long short-term memory (LSTM) layer.",
          "name" : "BNNSLayerParametersLSTM",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSLayerParametersLSTM"
        },
        {
          "description" : "Returns the minimum bytes capacity of the training cache buffer a long short-term memory (LSTM) layer uses when it’s applied.",
          "name" : "BNNSComputeLSTMTrainingCacheCapacity(_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSComputeLSTMTrainingCacheCapacity(_:)"
        },
        {
          "description" : "Applies a long short-term memory (LSTM) layer directly to an input.",
          "name" : "BNNSDirectApplyLSTMBatchTrainingCaching(_:_:_:_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSDirectApplyLSTMBatchTrainingCaching(_:_:_:_:)"
        },
        {
          "description" : "Applies a long short-term memory (LSTM) filter backward to generate gradients.",
          "name" : "BNNSDirectApplyLSTMBatchBackward(_:_:_:_:_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSDirectApplyLSTMBatchBackward(_:_:_:_:_:)"
        }
      ],
      "title" : "Recurrent layers"
    }
  ],
  "source" : "appleJSON",
  "title" : "Using Long Short-Term Memory Layers (LSTM)",
  "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/using-long-short-term-memory-layers-lstm"
}