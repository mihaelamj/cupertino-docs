{
  "abstract" : "Share image data between vDSP and vImage to visualize audio that a device microphone captures.",
  "codeExamples" : [
    {
      "code" : "\/\/\/ The number of samples per frame — the height of the spectrogram.\nstatic let sampleCount = 1024\n\n\/\/\/ The number of displayed buffers — the width of the spectrogram.\nstatic let bufferCount = 768\n\n\/\/\/ Determines the overlap between frames.\nstatic let hopCount = 512",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ A reusable array that contains the current frame of time-domain audio data as single-precision\n\/\/\/ values.\nvar timeDomainBuffer = [Float](repeating: 0,\n                               count: sampleCount)\n\n\/\/\/ A resuable array that contains the frequency-domain representation of the current frame of\n\/\/\/ audio data.\nvar frequencyDomainBuffer = [Float](repeating: 0,\n                                    count: sampleCount)",
      "language" : "swift"
    },
    {
      "code" : "vDSP.convertElements(of: values,\n                     to: &timeDomainBuffer)",
      "language" : "swift"
    },
    {
      "code" : "let hanningWindow = vDSP.window(ofType: Float.self,\n                                usingSequence: .hanningDenormalized,\n                                count: sampleCount,\n                                isHalfWindow: false)",
      "language" : "swift"
    },
    {
      "code" : "vDSP.multiply(timeDomainBuffer,\n              hanningWindow,\n              result: &timeDomainBuffer)\n\nforwardDCT.transform(timeDomainBuffer,\n                     result: &frequencyDomainBuffer)",
      "language" : "swift"
    },
    {
      "code" : "static var multidimensionalLookupTable: vImage.MultidimensionalLookupTable = {\n    let entriesPerChannel = UInt8(32)\n    let srcChannelCount = 1\n    let destChannelCount = 3\n    \n    let lookupTableElementCount = Int(pow(Float(entriesPerChannel),\n                                          Float(srcChannelCount))) *\n    Int(destChannelCount)\n    \n    let tableData = [UInt16](unsafeUninitializedCapacity: lookupTableElementCount) {\n        buffer, count in\n        \n        \/\/\/ Supply the samples in the range `0...65535`. The transform function\n        \/\/\/ interpolates these to the range `0...1`.\n        let multiplier = CGFloat(UInt16.max)\n        var bufferIndex = 0\n        \n        for gray in ( 0 ..< entriesPerChannel) {\n            \/\/\/ Create normalized red, green, and blue values in the range `0...1`.\n            let normalizedValue = CGFloat(gray) \/ CGFloat(entriesPerChannel - 1)\n          \n            \/\/ Define `hue` that's blue at `0.0` to red at `1.0`.\n            let hue = 0.6666 - (0.6666 * normalizedValue)\n            let brightness = sqrt(normalizedValue)\n            \n            let color = NSColor(hue: hue,\n                                saturation: 1,\n                                brightness: brightness,\n                                alpha: 1)\n            \n            var red = CGFloat()\n            var green = CGFloat()\n            var blue = CGFloat()\n            \n            color.getRed(&red,\n                         green: &green,\n                         blue: &blue,\n                         alpha: nil)\n \n            buffer[ bufferIndex ] = UInt16(green * multiplier)\n            bufferIndex += 1\n            buffer[ bufferIndex ] = UInt16(red * multiplier)\n            bufferIndex += 1\n            buffer[ bufferIndex ] = UInt16(blue * multiplier)\n            bufferIndex += 1\n        }\n        \n        count = lookupTableElementCount\n    }\n    \n    let entryCountPerSourceChannel = [UInt8](repeating: entriesPerChannel,\n                                             count: srcChannelCount)\n    \n    return vImage.MultidimensionalLookupTable(entryCountPerSourceChannel: entryCountPerSourceChannel,\n                                              destinationChannelCount: destChannelCount,\n                                              data: tableData)\n}()",
      "language" : "swift"
    },
    {
      "code" : "func makeAudioSpectrogramImage() -> CGImage {\n    frequencyDomainValues.withUnsafeMutableBufferPointer {\n        \n        let planarImageBuffer = vImage.PixelBuffer(\n            data: $0.baseAddress!,\n            width: AudioSpectrogram.sampleCount,\n            height: AudioSpectrogram.bufferCount,\n            byteCountPerRow: AudioSpectrogram.sampleCount * MemoryLayout<Float>.stride,\n            pixelFormat: vImage.PlanarF.self)\n        \n        AudioSpectrogram.multidimensionalLookupTable.apply(\n            sources: [planarImageBuffer],\n            destinations: [redBuffer, greenBuffer, blueBuffer],\n            interpolation: .half)\n        \n        rgbImageBuffer.interleave(\n            planarSourceBuffers: [redBuffer, greenBuffer, blueBuffer])\n    }\n    \n    return rgbImageBuffer.makeCGImage(cgImageFormat: rgbImageFormat) ?? AudioSpectrogram.emptyCGImage\n}",
      "language" : "swift"
    },
    {
      "code" : "func frequencyToMel(_ frequency: Float) -> Float {\n    return 2595 * log10(1 + (frequency \/ 700))\n}\n\nfunc melToFrequency(_ mel: Float) -> Float {\n    return 700 * (pow(10, mel \/ 2595) - 1)\n}\n\nlet minMel = frequencyToMel(frequencyRange.lowerBound)\nlet maxMel = frequencyToMel(frequencyRange.upperBound)\nlet bankWidth = (maxMel - minMel) \/ Float(filterBankCount - 1)\n\nlet melFilterBankFrequencies: [Int] = stride(from: minMel, to: maxMel, by: bankWidth).map {\n    let mel = Float($0)\n    let frequency = melToFrequency(mel)\n    \n    return Int((frequency \/ frequencyRange.upperBound) * Float(sampleCount))\n}",
      "language" : "swift"
    },
    {
      "code" : "for i in 0 ..< melFilterBankFrequencies.count {\n    \n    let row = i * sampleCount\n    \n    let startFrequency = melFilterBankFrequencies[ max(0, i - 1) ]\n    let centerFrequency = melFilterBankFrequencies[ i ]\n    let endFrequency = (i + 1) < melFilterBankFrequencies.count ?\n    melFilterBankFrequencies[ i + 1 ] : sampleCount - 1\n    \n    let attackWidth = centerFrequency - startFrequency + 1\n    let decayWidth = endFrequency - centerFrequency + 1\n    \n    \/\/ Create the attack phase of the triangle.\n    if attackWidth > 0 {\n        vDSP_vgen(&endValue,\n                  &baseValue,\n                  filterBank.baseAddress!.advanced(by: row + startFrequency),\n                  1,\n                  vDSP_Length(attackWidth))\n    }\n    \n    \/\/ Create the decay phase of the triangle.\n    if decayWidth > 0 {\n        vDSP_vgen(&baseValue,\n                  &endValue,\n                  filterBank.baseAddress!.advanced(by: row + centerFrequency),\n                  1,\n                  vDSP_Length(decayWidth))\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "values.withUnsafeBufferPointer { frequencyDomainValuesPtr in\n    cblas_sgemm(CblasRowMajor,\n                CblasTrans, CblasTrans,\n                1,\n                Int32(MelSpectrogram.filterBankCount),\n                Int32(sampleCount),\n                1,\n                frequencyDomainValuesPtr.baseAddress,\n                1,\n                filterBank.baseAddress, Int32(sampleCount),\n                0,\n                sgemmResult.baseAddress, Int32(MelSpectrogram.filterBankCount))\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "1977934a4166156fc64f96f5fdc8a81458e7eb51169e0f15c488aecf129e4732",
  "crawledAt" : "2025-12-02T15:46:14Z",
  "id" : "560F570A-C427-4BDB-9644-88E474BDB4F9",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "Accelerate",
  "overview" : "## Overview\n\nThis sample code project captures audio from a macOS device’s microphone and uses a combination of routines from vImage and vDSP to render the audio as an *audio spectrogram*. Audio spectrograms visualize audio in 2D using one axis to represent time and the other axis to represent frequency. Color represents the amplitude of the time-frequency pair.\n\nYou can use audio spectrograms for signal analysis. For example, a spectrogram can help identify audio issues, such as low- or high-frequency noise, or short-impulse noises like clicks and pops that may not be immediately obvious to the human ear. Spectrograms can also assist in audio classification using neural networks for tasks such as bird song and speech recognition.\n\nThe image below shows the audio spectrogram that this sample created from the *Stargate Opening* sound effect in [https:\/\/www.apple.com\/ios\/garageband\/]. The horizontal axis represents time, and the vertical axis represents frequency. The sample calculates the color that represents amplitude using a procedurally generated multidimensional lookup table.\n\n\n\nThe sample creates an audio spectrogram by performing a discrete cosine transform (DCT) on audio samples. The DCT computes the frequency components of an audio signal and represents the audio as a series of amplitudes at the component frequencies. DCTs are related to Fourier transforms, but use real values rather than complex values. You can learn more about Fourier transforms at [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/finding-the-component-frequencies-in-a-composite-sine-wave].\n\nThe spectrogram scrolls horizontally so that the most recent sample renders on the right side of the device’s screen.\n\nFor each sample buffer that AVFoundation provides, the app appends that data to the `rawAudioData` array.  At the same time, the app applies a DCT to the first `sampleCount` elements of `rawAudioData` and produces a single-precision frequency-domain representation.\n\nThe code appends the newly generated frequency-domain values to `frequencyDomainValues` and discards `sampleCount` elements from the beginning. It is this appending and discarding of data that generates the scrolling effect.\n\nA vImage pixel buffer, `planarImageBuffer`, shares data with `frequencyDomainValues` and the multidimensional lookup table uses that as a planar source to populate three additional planar buffers that represent the red, green, and blue channels of the spectrogram image. The sample app interleaves the red, green, and blue planar buffers to display the RGB spectrogram image.\n\nBefore exploring the code, build and run the app to familiarize yourself with the different visual results it generates from different sounds.\n\n### Define the spectrogram size\n\nThe sample defines two constants that specify the size of the spectrogram.\n\nThe sample also specifies a hop size that controls the overlap between frames of data and ensures that the spectrogram doesn’t lose any audio information at the start and end of each sample.\n\n### Process the audio data\n\nThe `processData(values:)` function processes the first `sampleCount` samples from  `rawAudioData` by performing the DCT and appending the frequency-domain representation data to the array that creates the vImage buffer and, ultimately, the audio spectrogram image.\n\nTo avoid recreating working arrays with each iteration, the following code creates reusable buffers that `processData(values:)` uses:\n\nThe sample calls [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vDSP\/convertElements(of:to:)-93xn9] to convert 16-bit integer audio samples to single-precision floating-point values.\n\nTo reduce spectral leakage, the sample multiplies the signal by a Hann window and performs the DCT. The following code generates the window:\n\nTo learn more about using windows to reduce spectral leakage, see [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/reducing-spectral-leakage-with-windowing].\n\nThe following code multiplies the time-domain data by the Hann window and performs the forward DCT:\n\n### Define the pseudocolor multidimensional lookup tables\n\nThe following code creates a [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vImage\/MultidimensionalLookupTable] that the sample uses to create the pseudocolor rendering. The function returns dark blue for low values, graduates through red, and returns full-brightness green for `1.0`.\n\nThe following image shows the color that the function returns with inputs from `0.0` through `1.0`:\n\n\n\n### Prepare the vImage pixel buffers to display the audio spectrogram\n\nTo display the audio spectrogram, the app creates a temporary planar [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vImage\/PixelBuffer] that shares memory with the frequency-domain values.\n\nThe following code applies the multidimensional lookup table to the grayscale information in the temporary buffer to populate three planar buffers that represent the red, green, and blue channels. Because the vImage functions that generate a Core Graphics image from a pixel buffer require an interleaved buffer, the code interleaves the red, green, and blue buffer into `rgbImageBuffer`.\n\n### Compute the mel spectrum using linear algebra\n\nIn addition to the linear audio spectrogram, the sample app provides a mode to render audio as a mel spectrogram. The `computeMelSpectrogram(values:)` function rescales the frequency-domain buffer from a linear scale to the mel scale.\n\nThe mel scale is a scale of pitches that human hearing generally perceives to be equidistant from each other. As frequency increases, the interval, in hertz, between mel scale values (or simply *mels*) increases. The name *mel* derives from *melody* and indicates that the scale is based on the comparison between pitches. The mel spectrogram remaps the values in hertz to the mel scale.\n\nThe linear audio spectrogram is ideally suited for use cases where all frequencies have equal importance, while mel spectrograms are better suited when modeling human hearing perception. Mel spectrogram data is also suited for use in audio classification.\n\nA mel spectrogram differs from a linearly scaled audio spectrogram in two ways:\n\nThe sample builds the filter bank from a series of overlapping triangular windows at a series of evenly spaced mels. The number of elements in a single frame in a mel spectrogram is equal to the number of filters in the filter bank.\n\nThe following image shows the linear audio spectrogram and the mel spectrogram of the same linearly increasing and decreasing tone. The tone starts at 20 Hz, rises to 22,050 Hz, and drops back to 20 Hz. The image shows that the audio spectrogram represents the objective signal, but the mel spectrogram mirrors human perception, that is, the curve flattens \u001fand indicates reduced differentiation between high frequencies.\n\n\n\nIn this case, the mel spectrogram consists of 40 filters, so the spectrogram has a lower vertical resolution than the linear spectrogram.\n\n### Define the mel frequencies\n\nThe sample creates an array, `melFilterBankFrequencies`, that contains the indices of `frequencyDomainBuffer` that represent the mel scale frequencies. For example, if the Nyquist frequency is 22,050 Hz and `frequencyDomainBuffer` contains 1024 elements, a value of 512 in `melFilterBankFrequencies` represents 11,025 Hz.\n\nThe `static MelSpectrogram.populateMelFilterBankFrequencies(_:maximumFrequency:)` function populates `melFilterBankFrequencies` with the logarithmically increasing indices based on the linearly interpolated increasing mel frequencies in  `melFilterBankFrequencies`.\n\nThe following line chart shows 16 generated mel frequencies as squares and the corresponding frequencies in hertz as circles:\n\n\n\n### Create the filter bank\n\nThe sample creates the filter bank matrix with `filterBankCount` rows and `sampleCount` columns. Each row contains a triangular window that starts at the previous frequency, peaks at the current frequency, and ends at the next frequency. For example, the following graphic illustrates the values for a filter bank that contains 16 values:\n\n\n\nThe `static MelSpectrogram.populateFilterBank(_:melFilterBankFrequencies:)` function populates the `filterBank` array. The function uses [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vDSP_vgen] to generate the attack and decay phases of each triangle.\n\n### Use a matrix multiply to compute the mel spectrogram\n\nThe sample performs a matrix multiply of each frame of frequency-domain data with the filter bank to produce a frame of mel-scaled values.\n\nThe following image shows the matrix multiply. The frame of 1024 frequency-domain values is multiplied by 16 overlapping triangular windows, returning the 16-element mel-scaled values.\n\n\n\nThe following code uses [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/cblas_sgemm(_:_:_:_:_:_:_:_:_:_:_:_:_:_:)] to perform the matrix multiply:\n\nOn return, the sample adds the result of the matrix multiply in `sgemmResult` to the `melSpectrumValues` that contain `bufferCount * filterBankCount` elements.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/Accelerate\/visualizing-sound-as-an-audio-spectrogram\ncrawled: 2025-12-02T15:46:14Z\n---\n\n# Visualizing sound as an audio spectrogram\n\n**Sample Code**\n\nShare image data between vDSP and vImage to visualize audio that a device microphone captures.\n\n## Overview\n\nThis sample code project captures audio from a macOS device’s microphone and uses a combination of routines from vImage and vDSP to render the audio as an *audio spectrogram*. Audio spectrograms visualize audio in 2D using one axis to represent time and the other axis to represent frequency. Color represents the amplitude of the time-frequency pair.\n\nYou can use audio spectrograms for signal analysis. For example, a spectrogram can help identify audio issues, such as low- or high-frequency noise, or short-impulse noises like clicks and pops that may not be immediately obvious to the human ear. Spectrograms can also assist in audio classification using neural networks for tasks such as bird song and speech recognition.\n\nThe image below shows the audio spectrogram that this sample created from the *Stargate Opening* sound effect in [https:\/\/www.apple.com\/ios\/garageband\/]. The horizontal axis represents time, and the vertical axis represents frequency. The sample calculates the color that represents amplitude using a procedurally generated multidimensional lookup table.\n\n\n\nThe sample creates an audio spectrogram by performing a discrete cosine transform (DCT) on audio samples. The DCT computes the frequency components of an audio signal and represents the audio as a series of amplitudes at the component frequencies. DCTs are related to Fourier transforms, but use real values rather than complex values. You can learn more about Fourier transforms at [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/finding-the-component-frequencies-in-a-composite-sine-wave].\n\nThe spectrogram scrolls horizontally so that the most recent sample renders on the right side of the device’s screen.\n\nFor each sample buffer that AVFoundation provides, the app appends that data to the `rawAudioData` array.  At the same time, the app applies a DCT to the first `sampleCount` elements of `rawAudioData` and produces a single-precision frequency-domain representation.\n\nThe code appends the newly generated frequency-domain values to `frequencyDomainValues` and discards `sampleCount` elements from the beginning. It is this appending and discarding of data that generates the scrolling effect.\n\nA vImage pixel buffer, `planarImageBuffer`, shares data with `frequencyDomainValues` and the multidimensional lookup table uses that as a planar source to populate three additional planar buffers that represent the red, green, and blue channels of the spectrogram image. The sample app interleaves the red, green, and blue planar buffers to display the RGB spectrogram image.\n\nBefore exploring the code, build and run the app to familiarize yourself with the different visual results it generates from different sounds.\n\n### Define the spectrogram size\n\nThe sample defines two constants that specify the size of the spectrogram.\n\n- `sampleCount` defines the number of individual samples that pass to the DCT, and the resolution of the displayed frequencies.\n- `bufferCount` controls the number of displayed buffers.\n\nThe sample also specifies a hop size that controls the overlap between frames of data and ensures that the spectrogram doesn’t lose any audio information at the start and end of each sample.\n\n```swift\n\/\/\/ The number of samples per frame — the height of the spectrogram.\nstatic let sampleCount = 1024\n\n\/\/\/ The number of displayed buffers — the width of the spectrogram.\nstatic let bufferCount = 768\n\n\/\/\/ Determines the overlap between frames.\nstatic let hopCount = 512\n```\n\n### Process the audio data\n\nThe `processData(values:)` function processes the first `sampleCount` samples from  `rawAudioData` by performing the DCT and appending the frequency-domain representation data to the array that creates the vImage buffer and, ultimately, the audio spectrogram image.\n\nTo avoid recreating working arrays with each iteration, the following code creates reusable buffers that `processData(values:)` uses:\n\n```swift\n\/\/\/ A reusable array that contains the current frame of time-domain audio data as single-precision\n\/\/\/ values.\nvar timeDomainBuffer = [Float](repeating: 0,\n                               count: sampleCount)\n\n\/\/\/ A resuable array that contains the frequency-domain representation of the current frame of\n\/\/\/ audio data.\nvar frequencyDomainBuffer = [Float](repeating: 0,\n                                    count: sampleCount)\n```\n\nThe sample calls [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vDSP\/convertElements(of:to:)-93xn9] to convert 16-bit integer audio samples to single-precision floating-point values.\n\n```swift\nvDSP.convertElements(of: values,\n                     to: &timeDomainBuffer)\n```\n\nTo reduce spectral leakage, the sample multiplies the signal by a Hann window and performs the DCT. The following code generates the window:\n\n```swift\nlet hanningWindow = vDSP.window(ofType: Float.self,\n                                usingSequence: .hanningDenormalized,\n                                count: sampleCount,\n                                isHalfWindow: false)\n```\n\nTo learn more about using windows to reduce spectral leakage, see [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/reducing-spectral-leakage-with-windowing].\n\nThe following code multiplies the time-domain data by the Hann window and performs the forward DCT:\n\n```swift\nvDSP.multiply(timeDomainBuffer,\n              hanningWindow,\n              result: &timeDomainBuffer)\n\nforwardDCT.transform(timeDomainBuffer,\n                     result: &frequencyDomainBuffer)\n```\n\n### Define the pseudocolor multidimensional lookup tables\n\nThe following code creates a [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vImage\/MultidimensionalLookupTable] that the sample uses to create the pseudocolor rendering. The function returns dark blue for low values, graduates through red, and returns full-brightness green for `1.0`.\n\n```swift\nstatic var multidimensionalLookupTable: vImage.MultidimensionalLookupTable = {\n    let entriesPerChannel = UInt8(32)\n    let srcChannelCount = 1\n    let destChannelCount = 3\n    \n    let lookupTableElementCount = Int(pow(Float(entriesPerChannel),\n                                          Float(srcChannelCount))) *\n    Int(destChannelCount)\n    \n    let tableData = [UInt16](unsafeUninitializedCapacity: lookupTableElementCount) {\n        buffer, count in\n        \n        \/\/\/ Supply the samples in the range `0...65535`. The transform function\n        \/\/\/ interpolates these to the range `0...1`.\n        let multiplier = CGFloat(UInt16.max)\n        var bufferIndex = 0\n        \n        for gray in ( 0 ..< entriesPerChannel) {\n            \/\/\/ Create normalized red, green, and blue values in the range `0...1`.\n            let normalizedValue = CGFloat(gray) \/ CGFloat(entriesPerChannel - 1)\n          \n            \/\/ Define `hue` that's blue at `0.0` to red at `1.0`.\n            let hue = 0.6666 - (0.6666 * normalizedValue)\n            let brightness = sqrt(normalizedValue)\n            \n            let color = NSColor(hue: hue,\n                                saturation: 1,\n                                brightness: brightness,\n                                alpha: 1)\n            \n            var red = CGFloat()\n            var green = CGFloat()\n            var blue = CGFloat()\n            \n            color.getRed(&red,\n                         green: &green,\n                         blue: &blue,\n                         alpha: nil)\n \n            buffer[ bufferIndex ] = UInt16(green * multiplier)\n            bufferIndex += 1\n            buffer[ bufferIndex ] = UInt16(red * multiplier)\n            bufferIndex += 1\n            buffer[ bufferIndex ] = UInt16(blue * multiplier)\n            bufferIndex += 1\n        }\n        \n        count = lookupTableElementCount\n    }\n    \n    let entryCountPerSourceChannel = [UInt8](repeating: entriesPerChannel,\n                                             count: srcChannelCount)\n    \n    return vImage.MultidimensionalLookupTable(entryCountPerSourceChannel: entryCountPerSourceChannel,\n                                              destinationChannelCount: destChannelCount,\n                                              data: tableData)\n}()\n```\n\nThe following image shows the color that the function returns with inputs from `0.0` through `1.0`:\n\n\n\n### Prepare the vImage pixel buffers to display the audio spectrogram\n\nTo display the audio spectrogram, the app creates a temporary planar [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vImage\/PixelBuffer] that shares memory with the frequency-domain values.\n\nThe following code applies the multidimensional lookup table to the grayscale information in the temporary buffer to populate three planar buffers that represent the red, green, and blue channels. Because the vImage functions that generate a Core Graphics image from a pixel buffer require an interleaved buffer, the code interleaves the red, green, and blue buffer into `rgbImageBuffer`.\n\n```swift\nfunc makeAudioSpectrogramImage() -> CGImage {\n    frequencyDomainValues.withUnsafeMutableBufferPointer {\n        \n        let planarImageBuffer = vImage.PixelBuffer(\n            data: $0.baseAddress!,\n            width: AudioSpectrogram.sampleCount,\n            height: AudioSpectrogram.bufferCount,\n            byteCountPerRow: AudioSpectrogram.sampleCount * MemoryLayout<Float>.stride,\n            pixelFormat: vImage.PlanarF.self)\n        \n        AudioSpectrogram.multidimensionalLookupTable.apply(\n            sources: [planarImageBuffer],\n            destinations: [redBuffer, greenBuffer, blueBuffer],\n            interpolation: .half)\n        \n        rgbImageBuffer.interleave(\n            planarSourceBuffers: [redBuffer, greenBuffer, blueBuffer])\n    }\n    \n    return rgbImageBuffer.makeCGImage(cgImageFormat: rgbImageFormat) ?? AudioSpectrogram.emptyCGImage\n}\n```\n\n### Compute the mel spectrum using linear algebra\n\nIn addition to the linear audio spectrogram, the sample app provides a mode to render audio as a mel spectrogram. The `computeMelSpectrogram(values:)` function rescales the frequency-domain buffer from a linear scale to the mel scale.\n\nThe mel scale is a scale of pitches that human hearing generally perceives to be equidistant from each other. As frequency increases, the interval, in hertz, between mel scale values (or simply *mels*) increases. The name *mel* derives from *melody* and indicates that the scale is based on the comparison between pitches. The mel spectrogram remaps the values in hertz to the mel scale.\n\nThe linear audio spectrogram is ideally suited for use cases where all frequencies have equal importance, while mel spectrograms are better suited when modeling human hearing perception. Mel spectrogram data is also suited for use in audio classification.\n\nA mel spectrogram differs from a linearly scaled audio spectrogram in two ways:\n\n- A mel spectrogram logarithmically renders frequencies above a certain threshold (the *corner frequency*). For example, in the linearly scaled spectrogram, the vertical space between 1000 Hz and 2000 Hz is half of the vertical space between 2000 Hz and 4000 Hz. In the mel spectrogram, the space between those ranges is approximately the same. This scaling is analogous to human hearing, where it’s easier to distinguish between similar low frequency sounds than similar high frequency sounds.\n- A mel spectrogram computes its output by multiplying frequency-domain values by a filter bank.\n\nThe sample builds the filter bank from a series of overlapping triangular windows at a series of evenly spaced mels. The number of elements in a single frame in a mel spectrogram is equal to the number of filters in the filter bank.\n\nThe following image shows the linear audio spectrogram and the mel spectrogram of the same linearly increasing and decreasing tone. The tone starts at 20 Hz, rises to 22,050 Hz, and drops back to 20 Hz. The image shows that the audio spectrogram represents the objective signal, but the mel spectrogram mirrors human perception, that is, the curve flattens \u001fand indicates reduced differentiation between high frequencies.\n\n\n\nIn this case, the mel spectrogram consists of 40 filters, so the spectrogram has a lower vertical resolution than the linear spectrogram.\n\n### Define the mel frequencies\n\nThe sample creates an array, `melFilterBankFrequencies`, that contains the indices of `frequencyDomainBuffer` that represent the mel scale frequencies. For example, if the Nyquist frequency is 22,050 Hz and `frequencyDomainBuffer` contains 1024 elements, a value of 512 in `melFilterBankFrequencies` represents 11,025 Hz.\n\nThe `static MelSpectrogram.populateMelFilterBankFrequencies(_:maximumFrequency:)` function populates `melFilterBankFrequencies` with the logarithmically increasing indices based on the linearly interpolated increasing mel frequencies in  `melFilterBankFrequencies`.\n\n```swift\nfunc frequencyToMel(_ frequency: Float) -> Float {\n    return 2595 * log10(1 + (frequency \/ 700))\n}\n\nfunc melToFrequency(_ mel: Float) -> Float {\n    return 700 * (pow(10, mel \/ 2595) - 1)\n}\n\nlet minMel = frequencyToMel(frequencyRange.lowerBound)\nlet maxMel = frequencyToMel(frequencyRange.upperBound)\nlet bankWidth = (maxMel - minMel) \/ Float(filterBankCount - 1)\n\nlet melFilterBankFrequencies: [Int] = stride(from: minMel, to: maxMel, by: bankWidth).map {\n    let mel = Float($0)\n    let frequency = melToFrequency(mel)\n    \n    return Int((frequency \/ frequencyRange.upperBound) * Float(sampleCount))\n}\n```\n\nThe following line chart shows 16 generated mel frequencies as squares and the corresponding frequencies in hertz as circles:\n\n\n\n### Create the filter bank\n\nThe sample creates the filter bank matrix with `filterBankCount` rows and `sampleCount` columns. Each row contains a triangular window that starts at the previous frequency, peaks at the current frequency, and ends at the next frequency. For example, the following graphic illustrates the values for a filter bank that contains 16 values:\n\n\n\nThe `static MelSpectrogram.populateFilterBank(_:melFilterBankFrequencies:)` function populates the `filterBank` array. The function uses [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vDSP_vgen] to generate the attack and decay phases of each triangle.\n\n```swift\nfor i in 0 ..< melFilterBankFrequencies.count {\n    \n    let row = i * sampleCount\n    \n    let startFrequency = melFilterBankFrequencies[ max(0, i - 1) ]\n    let centerFrequency = melFilterBankFrequencies[ i ]\n    let endFrequency = (i + 1) < melFilterBankFrequencies.count ?\n    melFilterBankFrequencies[ i + 1 ] : sampleCount - 1\n    \n    let attackWidth = centerFrequency - startFrequency + 1\n    let decayWidth = endFrequency - centerFrequency + 1\n    \n    \/\/ Create the attack phase of the triangle.\n    if attackWidth > 0 {\n        vDSP_vgen(&endValue,\n                  &baseValue,\n                  filterBank.baseAddress!.advanced(by: row + startFrequency),\n                  1,\n                  vDSP_Length(attackWidth))\n    }\n    \n    \/\/ Create the decay phase of the triangle.\n    if decayWidth > 0 {\n        vDSP_vgen(&baseValue,\n                  &endValue,\n                  filterBank.baseAddress!.advanced(by: row + centerFrequency),\n                  1,\n                  vDSP_Length(decayWidth))\n    }\n}\n```\n\n### Use a matrix multiply to compute the mel spectrogram\n\nThe sample performs a matrix multiply of each frame of frequency-domain data with the filter bank to produce a frame of mel-scaled values.\n\nThe following image shows the matrix multiply. The frame of 1024 frequency-domain values is multiplied by 16 overlapping triangular windows, returning the 16-element mel-scaled values.\n\n\n\nThe following code uses [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/cblas_sgemm(_:_:_:_:_:_:_:_:_:_:_:_:_:_:)] to perform the matrix multiply:\n\n```swift\nvalues.withUnsafeBufferPointer { frequencyDomainValuesPtr in\n    cblas_sgemm(CblasRowMajor,\n                CblasTrans, CblasTrans,\n                1,\n                Int32(MelSpectrogram.filterBankCount),\n                Int32(sampleCount),\n                1,\n                frequencyDomainValuesPtr.baseAddress,\n                1,\n                filterBank.baseAddress, Int32(sampleCount),\n                0,\n                sgemmResult.baseAddress, Int32(MelSpectrogram.filterBankCount))\n}\n```\n\nOn return, the sample adds the result of the matrix multiply in `sgemmResult` to the `melSpectrumValues` that contain `bufferCount * filterBankCount` elements.\n\n## Audio Processing\n\n- **Applying biquadratic filters to a music loop**: Change the frequency response of an audio signal using a cascaded biquadratic filter.\n- **Equalizing audio with discrete cosine transforms (DCTs)**: Change the frequency response of an audio signal by manipulating frequency-domain data.\n- **Biquadratic IIR filters**: Apply biquadratic filters to single-channel and multichannel data.\n- **Discrete Cosine transforms**: Transform vectors of temporal and spatial domain real values to the frequency domain, and vice versa.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Change the frequency response of an audio signal using a cascaded biquadratic filter.",
          "name" : "Applying biquadratic filters to a music loop",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/applying-biquadratic-filters-to-a-music-loop"
        },
        {
          "description" : "Change the frequency response of an audio signal by manipulating frequency-domain data.",
          "name" : "Equalizing audio with discrete cosine transforms (DCTs)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/equalizing-audio-with-discrete-cosine-transforms-dcts"
        },
        {
          "description" : "Apply biquadratic filters to single-channel and multichannel data.",
          "name" : "Biquadratic IIR filters",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/biquadratic-iir-filters"
        },
        {
          "description" : "Transform vectors of temporal and spatial domain real values to the frequency domain, and vice versa.",
          "name" : "Discrete Cosine transforms",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/discrete-cosine-transforms"
        }
      ],
      "title" : "Audio Processing"
    }
  ],
  "source" : "appleJSON",
  "title" : "Visualizing sound as an audio spectrogram",
  "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/visualizing-sound-as-an-audio-spectrogram"
}