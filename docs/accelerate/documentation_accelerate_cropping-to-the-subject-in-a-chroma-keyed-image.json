{
  "abstract" : "Convert a chroma-key color to alpha values and trim transparent pixels using Accelerate.",
  "codeExamples" : [
    {
      "code" : "let entriesPerChannel = UInt8(32)\nlet ramp = vDSP.ramp(in: 0 ... 1.0, count: Int(entriesPerChannel))\n\nlet sourceChannelCount = 3\nlet destinationChannelCount = 1\n\nlet lookupTableElementCount = Int(pow(Float(entriesPerChannel),\n                                      Float(sourceChannelCount))) * Int(destinationChannelCount)\n\nlet lookupTableData = UnsafeMutableBufferPointer<UInt16>.allocate(capacity: lookupTableElementCount)\ndefer {\n    lookupTableData.deallocate()\n}",
      "language" : "swift"
    },
    {
      "code" : "let chromaKeyRGB = chromaKeyColor.components ?? [0, 0, 0]\nlet chromaKeyLab = ColorConverter.rgbToLab(r: chromaKeyRGB[0],\n                                           g: chromaKeyRGB.count > 1 ? chromaKeyRGB[1] : chromaKeyRGB[0],\n                                           b: chromaKeyRGB.count > 2 ? chromaKeyRGB[2] : chromaKeyRGB[0])\n\nvar bufferIndex = 0\nfor red in ramp {\n    for green in ramp {\n        for blue in ramp {\n            \n            let lab = ColorConverter.rgbToLab(r: red, g: green, b: blue)\n            \n            let distance = simd_distance(chromaKeyLab, lab)\n            \n            let contrast = Float(20)\n            let offset = Float(0.25)\n            let alpha = saturate(tanh(((distance \/ tolerance ) - 0.5 - offset) * contrast))\n            \n            lookupTableData[bufferIndex] = UInt16(alpha * Float(UInt16.max))\n            bufferIndex += 1\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "alphaBuffer.withUnsafeBufferPointer { alphaPointer in\n    \n    let rowStride = alphaBuffer.rowStride\n    \n    \/\/ Find the bounding box top.\n    for i in 0 ..< alphaBuffer.height {\n        \n        let start = alphaPointer.baseAddress?.advanced(by: i * rowStride)\n        let row = UnsafeBufferPointer<Float>(start: start, count: alphaBuffer.width)\n        let sum = vDSP.sum(row)\n        \n        if sum != 0 {\n            top = i\n            break\n        }\n    }\n    \n    \/\/ Find the bounding box bottom.\n    for i in stride(from: alphaBuffer.height - 1, through: top, by: -1) {\n        \n        let start = alphaPointer.baseAddress?.advanced(by: i * rowStride)\n        let row = UnsafeBufferPointer<Float>(start: start, count: alphaBuffer.width)\n        let sum = vDSP.sum(row)\n        \n        if sum != 0 {\n            bottom = i\n            break\n        }\n    }\n    \n    let height = bottom - top",
      "language" : "swift"
    },
    {
      "code" : "    \/\/ Find the bounding box left.\n    for i in 0 ..< alphaBuffer.width {\n        \n        let columnStart = alphaPointer.baseAddress!.advanced(by: i + (top * rowStride))\n        \n        var sum = Float()\n        \n        vDSP_sve(columnStart, rowStride, &sum, vDSP_Length(height))\n        \n        if sum != 0 {\n            left = i\n            break\n        }\n    }\n    \n    \/\/ Find the bounding box right.\n    for i in stride(from: alphaBuffer.width - 1, through: 0, by: -1) {\n        \n        let columnStart = alphaPointer.baseAddress!.advanced(by: i + (top * rowStride))\n        \n        var sum = Float()\n        \n        vDSP_sve(columnStart, rowStride, &sum, vDSP_Length(height))\n        \n        if sum != 0 {\n            right = i\n            break\n        }\n    }\n} \/\/ Ends `alphaBuffer.withUnsafeBufferPointer { alphaPointer in`.",
      "language" : "swift"
    },
    {
      "code" : "return CGRect(x: left,\n              y: top,\n              width: right - left,\n              height: bottom - top)",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Create an `InterleavedFx4` pixel buffer from the original image.\nlet sourceBuffer = try! vImage.PixelBuffer<vImage.InterleavedFx4>(\n    cgImage: originalImage,\n    cgImageFormat: &cgImageFormatARGB)\n\n\/\/ Create a `PlanarF` pixel buffer that represents the alpha channel.\nlet alpha = Self.chromaKeyToAlpha(source: sourceBuffer,\n                                  chromaKeyColor: .init(red: 91 \/ 255,\n                                                        green: 155 \/ 255,\n                                                        blue: 244 \/ 255,\n                                                        alpha: 0),\n                                  tolerance: 60)\n\n\/\/ Compute the bounding box for nontransparent pixels.\nlet boundingBox = Self.boundingBoxForNonTransparentPixels(alphaBuffer: alpha)\n\n\/\/ Create an `InterleavedFx4` pixel buffer that's the cropped version\n\/\/ of the original image.\nlet cropped = sourceBuffer.cropped(to: boundingBox)\n\n\/\/ Overwrite the alpha channel of the cropped image so that the chroma-key\n\/\/ background is transparent.\ncropped.overwriteChannels([0],\n                          withPlanarBuffer: alpha.cropped(to: boundingBox),\n                          destination: cropped)\n\n\/\/ Create a Core Graphics image of the final result.\noutputImage = cropped.makeCGImage(cgImageFormat: cgImageFormatARGB)!",
      "language" : "swift"
    }
  ],
  "contentHash" : "65086e101b496b85ab93cc4e8b9e84e56f4ceef9b3f71362ae98ab8a1f49ab3f",
  "crawledAt" : "2025-12-02T15:27:55Z",
  "id" : "5FAA1B08-9AFF-4050-940F-95E58812BA02",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "Accelerate",
  "overview" : "## Overview\n\nThe Accelerate framework provides libraries that allow you to isolate objects from a chroma-key background and trim transparent pixels. This technique has applications in fields such as machine learning, where code needs to operate on the smallest useful region of an image to reduce processing time, optimize performance, and minimize energy consumption.\n\nThe image on the left below shows an original image with a chroma-key background. The middle image shows the chroma-key converted to alpha values, and the image on the right shows the original image cropped to the smallest bounding box that contains nontransparent pixels:\n\n\n\n### Convert the chroma-key color to alpha values\n\nThe sample code project includes separate functions to convert the chroma-key color to alpha values and to trim the transparent pixels.\n\nThe `static chromaKeyToAlpha(source:chromaKeyColor:tolerance:)` function accepts a four-channel ARGB (alpha, red, green, blue) [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vImage\/PixelBuffer] and uses a [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vImage\/MultidimensionalLookupTable] to generate a planar pixel buffer that contains the alpha channel.\n\nFor more information about multidimensional lookup tables, see [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/applying-color-transforms-to-images-with-a-multidimensional-lookup-table].\n\nThis sample code project uses the red, green, and blue channels of the source image and generates a single-channel output. The code below allocates the memory for the lookup table values:\n\nThe sample calculates the alpha channel value for each permutation of red, green, and blue as a function of the Euclidean distance between each permutation and the chroma-key color in L*a*b* color space.\n\nThe code below iterates over red, green, and blue values, and populates each lookup table entry with a corresponding alpha value.\n\n### Trim the transparent pixels\n\nThe `static boundingBoxForNonTransparentPixels(alphaBuffer:)` function accepts an alpha channel pixel buffer and returns the smallest bounding box that contains the pixel buffer’s nontransparent pixels. The function calculates the top of the bounding box by iterating over each row of pixels, starting at the top of the alpha buffer. For each iteration, the code calls [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vDSP\/sum(_:)-8hq9]. When the sum of the alpha values isn’t equal to `0`, the corresponding row contains nontransparent pixels and, therefore, that row is the top of the bounding box.\n\nThe function uses the same technique to find the bottom of the bounding box, but in this case, it starts from the bottom and works upward.\n\nThe code below finds the top and bottom of the bounding box:\n\nTo find the left and right sides of the bounding box, the code uses [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vDSP_sve] rather than [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vDSP\/sum(_:)-8hq9]. The `vDSP_sve(_:_:_:_:)` function supports the `stride` parameter that the code defines as the pixels buffer’s [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vImage\/PixelBuffer\/rowStride]. Setting this value specifies that the summation is computed along the buffer’s column rather than row.\n\nSimilar to the top and bottom calculation, when the horizontal iterations find a nonzero column sum, the function sets the column index as the left or right edge of the bounding box.\n\nAfter the code has completed the horizontal and vertical passes, the function returns a [doc:\/\/com.apple.documentation\/documentation\/CoreFoundation\/CGRect] structure that represents the smallest bounding box that doesn’t contain either a row or column of entirely transparent pixels.\n\n### Create the final image\n\nThe code below applies the `static chromaKeyToAlpha(source:chromaKeyColor:tolerance:)` and `static boundingBoxForNonTransparentPixels(alphaBuffer:)` functions to a [doc:\/\/com.apple.documentation\/documentation\/CoreGraphics\/CGImage] instance named `originalImage`.\n\nOn return, `outputImage` contains the original image, without transparent rows and columns, and with the chroma-key background transparent.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/Accelerate\/cropping-to-the-subject-in-a-chroma-keyed-image\ncrawled: 2025-12-02T15:27:55Z\n---\n\n# Cropping to the subject in a chroma-keyed image\n\n**Sample Code**\n\nConvert a chroma-key color to alpha values and trim transparent pixels using Accelerate.\n\n## Overview\n\nThe Accelerate framework provides libraries that allow you to isolate objects from a chroma-key background and trim transparent pixels. This technique has applications in fields such as machine learning, where code needs to operate on the smallest useful region of an image to reduce processing time, optimize performance, and minimize energy consumption.\n\nThe image on the left below shows an original image with a chroma-key background. The middle image shows the chroma-key converted to alpha values, and the image on the right shows the original image cropped to the smallest bounding box that contains nontransparent pixels:\n\n\n\n### Convert the chroma-key color to alpha values\n\nThe sample code project includes separate functions to convert the chroma-key color to alpha values and to trim the transparent pixels.\n\nThe `static chromaKeyToAlpha(source:chromaKeyColor:tolerance:)` function accepts a four-channel ARGB (alpha, red, green, blue) [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vImage\/PixelBuffer] and uses a [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vImage\/MultidimensionalLookupTable] to generate a planar pixel buffer that contains the alpha channel.\n\nFor more information about multidimensional lookup tables, see [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/applying-color-transforms-to-images-with-a-multidimensional-lookup-table].\n\nThis sample code project uses the red, green, and blue channels of the source image and generates a single-channel output. The code below allocates the memory for the lookup table values:\n\n```swift\nlet entriesPerChannel = UInt8(32)\nlet ramp = vDSP.ramp(in: 0 ... 1.0, count: Int(entriesPerChannel))\n\nlet sourceChannelCount = 3\nlet destinationChannelCount = 1\n\nlet lookupTableElementCount = Int(pow(Float(entriesPerChannel),\n                                      Float(sourceChannelCount))) * Int(destinationChannelCount)\n\nlet lookupTableData = UnsafeMutableBufferPointer<UInt16>.allocate(capacity: lookupTableElementCount)\ndefer {\n    lookupTableData.deallocate()\n}\n```\n\nThe sample calculates the alpha channel value for each permutation of red, green, and blue as a function of the Euclidean distance between each permutation and the chroma-key color in L*a*b* color space.\n\nThe code below iterates over red, green, and blue values, and populates each lookup table entry with a corresponding alpha value.\n\n```swift\nlet chromaKeyRGB = chromaKeyColor.components ?? [0, 0, 0]\nlet chromaKeyLab = ColorConverter.rgbToLab(r: chromaKeyRGB[0],\n                                           g: chromaKeyRGB.count > 1 ? chromaKeyRGB[1] : chromaKeyRGB[0],\n                                           b: chromaKeyRGB.count > 2 ? chromaKeyRGB[2] : chromaKeyRGB[0])\n\nvar bufferIndex = 0\nfor red in ramp {\n    for green in ramp {\n        for blue in ramp {\n            \n            let lab = ColorConverter.rgbToLab(r: red, g: green, b: blue)\n            \n            let distance = simd_distance(chromaKeyLab, lab)\n            \n            let contrast = Float(20)\n            let offset = Float(0.25)\n            let alpha = saturate(tanh(((distance \/ tolerance ) - 0.5 - offset) * contrast))\n            \n            lookupTableData[bufferIndex] = UInt16(alpha * Float(UInt16.max))\n            bufferIndex += 1\n        }\n    }\n}\n```\n\n### Trim the transparent pixels\n\nThe `static boundingBoxForNonTransparentPixels(alphaBuffer:)` function accepts an alpha channel pixel buffer and returns the smallest bounding box that contains the pixel buffer’s nontransparent pixels. The function calculates the top of the bounding box by iterating over each row of pixels, starting at the top of the alpha buffer. For each iteration, the code calls [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vDSP\/sum(_:)-8hq9]. When the sum of the alpha values isn’t equal to `0`, the corresponding row contains nontransparent pixels and, therefore, that row is the top of the bounding box.\n\nThe function uses the same technique to find the bottom of the bounding box, but in this case, it starts from the bottom and works upward.\n\nThe code below finds the top and bottom of the bounding box:\n\n```swift\nalphaBuffer.withUnsafeBufferPointer { alphaPointer in\n    \n    let rowStride = alphaBuffer.rowStride\n    \n    \/\/ Find the bounding box top.\n    for i in 0 ..< alphaBuffer.height {\n        \n        let start = alphaPointer.baseAddress?.advanced(by: i * rowStride)\n        let row = UnsafeBufferPointer<Float>(start: start, count: alphaBuffer.width)\n        let sum = vDSP.sum(row)\n        \n        if sum != 0 {\n            top = i\n            break\n        }\n    }\n    \n    \/\/ Find the bounding box bottom.\n    for i in stride(from: alphaBuffer.height - 1, through: top, by: -1) {\n        \n        let start = alphaPointer.baseAddress?.advanced(by: i * rowStride)\n        let row = UnsafeBufferPointer<Float>(start: start, count: alphaBuffer.width)\n        let sum = vDSP.sum(row)\n        \n        if sum != 0 {\n            bottom = i\n            break\n        }\n    }\n    \n    let height = bottom - top\n```\n\nTo find the left and right sides of the bounding box, the code uses [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vDSP_sve] rather than [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vDSP\/sum(_:)-8hq9]. The `vDSP_sve(_:_:_:_:)` function supports the `stride` parameter that the code defines as the pixels buffer’s [doc:\/\/com.apple.accelerate\/documentation\/Accelerate\/vImage\/PixelBuffer\/rowStride]. Setting this value specifies that the summation is computed along the buffer’s column rather than row.\n\nSimilar to the top and bottom calculation, when the horizontal iterations find a nonzero column sum, the function sets the column index as the left or right edge of the bounding box.\n\n```swift\n    \/\/ Find the bounding box left.\n    for i in 0 ..< alphaBuffer.width {\n        \n        let columnStart = alphaPointer.baseAddress!.advanced(by: i + (top * rowStride))\n        \n        var sum = Float()\n        \n        vDSP_sve(columnStart, rowStride, &sum, vDSP_Length(height))\n        \n        if sum != 0 {\n            left = i\n            break\n        }\n    }\n    \n    \/\/ Find the bounding box right.\n    for i in stride(from: alphaBuffer.width - 1, through: 0, by: -1) {\n        \n        let columnStart = alphaPointer.baseAddress!.advanced(by: i + (top * rowStride))\n        \n        var sum = Float()\n        \n        vDSP_sve(columnStart, rowStride, &sum, vDSP_Length(height))\n        \n        if sum != 0 {\n            right = i\n            break\n        }\n    }\n} \/\/ Ends `alphaBuffer.withUnsafeBufferPointer { alphaPointer in`.\n```\n\nAfter the code has completed the horizontal and vertical passes, the function returns a [doc:\/\/com.apple.documentation\/documentation\/CoreFoundation\/CGRect] structure that represents the smallest bounding box that doesn’t contain either a row or column of entirely transparent pixels.\n\n```swift\nreturn CGRect(x: left,\n              y: top,\n              width: right - left,\n              height: bottom - top)\n```\n\n### Create the final image\n\nThe code below applies the `static chromaKeyToAlpha(source:chromaKeyColor:tolerance:)` and `static boundingBoxForNonTransparentPixels(alphaBuffer:)` functions to a [doc:\/\/com.apple.documentation\/documentation\/CoreGraphics\/CGImage] instance named `originalImage`.\n\n```swift\n\/\/ Create an `InterleavedFx4` pixel buffer from the original image.\nlet sourceBuffer = try! vImage.PixelBuffer<vImage.InterleavedFx4>(\n    cgImage: originalImage,\n    cgImageFormat: &cgImageFormatARGB)\n\n\/\/ Create a `PlanarF` pixel buffer that represents the alpha channel.\nlet alpha = Self.chromaKeyToAlpha(source: sourceBuffer,\n                                  chromaKeyColor: .init(red: 91 \/ 255,\n                                                        green: 155 \/ 255,\n                                                        blue: 244 \/ 255,\n                                                        alpha: 0),\n                                  tolerance: 60)\n\n\/\/ Compute the bounding box for nontransparent pixels.\nlet boundingBox = Self.boundingBoxForNonTransparentPixels(alphaBuffer: alpha)\n\n\/\/ Create an `InterleavedFx4` pixel buffer that's the cropped version\n\/\/ of the original image.\nlet cropped = sourceBuffer.cropped(to: boundingBox)\n\n\/\/ Overwrite the alpha channel of the cropped image so that the chroma-key\n\/\/ background is transparent.\ncropped.overwriteChannels([0],\n                          withPlanarBuffer: alpha.cropped(to: boundingBox),\n                          destination: cropped)\n\n\/\/ Create a Core Graphics image of the final result.\noutputImage = cropped.makeCGImage(cgImageFormat: cgImageFormatARGB)!\n```\n\nOn return, `outputImage` contains the original image, without transparent rows and columns, and with the chroma-key background transparent.\n\n## Transforming with a multidimensional lookup table\n\n- **Applying color transforms to images with a multidimensional lookup table**: Precompute translation values to optimize color space conversion and other pointwise operations.\n- **Applying transformations to selected colors in an image**: Desaturate a range of colors in an image with a multidimensional lookup table.\n- **vImageMultidimensionalTable_Create(_:_:_:_:_:_:_:)**: Creates a multidimensional lookup table.\n- **vImageMultiDimensionalInterpolatedLookupTable_PlanarF(_:_:_:_:_:_:)**: Uses a multidimensional lookup table to transform a 32-bit planar image.\n- **vImageMultiDimensionalInterpolatedLookupTable_Planar16Q12(_:_:_:_:_:_:)**: Uses a multidimensional lookup table to transform a 16Q12 planar image.\n- **vImageMultidimensionalTable_Retain(_:)**: Retains a multidimensional table.\n- **vImageMultidimensionalTable_Release(_:)**: Releases a multidimensional table.\n- **vImage_MultidimensionalTable**: An opaque pointer that represents a multidimensional lookup table.\n- **vImageMDTableUsageHint**: Constants that indicate the use for a multidimensional lookup table.\n- **vImage_InterpolationMethod**: Constants that represent different interpolation methods.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Precompute translation values to optimize color space conversion and other pointwise operations.",
          "name" : "Applying color transforms to images with a multidimensional lookup table",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/applying-color-transforms-to-images-with-a-multidimensional-lookup-table"
        },
        {
          "description" : "Desaturate a range of colors in an image with a multidimensional lookup table.",
          "name" : "Applying transformations to selected colors in an image",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/applying-transformations-to-selected-colors-in-an-image"
        },
        {
          "description" : "Creates a multidimensional lookup table.",
          "name" : "vImageMultidimensionalTable_Create(_:_:_:_:_:_:_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/vImageMultidimensionalTable_Create(_:_:_:_:_:_:_:)"
        },
        {
          "description" : "Uses a multidimensional lookup table to transform a 32-bit planar image.",
          "name" : "vImageMultiDimensionalInterpolatedLookupTable_PlanarF(_:_:_:_:_:_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/vImageMultiDimensionalInterpolatedLookupTable_PlanarF(_:_:_:_:_:_:)"
        },
        {
          "description" : "Uses a multidimensional lookup table to transform a 16Q12 planar image.",
          "name" : "vImageMultiDimensionalInterpolatedLookupTable_Planar16Q12(_:_:_:_:_:_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/vImageMultiDimensionalInterpolatedLookupTable_Planar16Q12(_:_:_:_:_:_:)"
        },
        {
          "description" : "Retains a multidimensional table.",
          "name" : "vImageMultidimensionalTable_Retain(_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/vImageMultidimensionalTable_Retain(_:)"
        },
        {
          "description" : "Releases a multidimensional table.",
          "name" : "vImageMultidimensionalTable_Release(_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/vImageMultidimensionalTable_Release(_:)"
        },
        {
          "description" : "An opaque pointer that represents a multidimensional lookup table.",
          "name" : "vImage_MultidimensionalTable",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/vImage_MultidimensionalTable"
        },
        {
          "description" : "Constants that indicate the use for a multidimensional lookup table.",
          "name" : "vImageMDTableUsageHint",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/vImageMDTableUsageHint"
        },
        {
          "description" : "Constants that represent different interpolation methods.",
          "name" : "vImage_InterpolationMethod",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/vImage_InterpolationMethod"
        }
      ],
      "title" : "Transforming with a multidimensional lookup table"
    }
  ],
  "source" : "appleJSON",
  "title" : "Cropping to the subject in a chroma-keyed image",
  "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/cropping-to-the-subject-in-a-chroma-keyed-image"
}