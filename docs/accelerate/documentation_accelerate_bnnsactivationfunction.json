{
  "abstract" : "Constants that describe activation functions.",
  "codeExamples" : [

  ],
  "conformsTo" : [
    "BitwiseCopyable",
    "Equatable",
    "Hashable",
    "RawRepresentable",
    "Sendable"
  ],
  "contentHash" : "14301b714c0c2075b185dedbd98d7896e24318a2d0628a2823fa966bd67a2144",
  "crawledAt" : "2025-12-03T20:16:24Z",
  "declaration" : {
    "code" : "struct BNNSActivationFunction",
    "language" : "swift"
  },
  "id" : "4960E034-2B78-4997-A972-821B32543FD3",
  "kind" : "struct",
  "language" : "swift",
  "module" : "Accelerate",
  "platforms" : [
    "iOS",
    "iPadOS",
    "Mac Catalyst",
    "macOS",
    "tvOS",
    "visionOS",
    "watchOS"
  ],
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunction\ncrawled: 2025-12-03T20:16:24Z\n---\n\n# BNNSActivationFunction\n\n**Structure**\n\nConstants that describe activation functions.\n\n## Declaration\n\n```swift\nstruct BNNSActivationFunction\n```\n\n## Activation Functions\n\n- **abs**: An activation function that returns the absolute value of its input.\n- **clamp**: An activation function that returns its input clamped to a specified range.\n- **identity**: An activation function that returns its input.\n- **integerLinearSaturate**: An activation function that returns an arithmetic shift, preserving sign.\n- **integerLinearSaturatePerChannel**: An activation function that returns an arithmetic shift, preserving sign for each channel.\n- **leakyRectifiedLinear**: An activation function that returns its input when that is greater than or equal to zero, otherwise it returns its input multiplied by a specified value.\n- **linear**: An activation function that returns its input multiplied by a specified value.\n- **rectifiedLinear**: An activation function that returns its input when that is greater than or equal to zero, otherwise it returns zero.\n- **scaledTanh**: An activation function that returns the scaled hyperbolic tangent of its input.\n- **sigmoid**: An activation function that returns the sigmoid function of its input.\n- **softmax**: An activation function that returns the softmax function of its input.\n- **tanh**: An activation function that returns the hyperbolic tangent of its input.\n\n## Raw Values\n\n- **init(_:)**\n- **init(rawValue:)**\n- **rawValue**\n- **BNNSActivationFunctionAbs**\n- **BNNSActivationFunctionCELU**: An activation function that evaluates the continuously differentiable exponential linear units (CELU) on its input.\n- **BNNSActivationFunctionClampedLeakyRectifiedLinear**: An activation function that returns its input clamped to beta when that is greater than or equal to zero, otherwise it returns its input multiplied by alpha clamped to beta.\n- **BNNSActivationFunctionELU**: An activation function that evaluates the exponential linear units (ELU) on its input.\n- **BNNSActivationFunctionErf**\n- **BNNSActivationFunctionGELU**\n- **BNNSActivationFunctionGELUApproximation**: An activation function that evaluates the Gaussian error linear units (GELU) approximation on its input.\n- **BNNSActivationFunctionGELUApproximation2**: An activation function that provides a fast evaluation of the Gaussian error linear units (GELU) approximation on its input.\n- **BNNSActivationFunctionGELUApproximationSigmoid**\n- **BNNSActivationFunctionGumbel**: An activation function that returns random numbers from the Gumbel distribution.\n- **BNNSActivationFunctionGumbelMax**: An activation function that returns random numbers from the Gumbel distribution.\n- **BNNSActivationFunctionHardShrink**: An activation function that returns zero when the absolute input is less than alpha, otherwise it returns its input.\n- **BNNSActivationFunctionHardSigmoid**: An activation function that returns the hard sigmoid function of its input.\n- **BNNSActivationFunctionHardSwish**: An activation function that returns the hard swish function of its input.\n- **BNNSActivationFunctionIdentity**\n- **BNNSActivationFunctionLeakyRectifiedLinear**\n- **BNNSActivationFunctionLinearWithBias**: An activation function that returns its input multiplied by a scale and added to a bias.\n- **BNNSActivationFunctionLogSigmoid**: An activation function that returns the logarithm of the sigmoid function of its input.\n- **BNNSActivationFunctionLogSoftmax**: An activation function that returns the logarithm of the softmax function of its input.\n- **BNNSActivationFunctionPReLUPerChannel**: An activation function provides per-channel alpha values to Leaky Rectified Linear.\n- **BNNSActivationFunctionRectifiedLinear**\n- **BNNSActivationFunctionReLU6**\n- **BNNSActivationFunctionScaledTanh**\n- **BNNSActivationFunctionSELU**: An activation function that evaluates the scaled exponential linear units (SELU) on its input.\n- **BNNSActivationFunctionSigmoid**\n- **BNNSActivationFunctionSiLU**: An activation function that returns the sigmoid linear unit (SiLU) function of its input.\n- **BNNSActivationFunctionSoftplus**: An activation function that returns the softplus function of its input.\n- **BNNSActivationFunctionSoftShrink**: An activation function that returns zero when the absolute input is less than alpha, otherwise it returns its input minus alpha.\n- **BNNSActivationFunctionSoftsign**: An activation function that returns the softsign function of its input.\n- **BNNSActivationFunctionTanh**\n- **BNNSActivationFunctionTanhShrink**: An activation function that returns its input minus the hyperbolic tangent of its input.\n- **BNNSActivationFunctionThreshold**: An activation function that returns beta if its input is less than a specified threshold, otherwise it returns its input.\n\n## Activation layers\n\n- **BNNSFilterCreateVectorActivationLayer(_:_:_:_:)**\n- **BNNS.ActivationLayer**: A layer object that wraps an activation filter and manages its deinitialization.\n- **BNNSActivation**: A set of parameters that describe common activation functions.\n- **BNNSLayerParametersActivation**: A set of parameters that define an activation layer.\n- **BNNSFilterCreateLayerActivation(_:_:)**: Returns a new activation layer.\n- **BNNSDirectApplyActivationBatch(_:_:_:_:_:)**: Applies an activation filter to a set of input objects, writing out the result to a set of output objects.\n- **applyActivation(activation:axes:input:output:batchSize:filterParameters:)**: Applies an activation function on the specified axes.\n- **applyActivation(activation:input:output:batchSize:filterParameters:)**: Applies the specified activation function.\n\n## Conforms To\n\n- BitwiseCopyable\n- Equatable\n- Hashable\n- RawRepresentable\n- Sendable\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "An activation function that returns the absolute value of its input.",
          "name" : "abs",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunction\/abs"
        },
        {
          "description" : "An activation function that returns its input clamped to a specified range.",
          "name" : "clamp",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunction\/clamp"
        },
        {
          "description" : "An activation function that returns its input.",
          "name" : "identity",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunction\/identity"
        },
        {
          "description" : "An activation function that returns an arithmetic shift, preserving sign.",
          "name" : "integerLinearSaturate",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunction\/integerLinearSaturate"
        },
        {
          "description" : "An activation function that returns an arithmetic shift, preserving sign for each channel.",
          "name" : "integerLinearSaturatePerChannel",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunction\/integerLinearSaturatePerChannel"
        },
        {
          "description" : "An activation function that returns its input when that is greater than or equal to zero, otherwise it returns its input multiplied by a specified value.",
          "name" : "leakyRectifiedLinear",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunction\/leakyRectifiedLinear"
        },
        {
          "description" : "An activation function that returns its input multiplied by a specified value.",
          "name" : "linear",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunction\/linear"
        },
        {
          "description" : "An activation function that returns its input when that is greater than or equal to zero, otherwise it returns zero.",
          "name" : "rectifiedLinear",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunction\/rectifiedLinear"
        },
        {
          "description" : "An activation function that returns the scaled hyperbolic tangent of its input.",
          "name" : "scaledTanh",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunction\/scaledTanh"
        },
        {
          "description" : "An activation function that returns the sigmoid function of its input.",
          "name" : "sigmoid",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunction\/sigmoid"
        },
        {
          "description" : "An activation function that returns the softmax function of its input.",
          "name" : "softmax",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunction\/softmax"
        },
        {
          "description" : "An activation function that returns the hyperbolic tangent of its input.",
          "name" : "tanh",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunction\/tanh"
        }
      ],
      "title" : "Activation Functions"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "",
          "name" : "init(_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunction\/init(_:)"
        },
        {
          "description" : "",
          "name" : "init(rawValue:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunction\/init(rawValue:)"
        },
        {
          "description" : "",
          "name" : "rawValue",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunction\/rawValue"
        },
        {
          "description" : "",
          "name" : "BNNSActivationFunctionAbs",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionAbs"
        },
        {
          "description" : "An activation function that evaluates the continuously differentiable exponential linear units (CELU) on its input.",
          "name" : "BNNSActivationFunctionCELU",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionCELU"
        },
        {
          "description" : "An activation function that returns its input clamped to beta when that is greater than or equal to zero, otherwise it returns its input multiplied by alpha clamped to beta.",
          "name" : "BNNSActivationFunctionClampedLeakyRectifiedLinear",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionClampedLeakyRectifiedLinear"
        },
        {
          "description" : "An activation function that evaluates the exponential linear units (ELU) on its input.",
          "name" : "BNNSActivationFunctionELU",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionELU"
        },
        {
          "description" : "",
          "name" : "BNNSActivationFunctionErf",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionErf"
        },
        {
          "description" : "",
          "name" : "BNNSActivationFunctionGELU",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionGELU"
        },
        {
          "description" : "An activation function that evaluates the Gaussian error linear units (GELU) approximation on its input.",
          "name" : "BNNSActivationFunctionGELUApproximation",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionGELUApproximation"
        },
        {
          "description" : "An activation function that provides a fast evaluation of the Gaussian error linear units (GELU) approximation on its input.",
          "name" : "BNNSActivationFunctionGELUApproximation2",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionGELUApproximation2"
        },
        {
          "description" : "",
          "name" : "BNNSActivationFunctionGELUApproximationSigmoid",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionGELUApproximationSigmoid"
        },
        {
          "description" : "An activation function that returns random numbers from the Gumbel distribution.",
          "name" : "BNNSActivationFunctionGumbel",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionGumbel"
        },
        {
          "description" : "An activation function that returns random numbers from the Gumbel distribution.",
          "name" : "BNNSActivationFunctionGumbelMax",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionGumbelMax"
        },
        {
          "description" : "An activation function that returns zero when the absolute input is less than alpha, otherwise it returns its input.",
          "name" : "BNNSActivationFunctionHardShrink",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionHardShrink"
        },
        {
          "description" : "An activation function that returns the hard sigmoid function of its input.",
          "name" : "BNNSActivationFunctionHardSigmoid",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionHardSigmoid"
        },
        {
          "description" : "An activation function that returns the hard swish function of its input.",
          "name" : "BNNSActivationFunctionHardSwish",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionHardSwish"
        },
        {
          "description" : "",
          "name" : "BNNSActivationFunctionIdentity",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionIdentity"
        },
        {
          "description" : "",
          "name" : "BNNSActivationFunctionLeakyRectifiedLinear",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionLeakyRectifiedLinear"
        },
        {
          "description" : "An activation function that returns its input multiplied by a scale and added to a bias.",
          "name" : "BNNSActivationFunctionLinearWithBias",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionLinearWithBias"
        },
        {
          "description" : "An activation function that returns the logarithm of the sigmoid function of its input.",
          "name" : "BNNSActivationFunctionLogSigmoid",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionLogSigmoid"
        },
        {
          "description" : "An activation function that returns the logarithm of the softmax function of its input.",
          "name" : "BNNSActivationFunctionLogSoftmax",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionLogSoftmax"
        },
        {
          "description" : "An activation function provides per-channel alpha values to Leaky Rectified Linear.",
          "name" : "BNNSActivationFunctionPReLUPerChannel",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionPReLUPerChannel"
        },
        {
          "description" : "",
          "name" : "BNNSActivationFunctionRectifiedLinear",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionRectifiedLinear"
        },
        {
          "description" : "",
          "name" : "BNNSActivationFunctionReLU6",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionReLU6"
        },
        {
          "description" : "",
          "name" : "BNNSActivationFunctionScaledTanh",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionScaledTanh"
        },
        {
          "description" : "An activation function that evaluates the scaled exponential linear units (SELU) on its input.",
          "name" : "BNNSActivationFunctionSELU",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionSELU"
        },
        {
          "description" : "",
          "name" : "BNNSActivationFunctionSigmoid",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionSigmoid"
        },
        {
          "description" : "An activation function that returns the sigmoid linear unit (SiLU) function of its input.",
          "name" : "BNNSActivationFunctionSiLU",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionSiLU"
        },
        {
          "description" : "An activation function that returns the softplus function of its input.",
          "name" : "BNNSActivationFunctionSoftplus",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionSoftplus"
        },
        {
          "description" : "An activation function that returns zero when the absolute input is less than alpha, otherwise it returns its input minus alpha.",
          "name" : "BNNSActivationFunctionSoftShrink",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionSoftShrink"
        },
        {
          "description" : "An activation function that returns the softsign function of its input.",
          "name" : "BNNSActivationFunctionSoftsign",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionSoftsign"
        },
        {
          "description" : "",
          "name" : "BNNSActivationFunctionTanh",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionTanh"
        },
        {
          "description" : "An activation function that returns its input minus the hyperbolic tangent of its input.",
          "name" : "BNNSActivationFunctionTanhShrink",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionTanhShrink"
        },
        {
          "description" : "An activation function that returns beta if its input is less than a specified threshold, otherwise it returns its input.",
          "name" : "BNNSActivationFunctionThreshold",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunctionThreshold"
        }
      ],
      "title" : "Raw Values"
    },
    {
      "content" : "",
      "items" : [
        {
          "description" : "",
          "name" : "BNNSFilterCreateVectorActivationLayer(_:_:_:_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSFilterCreateVectorActivationLayer(_:_:_:_:)"
        },
        {
          "description" : "A layer object that wraps an activation filter and manages its deinitialization.",
          "name" : "BNNS.ActivationLayer",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/ActivationLayer"
        },
        {
          "description" : "A set of parameters that describe common activation functions.",
          "name" : "BNNSActivation",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivation"
        },
        {
          "description" : "A set of parameters that define an activation layer.",
          "name" : "BNNSLayerParametersActivation",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSLayerParametersActivation"
        },
        {
          "description" : "Returns a new activation layer.",
          "name" : "BNNSFilterCreateLayerActivation(_:_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSFilterCreateLayerActivation(_:_:)"
        },
        {
          "description" : "Applies an activation filter to a set of input objects, writing out the result to a set of output objects.",
          "name" : "BNNSDirectApplyActivationBatch(_:_:_:_:_:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSDirectApplyActivationBatch(_:_:_:_:_:)"
        },
        {
          "description" : "Applies an activation function on the specified axes.",
          "name" : "applyActivation(activation:axes:input:output:batchSize:filterParameters:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/applyActivation(activation:axes:input:output:batchSize:filterParameters:)"
        },
        {
          "description" : "Applies the specified activation function.",
          "name" : "applyActivation(activation:input:output:batchSize:filterParameters:)",
          "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNS\/applyActivation(activation:input:output:batchSize:filterParameters:)"
        }
      ],
      "title" : "Activation layers"
    }
  ],
  "source" : "appleJSON",
  "title" : "BNNSActivationFunction",
  "url" : "https:\/\/developer.apple.com\/documentation\/Accelerate\/BNNSActivationFunction"
}