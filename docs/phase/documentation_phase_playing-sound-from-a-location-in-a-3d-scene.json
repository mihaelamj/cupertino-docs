{
  "abstract" : "Position sound from a specific direction and automatically raise or lower volume based on the environment.",
  "codeExamples" : [
    {
      "code" : "var chessPiecePose: simd_float4x4 = matrix_identity_float4x4",
      "language" : "swift"
    },
    {
      "code" : "let chessPiecePointSource = PHASESource(engine: engine)",
      "language" : "swift"
    },
    {
      "code" : "do { try engine.rootObject.addChild(chessPiecePointSource) } \ncatch { print (\"Failed to add a child object to the scene.\") }",
      "language" : "swift"
    },
    {
      "code" : "chessPiecePose.columns.3.z -= 6.0\nchessPiecePointSource.transform = chessPiecePose",
      "language" : "swift"
    },
    {
      "code" : "let fenceMesh = MDLMesh(\n   planeWithExtent: vector3(0.1, 0.1, 0.1), \n   segments: vector2(1, 1), \n   geometryType: .triangles, \n   allocator: nil)\n\nlet fenceShape = PHASEShape(engine: engine, mesh: fenceMesh)\nlet volumetricFenceSource = PHASESource(engine: engine, shapes: [fenceShape])\n\ndo { try engine.rootObject.addChild(volumetricFenceSource) } \ncatch { print (\"Failed to add a child object to the scene.\") }",
      "language" : "swift"
    },
    {
      "code" : "let origin: simd_float4x4 = matrix_identity_float4x4\nlet listener = PHASEListener(engine: engine)\nlistener.transform = origin",
      "language" : "swift"
    },
    {
      "code" : "do { try engine.rootObject.addChild(listener) } \ncatch { print (\"Failed to add child object to the scene.\") }",
      "language" : "swift"
    },
    {
      "code" : "defaultOccluderSize: Float = 1.0\n\nlet pillarOccluderMesh = MDLMesh.newCylinder(withHeight: 10.0, \n    radii: vector_float2(defaultOccluderSize, defaultOccluderSize), \n    radialSegments: 9, \n    verticalSegments: 1, \n    geometryType: MDLGeometryType.triangles, \n    inwardNormals: false, \n    allocator: nil)",
      "language" : "swift"
    },
    {
      "code" : "let occluderMaterial = PHASEMaterial(engine: engine, preset: .concrete)\nlet occluderShape = PHASEShape(engine: engine,\n   mesh: pillarOccluderMesh,\n   materials: [occluderMaterial])\n\nlet occluder = PHASEOccluder(engine: engine,\n   shapes: [occluderShape])",
      "language" : "swift"
    },
    {
      "code" : "let defaultSourceDistance: Float = 10.0\n\nvar occluderTransform: simd_float4x4 = origin\noccluderTransform.columns.3.z -= defaultSourceDistance \/ 2.0\noccluder.transform = occluderTransform",
      "language" : "swift"
    },
    {
      "code" : "do { try engine.rootObject.addChild(occluder) } \ncatch { print (\"Failed to add a child object to the scene.\") }",
      "language" : "swift"
    },
    {
      "code" : "let chessPieceSpatialMixer = PHASESpatialMixerDefinition(\n    spatialPipeline: PHASESpatialPipeline(\n        flags: .directPathTransmission)!)\n\nlet fenceSpatialMixer = PHASESpatialMixerDefinition(\n    spatialPipeline: PHASESpatialPipeline(\n        flags: .directPathTransmission)!)",
      "language" : "swift"
    },
    {
      "code" : "let simpleModel = PHASEGeometricSpreadingDistanceModelParameters()\nsimpleModel.rolloffFactor = 1.0\n\nchessPieceSpatialMixer.distanceModelParameters = simpleModel\nfenceSpatialMixer.distanceModelParameters = simpleModel",
      "language" : "swift"
    },
    {
      "code" : "let shufflingSoundUrl = Bundle.main.url(forResource: \"shuffling\", withExtension: \"wav\")!\nvar shufflingSoundAsset:PHASESoundAsset!\ndo {\n    shufflingSoundAsset = try engine.assetRegistry.registerSoundAsset(\n    url: shufflingSoundUrl,\n    identifier: \"shufflingSound\",\n    assetType: PHASEAsset.AssetType.resident,\n    channelLayout: nil,\n    normalizationMode: .dynamic)\n} catch {\n    print(\"Failed to register the sound asset.\")\n}\n\nlet buzzingSoundUrl = Bundle.main.url(forResource: \"buzzing\", withExtension: \"wav\")!\nvar electricBuzzingSoundAsset:PHASESoundAsset!\ndo {\n    electricBuzzingSoundAsset = try engine!.assetRegistry.registerSoundAsset(\n    url: buzzingSoundUrl,\n    identifier: \"buzzingSound\",\n    assetType: .resident,\n    channelLayout: nil,\n    normalizationMode: .dynamic)\n} catch {\n    print(\"Failed to register the sound asset.\")\n}",
      "language" : "swift"
    },
    {
      "code" : "\/\/ Define the shuffling chess piece one-shot sound.\nlet shufflingSamplerNode = PHASESamplerNodeDefinition(\n    soundAssetIdentifier: shufflingSoundAsset.identifier,\n    mixerDefinition: chessPieceSpatialMixer,\n    identifier: shufflingSoundAsset.identifier + \"_SamplerNode\")\n\nshufflingSamplerNode.playbackMode = .oneShot\n\n\/\/ Define the buzzing electric fence looping sound.\nlet electricBuzzingSamplerNode = PHASESamplerNodeDefinition(\n    soundAssetIdentifier: electricBuzzingSoundAsset.identifier,\n    mixerDefinition: fenceSpatialMixer,\n    identifier: electricBuzzingSoundAsset.identifier + \"_SamplerNode\")\n\nelectricBuzzingSamplerNode.playbackMode = .looping",
      "language" : "swift"
    },
    {
      "code" : "var shufflingSoundEventAsset: PHASESoundEventNodeAsset!\ndo { shufflingSoundEventAsset = try \n    engine.assetRegistry.registerSoundEventAsset(  \n    rootNode: shufflingSamplerNode,\n    identifier: shufflingSoundAsset.identifier + \"_SoundEventAsset\")\n} catch { print(\"Failed to register a sound event asset.\") } \n\nvar buzzingSoundEventAsset: PHASESoundEventNodeAsset!\ndo { buzzingSoundEventAsset = try \n    engine.assetRegistry.registerSoundEventAsset(  \n    rootNode: electricBuzzingSamplerNode,\n    identifier: electricBuzzingSoundAsset.identifier + \"_SoundEventAsset\")\n} catch { print(\"Failed to register a sound event asset.\") }",
      "language" : "swift"
    },
    {
      "code" : "let chessPieceSpatialMixerParams = PHASEMixerParameters()\nchessPieceSpatialMixerParams.addSpatialMixerParameters(\n    identifier: chessPieceSpatialMixer.identifier,\n    source: chessPiecePointSource,\n    listener: listener)\n\nlet fenceSpatialMixerParams = PHASEMixerParameters()\nfenceSpatialMixerParams.addSpatialMixerParameters(\n    identifier: fenceSpatialMixer.identifier,\n    source: volumetricFenceSource,\n    listener: listener)",
      "language" : "swift"
    },
    {
      "code" : "let buzzingSoundEvent = try! PHASESoundEvent(engine: engine,\n    assetIdentifier: buzzingSoundEventAsset.identifier,\n    mixerParameters: fenceSpatialMixerParams)\n\nbuzzingSoundEvent.start(completion: nil)\n\nlet shufflingSoundEvent = try! PHASESoundEvent(engine: engine,\n    assetIdentifier: shufflingSoundEventAsset.identifier,\n    mixerParameters: chessPieceSpatialMixerParams)\n\nshufflingSoundEvent.start(completion: nil)",
      "language" : "swift"
    }
  ],
  "contentHash" : "5972c1799e897098d46995abff8825606848490952134194b4d2248629961dba",
  "crawledAt" : "2025-12-03T03:55:59Z",
  "id" : "4CFF6CB4-1703-4102-A548-0C20A2365A32",
  "kind" : "article",
  "language" : "swift",
  "module" : "PHASE",
  "overview" : "## Overview\n\nAudio that adjusts its volume based on distance to a specific reference point is called *spatial audio*. Spatial audio transmits sound from a particular location, or source, in a specific direction that you define by describing the sound’s 3D position and orientation. Your app can leverage spatial audio to define the point of reference, or define a listener as a player in a game. For example, to indicate that a horn resides on the left-hand side of a scene, PHASE outputs audio more through the left speaker.\n\nPHASE spatial audio accommodates a scene layout in several ways: by observing objects in the scene, the shape of the sound’s source, obstructions, and the point of reference of a listener. To complement obstacles and indoor locations in your scene, you can use spatial audio to layer environmental effects, for example, a reflection or reverberation, on top of spatial sounds.\n\nAfter describing your app’s scene, you queue sound to play by creating an event node asset that instructs PHASE on what to play and when. At runtime, PHASE checks your app’s state through the event node asset and plays sounds at the right moment that react in accordance to your app’s visual appearance.\n\n### Set an object’s scene position and orientation\n\nBefore PHASE plays sound in 3D, you create several objects to model your app’s visual scene. Initialize and add the following classes to [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEEngine\/rootObject], the engine’s root object:\n\nEach class derives from [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEObject], which defines a transform to provide it with a unique 3D position and orientation in the scene.\n\nAn app can define the scene as a level in a game that’s composed of a detailed set of visual properties, for example, textured pixel data, lighting, and animation. However, to play sound in accordance with the visual scene, PHASE only needs to know the scene’s geometric shape. PHASE interprets the shape through a tranform (of type [doc:\/\/com.apple.documentation\/documentation\/simd\/simd_float4x4]), and in particular, its array of four-column vectors. The following transform defines a zeroed orientation and position for a board-game piece:\n\n### Define the location from which sound plays\n\nThe first type of object you define in the PHASE scene describes a location from which sound originates, such as a chess piece that makes a shuffling noise as it slides to a location on the board. A source emanates sound from a single point in space, or a *voluminous area*. To produce sound at a point in the scene, create a [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASESource] with the engine parameter, for example:\n\nThen, add the source to the scene by handing it to the engine. You can add an object as a child to any other object by calling [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEObject\/addChild(_:)] on the parent. The result models a scene as a connected graph of objects, or an *object hierarchy*. The following code adds the chess piece as child of the engine’s root:\n\nPHASE interprets object positions in the coordinate space of the parent, which for the root object is the coordinate space of the scene, or the *world* space. Set an object’s position by assigning the transform’s first three elements of the last column. The following code sets a chess piece’s position to `(0,0,-6)`, which is 6 meters in front of the world origin `(0,0,0)`:\n\n### Originate sound from a geometric area\n\nFor spatial sounds, the framework requires a 3D position from which the sound originates. To emanate sound from an area larger than a point, for example, the full length of an electric fence in a game, describe the region by configuring a [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEShape] object. The following code models a fence by using a [doc:\/\/com.apple.documentation\/documentation\/ModelIO] plane:\n\n### Create an object that hears sound\n\nA [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEListener] is an object within an app that hears sound; it’s the central position and orientation at which the user experiences spatial audio. The framework adjusts the volume of a sound source based on its unique position and orientation with respect to the listener. For example, a sound that plays far off in the distance from the listener plays quietly, and a closer sound plays more loudly. The following code creates a listener and defines its position and orientation in the scene:\n\nAdd the listener to the scene by inserting it into the object hierarchy. The following code adds the listener as a child of the engine’s root object:\n\n### Set up occluder options for obstructed sound\n\nThe framework lowers the volume of a sound source when an obstacle blocks its path to the listener. The result creates a realistic effect in cases where, for example, the player takes cover behind a gallery pillar, which reduces the volume of commotion from the other side of the room.\n\nTo define an obstacle, create a [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEOccluder] with a shape and position that matches the pillar’s visual counterpart. The following code defines an occluder’s geometric shape using [doc:\/\/com.apple.documentation\/documentation\/ModelIO]:\n\nYou can tweak the reflectivity of a particular occluder by choosing a physical material that makes up the occluder. The material you choose implements a blend between how much an occluder reflects sound and how much sound passes through it. To select a material, pass its corresponding preset, [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEMaterialPreset], into a new [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEMaterial] object:\n\nPosition an occluder in the scene by setting its [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEObject\/transform]. The following code places an occluder midway between the source and listener by dividing the custom `defaultSourceDistance` variable by two:\n\nActivate the occluder by adding it into the scene. The following code adds the occluder as a child of the root object:\n\n### Describe the output pipeline\n\nAs one of the final stages in audio playback configuration, the app specifies the particular object, or *mixer*, that combines in-flight audio signals for transmission to the output device. For spatial audio, the app creates a spatial mixer, [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASESpatialMixerDefinition]. The following code defines a spatial mixer for two sources:\n\nThe spatial mixer can add environmental layers to the output, such as reflections or reverb, by including the [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASESpatialPipeline\/Flags-swift.struct\/earlyReflections] or [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASESpatialPipeline\/Flags-swift.struct\/lateReverb] cases, in addition to [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASESpatialPipeline\/Flags-swift.struct\/directPathTransmission], to the `flags` argument.\n\n### Adjust volume based on distance\n\nPHASE attenuates sound over the distance between a source and a listener by observing the distance model you define on the spatial mixer. As a source emits sound, the spatial mixer adjusts its volume based on the distance from the listener. The farther away the source is from the listener, the more the volume attenuates and the quieter the sound gets with respect to the listener.\n\nFor more information about distance modeling and its various types, *geometric* and *envelope*, see [doc:\/\/com.apple.phase\/documentation\/PHASE\/spatial-mixing].\n\n[doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEGeometricSpreadingDistanceModelParameters] is a model that simulates sound loss over distance realistically. The distance model [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEGeometricSpreadingDistanceModelParameters\/rolloffFactor] emphasizes or deemphasizes this model. At `1.0`, sound that emanates between the source and listener loses 6 dB every time distance doubles. At `2.0`, the loss doubles. At `0.5`, the loss halves, and so on. The following code defines a geometric spatial model for the scene’s spatial mixers:\n\n### Generate a sound event\n\nWhen PHASE understands the layout and configuration of the scene, sound reacts in accordance when your app plays a sound event by using [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASESoundEvent]. You generate a sound event from a sound-event node that describes your particular playback needs.\n\nFirst, identify the sound assets for your sound events and register them with the engine’s asset registry. The following code loads two files bundled with the project titled “shuffling.wav” and “buzzing.wav”:\n\nThe following code creates sampler nodes for both the shuffling chess piece and electrified fence:\n\nGive PHASE information about the sound-event nodes by registering their assets with the engine:\n\nDefine which sound source plays the audio and the listener that hears it by configuring a [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEMixerParameters] object for each spatial mixer:\n\nTo play the sounds, generate a [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASESoundEvent] instance for each node and call [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASESoundEvent\/start(completion:)] on the sound event. Associate the source to the sound event by passing its `spatialMixerParams` through the [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASESoundEvent] initializer `mixerParameters` argument:\n\nBecause `electricBuzzingSamplerNode` and `shufflingSamplerNode` have no children, both `buzzingSoundEventAsset` and `shufflingSoundEventAsset` contain a node hierarchy of only one node. When the app invokes a sound event from these assets, the same audio plays consistently.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/PHASE\/playing-sound-from-a-location-in-a-3d-scene\ncrawled: 2025-12-03T03:55:59Z\n---\n\n# Playing sound from a location in a 3D scene\n\n**Article**\n\nPosition sound from a specific direction and automatically raise or lower volume based on the environment.\n\n## Overview\n\nAudio that adjusts its volume based on distance to a specific reference point is called *spatial audio*. Spatial audio transmits sound from a particular location, or source, in a specific direction that you define by describing the sound’s 3D position and orientation. Your app can leverage spatial audio to define the point of reference, or define a listener as a player in a game. For example, to indicate that a horn resides on the left-hand side of a scene, PHASE outputs audio more through the left speaker.\n\nPHASE spatial audio accommodates a scene layout in several ways: by observing objects in the scene, the shape of the sound’s source, obstructions, and the point of reference of a listener. To complement obstacles and indoor locations in your scene, you can use spatial audio to layer environmental effects, for example, a reflection or reverberation, on top of spatial sounds.\n\nAfter describing your app’s scene, you queue sound to play by creating an event node asset that instructs PHASE on what to play and when. At runtime, PHASE checks your app’s state through the event node asset and plays sounds at the right moment that react in accordance to your app’s visual appearance.\n\n### Set an object’s scene position and orientation\n\nBefore PHASE plays sound in 3D, you create several objects to model your app’s visual scene. Initialize and add the following classes to [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEEngine\/rootObject], the engine’s root object:\n\n- [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASESource] for sound sources\n- [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEListener] for a listener\n- [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEOccluder] for optional occluders\n\nEach class derives from [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEObject], which defines a transform to provide it with a unique 3D position and orientation in the scene.\n\nAn app can define the scene as a level in a game that’s composed of a detailed set of visual properties, for example, textured pixel data, lighting, and animation. However, to play sound in accordance with the visual scene, PHASE only needs to know the scene’s geometric shape. PHASE interprets the shape through a tranform (of type [doc:\/\/com.apple.documentation\/documentation\/simd\/simd_float4x4]), and in particular, its array of four-column vectors. The following transform defines a zeroed orientation and position for a board-game piece:\n\n```swift\nvar chessPiecePose: simd_float4x4 = matrix_identity_float4x4\n```\n\n### Define the location from which sound plays\n\nThe first type of object you define in the PHASE scene describes a location from which sound originates, such as a chess piece that makes a shuffling noise as it slides to a location on the board. A source emanates sound from a single point in space, or a *voluminous area*. To produce sound at a point in the scene, create a [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASESource] with the engine parameter, for example:\n\n```swift\nlet chessPiecePointSource = PHASESource(engine: engine)\n```\n\nThen, add the source to the scene by handing it to the engine. You can add an object as a child to any other object by calling [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEObject\/addChild(_:)] on the parent. The result models a scene as a connected graph of objects, or an *object hierarchy*. The following code adds the chess piece as child of the engine’s root:\n\n```swift\ndo { try engine.rootObject.addChild(chessPiecePointSource) } \ncatch { print (\"Failed to add a child object to the scene.\") }\n```\n\nPHASE interprets object positions in the coordinate space of the parent, which for the root object is the coordinate space of the scene, or the *world* space. Set an object’s position by assigning the transform’s first three elements of the last column. The following code sets a chess piece’s position to `(0,0,-6)`, which is 6 meters in front of the world origin `(0,0,0)`:\n\n```swift\nchessPiecePose.columns.3.z -= 6.0\nchessPiecePointSource.transform = chessPiecePose\n```\n\n\n\n### Originate sound from a geometric area\n\nFor spatial sounds, the framework requires a 3D position from which the sound originates. To emanate sound from an area larger than a point, for example, the full length of an electric fence in a game, describe the region by configuring a [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEShape] object. The following code models a fence by using a [doc:\/\/com.apple.documentation\/documentation\/ModelIO] plane:\n\n```swift\nlet fenceMesh = MDLMesh(\n   planeWithExtent: vector3(0.1, 0.1, 0.1), \n   segments: vector2(1, 1), \n   geometryType: .triangles, \n   allocator: nil)\n\nlet fenceShape = PHASEShape(engine: engine, mesh: fenceMesh)\nlet volumetricFenceSource = PHASESource(engine: engine, shapes: [fenceShape])\n\ndo { try engine.rootObject.addChild(volumetricFenceSource) } \ncatch { print (\"Failed to add a child object to the scene.\") }\n```\n\n\n\n### Create an object that hears sound\n\nA [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEListener] is an object within an app that hears sound; it’s the central position and orientation at which the user experiences spatial audio. The framework adjusts the volume of a sound source based on its unique position and orientation with respect to the listener. For example, a sound that plays far off in the distance from the listener plays quietly, and a closer sound plays more loudly. The following code creates a listener and defines its position and orientation in the scene:\n\n```swift\nlet origin: simd_float4x4 = matrix_identity_float4x4\nlet listener = PHASEListener(engine: engine)\nlistener.transform = origin\n```\n\nAdd the listener to the scene by inserting it into the object hierarchy. The following code adds the listener as a child of the engine’s root object:\n\n```swift\ndo { try engine.rootObject.addChild(listener) } \ncatch { print (\"Failed to add child object to the scene.\") }\n```\n\n### Set up occluder options for obstructed sound\n\nThe framework lowers the volume of a sound source when an obstacle blocks its path to the listener. The result creates a realistic effect in cases where, for example, the player takes cover behind a gallery pillar, which reduces the volume of commotion from the other side of the room.\n\nTo define an obstacle, create a [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEOccluder] with a shape and position that matches the pillar’s visual counterpart. The following code defines an occluder’s geometric shape using [doc:\/\/com.apple.documentation\/documentation\/ModelIO]:\n\n```swift\ndefaultOccluderSize: Float = 1.0\n\nlet pillarOccluderMesh = MDLMesh.newCylinder(withHeight: 10.0, \n    radii: vector_float2(defaultOccluderSize, defaultOccluderSize), \n    radialSegments: 9, \n    verticalSegments: 1, \n    geometryType: MDLGeometryType.triangles, \n    inwardNormals: false, \n    allocator: nil)\n```\n\nYou can tweak the reflectivity of a particular occluder by choosing a physical material that makes up the occluder. The material you choose implements a blend between how much an occluder reflects sound and how much sound passes through it. To select a material, pass its corresponding preset, [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEMaterialPreset], into a new [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEMaterial] object:\n\n```swift\nlet occluderMaterial = PHASEMaterial(engine: engine, preset: .concrete)\nlet occluderShape = PHASEShape(engine: engine,\n   mesh: pillarOccluderMesh,\n   materials: [occluderMaterial])\n\nlet occluder = PHASEOccluder(engine: engine,\n   shapes: [occluderShape])\n```\n\nPosition an occluder in the scene by setting its [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEObject\/transform]. The following code places an occluder midway between the source and listener by dividing the custom `defaultSourceDistance` variable by two:\n\n```swift\nlet defaultSourceDistance: Float = 10.0\n\nvar occluderTransform: simd_float4x4 = origin\noccluderTransform.columns.3.z -= defaultSourceDistance \/ 2.0\noccluder.transform = occluderTransform\n```\n\nActivate the occluder by adding it into the scene. The following code adds the occluder as a child of the root object:\n\n```swift\ndo { try engine.rootObject.addChild(occluder) } \ncatch { print (\"Failed to add a child object to the scene.\") }\n```\n\n### Describe the output pipeline\n\nAs one of the final stages in audio playback configuration, the app specifies the particular object, or *mixer*, that combines in-flight audio signals for transmission to the output device. For spatial audio, the app creates a spatial mixer, [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASESpatialMixerDefinition]. The following code defines a spatial mixer for two sources:\n\n```swift\nlet chessPieceSpatialMixer = PHASESpatialMixerDefinition(\n    spatialPipeline: PHASESpatialPipeline(\n        flags: .directPathTransmission)!)\n\nlet fenceSpatialMixer = PHASESpatialMixerDefinition(\n    spatialPipeline: PHASESpatialPipeline(\n        flags: .directPathTransmission)!)\n```\n\nThe spatial mixer can add environmental layers to the output, such as reflections or reverb, by including the [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASESpatialPipeline\/Flags-swift.struct\/earlyReflections] or [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASESpatialPipeline\/Flags-swift.struct\/lateReverb] cases, in addition to [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASESpatialPipeline\/Flags-swift.struct\/directPathTransmission], to the `flags` argument.\n\n### Adjust volume based on distance\n\nPHASE attenuates sound over the distance between a source and a listener by observing the distance model you define on the spatial mixer. As a source emits sound, the spatial mixer adjusts its volume based on the distance from the listener. The farther away the source is from the listener, the more the volume attenuates and the quieter the sound gets with respect to the listener.\n\n\n\nFor more information about distance modeling and its various types, *geometric* and *envelope*, see [doc:\/\/com.apple.phase\/documentation\/PHASE\/spatial-mixing].\n\n[doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEGeometricSpreadingDistanceModelParameters] is a model that simulates sound loss over distance realistically. The distance model [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEGeometricSpreadingDistanceModelParameters\/rolloffFactor] emphasizes or deemphasizes this model. At `1.0`, sound that emanates between the source and listener loses 6 dB every time distance doubles. At `2.0`, the loss doubles. At `0.5`, the loss halves, and so on. The following code defines a geometric spatial model for the scene’s spatial mixers:\n\n```swift\nlet simpleModel = PHASEGeometricSpreadingDistanceModelParameters()\nsimpleModel.rolloffFactor = 1.0\n\nchessPieceSpatialMixer.distanceModelParameters = simpleModel\nfenceSpatialMixer.distanceModelParameters = simpleModel\n```\n\n### Generate a sound event\n\nWhen PHASE understands the layout and configuration of the scene, sound reacts in accordance when your app plays a sound event by using [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASESoundEvent]. You generate a sound event from a sound-event node that describes your particular playback needs.\n\nFirst, identify the sound assets for your sound events and register them with the engine’s asset registry. The following code loads two files bundled with the project titled “shuffling.wav” and “buzzing.wav”:\n\n```swift\nlet shufflingSoundUrl = Bundle.main.url(forResource: \"shuffling\", withExtension: \"wav\")!\nvar shufflingSoundAsset:PHASESoundAsset!\ndo {\n    shufflingSoundAsset = try engine.assetRegistry.registerSoundAsset(\n    url: shufflingSoundUrl,\n    identifier: \"shufflingSound\",\n    assetType: PHASEAsset.AssetType.resident,\n    channelLayout: nil,\n    normalizationMode: .dynamic)\n} catch {\n    print(\"Failed to register the sound asset.\")\n}\n\nlet buzzingSoundUrl = Bundle.main.url(forResource: \"buzzing\", withExtension: \"wav\")!\nvar electricBuzzingSoundAsset:PHASESoundAsset!\ndo {\n    electricBuzzingSoundAsset = try engine!.assetRegistry.registerSoundAsset(\n    url: buzzingSoundUrl,\n    identifier: \"buzzingSound\",\n    assetType: .resident,\n    channelLayout: nil,\n    normalizationMode: .dynamic)\n} catch {\n    print(\"Failed to register the sound asset.\")\n}\n```\n\nThe following code creates sampler nodes for both the shuffling chess piece and electrified fence:\n\n```swift\n\/\/ Define the shuffling chess piece one-shot sound.\nlet shufflingSamplerNode = PHASESamplerNodeDefinition(\n    soundAssetIdentifier: shufflingSoundAsset.identifier,\n    mixerDefinition: chessPieceSpatialMixer,\n    identifier: shufflingSoundAsset.identifier + \"_SamplerNode\")\n\nshufflingSamplerNode.playbackMode = .oneShot\n\n\/\/ Define the buzzing electric fence looping sound.\nlet electricBuzzingSamplerNode = PHASESamplerNodeDefinition(\n    soundAssetIdentifier: electricBuzzingSoundAsset.identifier,\n    mixerDefinition: fenceSpatialMixer,\n    identifier: electricBuzzingSoundAsset.identifier + \"_SamplerNode\")\n\nelectricBuzzingSamplerNode.playbackMode = .looping\n```\n\nGive PHASE information about the sound-event nodes by registering their assets with the engine:\n\n```swift\nvar shufflingSoundEventAsset: PHASESoundEventNodeAsset!\ndo { shufflingSoundEventAsset = try \n    engine.assetRegistry.registerSoundEventAsset(  \n    rootNode: shufflingSamplerNode,\n    identifier: shufflingSoundAsset.identifier + \"_SoundEventAsset\")\n} catch { print(\"Failed to register a sound event asset.\") } \n\nvar buzzingSoundEventAsset: PHASESoundEventNodeAsset!\ndo { buzzingSoundEventAsset = try \n    engine.assetRegistry.registerSoundEventAsset(  \n    rootNode: electricBuzzingSamplerNode,\n    identifier: electricBuzzingSoundAsset.identifier + \"_SoundEventAsset\")\n} catch { print(\"Failed to register a sound event asset.\") }\n```\n\nDefine which sound source plays the audio and the listener that hears it by configuring a [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASEMixerParameters] object for each spatial mixer:\n\n```swift\nlet chessPieceSpatialMixerParams = PHASEMixerParameters()\nchessPieceSpatialMixerParams.addSpatialMixerParameters(\n    identifier: chessPieceSpatialMixer.identifier,\n    source: chessPiecePointSource,\n    listener: listener)\n\nlet fenceSpatialMixerParams = PHASEMixerParameters()\nfenceSpatialMixerParams.addSpatialMixerParameters(\n    identifier: fenceSpatialMixer.identifier,\n    source: volumetricFenceSource,\n    listener: listener)\n```\n\nTo play the sounds, generate a [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASESoundEvent] instance for each node and call [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASESoundEvent\/start(completion:)] on the sound event. Associate the source to the sound event by passing its `spatialMixerParams` through the [doc:\/\/com.apple.phase\/documentation\/PHASE\/PHASESoundEvent] initializer `mixerParameters` argument:\n\n```swift\nlet buzzingSoundEvent = try! PHASESoundEvent(engine: engine,\n    assetIdentifier: buzzingSoundEventAsset.identifier,\n    mixerParameters: fenceSpatialMixerParams)\n\nbuzzingSoundEvent.start(completion: nil)\n\nlet shufflingSoundEvent = try! PHASESoundEvent(engine: engine,\n    assetIdentifier: shufflingSoundEventAsset.identifier,\n    mixerParameters: chessPieceSpatialMixerParams)\n\nshufflingSoundEvent.start(completion: nil)\n```\n\nBecause `electricBuzzingSamplerNode` and `shufflingSamplerNode` have no children, both `buzzingSoundEventAsset` and `shufflingSoundEventAsset` contain a node hierarchy of only one node. When the app invokes a sound event from these assets, the same audio plays consistently.\n\n\n\n## Essentials\n\n- **Personalizing spatial audio in your app**: Enhance the realism of spatial audio output by tracking a person’s head movement and accounting for their personal spatial audio profile.\n- **PHASE updates**: Learn about important changes to PHASE.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Enhance the realism of spatial audio output by tracking a person’s head movement and accounting for their personal spatial audio profile.",
          "name" : "Personalizing spatial audio in your app",
          "url" : "https:\/\/developer.apple.com\/documentation\/PHASE\/personalizing-spatial-audio-in-your-app"
        },
        {
          "description" : "Learn about important changes to PHASE.",
          "name" : "PHASE updates",
          "url" : "https:\/\/developer.apple.com\/documentation\/Updates\/PHASE"
        }
      ],
      "title" : "Essentials"
    }
  ],
  "source" : "appleJSON",
  "title" : "Playing sound from a location in a 3D scene",
  "url" : "https:\/\/developer.apple.com\/documentation\/PHASE\/playing-sound-from-a-location-in-a-3d-scene"
}