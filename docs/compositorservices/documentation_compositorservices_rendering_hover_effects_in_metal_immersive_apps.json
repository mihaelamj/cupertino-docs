{
  "abstract" : "Change the appearance of a rendered onscreen element when a player gazes at it.",
  "codeExamples" : [
    {
      "code" : "ImmersiveSpace(id: AppModel.immersiveSpaceId) {\n    makeCompositorLayer()\n}\n.immersionStyle(selection: .constant(.full), in: .full)",
      "language" : "swift"
    },
    {
      "code" : "func makeCompositorLayer() -> CompositorLayer {\n    CompositorLayer(configuration: { capabilities, configuration in\n        \n        \/\/ Set the buffer formats for the depth and color buffer.\n        configuration.depthFormat = .depth32Float\n        configuration.colorFormat = .bgra8Unorm_srgb",
      "language" : "swift"
    },
    {
      "code" : "        \/\/ If the device supports foveation, enable or disable it according to the user's stored preferences in the app model.\n        if capabilities.supportsFoveation {\n            configuration.isFoveationEnabled = appModel.foveation\n        }",
      "language" : "swift"
    },
    {
      "code" : "        \/\/ Set up features requiring visionOS 26 or later.\n        if #available(visionOS 26.0, *), appModel.withHover {\n            \/\/ Enable the tracking area buffer for Metal.\n            configuration.trackingAreasFormat = .r8Uint",
      "language" : "swift"
    },
    {
      "code" : "            \/\/ Specify how to use the data.\n            if appModel.withHover && appModel.useMSAA {\n                configuration.trackingAreasUsage = [.shaderWrite, .shaderRead]\n            } else {\n                configuration.trackingAreasUsage = [.renderTarget, .shaderRead]\n            }",
      "language" : "swift"
    },
    {
      "code" : "            \n            \/\/ Override the render-quality resolution, if requested.\n            if appModel.overrideResolution {\n                configuration.maxRenderQuality = .init(appModel.resolution)\n            }\n        }",
      "language" : "swift"
    },
    {
      "code" : "}) { renderer in\n        render(renderer)\n}",
      "language" : "swift"
    },
    {
      "code" : "Task(priority: .high) {\n    let renderData = RenderData(renderer, theAppModel: appModel)\n    await renderData.setUpWorldTracking()\n    await renderData.loadAssets()\n    await renderData.setUpTileResolvePipeline()\n    await renderData.setUpShaderPipeline()",
      "language" : "swift"
    },
    {
      "code" : "if #available(visionOS 26.0, *) {\n    renderer.onSpatialEvent = { events in\n        for event in events {\n            logger.log(level: .info, \"Received spatial event:\\(String(describing: event), privacy: .public)\")\n            let id = event.trackingAreaIdentifier.rawValue\n            let phase = event.phase\n            if id != 0 && phase == .ended {\n                Task(priority: .userInitiated) {\n                    await renderData.tap(on: id)\n                }\n            }\n        }\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "    await renderData.renderLoop()",
      "language" : "swift"
    },
    {
      "code" : "\/\/\/ Performs the rendering loop.\nfunc renderLoop() async {\n    while renderer.state != .invalidated {\n        guard let frame = renderer.queryNextFrame() else { continue }\n        frame.startUpdate()\n        frame.endUpdate()",
      "language" : "swift"
    },
    {
      "code" : "        guard let timing = frame.predictTiming() else { continue }\n        LayerRenderer.Clock().wait(until: timing.optimalInputTime)",
      "language" : "swift"
    },
    {
      "code" : "        frame.startSubmission()\n\n        buffer = queue.makeCommandBuffer()!\n        let drawables = {\n            if #available(visionOS 26.0, *) {\n                return frame.queryDrawables()\n            } else {\n                return frame.queryDrawable().map { [$0] } ?? []\n            }\n        }()",
      "language" : "swift"
    },
    {
      "code" : "        if drawables.isEmpty { break }\n        for pair in drawables.enumerated() {\n            let drawable = pair.element\n            let offset = pair.offset\n            await handleDrawable(drawable, offset)\n        }\n\n        buffer?.commit()\n        frame.endSubmission()\n    }\n}",
      "language" : "swift"
    },
    {
      "code" : "func setUpMSAA(drawable: LayerRenderer.Drawable,\n               offset: Int) async {\n    \n    if appModel.useMSAA {\n        if colorTextureCache.perDrawable[offset] == nil {\n            colorTextureCache.perDrawable[offset] = memorylessTexture(from: drawable.colorTextures[0])\n        }\n        \n        if #available(visionOS 26.0, *), appModel.withHover {\n            if indexTextureCache.perDrawable[offset] == nil {\n                indexTextureCache.perDrawable[offset] = memorylessTexture(from: drawable.trackingAreasTextures[0])\n            }\n        }\n        \n        if depthTextureCache.perDrawable[offset] == nil {\n            depthTextureCache.perDrawable[offset] = memorylessTexture(from: drawable.depthTextures[0])\n        }\n    }",
      "language" : "swift"
    },
    {
      "code" : "if #available(visionOS 26.0, *), appModel.withHover, drawCall.hasHover {\n    let trackingArea = drawable.addTrackingArea(identifier: .init(UInt64(id)))\n    trackingArea.addHoverEffect(.automatic)\n    uniforms.hoverIndex = trackingArea.renderValue.rawValue\n}",
      "language" : "swift"
    },
    {
      "code" : "struct FragmentOut {\n    float4 color [[color(0)]];\n    uint16_t index [[color(1)]];\n};",
      "language" : "swift"
    },
    {
      "code" : "return FragmentOut {\n    float4(outColor, 1.0),\n    uniformsArray.hoverIndex\n};",
      "language" : "swift"
    },
    {
      "code" : "kernel void block_resolve(imageblock<FragmentOut> block,\n                          ushort2 tid [[thread_position_in_threadgroup]],\n                          uint2 gid [[thread_position_in_grid]],\n                          ushort array_index [[render_target_array_index]],\n                          texture2d_array<uint16_t, access::write> resolvedTextureArray [[texture(0), function_constant(use_texture_array)]],\n                          texture2d<uint16_t, access::write> resolvedTexture [[texture(0), function_constant(not_texture_array)]])\n{\n\n    const ushort pixelCount = block.get_num_colors(tid);\n    ushort index = 0;\n\n    for (ushort i = 0; i < pixelCount; ++i) {\n        const FragmentOut color = block.read(tid, i, imageblock_data_rate::color);\n        index = max(index, color.index);\n    }\n\n    if (use_texture_array) {\n        resolvedTextureArray.write(index, gid, array_index);\n    } else {\n        resolvedTexture.write(index, gid);\n    }\n}",
      "language" : "swift"
    }
  ],
  "contentHash" : "8687eeb8778aa0e616b68ec64d7fe8bce00d649ecafaeea3d524da1544b986ba",
  "crawledAt" : "2025-12-03T08:02:58Z",
  "id" : "DB3BEDBC-219C-46AB-BB06-4AEC8CB32369",
  "kind" : "unknown",
  "language" : "swift",
  "module" : "Compositor Services",
  "overview" : "## Overview\n\nIn visionOS, both SwiftUI views and RealityKit entities can take advantage of *hover effects*, which change the appearance of a rendered onscreen element when a player gazes at it or highlights it using assistive technologies. In visionOS 26, fully immersive apps that render their own content using Metal can also use hover effects.\n\nThis sample code project demonstrates how to pass in uniforms and attributes to your Metal shaders so your app can implement system-provided hover effects in a privacy-preserving way. On launch, the app opens to an immersive virtual space with a large shape that shatters into several pieces. If a player looks at one of the pieces, it highlights, much like a RealityKit entity with a [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/HoverEffectComponent] does. If the player taps while gazing at the various pieces, they return to their original position, reassembling the original shape.\n\n## Understand the flow\n\nTo protect privacy, Metal shaders can’t get information about where a person using an Apple Vision Pro device is currently looking. RealityKit apps can add gaze highlighting using a [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/HoverEffectComponent]. For Full Space Metal apps, that’s not an option. Instead, Compositor Services provides a privacy-preserving mechanism for highlighting the virtual objects that Metal renders. The system does the gaze testing and highlight rendering, not the app.\n\nCompositor Services provides your app with an `Integer` frame buffer for passing in the indices of the draw calls your app needs the system to highlight. The system then does gaze hit-testing on those items, and when a player looks at one of them, it renders a highlight over it. It does this out of process, and your app can’t access the actual gaze data, but can respond to spatial gestures that a player generates by tapping those items.\n\n## Declare the immersive space\n\nIn `CompositorServicesHoverEffectApp`, the app declares a [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/WindowGroup] for a small SwiftUI window that shows at launch, and an [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/ImmersiveSpace] that puts the app in Full Space mode when a player activates it. The app declares the space to [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/ImmersionStyle] with [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/ImmersionStyle\/full]. The body of the `ImmersiveSpace` displays a `CompositorLayer` that the `makeCompositorLayer()` function creates, configures, and returns.\n\n## Set up the compositor layer\n\nThe `makeCompositorLayer()` function first sets up the depth and color buffers by specifying their format.\n\nThen it enables foveation if the device supports it. For more information on using foveation in your Metal apps, see [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/drawing-fully-immersive-content-using-metal].\n\n## Set up tracking areas\n\nThen `makeCompositorLayer()` checks whether the app is running in visionOS 26 or later. If it is, it enables hover effects by specifying a [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Configuration-swift.struct\/trackingAreasFormat] value of [doc:\/\/com.apple.documentation\/documentation\/Metal\/MTLPixelFormat\/r8Uint], which tells the system to enable gaze tracking and highlighting using unsigned integer values to represent different draw calls.\n\nAfter that, `makeCompositorLayer()` specifies the type of access its shaders need to the tracking-area data. Because the app offers multisample antialiasing (MSAA) as an option, it specifies different access depending on whether MSAA is available on the device and enabled in the app’s preferences. Metal handles MSAA for color buffers automatically, but not for integer buffers. When using both hover effects and MSAA, the app configures a usage of “.shaderWrite” instead of “.renderTarget” on the tracking area texture, because the texture will be the output of a custom tile resolver.\n\nRendered scenes look better with MSAA, so the app offers it as a configurable option. To work around the fact that some pixels don’t reference the correct draw call identifier, it implements a compute shader to ensure the draw call for any specific pixel is correct. To do that, the app’s shaders need `.shaderWrite` access when MSAA is enabled. If MSAA isn’t enabled, the configuration only needs write access to the render target, and uses the provided draw call identifier values.\n\n## Override maximum render quality\n\nIf the player requests a specific render quality, the function passes that value into the configuration.\n\nThen, in the trailing closure, it calls the `render(_:)` function.\n\n## Set up rendering\n\nThe `render(_:)` function creates a high-priority asynchronous task so that the rendering work doesn’t occur on the main thread. Then it creates a `RenderData` object, which is an [doc:\/\/com.apple.documentation\/documentation\/Swift\/Actor] object that holds all of the app’s render-related objects. Using an `actor` ensures that all code affecting the rendering data runs in the same global concurrency thread pool.\n\nThe function starts by setting up world tracking, loading assets, and implementing the render pipelines for its shaders. For more information about world tracking, see [doc:\/\/com.apple.documentation\/documentation\/visionOS\/tracking-points-in-world-space].\n\n## Listen for spatial events\n\nNext, the sample app checks whether it’s running in visionOS 26 or later again. If it is, it registers a closure that the renderer calls whenever that renderer generates a spatial event, such as a gesture the player makes while gazing at content with hover effects on.\n\nThe system calls the app’s closure when a tap gesture starts, as well as when it ends. The app checks specifically for the `.ended` event so that a piece only returns to its original position after the gesture is over. It gets the tracking-area identifier from the event and stores it in the render data to inform the shaders. It also begins another task that sleeps for a period of time and then unhides the tapped item.\n\n## Start the render loop\n\nThe last thing the `render(_:renderData:)` function does is call the `renderLoop()` function on the render data actor. By putting the render loop on the actor, it ensures all attempts to read and write render data happen in the same concurrent context.\n\nThe `renderLoop()` function enters a `while` loop until the system invalidates the renderer. Each time through the loop, it gets the next frame from the renderer.\n\nThen it calcuates the optimal time to wait before submitting data for the next frame, and waits for that length of time.\n\nAfter that, the app retrieves the drawables from the current frame object. When running in visionOS 26 or later, it uses ` LayerRenderer\/Frame\/queryDrawables()` to retrieve all available `LayerRenderer.Drawable` objects. In earlier visionOS versions, it retrieves a single `Drawable` and puts it in an array so the output of the calls are the same type, regardless of the OS version.\n\nNext, it iterates through the array of `LayerRenderer.Drawable` objects and passes each `Drawable`, along with its offset index, to `handleDrawable(_:_:)`, which handles processing a single drawable.\n\n## Handle draw calls and MSAA\n\nBefore processing a draw call, `handleDrawable(_:_:)` retrieves all the objects it needs, including the draw calls, device anchor, renderer pass descriptor, encoder, and viewports. It also calls the `setUpMSAA(drawable:offset:)` function to cache the textures the system needs when resolving the correct object ID for pixels that system anti-aliasing impacts in a compute shader. The function adds the color, depth, and tracking to the drawable’s texture cache because the compute shader needs access to that information.\n\n## Add a hover effect to the drawable’s tracking area\n\nThe sample app then iterates through each of the drawable’s draw calls using `handleDrawCall(encoder:drawable:drawCall:id:)`. A critical step for gaze tracking happens here. When running in visionOS 26 or later, the system creates a tracking area for the drawable and then calls `addHoverEffect()` on it, passing the raw value from the tracking area into a *uniform*, which is a constant value that the app passes to its shaders.\n\n## Return hover indices from your fragment shader\n\nIn addition to returning the fragment color, for hover effects to work, the app’s fragment shader also needs to return the object index of the fragment’s corresponding draw call. Without the object index, the system can’t perform gaze testing or highlighting.\n\nThe sample does this by declaring a Metal `struct` in `shaders.metal`. This lets the fragment shader return both the color and the object index.\n\nThen, in the fragment shader, after doing any other fragment processing the app needs to render its content, the sample returns the calculated color for the fragment, and its object index passes in from the spatial event closure.\n\n## Implement a custom resolver for MSAA support\n\nAt this point, hover effects work, however, gaze tracking isn’t taking system-provided anti-aliasing into account. Because Metal’s MSAA resolve ignores integer buffers, and MSAA needs to compute a value from all the MSAA samples, the app uses a compute shader to resolve the MSAA samples of the tracking area textures.\n\nThe system calls this compute function for every pixel. The function loops through the fragments and selects the highest object index of any of MSAA samples. The function then writes the value to the texture.",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/compositorservices\/rendering_hover_effects_in_metal_immersive_apps\ncrawled: 2025-12-03T08:02:58Z\n---\n\n# Rendering hover effects in Metal immersive apps\n\n**Sample Code**\n\nChange the appearance of a rendered onscreen element when a player gazes at it.\n\n## Overview\n\nIn visionOS, both SwiftUI views and RealityKit entities can take advantage of *hover effects*, which change the appearance of a rendered onscreen element when a player gazes at it or highlights it using assistive technologies. In visionOS 26, fully immersive apps that render their own content using Metal can also use hover effects.\n\nThis sample code project demonstrates how to pass in uniforms and attributes to your Metal shaders so your app can implement system-provided hover effects in a privacy-preserving way. On launch, the app opens to an immersive virtual space with a large shape that shatters into several pieces. If a player looks at one of the pieces, it highlights, much like a RealityKit entity with a [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/HoverEffectComponent] does. If the player taps while gazing at the various pieces, they return to their original position, reassembling the original shape.\n\n\n\n## Understand the flow\n\nTo protect privacy, Metal shaders can’t get information about where a person using an Apple Vision Pro device is currently looking. RealityKit apps can add gaze highlighting using a [doc:\/\/com.apple.documentation\/documentation\/RealityKit\/HoverEffectComponent]. For Full Space Metal apps, that’s not an option. Instead, Compositor Services provides a privacy-preserving mechanism for highlighting the virtual objects that Metal renders. The system does the gaze testing and highlight rendering, not the app.\n\nCompositor Services provides your app with an `Integer` frame buffer for passing in the indices of the draw calls your app needs the system to highlight. The system then does gaze hit-testing on those items, and when a player looks at one of them, it renders a highlight over it. It does this out of process, and your app can’t access the actual gaze data, but can respond to spatial gestures that a player generates by tapping those items.\n\n\n\n## Declare the immersive space\n\nIn `CompositorServicesHoverEffectApp`, the app declares a [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/WindowGroup] for a small SwiftUI window that shows at launch, and an [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/ImmersiveSpace] that puts the app in Full Space mode when a player activates it. The app declares the space to [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/ImmersionStyle] with [doc:\/\/com.apple.documentation\/documentation\/SwiftUI\/ImmersionStyle\/full]. The body of the `ImmersiveSpace` displays a `CompositorLayer` that the `makeCompositorLayer()` function creates, configures, and returns.\n\n```swift\nImmersiveSpace(id: AppModel.immersiveSpaceId) {\n    makeCompositorLayer()\n}\n.immersionStyle(selection: .constant(.full), in: .full)\n```\n\n## Set up the compositor layer\n\nThe `makeCompositorLayer()` function first sets up the depth and color buffers by specifying their format.\n\n```swift\nfunc makeCompositorLayer() -> CompositorLayer {\n    CompositorLayer(configuration: { capabilities, configuration in\n        \n        \/\/ Set the buffer formats for the depth and color buffer.\n        configuration.depthFormat = .depth32Float\n        configuration.colorFormat = .bgra8Unorm_srgb\n```\n\nThen it enables foveation if the device supports it. For more information on using foveation in your Metal apps, see [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/drawing-fully-immersive-content-using-metal].\n\n```swift\n        \/\/ If the device supports foveation, enable or disable it according to the user's stored preferences in the app model.\n        if capabilities.supportsFoveation {\n            configuration.isFoveationEnabled = appModel.foveation\n        }\n```\n\n## Set up tracking areas\n\nThen `makeCompositorLayer()` checks whether the app is running in visionOS 26 or later. If it is, it enables hover effects by specifying a [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Configuration-swift.struct\/trackingAreasFormat] value of [doc:\/\/com.apple.documentation\/documentation\/Metal\/MTLPixelFormat\/r8Uint], which tells the system to enable gaze tracking and highlighting using unsigned integer values to represent different draw calls.\n\n\n\n```swift\n        \/\/ Set up features requiring visionOS 26 or later.\n        if #available(visionOS 26.0, *), appModel.withHover {\n            \/\/ Enable the tracking area buffer for Metal.\n            configuration.trackingAreasFormat = .r8Uint\n```\n\nAfter that, `makeCompositorLayer()` specifies the type of access its shaders need to the tracking-area data. Because the app offers multisample antialiasing (MSAA) as an option, it specifies different access depending on whether MSAA is available on the device and enabled in the app’s preferences. Metal handles MSAA for color buffers automatically, but not for integer buffers. When using both hover effects and MSAA, the app configures a usage of “.shaderWrite” instead of “.renderTarget” on the tracking area texture, because the texture will be the output of a custom tile resolver.\n\nRendered scenes look better with MSAA, so the app offers it as a configurable option. To work around the fact that some pixels don’t reference the correct draw call identifier, it implements a compute shader to ensure the draw call for any specific pixel is correct. To do that, the app’s shaders need `.shaderWrite` access when MSAA is enabled. If MSAA isn’t enabled, the configuration only needs write access to the render target, and uses the provided draw call identifier values.\n\n```swift\n            \/\/ Specify how to use the data.\n            if appModel.withHover && appModel.useMSAA {\n                configuration.trackingAreasUsage = [.shaderWrite, .shaderRead]\n            } else {\n                configuration.trackingAreasUsage = [.renderTarget, .shaderRead]\n            }\n```\n\n## Override maximum render quality\n\nIf the player requests a specific render quality, the function passes that value into the configuration.\n\n```swift\n            \n            \/\/ Override the render-quality resolution, if requested.\n            if appModel.overrideResolution {\n                configuration.maxRenderQuality = .init(appModel.resolution)\n            }\n        }\n```\n\n\n\nThen, in the trailing closure, it calls the `render(_:)` function.\n\n```swift\n}) { renderer in\n        render(renderer)\n}\n```\n\n## Set up rendering\n\nThe `render(_:)` function creates a high-priority asynchronous task so that the rendering work doesn’t occur on the main thread. Then it creates a `RenderData` object, which is an [doc:\/\/com.apple.documentation\/documentation\/Swift\/Actor] object that holds all of the app’s render-related objects. Using an `actor` ensures that all code affecting the rendering data runs in the same global concurrency thread pool.\n\nThe function starts by setting up world tracking, loading assets, and implementing the render pipelines for its shaders. For more information about world tracking, see [doc:\/\/com.apple.documentation\/documentation\/visionOS\/tracking-points-in-world-space].\n\n```swift\nTask(priority: .high) {\n    let renderData = RenderData(renderer, theAppModel: appModel)\n    await renderData.setUpWorldTracking()\n    await renderData.loadAssets()\n    await renderData.setUpTileResolvePipeline()\n    await renderData.setUpShaderPipeline()\n```\n\n## Listen for spatial events\n\nNext, the sample app checks whether it’s running in visionOS 26 or later again. If it is, it registers a closure that the renderer calls whenever that renderer generates a spatial event, such as a gesture the player makes while gazing at content with hover effects on.\n\nThe system calls the app’s closure when a tap gesture starts, as well as when it ends. The app checks specifically for the `.ended` event so that a piece only returns to its original position after the gesture is over. It gets the tracking-area identifier from the event and stores it in the render data to inform the shaders. It also begins another task that sleeps for a period of time and then unhides the tapped item.\n\n```swift\nif #available(visionOS 26.0, *) {\n    renderer.onSpatialEvent = { events in\n        for event in events {\n            logger.log(level: .info, \"Received spatial event:\\(String(describing: event), privacy: .public)\")\n            let id = event.trackingAreaIdentifier.rawValue\n            let phase = event.phase\n            if id != 0 && phase == .ended {\n                Task(priority: .userInitiated) {\n                    await renderData.tap(on: id)\n                }\n            }\n        }\n    }\n}\n```\n\n## Start the render loop\n\nThe last thing the `render(_:renderData:)` function does is call the `renderLoop()` function on the render data actor. By putting the render loop on the actor, it ensures all attempts to read and write render data happen in the same concurrent context.\n\n```swift\n    await renderData.renderLoop()\n```\n\nThe `renderLoop()` function enters a `while` loop until the system invalidates the renderer. Each time through the loop, it gets the next frame from the renderer.\n\n```swift\n\/\/\/ Performs the rendering loop.\nfunc renderLoop() async {\n    while renderer.state != .invalidated {\n        guard let frame = renderer.queryNextFrame() else { continue }\n        frame.startUpdate()\n        frame.endUpdate()\n```\n\nThen it calcuates the optimal time to wait before submitting data for the next frame, and waits for that length of time.\n\n```swift\n        guard let timing = frame.predictTiming() else { continue }\n        LayerRenderer.Clock().wait(until: timing.optimalInputTime)\n```\n\nAfter that, the app retrieves the drawables from the current frame object. When running in visionOS 26 or later, it uses ` LayerRenderer\/Frame\/queryDrawables()` to retrieve all available `LayerRenderer.Drawable` objects. In earlier visionOS versions, it retrieves a single `Drawable` and puts it in an array so the output of the calls are the same type, regardless of the OS version.\n\n```swift\n        frame.startSubmission()\n\n        buffer = queue.makeCommandBuffer()!\n        let drawables = {\n            if #available(visionOS 26.0, *) {\n                return frame.queryDrawables()\n            } else {\n                return frame.queryDrawable().map { [$0] } ?? []\n            }\n        }()\n```\n\nNext, it iterates through the array of `LayerRenderer.Drawable` objects and passes each `Drawable`, along with its offset index, to `handleDrawable(_:_:)`, which handles processing a single drawable.\n\n```swift\n        if drawables.isEmpty { break }\n        for pair in drawables.enumerated() {\n            let drawable = pair.element\n            let offset = pair.offset\n            await handleDrawable(drawable, offset)\n        }\n\n        buffer?.commit()\n        frame.endSubmission()\n    }\n}\n```\n\n## Handle draw calls and MSAA\n\nBefore processing a draw call, `handleDrawable(_:_:)` retrieves all the objects it needs, including the draw calls, device anchor, renderer pass descriptor, encoder, and viewports. It also calls the `setUpMSAA(drawable:offset:)` function to cache the textures the system needs when resolving the correct object ID for pixels that system anti-aliasing impacts in a compute shader. The function adds the color, depth, and tracking to the drawable’s texture cache because the compute shader needs access to that information.\n\n```swift\nfunc setUpMSAA(drawable: LayerRenderer.Drawable,\n               offset: Int) async {\n    \n    if appModel.useMSAA {\n        if colorTextureCache.perDrawable[offset] == nil {\n            colorTextureCache.perDrawable[offset] = memorylessTexture(from: drawable.colorTextures[0])\n        }\n        \n        if #available(visionOS 26.0, *), appModel.withHover {\n            if indexTextureCache.perDrawable[offset] == nil {\n                indexTextureCache.perDrawable[offset] = memorylessTexture(from: drawable.trackingAreasTextures[0])\n            }\n        }\n        \n        if depthTextureCache.perDrawable[offset] == nil {\n            depthTextureCache.perDrawable[offset] = memorylessTexture(from: drawable.depthTextures[0])\n        }\n    }\n```\n\n## Add a hover effect to the drawable’s tracking area\n\nThe sample app then iterates through each of the drawable’s draw calls using `handleDrawCall(encoder:drawable:drawCall:id:)`. A critical step for gaze tracking happens here. When running in visionOS 26 or later, the system creates a tracking area for the drawable and then calls `addHoverEffect()` on it, passing the raw value from the tracking area into a *uniform*, which is a constant value that the app passes to its shaders.\n\n```swift\nif #available(visionOS 26.0, *), appModel.withHover, drawCall.hasHover {\n    let trackingArea = drawable.addTrackingArea(identifier: .init(UInt64(id)))\n    trackingArea.addHoverEffect(.automatic)\n    uniforms.hoverIndex = trackingArea.renderValue.rawValue\n}\n```\n\n## Return hover indices from your fragment shader\n\nIn addition to returning the fragment color, for hover effects to work, the app’s fragment shader also needs to return the object index of the fragment’s corresponding draw call. Without the object index, the system can’t perform gaze testing or highlighting.\n\nThe sample does this by declaring a Metal `struct` in `shaders.metal`. This lets the fragment shader return both the color and the object index.\n\n```swift\nstruct FragmentOut {\n    float4 color [[color(0)]];\n    uint16_t index [[color(1)]];\n};\n```\n\nThen, in the fragment shader, after doing any other fragment processing the app needs to render its content, the sample returns the calculated color for the fragment, and its object index passes in from the spatial event closure.\n\n```swift\nreturn FragmentOut {\n    float4(outColor, 1.0),\n    uniformsArray.hoverIndex\n};\n```\n\n## Implement a custom resolver for MSAA support\n\nAt this point, hover effects work, however, gaze tracking isn’t taking system-provided anti-aliasing into account. Because Metal’s MSAA resolve ignores integer buffers, and MSAA needs to compute a value from all the MSAA samples, the app uses a compute shader to resolve the MSAA samples of the tracking area textures.\n\n```swift\nkernel void block_resolve(imageblock<FragmentOut> block,\n                          ushort2 tid [[thread_position_in_threadgroup]],\n                          uint2 gid [[thread_position_in_grid]],\n                          ushort array_index [[render_target_array_index]],\n                          texture2d_array<uint16_t, access::write> resolvedTextureArray [[texture(0), function_constant(use_texture_array)]],\n                          texture2d<uint16_t, access::write> resolvedTexture [[texture(0), function_constant(not_texture_array)]])\n{\n\n    const ushort pixelCount = block.get_num_colors(tid);\n    ushort index = 0;\n\n    for (ushort i = 0; i < pixelCount; ++i) {\n        const FragmentOut color = block.read(tid, i, imageblock_data_rate::color);\n        index = max(index, color.index);\n    }\n\n    if (use_texture_array) {\n        resolvedTextureArray.write(index, gid, array_index);\n    } else {\n        resolvedTexture.write(index, gid);\n    }\n}\n```\n\nThe system calls this compute function for every pixel. The function loops through the fragments and selects the highest object index of any of MSAA samples. The function then writes the value to the texture.\n\n## App integration\n\n- **Drawing fully immersive content using Metal**: Create a fully immersive experience in visionOS using a custom Metal-based rendering engine.\n- **Interacting with virtual content blended with passthrough**: Present a mixed immersion style space to draw content in a person’s surroundings, and choose how upper limbs appear with respect to rendered content.\n- **CompositorLayer**: A type that you use with an immersive space to display fully immersive content using Metal.\n- **CompositorLayerConfiguration**: An interface for specifying the texture configurations and rendering behaviors to use with your Metal rendering engine.\n- **DefaultCompositorLayerConfiguration**: A type that configures the layer with the default texture configurations and rendering behaviors for the current device.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Create a fully immersive experience in visionOS using a custom Metal-based rendering engine.",
          "name" : "Drawing fully immersive content using Metal",
          "url" : "https:\/\/developer.apple.com\/documentation\/CompositorServices\/drawing-fully-immersive-content-using-metal"
        },
        {
          "description" : "Present a mixed immersion style space to draw content in a person’s surroundings, and choose how upper limbs appear with respect to rendered content.",
          "name" : "Interacting with virtual content blended with passthrough",
          "url" : "https:\/\/developer.apple.com\/documentation\/CompositorServices\/interacting-with-virtual-content-blended-with-passthrough"
        },
        {
          "description" : "A type that you use with an immersive space to display fully immersive content using Metal.",
          "name" : "CompositorLayer",
          "url" : "https:\/\/developer.apple.com\/documentation\/CompositorServices\/CompositorLayer"
        },
        {
          "description" : "An interface for specifying the texture configurations and rendering behaviors to use with your Metal rendering engine.",
          "name" : "CompositorLayerConfiguration",
          "url" : "https:\/\/developer.apple.com\/documentation\/CompositorServices\/CompositorLayerConfiguration"
        },
        {
          "description" : "A type that configures the layer with the default texture configurations and rendering behaviors for the current device.",
          "name" : "DefaultCompositorLayerConfiguration",
          "url" : "https:\/\/developer.apple.com\/documentation\/CompositorServices\/DefaultCompositorLayerConfiguration"
        }
      ],
      "title" : "App integration"
    }
  ],
  "source" : "appleJSON",
  "title" : "Rendering hover effects in Metal immersive apps",
  "url" : "https:\/\/developer.apple.com\/documentation\/compositorservices\/rendering_hover_effects_in_metal_immersive_apps"
}