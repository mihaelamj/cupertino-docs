{
  "abstract" : "Create a fully immersive experience in visionOS using a custom Metal-based rendering engine.",
  "codeExamples" : [
    {
      "code" : "@main\nstruct MyApp: App {\n    var body: some Scene {\n        \n        \/\/ Display a fully immersive scene that uses Metal for drawing.\n        ImmersiveSpace(id: \"MyContent\") {\n            CompositorLayer { layerRenderer in\n                \/\/ Set up and run the Metal render loop.\n                let renderThread = Thread {\n                    let engine = myEngineCreate(layerRenderer)\n                    myEngineRenderLoop(engine)\n                }\n                renderThread.name = \"Render Thread\"\n                renderThread.start()\n            }\n        }\n\n        \/\/ Display a 2D window.\n        WindowGroup {\n            ContentView()\n        }\n    }\n}\n",
      "language" : "swift"
    },
    {
      "code" : "struct MyContentConfiguration: CompositorLayerConfiguration {\n    func makeConfiguration(\n        capabilities: LayerRenderer.Capabilities, \n        configuration: inout LayerRenderer.Configuration\n    ) {\n        let supportsFoveation = layerCapabilites.supportsFoveation\n        let supportedLayouts = supportedLayouts(options: supportsFoveation ?\n                                                [.foveationEnabled] : [])\n\n        configuration.layout = supportedLayouts.contains(.layered) ? .layered : .dedicated\n        configuration.isFoveationEnabled = supportsFoveation\n\n        \/\/ HDR support\n        configuration.colorFormat = .rgba16Float\n\n    }\n",
      "language" : "swift"
    },
    {
      "code" : "@main\nstruct MyApp: App {\n    var body: some Scene {\n        \n        ImmersiveSpace(id: \"MyContent\") {\n            CompositorLayer (configuration: MyContentConfiguration()) { layerRenderer in\n                \/\/ Set up and run the Metal render loop.\n                let renderThread = Thread {\n                    let engine = myEngineCreate(layerRenderer)\n                    myEngineRenderLoop(engine)\n                }\n                renderThread.name = \"Render Thread\"\n                renderThread.start()\n            }\n        }\n        \/\/ Other scenes...\n    }\n}\n",
      "language" : "swift"
    },
    {
      "code" : "void myEngineRenderLoop(my_engine *engine) {\n    my_engine_setup_render_pipeline(engine);\n\n    bool is_rendering = true;\n    while (is_rendering) @autoreleasepool {\n        switch (cp_layer_renderer_get_state(engine->layer_renderer)) {\n            case cp_layer_renderer_state_paused:\n                \/\/ Wait until the scene appears.\n                cp_layer_renderer_wait_until_running(engine->layer_renderer);\n                break;\n\n            case cp_layer_renderer_state_running:\n                \/\/ Render the next frame.\n                my_engine_render_new_frame(engine);\n                break;\n\n            case cp_layer_renderer_state_invalidated:\n                \/\/ Exit the render loop.\n                is_rendering = false;\n                break;\n        }\n    }\n\n    my_engine_invalidate(engine);\n}",
      "language" : "objc"
    },
    {
      "code" : "void my_engine_render_new_frame(my_engine *engine) {\n    \/\/ Get the next frame.\n    cp_frame_t frame = cp_layer_renderer_query_next_frame(engine->layer_renderer);\n    if (frame == nullptr) { return; }\n    \n    \/\/ Fetch the predicted timing information.\n    cp_frame_timing_t timing = cp_frame_predict_timing(frame);\n    if (timing == nullptr) { return; }\n\n    \/\/ Update the frame...\n    cp_frame_start_update(frame);\n\n    \/\/ Update any position- or orientation-independent information.\n    my_input_state input_state = my_engine_gather_inputs(engine, timing);\n    my_engine_update_frame(engine, timing, input_state);\n\n    cp_frame_end_update(frame);\n\n    \/\/ Wait until the optimal time for querying the input.\n    cp_time_wait_until(cp_frame_timing_get_optimal_input_time(timing));\n\n    \/\/ Submit the frame...\n    cp_frame_start_submission(frame);\n    cp_drawable_t drawable = cp_frame_query_drawable(frame);\n    if (drawable == nullptr) { return; }\n\n    cp_frame_timing_t timing = cp_drawable_get_frame_timing(frame);\n    ar_device_anchor_t anchor = my_engine_get_ar_device_anchor(engine, timing);\n    cp_drawable_set_ar_device_anchor(drawable, anchor);\n\n    my_engine_draw_and_submit_frame(engine, frame, drawable);\n\n    cp_frame_end_submission(frame);\n }",
      "language" : "objc"
    },
    {
      "code" : "MTLRenderPassDescriptor* my_renderer_create_render_descriptor(my_renderer *renderer,\n                                                              cp_drawable_t *drawable) {    \n    MTLRenderPassDescriptor *pass_descriptor = [[MTLRenderPassDescriptor alloc] init];\n\n    pass_descriptor.colorAttachments[0].texture = cp_drawable_get_color_texture(drawable, 0);\n    pass_descriptor.colorAttachments[0].storeAction = MTLStoreActionStore;\n    \n    pass_descriptor.depthAttachment.texture = cp_drawable_get_depth_texture(drawable, 0);\n    pass_descriptor.depthAttachment.storeAction = MTLStoreActionStore;\n\n    pass_descriptor.renderTargetArrayLength = cp_drawable_get_view_count(drawable);\n\n    \/\/ Foveation support\n    pass_descriptor.rasterizationRateMap = cp_drawable_get_rasterization_rate_map(drawable, 0);\n\n    return pass_descriptor;\n}",
      "language" : "objc"
    },
    {
      "code" : "ar_device_anchor_t my_engine_get_ar_device_anchor(my_engine *engine, cp_frame_timing_t timing) {   \n    ar_device_anchor_t anchor = ar_device_anchor_create();\n    ar_world_tracking_provider_t provider = engine->my_world_tracking_provider;\n\n    \/\/ Fetch the device anchor from ARKit.\n    auto p_time = cp_time_to_cf_time_interval(cp_frame_timing_get_presentation_time(timing));\n    auto anchor_status = ar_world_tracking_provider_query_device_anchor_at_timestamp(provider, p_time, &anchor);\n    if (anchor_status == ar_device_anchor_query_status_success) {\n        return anchor;\n    }\n    return nil;\n}",
      "language" : "objc"
    },
    {
      "code" : "typedef struct{\n    simd_float4x4 projectionMatrix;\n    simd_float4x4 viewMatrix;\n} Uniforms;\n\nstatic const NSUInteger MaxBuffersInFlight = 3;\n@implementation Renderer (UniformsExtension)\n   id <MTLBuffer> _uniformBufferAddress[MaxBuffersInFlight];\n\n- (void)updateUniformsForRenderer:(Renderer*)renderer\n                     withDrawable:(cp_drawable_t)drawable\n                          atIndex:(size_t)index {\n\n    Uniforms *uniforms = (Uniforms*)_uniformBufferAddress;\n\n    \/\/ Get the current device anchor value.\n    ar_device_anchor_t device_anchor = cp_drawable_get_device_anchor(drawable);\n    simd_float4x4 head_position = ar_anchor_get_origin_from_anchor_transform(device_anchor);\n\n    cp_view_t view = cp_drawable_get_view(drawable, index);\n    simd_float4 tangents = cp_view_get_tangents(view);\n    simd_float2 depth_range = cp_drawable_get_depth_range(drawable);\n    simd_float4x4 transform = makeProjectiveTransformFromTangents(tangents[0], \/* left *\/\n                                                                  tangents[1], \/* right *\/\n                                                                  tangents[2], \/* top *\/\n                                                                  tangents[3], \/* bottom *\/\n                                                                  depth_range[1], \/* nearZ *\/\n                                                                  depth_range[0], \/* farZ *\/\n                                                                  true); \/* reverseZ *\/\n    uniforms[index].projectionMatrix = transform;\n\n    \/\/ Adjust the camera transform for the current eye position.\n    simd_float4x4 camera_transform = simd_mul(head_position, cp_view_get_transform(view));\n    uniforms[index].viewMatrix = simd_inverse(camera_transform);\n}\n\n@end\n",
      "language" : "objc"
    },
    {
      "code" : "@main\nstruct MyApp: App {\n    var body: some Scene {\n        \n        \/\/ Create a fully immersive scene.\n        ImmersiveSpace(id: \"MyContent\") {\n            CompositorLayer (configuration: MyContentConfiguration()) { layerRenderer in\n                \/\/ Set up and run the Metal render loop.\n                let renderThread = Thread {\n                    let engine = myEngineCreate(layerRenderer)\n                    myEngineRenderLoop(engine)\n                }\n                renderThread.name = \"Render Thread\"\n                renderThread.start()\n\n                \/\/ Handle any events in the scene.\n                layerRenderer.onSpatialEvent = { eventCollection in\n                    var events = eventCollection.map { mySpatialEvent($0) }\n                    myEnginePushSpatialEvents(engine, &events, events.count)\n                }\n            }\n        }\n         \/\/ Other scenes...\n    }\n}\n",
      "language" : "swift"
    }
  ],
  "contentHash" : "9feb7d5d1b47c91d10a95786dc1cb030b8a0b25e104abc1a6d3da1f80b69d510",
  "crawledAt" : "2025-12-03T08:02:56Z",
  "id" : "6059AD6C-C8F7-4FDF-BD97-777001021C31",
  "kind" : "article",
  "language" : "swift",
  "module" : "Compositor Services",
  "overview" : "## Overview\n\nIf your app draws fully immersive content using Metal, Compositor Services provides a bridge between your SwiftUI code and your Metal rendering engine code. Use this framework to present a fully immersive scene that supports Metal rendering. When you present the scene from your app, Compositor Services provides a layer, which contains the Metal types, textures, and other information you need. The layer also provides timing information to help you manage your app’s rendering loop and deliver frames of content in a timely manner.\n\nWhen creating fully immersive content using Metal, you draw everything the person sees. The result of your drawing efforts is two images, one for each eye, to create a stereosopic effect when viewed on Apple Vision Pro. Use the timing information in the layer to render up to 90 frames a second using a custom rendering loop.\n\nFor information about how to draw content with Metal, see [doc:\/\/com.apple.documentation\/documentation\/Metal].\n\n### Add an immersive space for your content\n\nTo present your fully immersive experience, configure your app with an `ImmersiveSpace` scene that gets its content from a [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/CompositorLayer] type. This type provides the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer] type you need to set up and run your app’s custom rendering loop. The following example shows how to set up the space and your app’s content. In the closure for the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/CompositorLayer] type, create a new thread to configure and start your app’s render loop.\n\nDon’t include any style modifiers on a space that contains a [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/CompositorLayer] type. The system automatically configures a space with [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/CompositorLayer] content as fully immersive, and ignores any style modifiers.\n\nTypically, apps don’t display an immersive space immediately at launch. Transitioning to a fully immersive experience can be jarring if someone isn’t ready for it, so it’s preferable to display a window first and let someone enter the space when they’re ready. However, if you need to display a space first, add the `UIApplicationPreferredDefaultSceneSessionRole` key to the [doc:\/\/com.apple.documentation\/documentation\/BundleResources\/Information-Property-List\/UIApplicationSceneManifest] in your app’s `Info.plist` file. Set the value of this key to `CPSceneSessionRoleImmersiveSpaceApplication`. When this key is present, the system displays the first space it finds in your app’s list of scenes.\n\n### Customize the configuration of your layer\n\nIf your Metal rendering engine requires specific texture layouts, pixel formats, or rendering options, specify those details when you configure your [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/CompositorLayer] type. In your scene creation code, pass a type that adopts [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/CompositorLayerConfiguration] as a parameter to your scene content. The system uses that information to configure the Metal textures your [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer] provides. If you don’t provide a custom configuration, Compositor Services uses a set of default configuration values.\n\nTo specify a set of custom options, define a type that adopts the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/CompositorLayerConfiguration] protocol and implement its [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/CompositorLayerConfiguration\/makeConfiguration(capabilities:configuration:)] method. In your implementation of that method, update the default values in the `configuration` parameter with your preferred choices. Change only the values you want and leave the other values alone. Configuration options that are available on a device might not be available in Simulator, so use the `capabilities` parameter to validate your choices before making them. The following example changes the pixel format to one that supports HDR values, and configures the texture layout based on the current foveation setting:\n\nTo use your configuration options for rendering, pass your custom [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/CompositorLayerConfiguration] type to your [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/CompositorLayer] at initialization time. The following example modifies the previous scene’s setup code to include custom configuration data. Compositor Services integrates your configuration details into the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer] type it creates.\n\n### Configure your app’s rendering loop\n\nWhen your app displays a space with a [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/CompositorLayer], the system runs the code you provide. Use that code to configure your Metal rendering engine and spawn a thread for your rendering loop, but don’t start rendering your content immediately. Instead, check the state of the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer] type to see if the scene is actually running. The system might leave a scene in the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/State-swift.enum\/paused] state while it confirms the person wants to enter the fully immersive experience. The system changes the state to [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/State-swift.enum\/running] only when it’s ready to display your scene’s content.\n\nThe following example shows the logic you might use to check the state of your loop each time through your rendering loop. While the layer is paused, the code pauses the render loop thread and waits until the layer starts running again. When the system or a person dismisses the scene, the layer moves to the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/State-swift.enum\/invalidated] state to let you know it’s time to stop your rendering loop.\n\nCreating Metal pipeline state information is potentially expensive, so use the setup phase of your render loop to configure as much of your Metal code as possible. Start loading textures and shader code, and set up the render and compute descriptors you need for your content. You can also use your setup code to configure the ARKit code you need to fetch device anchor information.\n\nUntil your scene is visible, you can’t fetch new frames from the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer] and use them to configure your rendering code. If you need information about the configuration of textures, create a [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Properties-swift.struct] type using the same [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/CompositorLayerConfiguration] information you used to configure your scene. The [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Properties-swift.struct] type contains the number of views to draw and the organization of textures for each frame.\n\n### Update and encode a single frame of content\n\nWhile your layer is in the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/State-swift.enum\/running] state, fetch a new frame and fill it with content each time through your render loop. The layer manages a finite number of frames, so render only one frame at a time and submit it. Compositor Services provides timing information with each frame to help you start work on the frame at the appropriate time and submit your changes before the system needs them.\n\nThe following sequence shows the steps to create a single frame of content. Perform these steps each time through your app’s render loop.\n\nThe system uses data from the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Frame\/startUpdate()], [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Frame\/endUpdate()], [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Frame\/startSubmission()], and [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Frame\/endSubmission()] functions to improve the timing information for subsequent frames. Call these functions to ensure your app has accurate timing information, and to help the system manage CPU and GPU resources efficiently.\n\nThe following example shows the structure of the drawing code for rendering one frame of content. The custom `my_engine_gather_inputs`, `my_engine_update_frame`, and `my_engine_draw_and_submit_frame` functions perform custom tasks the app needs to update its data structures and render the content of the frame. The code also fetches the current device anchor from ARKit using the custom `my_engine_get_ar_device_anchor` function and associates that information with the frame.\n\nFor information about how to set up Metal command buffers and command encoders, see [doc:\/\/com.apple.documentation\/documentation\/Metal\/setting-up-a-command-structure].\n\n### Configure the render pass descriptor for the frame\n\nDuring drawing, add the textures from your frame’s [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Drawable] to your render pass descriptor. The render pass descriptor tells Metal where to deliver the output of your rendering commands. Because each frame of content relies on different textures, you must create and configure a render pass descriptor with the current frame’s textures each time through your render loop.\n\nThe following example shows a function that creates a new render pass descriptor and configures its texture information. The [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Drawable] in the example uses the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Layout\/layered] layout, which uses a single texture of type [doc:\/\/com.apple.documentation\/documentation\/Metal\/MTLTextureType\/type2DArray]. You could use similar code to set up the render pass descriptor for the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Layout\/shared] layout.\n\nFor a [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Layout\/dedicated] layout, you must perform two render passes on your content and create a separate render pass descriptor for each one. Configure each render pass descriptor with the texture at a different index in the arrays of the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Drawable] type.\n\n### Retrieve device anchor information and attach it to the frame\n\nTo prevent the person viewing your content from experiencing disorientation or physical discomfort, it’s essential to match the position of the camera in your scene to the location of the person’s head. Matching the person’s head movements ensures that what they see doesn’t conflict with the input their body receives from the real world.\n\nBecause you render your app’s content in advance, you also need to know the position and orientation of the device in advance. To retrieve this information, use ARKit to call [doc:\/\/com.apple.documentation\/documentation\/ARKit\/ar_world_tracking_provider_query_device_anchor_at_timestamp] during the encoding process for your frame. ARKit provides this function to deliver the expected device anchor at the time you specify. Use this information to configure any camera positions during rendering.\n\nThe following example shows how to retrieve the predicted device anchor using ARKit. Use the timing information from the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Drawable] to get the most accurate presentation time for the frame. Return the device anchor upon success or return `nil` if an error occurs.\n\nWhen it displays your frame, the system checks for a discrepancy between the predicted device anchor you provided for your frame and the actual device anchor the hardware reports. If there’s a difference, the system automatically adjusts the rendered frame to compensate for the movement. If you don’t want the system to make this adjustment, don’t specify a device anchor using the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/cp_drawable_set_device_anchor] function.\n\nFor more information about how to track the device anchor, see [doc:\/\/com.apple.documentation\/documentation\/ARKit].\n\n### Render each view with the correct perspective\n\nThe goal of your Metal rendering engine is to produce 2D textures to display to the viewer. When your content is 3D, you need to map points in your scene to the 2D texture in a way that makes the content look realistically 3D to someone viewing it. This process requires you to create a projection matrix that maps points in your 3D content to points on the texture for each view. For stereoscopic rendering, you also have to account for the positional differences between the device anchor and the position of the person’s eyes.\n\nDuring rendering, the rendering engine calls the method in the following example once for each view in the frame. It uses the device anchor it assigned to the frame earlier to create a transform to compensate for any differences between the device position and the view’s position. It also creates a projection matrix using the view’s tangents information and the distances to the near and far projection planes. The `makeProjectiveTransformFromTangents` function assembles the actual matrix values in the same way as [doc:\/\/com.apple.documentation\/documentation\/Spatial\/ProjectiveTransform3D\/init(leftTangent:rightTangent:topTangent:bottomTangent:nearZ:farZ:reverseZ:)].\n\n### Respond to interactions with your custom content\n\nWhen your scene is visible, you’re responsible for managing all interactions with your custom content. Because you render everything yourself using Metal, you can’t rely on view-based events or gesture recognizers for input. Instead, use one of the following techniques:\n\nWhen the system detects any direct or indirect touch events, it reports them to the `.onSpatialEvent` callback of the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer]. Use this callback to handle any interactions with your custom content. The system executes your callback on the main thread each time a new touch occurs or an active touch changes, so keep your callback code brief. The following example shows how to add this callback to your layer:\n\nFor information about ARKit hand tracking, see [doc:\/\/com.apple.documentation\/documentation\/ARKit].",
  "rawMarkdown" : "---\nsource: https:\/\/developer.apple.com\/documentation\/compositorservices\/drawing-fully-immersive-content-using-metal\ncrawled: 2025-12-03T08:02:56Z\n---\n\n# Drawing fully immersive content using Metal\n\n**Article**\n\nCreate a fully immersive experience in visionOS using a custom Metal-based rendering engine.\n\n## Overview\n\nIf your app draws fully immersive content using Metal, Compositor Services provides a bridge between your SwiftUI code and your Metal rendering engine code. Use this framework to present a fully immersive scene that supports Metal rendering. When you present the scene from your app, Compositor Services provides a layer, which contains the Metal types, textures, and other information you need. The layer also provides timing information to help you manage your app’s rendering loop and deliver frames of content in a timely manner.\n\n\n\nWhen creating fully immersive content using Metal, you draw everything the person sees. The result of your drawing efforts is two images, one for each eye, to create a stereosopic effect when viewed on Apple Vision Pro. Use the timing information in the layer to render up to 90 frames a second using a custom rendering loop.\n\nFor information about how to draw content with Metal, see [doc:\/\/com.apple.documentation\/documentation\/Metal].\n\n### Add an immersive space for your content\n\nTo present your fully immersive experience, configure your app with an `ImmersiveSpace` scene that gets its content from a [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/CompositorLayer] type. This type provides the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer] type you need to set up and run your app’s custom rendering loop. The following example shows how to set up the space and your app’s content. In the closure for the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/CompositorLayer] type, create a new thread to configure and start your app’s render loop.\n\n```swift\n@main\nstruct MyApp: App {\n    var body: some Scene {\n        \n        \/\/ Display a fully immersive scene that uses Metal for drawing.\n        ImmersiveSpace(id: \"MyContent\") {\n            CompositorLayer { layerRenderer in\n                \/\/ Set up and run the Metal render loop.\n                let renderThread = Thread {\n                    let engine = myEngineCreate(layerRenderer)\n                    myEngineRenderLoop(engine)\n                }\n                renderThread.name = \"Render Thread\"\n                renderThread.start()\n            }\n        }\n\n        \/\/ Display a 2D window.\n        WindowGroup {\n            ContentView()\n        }\n    }\n}\n\n```\n\nDon’t include any style modifiers on a space that contains a [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/CompositorLayer] type. The system automatically configures a space with [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/CompositorLayer] content as fully immersive, and ignores any style modifiers.\n\nTypically, apps don’t display an immersive space immediately at launch. Transitioning to a fully immersive experience can be jarring if someone isn’t ready for it, so it’s preferable to display a window first and let someone enter the space when they’re ready. However, if you need to display a space first, add the `UIApplicationPreferredDefaultSceneSessionRole` key to the [doc:\/\/com.apple.documentation\/documentation\/BundleResources\/Information-Property-List\/UIApplicationSceneManifest] in your app’s `Info.plist` file. Set the value of this key to `CPSceneSessionRoleImmersiveSpaceApplication`. When this key is present, the system displays the first space it finds in your app’s list of scenes.\n\n### Customize the configuration of your layer\n\nIf your Metal rendering engine requires specific texture layouts, pixel formats, or rendering options, specify those details when you configure your [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/CompositorLayer] type. In your scene creation code, pass a type that adopts [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/CompositorLayerConfiguration] as a parameter to your scene content. The system uses that information to configure the Metal textures your [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer] provides. If you don’t provide a custom configuration, Compositor Services uses a set of default configuration values.\n\nTo specify a set of custom options, define a type that adopts the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/CompositorLayerConfiguration] protocol and implement its [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/CompositorLayerConfiguration\/makeConfiguration(capabilities:configuration:)] method. In your implementation of that method, update the default values in the `configuration` parameter with your preferred choices. Change only the values you want and leave the other values alone. Configuration options that are available on a device might not be available in Simulator, so use the `capabilities` parameter to validate your choices before making them. The following example changes the pixel format to one that supports HDR values, and configures the texture layout based on the current foveation setting:\n\n```swift\nstruct MyContentConfiguration: CompositorLayerConfiguration {\n    func makeConfiguration(\n        capabilities: LayerRenderer.Capabilities, \n        configuration: inout LayerRenderer.Configuration\n    ) {\n        let supportsFoveation = layerCapabilites.supportsFoveation\n        let supportedLayouts = supportedLayouts(options: supportsFoveation ?\n                                                [.foveationEnabled] : [])\n\n        configuration.layout = supportedLayouts.contains(.layered) ? .layered : .dedicated\n        configuration.isFoveationEnabled = supportsFoveation\n\n        \/\/ HDR support\n        configuration.colorFormat = .rgba16Float\n\n    }\n\n```\n\n\n\nTo use your configuration options for rendering, pass your custom [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/CompositorLayerConfiguration] type to your [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/CompositorLayer] at initialization time. The following example modifies the previous scene’s setup code to include custom configuration data. Compositor Services integrates your configuration details into the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer] type it creates.\n\n```swift\n@main\nstruct MyApp: App {\n    var body: some Scene {\n        \n        ImmersiveSpace(id: \"MyContent\") {\n            CompositorLayer (configuration: MyContentConfiguration()) { layerRenderer in\n                \/\/ Set up and run the Metal render loop.\n                let renderThread = Thread {\n                    let engine = myEngineCreate(layerRenderer)\n                    myEngineRenderLoop(engine)\n                }\n                renderThread.name = \"Render Thread\"\n                renderThread.start()\n            }\n        }\n        \/\/ Other scenes...\n    }\n}\n\n```\n\n### Configure your app’s rendering loop\n\nWhen your app displays a space with a [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/CompositorLayer], the system runs the code you provide. Use that code to configure your Metal rendering engine and spawn a thread for your rendering loop, but don’t start rendering your content immediately. Instead, check the state of the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer] type to see if the scene is actually running. The system might leave a scene in the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/State-swift.enum\/paused] state while it confirms the person wants to enter the fully immersive experience. The system changes the state to [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/State-swift.enum\/running] only when it’s ready to display your scene’s content.\n\nThe following example shows the logic you might use to check the state of your loop each time through your rendering loop. While the layer is paused, the code pauses the render loop thread and waits until the layer starts running again. When the system or a person dismisses the scene, the layer moves to the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/State-swift.enum\/invalidated] state to let you know it’s time to stop your rendering loop.\n\n```objc\nvoid myEngineRenderLoop(my_engine *engine) {\n    my_engine_setup_render_pipeline(engine);\n\n    bool is_rendering = true;\n    while (is_rendering) @autoreleasepool {\n        switch (cp_layer_renderer_get_state(engine->layer_renderer)) {\n            case cp_layer_renderer_state_paused:\n                \/\/ Wait until the scene appears.\n                cp_layer_renderer_wait_until_running(engine->layer_renderer);\n                break;\n\n            case cp_layer_renderer_state_running:\n                \/\/ Render the next frame.\n                my_engine_render_new_frame(engine);\n                break;\n\n            case cp_layer_renderer_state_invalidated:\n                \/\/ Exit the render loop.\n                is_rendering = false;\n                break;\n        }\n    }\n\n    my_engine_invalidate(engine);\n}\n```\n\n\n\nCreating Metal pipeline state information is potentially expensive, so use the setup phase of your render loop to configure as much of your Metal code as possible. Start loading textures and shader code, and set up the render and compute descriptors you need for your content. You can also use your setup code to configure the ARKit code you need to fetch device anchor information.\n\nUntil your scene is visible, you can’t fetch new frames from the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer] and use them to configure your rendering code. If you need information about the configuration of textures, create a [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Properties-swift.struct] type using the same [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/CompositorLayerConfiguration] information you used to configure your scene. The [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Properties-swift.struct] type contains the number of views to draw and the organization of textures for each frame.\n\n### Update and encode a single frame of content\n\nWhile your layer is in the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/State-swift.enum\/running] state, fetch a new frame and fill it with content each time through your render loop. The layer manages a finite number of frames, so render only one frame at a time and submit it. Compositor Services provides timing information with each frame to help you start work on the frame at the appropriate time and submit your changes before the system needs them.\n\nThe following sequence shows the steps to create a single frame of content. Perform these steps each time through your app’s render loop.\n\n1. Call [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/queryNextFrame()] to fetch the next frame to use for drawing.\n2. Call [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Frame\/predictTiming()] (or [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/cp_frame_predict_timing]) to get the predicted render deadlines for your code. You use this information later to pause your thread until the optimal rendering time.\n3. Call [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Frame\/startUpdate()] to mark the start of the update phase.\n4. Apply user interactions to your content and update any app-specific data.\n5. Perform any rendering-related work that doesn’t rely on the device anchor information.\n6. Call [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Frame\/endUpdate()] to mark the end of the update phase.\n7. Call [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Clock\/wait(until:tolerance:)] (or [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/cp_time_wait_until]) to pause your render loop until the optimal rendering time.\n8. Call [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Frame\/startSubmission()] to mark the start of submission phase.\n9. Fetch the predicted device anchor from ARKit using the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Drawable\/frameTiming] information, and apply that anchor to your frame.\n10. Encode any drawing commands that depend on the device position or orientation.\n11. Call [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Drawable\/encodePresent(commandBuffer:)] to encode a presentation event into your command buffer.\n12. Commit your command buffer.\n13. Call [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Frame\/endSubmission()] to mark the end of your GPU submission.\n\nThe system uses data from the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Frame\/startUpdate()], [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Frame\/endUpdate()], [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Frame\/startSubmission()], and [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Frame\/endSubmission()] functions to improve the timing information for subsequent frames. Call these functions to ensure your app has accurate timing information, and to help the system manage CPU and GPU resources efficiently.\n\nThe following example shows the structure of the drawing code for rendering one frame of content. The custom `my_engine_gather_inputs`, `my_engine_update_frame`, and `my_engine_draw_and_submit_frame` functions perform custom tasks the app needs to update its data structures and render the content of the frame. The code also fetches the current device anchor from ARKit using the custom `my_engine_get_ar_device_anchor` function and associates that information with the frame.\n\n```objc\nvoid my_engine_render_new_frame(my_engine *engine) {\n    \/\/ Get the next frame.\n    cp_frame_t frame = cp_layer_renderer_query_next_frame(engine->layer_renderer);\n    if (frame == nullptr) { return; }\n    \n    \/\/ Fetch the predicted timing information.\n    cp_frame_timing_t timing = cp_frame_predict_timing(frame);\n    if (timing == nullptr) { return; }\n\n    \/\/ Update the frame...\n    cp_frame_start_update(frame);\n\n    \/\/ Update any position- or orientation-independent information.\n    my_input_state input_state = my_engine_gather_inputs(engine, timing);\n    my_engine_update_frame(engine, timing, input_state);\n\n    cp_frame_end_update(frame);\n\n    \/\/ Wait until the optimal time for querying the input.\n    cp_time_wait_until(cp_frame_timing_get_optimal_input_time(timing));\n\n    \/\/ Submit the frame...\n    cp_frame_start_submission(frame);\n    cp_drawable_t drawable = cp_frame_query_drawable(frame);\n    if (drawable == nullptr) { return; }\n\n    cp_frame_timing_t timing = cp_drawable_get_frame_timing(frame);\n    ar_device_anchor_t anchor = my_engine_get_ar_device_anchor(engine, timing);\n    cp_drawable_set_ar_device_anchor(drawable, anchor);\n\n    my_engine_draw_and_submit_frame(engine, frame, drawable);\n\n    cp_frame_end_submission(frame);\n }\n```\n\nFor information about how to set up Metal command buffers and command encoders, see [doc:\/\/com.apple.documentation\/documentation\/Metal\/setting-up-a-command-structure].\n\n### Configure the render pass descriptor for the frame\n\nDuring drawing, add the textures from your frame’s [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Drawable] to your render pass descriptor. The render pass descriptor tells Metal where to deliver the output of your rendering commands. Because each frame of content relies on different textures, you must create and configure a render pass descriptor with the current frame’s textures each time through your render loop.\n\nThe following example shows a function that creates a new render pass descriptor and configures its texture information. The [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Drawable] in the example uses the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Layout\/layered] layout, which uses a single texture of type [doc:\/\/com.apple.documentation\/documentation\/Metal\/MTLTextureType\/type2DArray]. You could use similar code to set up the render pass descriptor for the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Layout\/shared] layout.\n\n```objc\nMTLRenderPassDescriptor* my_renderer_create_render_descriptor(my_renderer *renderer,\n                                                              cp_drawable_t *drawable) {    \n    MTLRenderPassDescriptor *pass_descriptor = [[MTLRenderPassDescriptor alloc] init];\n\n    pass_descriptor.colorAttachments[0].texture = cp_drawable_get_color_texture(drawable, 0);\n    pass_descriptor.colorAttachments[0].storeAction = MTLStoreActionStore;\n    \n    pass_descriptor.depthAttachment.texture = cp_drawable_get_depth_texture(drawable, 0);\n    pass_descriptor.depthAttachment.storeAction = MTLStoreActionStore;\n\n    pass_descriptor.renderTargetArrayLength = cp_drawable_get_view_count(drawable);\n\n    \/\/ Foveation support\n    pass_descriptor.rasterizationRateMap = cp_drawable_get_rasterization_rate_map(drawable, 0);\n\n    return pass_descriptor;\n}\n```\n\nFor a [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Layout\/dedicated] layout, you must perform two render passes on your content and create a separate render pass descriptor for each one. Configure each render pass descriptor with the texture at a different index in the arrays of the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Drawable] type.\n\n### Retrieve device anchor information and attach it to the frame\n\nTo prevent the person viewing your content from experiencing disorientation or physical discomfort, it’s essential to match the position of the camera in your scene to the location of the person’s head. Matching the person’s head movements ensures that what they see doesn’t conflict with the input their body receives from the real world.\n\nBecause you render your app’s content in advance, you also need to know the position and orientation of the device in advance. To retrieve this information, use ARKit to call [doc:\/\/com.apple.documentation\/documentation\/ARKit\/ar_world_tracking_provider_query_device_anchor_at_timestamp] during the encoding process for your frame. ARKit provides this function to deliver the expected device anchor at the time you specify. Use this information to configure any camera positions during rendering.\n\nThe following example shows how to retrieve the predicted device anchor using ARKit. Use the timing information from the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer\/Drawable] to get the most accurate presentation time for the frame. Return the device anchor upon success or return `nil` if an error occurs.\n\n```objc\nar_device_anchor_t my_engine_get_ar_device_anchor(my_engine *engine, cp_frame_timing_t timing) {   \n    ar_device_anchor_t anchor = ar_device_anchor_create();\n    ar_world_tracking_provider_t provider = engine->my_world_tracking_provider;\n\n    \/\/ Fetch the device anchor from ARKit.\n    auto p_time = cp_time_to_cf_time_interval(cp_frame_timing_get_presentation_time(timing));\n    auto anchor_status = ar_world_tracking_provider_query_device_anchor_at_timestamp(provider, p_time, &anchor);\n    if (anchor_status == ar_device_anchor_query_status_success) {\n        return anchor;\n    }\n    return nil;\n}\n```\n\nWhen it displays your frame, the system checks for a discrepancy between the predicted device anchor you provided for your frame and the actual device anchor the hardware reports. If there’s a difference, the system automatically adjusts the rendered frame to compensate for the movement. If you don’t want the system to make this adjustment, don’t specify a device anchor using the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/cp_drawable_set_device_anchor] function.\n\nFor more information about how to track the device anchor, see [doc:\/\/com.apple.documentation\/documentation\/ARKit].\n\n### Render each view with the correct perspective\n\nThe goal of your Metal rendering engine is to produce 2D textures to display to the viewer. When your content is 3D, you need to map points in your scene to the 2D texture in a way that makes the content look realistically 3D to someone viewing it. This process requires you to create a projection matrix that maps points in your 3D content to points on the texture for each view. For stereoscopic rendering, you also have to account for the positional differences between the device anchor and the position of the person’s eyes.\n\nDuring rendering, the rendering engine calls the method in the following example once for each view in the frame. It uses the device anchor it assigned to the frame earlier to create a transform to compensate for any differences between the device position and the view’s position. It also creates a projection matrix using the view’s tangents information and the distances to the near and far projection planes. The `makeProjectiveTransformFromTangents` function assembles the actual matrix values in the same way as [doc:\/\/com.apple.documentation\/documentation\/Spatial\/ProjectiveTransform3D\/init(leftTangent:rightTangent:topTangent:bottomTangent:nearZ:farZ:reverseZ:)].\n\n```objc\ntypedef struct{\n    simd_float4x4 projectionMatrix;\n    simd_float4x4 viewMatrix;\n} Uniforms;\n\nstatic const NSUInteger MaxBuffersInFlight = 3;\n@implementation Renderer (UniformsExtension)\n   id <MTLBuffer> _uniformBufferAddress[MaxBuffersInFlight];\n\n- (void)updateUniformsForRenderer:(Renderer*)renderer\n                     withDrawable:(cp_drawable_t)drawable\n                          atIndex:(size_t)index {\n\n    Uniforms *uniforms = (Uniforms*)_uniformBufferAddress;\n\n    \/\/ Get the current device anchor value.\n    ar_device_anchor_t device_anchor = cp_drawable_get_device_anchor(drawable);\n    simd_float4x4 head_position = ar_anchor_get_origin_from_anchor_transform(device_anchor);\n\n    cp_view_t view = cp_drawable_get_view(drawable, index);\n    simd_float4 tangents = cp_view_get_tangents(view);\n    simd_float2 depth_range = cp_drawable_get_depth_range(drawable);\n    simd_float4x4 transform = makeProjectiveTransformFromTangents(tangents[0], \/* left *\/\n                                                                  tangents[1], \/* right *\/\n                                                                  tangents[2], \/* top *\/\n                                                                  tangents[3], \/* bottom *\/\n                                                                  depth_range[1], \/* nearZ *\/\n                                                                  depth_range[0], \/* farZ *\/\n                                                                  true); \/* reverseZ *\/\n    uniforms[index].projectionMatrix = transform;\n\n    \/\/ Adjust the camera transform for the current eye position.\n    simd_float4x4 camera_transform = simd_mul(head_position, cp_view_get_transform(view));\n    uniforms[index].viewMatrix = simd_inverse(camera_transform);\n}\n\n@end\n\n```\n\n### Respond to interactions with your custom content\n\nWhen your scene is visible, you’re responsible for managing all interactions with your custom content. Because you render everything yourself using Metal, you can’t rely on view-based events or gesture recognizers for input. Instead, use one of the following techniques:\n\n- Add an `.onSpatialEvent` callback to your layer and map touch events to your content.\n- Use ARKit hand tracking to manage input yourself.\n\nWhen the system detects any direct or indirect touch events, it reports them to the `.onSpatialEvent` callback of the [doc:\/\/com.apple.compositorservices\/documentation\/CompositorServices\/LayerRenderer]. Use this callback to handle any interactions with your custom content. The system executes your callback on the main thread each time a new touch occurs or an active touch changes, so keep your callback code brief. The following example shows how to add this callback to your layer:\n\n```swift\n@main\nstruct MyApp: App {\n    var body: some Scene {\n        \n        \/\/ Create a fully immersive scene.\n        ImmersiveSpace(id: \"MyContent\") {\n            CompositorLayer (configuration: MyContentConfiguration()) { layerRenderer in\n                \/\/ Set up and run the Metal render loop.\n                let renderThread = Thread {\n                    let engine = myEngineCreate(layerRenderer)\n                    myEngineRenderLoop(engine)\n                }\n                renderThread.name = \"Render Thread\"\n                renderThread.start()\n\n                \/\/ Handle any events in the scene.\n                layerRenderer.onSpatialEvent = { eventCollection in\n                    var events = eventCollection.map { mySpatialEvent($0) }\n                    myEnginePushSpatialEvents(engine, &events, events.count)\n                }\n            }\n        }\n         \/\/ Other scenes...\n    }\n}\n\n```\n\n\n\nFor information about ARKit hand tracking, see [doc:\/\/com.apple.documentation\/documentation\/ARKit].\n\n## App integration\n\n- **Interacting with virtual content blended with passthrough**: Present a mixed immersion style space to draw content in a person’s surroundings, and choose how upper limbs appear with respect to rendered content.\n- **Rendering hover effects in Metal immersive apps**: Change the appearance of a rendered onscreen element when a player gazes at it.\n- **CompositorLayer**: A type that you use with an immersive space to display fully immersive content using Metal.\n- **CompositorLayerConfiguration**: An interface for specifying the texture configurations and rendering behaviors to use with your Metal rendering engine.\n- **DefaultCompositorLayerConfiguration**: A type that configures the layer with the default texture configurations and rendering behaviors for the current device.\n\n",
  "sections" : [
    {
      "content" : "",
      "items" : [
        {
          "description" : "Present a mixed immersion style space to draw content in a person’s surroundings, and choose how upper limbs appear with respect to rendered content.",
          "name" : "Interacting with virtual content blended with passthrough",
          "url" : "https:\/\/developer.apple.com\/documentation\/CompositorServices\/interacting-with-virtual-content-blended-with-passthrough"
        },
        {
          "description" : "Change the appearance of a rendered onscreen element when a player gazes at it.",
          "name" : "Rendering hover effects in Metal immersive apps",
          "url" : "https:\/\/developer.apple.com\/documentation\/CompositorServices\/rendering_hover_effects_in_metal_immersive_apps"
        },
        {
          "description" : "A type that you use with an immersive space to display fully immersive content using Metal.",
          "name" : "CompositorLayer",
          "url" : "https:\/\/developer.apple.com\/documentation\/CompositorServices\/CompositorLayer"
        },
        {
          "description" : "An interface for specifying the texture configurations and rendering behaviors to use with your Metal rendering engine.",
          "name" : "CompositorLayerConfiguration",
          "url" : "https:\/\/developer.apple.com\/documentation\/CompositorServices\/CompositorLayerConfiguration"
        },
        {
          "description" : "A type that configures the layer with the default texture configurations and rendering behaviors for the current device.",
          "name" : "DefaultCompositorLayerConfiguration",
          "url" : "https:\/\/developer.apple.com\/documentation\/CompositorServices\/DefaultCompositorLayerConfiguration"
        }
      ],
      "title" : "App integration"
    }
  ],
  "source" : "appleJSON",
  "title" : "Drawing fully immersive content using Metal",
  "url" : "https:\/\/developer.apple.com\/documentation\/compositorservices\/drawing-fully-immersive-content-using-metal"
}